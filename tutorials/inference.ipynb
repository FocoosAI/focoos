{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• How to use a Computer Vision Model with Focoos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üêç Setup Focoos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® There are three ways to use a model:\n",
    "\n",
    "1. Use it on the Focoos' efficient servers with the RemoteModel\n",
    "2. Use the model in PyTorch\n",
    "3. Use the exported optimized version of the model using a supported inference runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üêç Connect with Focoos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.hub import FocoosHUB\n",
    "\n",
    "FOCOOS_API_KEY = None  # write here your API key\n",
    "hub = FocoosHUB(api_key=FOCOOS_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also download a sample image to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# Download the image\n",
    "url = \"https://public.focoos.ai/samples/pexels-abby-chung.jpg\"\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ See your models\n",
    "You can see the models available for you on the platform with an intuitive user interface.\n",
    "However, you can also list them using the Hub functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(hub.list_remote_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Remote Inference\n",
    "\n",
    "In this section, you'll run a model on the Focoos' servers instead of on your machine. The image will be packed and sent on the network to the servers, where it is processed and the results is retured to your machine, all in few milliseconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = \"fai-detr-l-obj365\"  # use any of your models here\n",
    "\n",
    "model = hub.get_remote_model(model_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model is as simple as it could! Just call it with an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = model(image)\n",
    "pprint(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to visualize the result on the image, there's a utily for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.utils.vision import annotate_image\n",
    "\n",
    "display(annotate_image(image, detections, task=model.model_info.task, classes=model.model_info.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Torch Inference\n",
    "\n",
    "This section demonstrates how to perform local inference using a plain Pytorch model.\n",
    "We will load a model and then run inference on a sample image.\n",
    "\n",
    "First, let's get a model. We need to use the `ModelManager` that will take care of instaciating the right model starting from a model reference (for example, the `hub://fai-detr-l-obj365`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.model_manager import ModelManager\n",
    "\n",
    "model_ref = \"hub://fai-detr-l-obj365\"  # use any of your models here\n",
    "\n",
    "model = ModelManager.get(model_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now run the model by simply passing it an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "detections = model(image)\n",
    "pprint(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and visualize the results using the annotate_image utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.utils.vision import annotate_image\n",
    "\n",
    "display(annotate_image(image, detections, task=model.model_info.task, classes=model.model_info.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How fast is this model locally? We can compute it's speed by using the benchmark utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.benchmark(iterations=10, size=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî® Optimized Inference\n",
    "\n",
    "As you can see, using the torch model is great, but we can achieve better performance by exporting and running it with a optimized runtime, such as Torchscript, TensorRT, CoreML or the ones available on ONNXRuntime.\n",
    "\n",
    "In the following cells, we will export the previous model for one of these and run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchscript\n",
    "\n",
    "We already provide multiple inference runtime, that you can see on the `RuntimeTypes` enum. Let's select Torchscript as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.ports import RuntimeType\n",
    "\n",
    "runtime = RuntimeType.TORCHSCRIPT_32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to export the model. We can use the export method of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = model.export(runtime_type=runtime, image_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the output. As you will see, there are not differences from the model in pure torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.utils.vision import annotate_image\n",
    "\n",
    "detections = optimized_model(image)\n",
    "display(annotate_image(image, detections, task=model.model_info.task, classes=model.model_info.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, let's see its latency! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.benchmark(iterations=10, size=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That's a lot faster! And without losing a bit in performance!\n",
    "\n",
    "You can also try different runtimes. Please note that you need to install the relative packages for onnx and tensorRT for using them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX with CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.ports import RuntimeType\n",
    "from focoos.utils.vision import annotate_image\n",
    "\n",
    "runtime = RuntimeType.ONNX_CUDA32\n",
    "optimized_model = model.export(runtime_type=runtime)\n",
    "\n",
    "detections = optimized_model(image)\n",
    "display(annotate_image(image, detections, task=model.model_info.task, classes=model.model_info.classes))\n",
    "\n",
    "optimized_model.benchmark(iterations=10, size=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX with TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.ports import RuntimeType\n",
    "from focoos.utils.vision import annotate_image\n",
    "\n",
    "runtime = RuntimeType.ONNX_TRT16\n",
    "optimized_model = model.export(runtime_type=runtime)\n",
    "\n",
    "detections = optimized_model(image)\n",
    "display(annotate_image(image, detections, task=model.model_info.task, classes=model.model_info.classes))\n",
    "\n",
    "optimized_model.benchmark(iterations=10, size=640)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
