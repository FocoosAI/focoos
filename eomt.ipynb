{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from focoos.nn.backbone.vit import VisionTransformer\n",
    "\n",
    "# 'vit_small_patch14_reg4_dinov2.lvd142m': _cfg(\n",
    "#     url='https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth',\n",
    "#     hf_hub_id='timm/',\n",
    "#     license='apache-2.0',\n",
    "#     mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,\n",
    "#     input_size=(3, 518, 518), crop_pct=1.0),\n",
    "# patch_size=14, embed_dim=384, depth=12, num_heads=6, init_values=1e-5, reg_tokens=4, no_embed_class=True,\n",
    "# 'vit_base_patch14_reg4_dinov2.lvd142m': _cfg(\n",
    "#     url='https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth',\n",
    "#     hf_hub_id='timm/',\n",
    "#     license='apache-2.0',\n",
    "#     mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,\n",
    "#     input_size=(3, 518, 518), crop_pct=1.0),\n",
    "# patch_size=14, embed_dim=768, depth=12, num_heads=12, init_values=1e-5, reg_tokens=4, no_embed_class=True,\n",
    "# 'vit_large_patch14_reg4_dinov2.lvd142m': _cfg(\n",
    "#     url='https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth',\n",
    "#     hf_hub_id='timm/',\n",
    "#     license='apache-2.0',\n",
    "#     mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,\n",
    "#     input_size=(3, 518, 518), crop_pct=1.0),\n",
    "#     patch_size=14, embed_dim=1024, depth=24, num_heads=16, init_values=1e-5, reg_tokens=4, no_embed_class=True,\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: tuple[int, int],\n",
    "        patch_size=14,\n",
    "        ckpt_path: Optional[str] = \"dinov2_vits14_reg4_pretrain_timm.pth\",\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        reg_tokens=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = VisionTransformer(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            embed_dim=embed_dim,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            init_values=1e-5,\n",
    "            reg_tokens=reg_tokens,\n",
    "            no_embed_class=True,\n",
    "        )\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.backbone.load_state_dict(torch.load(ckpt_path), strict=False)\n",
    "        IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "        IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "        pixel_mean = torch.tensor(IMAGENET_DEFAULT_MEAN).reshape(1, -1, 1, 1)\n",
    "        pixel_std = torch.tensor(IMAGENET_DEFAULT_STD).reshape(1, -1, 1, 1)\n",
    "\n",
    "        self.register_buffer(\"pixel_mean\", pixel_mean)\n",
    "        self.register_buffer(\"pixel_std\", pixel_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.pixel_mean) / self.pixel_std\n",
    "        x = self.backbone.forward_features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ViT(img_size=(518, 518), patch_size=14, ckpt_path=\"dinov2_vits14_reg4_pretrain_timm.pth\")\n",
    "\n",
    "encoder.eval()\n",
    "\n",
    "encoder(torch.randn(1, 3, 518, 518))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dinov2(state_dict):\n",
    "    import re\n",
    "\n",
    "    out_dict = {}\n",
    "    state_dict.pop(\"mask_token\", None)\n",
    "    if \"register_tokens\" in state_dict:\n",
    "        # convert dinov2 w/ registers to no_embed_class timm model (neither cls or reg tokens overlap pos embed)\n",
    "        out_dict[\"reg_token\"] = state_dict.pop(\"register_tokens\")\n",
    "        out_dict[\"cls_token\"] = state_dict.pop(\"cls_token\") + state_dict[\"pos_embed\"][:, 0]\n",
    "        out_dict[\"pos_embed\"] = state_dict.pop(\"pos_embed\")[:, 1:]\n",
    "    for k, v in state_dict.items():\n",
    "        if re.match(r\"blocks\\.(\\d+)\\.mlp\\.w12\\.(?:weight|bias)\", k):\n",
    "            out_dict[k.replace(\"w12\", \"fc1\")] = v\n",
    "            continue\n",
    "        elif re.match(r\"blocks\\.(\\d+)\\.mlp\\.w3\\.(?:weight|bias)\", k):\n",
    "            out_dict[k.replace(\"w3\", \"fc2\")] = v\n",
    "            continue\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "state_dict = torch.load(\"dinov2_vits14_reg4_pretrain.pth\")\n",
    "\n",
    "out_dict = convert_dinov2(state_dict)\n",
    "torch.save(out_dict, \"dinov2_vits14_reg4_pretrain_timm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from timm.models.vision_transformer import VisionTransformer as TimmVisionTransformer\n",
    "\n",
    "# Test different resolutions that are multiples of patch_size\n",
    "resolutions = [518]\n",
    "times = []\n",
    "\n",
    "for res in resolutions:\n",
    "    vit = TimmVisionTransformer(\n",
    "        img_size=(res, res),  # Will update dynamically\n",
    "        patch_size=14,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        init_values=1e-5,\n",
    "        reg_tokens=4,\n",
    "    )\n",
    "\n",
    "    vit.eval()\n",
    "    # Create square image of size res x res\n",
    "    image = torch.randn(1, 3, res, res)\n",
    "\n",
    "    # Time the forward pass\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        vit.patch_embed.img_size = (res, res)  # Update image size\n",
    "        features = vit.forward_features(image)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(resolutions, times, \"-o\")\n",
    "plt.xlabel(\"Image Resolution\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"ViT Forward Pass Time vs Image Resolution\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print some key points\n",
    "print(f\"Min time: {min(times):.4f}s at resolution {resolutions[np.argmin(times)]}\")\n",
    "print(f\"Max time: {max(times):.4f}s at resolution {resolutions[np.argmax(times)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
