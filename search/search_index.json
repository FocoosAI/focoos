{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Focoos AI \ud83d\udd25","text":"<p>Focoos AI provides an advanced development platform designed to empower developers and businesses with efficient, customizable computer vision solutions. Whether you're working with data from cloud infrastructures or deploying on edge devices, Focoos AI enables you to select, fine-tune, and deploy state-of-the-art models optimized for your unique needs.</p>"},{"location":"#what-we-offer","title":"What We Offer \ud83c\udfaf","text":""},{"location":"#ai-ready-models-platform-for-computer-vision-applications","title":"AI-Ready Models Platform for Computer Vision Applications \ud83e\udd16","text":"<p>Focoos AI offers a versatile platform for developing computer vision solutions. Our platform includes a suite of services to support the end-to-end development process:</p> <ul> <li>Ready-to-use models: Choose from a variety of pre-trained models, optimized for different data, applications, and hardware.</li> <li>Customization: Tailor models to your specific needs by selecting relevant classes and fine-tuning them on your own dataset.</li> <li>Testing and Validation: Verify model accuracy and efficiency using your own data samples, ensuring the model meets your requirements before deployment.</li> </ul>"},{"location":"#key-features","title":"Key Features \ud83d\udd11","text":"<ol> <li> <p>Select Ready-to-use Models \ud83e\udde9    Get started quickly by selecting one of our efficient, pre-trained models that best suits your data and application needs.</p> </li> <li> <p>Personalize Your Model \u2728    Customize the selected model for higher accuracy through fine-tuning. Adapt the model to your specific use case by training it on your own dataset and selecting useful classes.</p> </li> <li> <p>Test and Validate \ud83e\uddea    Upload your data sample to test the model\u2019s accuracy and efficiency. Iterate the process to ensure the model performs to your expectations.</p> </li> <li> <p>Cloud Deployment \u2601\ufe0f    Deploy the model on your preferred cloud infrastructure, whether it's your own private cloud or a public cloud service. Your data stays private, as it remains within your servers.</p> </li> <li> <p>Edge Deployment \ud83d\udda5\ufe0f    Deploy the model on edge devices. Download the Focoos Engine to run the model locally, without sending any data over the network, ensuring full privacy.</p> </li> </ol>"},{"location":"#why-choose-focoos-ai","title":"Why Choose Focoos AI? \ud83e\udd29","text":"<p>Using Focoos AI helps you save both time and money while delivering high-performance AI models:</p> <ul> <li>80% Faster Development \u23f3: Save significant development time compared to traditional methods.</li> <li>+5% Model Accuracy \ud83c\udfaf: Achieve some of the most accurate models in the market, as demonstrated by our scientific benchmarks.</li> <li>Up to 20x Faster Models \u26a1: Run real-time data analysis with some of the fastest models available today.</li> </ul>"},{"location":"#pre-trained-models-with-minimum-training-data","title":"Pre-Trained Models with Minimum Training Data \ud83d\udcca","text":"<p>Our pre-trained models reduce the need for large datasets, making it easier to deploy computer vision solutions. Here's how Focoos AI helps you minimize your resources:</p> <ul> <li>80% Less Training Data \ud83d\udcc9: Leverage pre-trained models that are ready to tackle a variety of use cases.</li> <li>50% Lower Infrastructure Costs \ud83d\udca1: Use less expensive hardware and reduce energy consumption.</li> <li>75% Reduction in CO2 Emissions \ud83c\udf31: Deploy energy-efficient models that help you reduce your carbon footprint.</li> </ul>"},{"location":"#proven-efficiency-and-accuracy","title":"Proven Efficiency and Accuracy \ud83d\udd0d","text":"<p>Focoos AI models outperform other solutions in terms of both accuracy and efficiency. Our technical report highlights how our models lead in academic benchmarks across multiple domains. Contact us to learn more about the scientific benchmarks that set Focoos AI apart.</p>"},{"location":"#pricing-model","title":"Pricing Model \ud83d\udcb5","text":"<p>We offer a flexible pricing model based on your deployment preferences:</p> <ul> <li>Public Cloud \ud83c\udf10: Pay for model usage when deployed on public cloud providers.</li> <li>Private Infrastructure \ud83c\udfe2: Pay for usage when deploying on your own infrastructure.</li> </ul> <p>Contact us for a tailored quote based on your specific use case.</p> <p>By choosing Focoos AI, you can save time, reduce costs, and achieve superior model performance, all while ensuring the privacy and efficiency of your deployments. Ready to get started? Reach out to us today to explore how Focoos AI can power your computer vision projects. \ud83d\ude80</p>"},{"location":"datasets/","title":"Datasets","text":"<p>With the Focoos SDK, you can leverage a diverse collection of foundational datasets specifically tailored for computer vision tasks. These datasets, spanning tasks such as segmentation, detection, and instance segmentation, provide a strong foundation for building and optimizing models across a variety of domains.</p> <p>Datasets:</p> Name Task Description Layout Aeroscapes semseg A drone dataset to recognize many classes! supervisely Blister instseg A dataset to find blisters roboflow_coco Boxes detection Finding different boxes on the conveyor belt roboflow_coco Cable detection A dataset for detecting damages in cables (from Roboflow 100) - https://universe.roboflow.com/roboflow-100/cable-damage/dataset/2# roboflow_coco Circuit dataset detection A dataset with electronic circuits roboflow_coco Concrete instseg A dataset to find defect in concrete roboflow_coco Crack Segmentation instseg A dataset for segmenting cracks in buildings with 4k images. roboflow_coco Football-detection detection Football-detection by Roboflow roboflow_coco Peanuts detection Finding Molded or non Molded Peanuts roboflow_coco Strawberries instseg Finding defects on strawberries roboflow_coco aquarium detection aquarium roboflow_coco bottles detection bottles roboflow_coco chess_pieces detection A chess detector dataset by roboflow https://universe.roboflow.com/roboflow-100/chess-pieces-mjzgj roboflow_coco coco_2017_det detection COCO Detection catalog halo detection Halo fps by Roboflow roboflow_coco lettuce detection A dataset to find lettuce roboflow_coco safety detection From roboflow Universe: https://universe.roboflow.com/roboflow-100/construction-safety-gsnvb roboflow_coco screw detection Screw by Roboflow roboflow_coco"},{"location":"models/","title":"Focoos Foundational Models","text":"<p>With the Focoos SDK, you can take advantage of a collection of foundational models that are optimized for a range of computer vision tasks. These pre-trained models, covering detection and semantic segmentation across various domains, provide an excellent starting point for your specific use case. Whether you need to fine-tune for custom requirements or adapt them to your application, these models offer a solid foundation to accelerate your development process.</p> <p>Models:</p> Model Name Task Metrics Domain focoos_object365 Detection - Common Objects, 365 classes focoos_rtdetr Detection - Common Objects, 80 classes focoos_cts_medium Semantic Segmentation - Autonomous driving, 30 classes focoos_cts_large Semantic Segmentation - Autonomous driving, 30 classes focoos_ade_nano Semantic Segmentation - Common Scenes, 150 classes focoos_ade_small Semantic Segmentation - Common Scenes, 150 classes focoos_ade_medium Semantic Segmentation - Common Scenes, 150 classes focoos_ade_large Semantic Segmentation - Common Scenes, 150 classes focoos_aeroscapes Semantic Segmentation - Drone Aerial Scenes, 11 classes focoos_isaid_nano Semantic Segmentation - Satellite Imagery, 15 classes focoos_isaid_medium Semantic Segmentation - Satellite Imagery, 15 classes"},{"location":"api/focoos/","title":"focoos","text":"<p>Focoos Module</p> <p>This module provides a Python interface for interacting with Focoos APIs, allowing users to manage machine learning models and datasets in the Focoos ecosystem. The module supports operations such as retrieving model metadata, downloading models, and listing shared datasets.</p> <p>Classes:</p> Name Description <code>Focoos</code> <p>Main class to interface with Focoos APIs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised for invalid API responses or missing parameters.</p>"},{"location":"api/focoos/#focoos.focoos.Focoos","title":"<code>Focoos</code>","text":"<p>Main class to interface with Focoos APIs.</p> <p>This class provides methods to interact with Focoos-hosted models and datasets. It supports functionalities such as listing models, retrieving model metadata, downloading models, and creating new models.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key for authentication.</p> <code>http_client</code> <code>HttpClient</code> <p>HTTP client for making API requests.</p> <code>user_info</code> <code>dict</code> <p>Information about the currently authenticated user.</p> <code>cache_dir</code> <code>str</code> <p>Local directory for caching downloaded models.</p> Source code in <code>focoos/focoos.py</code> <pre><code>class Focoos:\n    \"\"\"\n    Main class to interface with Focoos APIs.\n\n    This class provides methods to interact with Focoos-hosted models and datasets.\n    It supports functionalities such as listing models, retrieving model metadata,\n    downloading models, and creating new models.\n\n    Attributes:\n        api_key (str): The API key for authentication.\n        http_client (HttpClient): HTTP client for making API requests.\n        user_info (dict): Information about the currently authenticated user.\n        cache_dir (str): Local directory for caching downloaded models.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        host_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes the Focoos API client.\n\n        This client provides authenticated access to the Focoos API, enabling various operations\n        through the configured HTTP client. It retrieves user information upon initialization and\n        logs the environment details.\n\n        Args:\n            api_key (Optional[str]): API key for authentication. Defaults to the `focoos_api_key`\n                specified in the FOCOOS_CONFIG.\n            host_url (Optional[str]): Base URL for the Focoos API. Defaults to the `default_host_url`\n                specified in the FOCOOS_CONFIG.\n\n        Raises:\n            ValueError: If the API key is not provided, or if the host URL is not specified in the\n                arguments or the configuration.\n\n        Attributes:\n            api_key (str): The API key used for authentication.\n            http_client (HttpClient): An HTTP client instance configured with the API key and host URL.\n            user_info (dict): Information about the authenticated user retrieved from the API.\n            cache_dir (str): Path to the cache directory used by the client.\n\n        Logs:\n            - Error if the API key or host URL is missing.\n            - Info about the authenticated user and environment upon successful initialization.\n        \"\"\"\n        self.api_key = api_key or FOCOOS_CONFIG.focoos_api_key\n        if not self.api_key:\n            logger.error(\"API key is required \ud83e\udd16\")\n            raise ValueError(\"API key is required \ud83e\udd16\")\n\n        host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n        self.http_client = HttpClient(self.api_key, host_url)\n        self.user_info = self.get_user_info()\n        self.cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"focoos\")\n        logger.info(f\"Currently logged as: {self.user_info.email} environment: {host_url}\")\n\n    def get_user_info(self) -&gt; User:\n        \"\"\"\n        Retrieves information about the authenticated user.\n\n        Returns:\n            dict: Information about the user (e.g., email).\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.get(\"user/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get user info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get user info: {res.status_code} {res.text}\")\n        return User.from_json(res.json())\n\n    def get_model_info(self, model_name: str) -&gt; ModelMetadata:\n        \"\"\"\n        Retrieves metadata for a specific model.\n\n        Args:\n            model_name (str): Name of the model.\n\n        Returns:\n            ModelMetadata: Metadata of the specified model.\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.get(f\"models/{model_name}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n        return ModelMetadata.from_json(res.json())\n\n    def list_models(self) -&gt; list[ModelPreview]:\n        \"\"\"\n        Lists all available models.\n\n        Returns:\n            list[ModelPreview]: List of model previews.\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.get(\"models/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list models: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list models: {res.status_code} {res.text}\")\n        return [ModelPreview.from_json(r) for r in res.json()]\n\n    def list_focoos_models(self) -&gt; list[ModelPreview]:\n        \"\"\"\n        Lists models specific to Focoos.\n\n        Returns:\n            list[ModelPreview]: List of Focoos models.\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.get(\"models/focoos-models\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n        return [ModelPreview.from_json(r) for r in res.json()]\n\n    def get_local_model(\n        self,\n        model_ref: str,\n        runtime_type: Optional[RuntimeTypes] = None,\n    ) -&gt; LocalModel:\n        \"\"\"\n        Retrieves a local model for the specified reference.\n\n        Downloads the model if it does not already exist in the local cache.\n\n        Args:\n            model_ref (str): Reference identifier for the model.\n            runtime_type (Optional[RuntimeTypes]): Runtime type for the model. Defaults to the\n                `runtime_type` specified in FOCOOS_CONFIG.\n\n        Returns:\n            LocalModel: An instance of the local model.\n\n        Raises:\n            ValueError: If the runtime type is not specified.\n\n        Notes:\n            The model is cached in the directory specified by `self.cache_dir`.\n        \"\"\"\n        runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n        model_dir = os.path.join(self.cache_dir, model_ref)\n        if not os.path.exists(os.path.join(model_dir, \"model.onnx\")):\n            self._download_model(model_ref)\n        return LocalModel(model_dir, runtime_type)\n\n    def get_remote_model(self, model_ref: str) -&gt; RemoteModel:\n        \"\"\"\n        Retrieves a remote model instance.\n\n        Args:\n            model_ref (str): Reference name of the model.\n\n        Returns:\n            RemoteModel: The remote model instance.\n        \"\"\"\n        return RemoteModel(model_ref, self.http_client)\n\n    def new_model(self, name: str, focoos_model: str, description: str) -&gt; RemoteModel:\n        \"\"\"\n        Creates a new model in the Focoos system.\n\n        Args:\n            name (str): Name of the new model.\n            focoos_model (str): Reference to the base Focoos model.\n            description (str): Description of the new model.\n\n        Returns:\n            Optional[RemoteModel]: The created model instance, or None if creation fails.\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.post(\n            \"models/\",\n            data={\n                \"name\": name,\n                \"focoos_model\": focoos_model,\n                \"description\": description,\n            },\n        )\n        if res.status_code in [200, 201]:\n            return RemoteModel(res.json()[\"ref\"], self.http_client)\n        if res.status_code == 409:\n            logger.warning(f\"Model already exists: {name}\")\n            return self.get_model_by_name(name, remote=True)\n        logger.warning(f\"Failed to create new model: {res.status_code} {res.text}\")\n\n    def list_shared_datasets(self) -&gt; list[DatasetMetadata]:\n        \"\"\"\n        Lists datasets shared with the user.\n\n        Returns:\n            list[DatasetMetadata]: List of shared datasets.\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.get(\"datasets/shared\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        return [DatasetMetadata.from_json(dataset) for dataset in res.json()]\n\n    def _download_model(self, model_ref: str) -&gt; str:\n        \"\"\"\n        Downloads a model from the Focoos API.\n\n        Args:\n            model_ref (str): Reference name of the model.\n\n        Returns:\n            str: Path to the downloaded model.\n\n        Raises:\n            ValueError: If the API request fails or the download fails.\n        \"\"\"\n        model_dir = os.path.join(self.cache_dir, model_ref)\n        model_path = os.path.join(model_dir, \"model.onnx\")\n        metadata_path = os.path.join(model_dir, \"focoos_metadata.json\")\n        if os.path.exists(model_path) and os.path.exists(metadata_path):\n            logger.info(\"\ud83d\udce5 Model already downloaded\")\n            return model_path\n\n        ## download model metadata\n        res = self.http_client.get(f\"models/{model_ref}/download?format=onnx\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to download model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to download model: {res.status_code} {res.text}\")\n\n        download_data = res.json()\n        metadata = ModelMetadata.from_json(download_data[\"model_metadata\"])\n        download_uri = download_data[\"download_uri\"]\n\n        ## download model from Focoos Cloud\n        logger.debug(f\"Model URI: {download_uri}\")\n        logger.info(\"\ud83d\udce5 Downloading model from Focoos Cloud.. \")\n        response = self.http_client.get_external_url(download_uri, stream=True)\n        if response.status_code != 200:\n            logger.error(f\"Failed to download model: {response.status_code} {response.text}\")\n            raise ValueError(f\"Failed to download model: {response.status_code} {response.text}\")\n        total_size = int(response.headers.get(\"content-length\", 0))\n        logger.info(f\"\ud83d\udce5 Size: {total_size / (1024**2):.2f} MB\")\n\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir)\n\n        with open(metadata_path, \"w\") as f:\n            f.write(metadata.model_dump_json())\n        logger.debug(f\"Dumped metadata to {metadata_path}\")\n\n        with (\n            open(model_path, \"wb\") as f,\n            tqdm(\n                desc=str(model_path).split(\"/\")[-1],\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as bar,\n        ):\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n                bar.update(len(chunk))\n        logger.info(f\"\ud83d\udce5 File downloaded: {model_path}\")\n        return model_path\n\n    def get_dataset_by_name(self, name: str) -&gt; Optional[DatasetMetadata]:\n        \"\"\"\n        Retrieves a dataset by its name.\n\n        Args:\n            name (str): Name of the dataset.\n\n        Returns:\n            Optional[DatasetMetadata]: The dataset metadata if found, or None otherwise.\n        \"\"\"\n        datasets = self.list_shared_datasets()\n        name_lower = name.lower()\n        for dataset in datasets:\n            if name_lower == dataset.name.lower():\n                return dataset\n\n    def get_model_by_name(self, name: str, remote: bool = True) -&gt; Union[RemoteModel, LocalModel]:\n        \"\"\"\n        Retrieves a model by its name.\n\n        Args:\n            name (str): Name of the model.\n            remote (bool): If True, retrieve as a RemoteModel. Otherwise, as a LocalModel. Defaults to True.\n\n        Returns:\n            Optional[Union[RemoteModel, LocalModel]]: The model instance if found, or None otherwise.\n        \"\"\"\n        models = self.list_models()\n        name_lower = name.lower()\n        for model in models:\n            if name_lower == model.name.lower():\n                if remote:\n                    return self.get_remote_model(model.ref)\n                else:\n                    return self.get_local_model(model.ref)\n        raise ModelNotFound(f\"Model not found: {name}\")\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.__init__","title":"<code>__init__(api_key=None, host_url=None)</code>","text":"<p>Initializes the Focoos API client.</p> <p>This client provides authenticated access to the Focoos API, enabling various operations through the configured HTTP client. It retrieves user information upon initialization and logs the environment details.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>API key for authentication. Defaults to the <code>focoos_api_key</code> specified in the FOCOOS_CONFIG.</p> <code>None</code> <code>host_url</code> <code>Optional[str]</code> <p>Base URL for the Focoos API. Defaults to the <code>default_host_url</code> specified in the FOCOOS_CONFIG.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API key is not provided, or if the host URL is not specified in the arguments or the configuration.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key used for authentication.</p> <code>http_client</code> <code>HttpClient</code> <p>An HTTP client instance configured with the API key and host URL.</p> <code>user_info</code> <code>dict</code> <p>Information about the authenticated user retrieved from the API.</p> <code>cache_dir</code> <code>str</code> <p>Path to the cache directory used by the client.</p> Logs <ul> <li>Error if the API key or host URL is missing.</li> <li>Info about the authenticated user and environment upon successful initialization.</li> </ul> Source code in <code>focoos/focoos.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = None,\n    host_url: Optional[str] = None,\n):\n    \"\"\"\n    Initializes the Focoos API client.\n\n    This client provides authenticated access to the Focoos API, enabling various operations\n    through the configured HTTP client. It retrieves user information upon initialization and\n    logs the environment details.\n\n    Args:\n        api_key (Optional[str]): API key for authentication. Defaults to the `focoos_api_key`\n            specified in the FOCOOS_CONFIG.\n        host_url (Optional[str]): Base URL for the Focoos API. Defaults to the `default_host_url`\n            specified in the FOCOOS_CONFIG.\n\n    Raises:\n        ValueError: If the API key is not provided, or if the host URL is not specified in the\n            arguments or the configuration.\n\n    Attributes:\n        api_key (str): The API key used for authentication.\n        http_client (HttpClient): An HTTP client instance configured with the API key and host URL.\n        user_info (dict): Information about the authenticated user retrieved from the API.\n        cache_dir (str): Path to the cache directory used by the client.\n\n    Logs:\n        - Error if the API key or host URL is missing.\n        - Info about the authenticated user and environment upon successful initialization.\n    \"\"\"\n    self.api_key = api_key or FOCOOS_CONFIG.focoos_api_key\n    if not self.api_key:\n        logger.error(\"API key is required \ud83e\udd16\")\n        raise ValueError(\"API key is required \ud83e\udd16\")\n\n    host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n    self.http_client = HttpClient(self.api_key, host_url)\n    self.user_info = self.get_user_info()\n    self.cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"focoos\")\n    logger.info(f\"Currently logged as: {self.user_info.email} environment: {host_url}\")\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_dataset_by_name","title":"<code>get_dataset_by_name(name)</code>","text":"<p>Retrieves a dataset by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset.</p> required <p>Returns:</p> Type Description <code>Optional[DatasetMetadata]</code> <p>Optional[DatasetMetadata]: The dataset metadata if found, or None otherwise.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_dataset_by_name(self, name: str) -&gt; Optional[DatasetMetadata]:\n    \"\"\"\n    Retrieves a dataset by its name.\n\n    Args:\n        name (str): Name of the dataset.\n\n    Returns:\n        Optional[DatasetMetadata]: The dataset metadata if found, or None otherwise.\n    \"\"\"\n    datasets = self.list_shared_datasets()\n    name_lower = name.lower()\n    for dataset in datasets:\n        if name_lower == dataset.name.lower():\n            return dataset\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_local_model","title":"<code>get_local_model(model_ref, runtime_type=None)</code>","text":"<p>Retrieves a local model for the specified reference.</p> <p>Downloads the model if it does not already exist in the local cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference identifier for the model.</p> required <code>runtime_type</code> <code>Optional[RuntimeTypes]</code> <p>Runtime type for the model. Defaults to the <code>runtime_type</code> specified in FOCOOS_CONFIG.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LocalModel</code> <code>LocalModel</code> <p>An instance of the local model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the runtime type is not specified.</p> Notes <p>The model is cached in the directory specified by <code>self.cache_dir</code>.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_local_model(\n    self,\n    model_ref: str,\n    runtime_type: Optional[RuntimeTypes] = None,\n) -&gt; LocalModel:\n    \"\"\"\n    Retrieves a local model for the specified reference.\n\n    Downloads the model if it does not already exist in the local cache.\n\n    Args:\n        model_ref (str): Reference identifier for the model.\n        runtime_type (Optional[RuntimeTypes]): Runtime type for the model. Defaults to the\n            `runtime_type` specified in FOCOOS_CONFIG.\n\n    Returns:\n        LocalModel: An instance of the local model.\n\n    Raises:\n        ValueError: If the runtime type is not specified.\n\n    Notes:\n        The model is cached in the directory specified by `self.cache_dir`.\n    \"\"\"\n    runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n    model_dir = os.path.join(self.cache_dir, model_ref)\n    if not os.path.exists(os.path.join(model_dir, \"model.onnx\")):\n        self._download_model(model_ref)\n    return LocalModel(model_dir, runtime_type)\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_model_by_name","title":"<code>get_model_by_name(name, remote=True)</code>","text":"<p>Retrieves a model by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>remote</code> <code>bool</code> <p>If True, retrieve as a RemoteModel. Otherwise, as a LocalModel. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[RemoteModel, LocalModel]</code> <p>Optional[Union[RemoteModel, LocalModel]]: The model instance if found, or None otherwise.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_model_by_name(self, name: str, remote: bool = True) -&gt; Union[RemoteModel, LocalModel]:\n    \"\"\"\n    Retrieves a model by its name.\n\n    Args:\n        name (str): Name of the model.\n        remote (bool): If True, retrieve as a RemoteModel. Otherwise, as a LocalModel. Defaults to True.\n\n    Returns:\n        Optional[Union[RemoteModel, LocalModel]]: The model instance if found, or None otherwise.\n    \"\"\"\n    models = self.list_models()\n    name_lower = name.lower()\n    for model in models:\n        if name_lower == model.name.lower():\n            if remote:\n                return self.get_remote_model(model.ref)\n            else:\n                return self.get_local_model(model.ref)\n    raise ModelNotFound(f\"Model not found: {name}\")\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_model_info","title":"<code>get_model_info(model_name)</code>","text":"<p>Retrieves metadata for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <p>Returns:</p> Name Type Description <code>ModelMetadata</code> <code>ModelMetadata</code> <p>Metadata of the specified model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_model_info(self, model_name: str) -&gt; ModelMetadata:\n    \"\"\"\n    Retrieves metadata for a specific model.\n\n    Args:\n        model_name (str): Name of the model.\n\n    Returns:\n        ModelMetadata: Metadata of the specified model.\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.get(f\"models/{model_name}\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n    return ModelMetadata.from_json(res.json())\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_remote_model","title":"<code>get_remote_model(model_ref)</code>","text":"<p>Retrieves a remote model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference name of the model.</p> required <p>Returns:</p> Name Type Description <code>RemoteModel</code> <code>RemoteModel</code> <p>The remote model instance.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_remote_model(self, model_ref: str) -&gt; RemoteModel:\n    \"\"\"\n    Retrieves a remote model instance.\n\n    Args:\n        model_ref (str): Reference name of the model.\n\n    Returns:\n        RemoteModel: The remote model instance.\n    \"\"\"\n    return RemoteModel(model_ref, self.http_client)\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_user_info","title":"<code>get_user_info()</code>","text":"<p>Retrieves information about the authenticated user.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>User</code> <p>Information about the user (e.g., email).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_user_info(self) -&gt; User:\n    \"\"\"\n    Retrieves information about the authenticated user.\n\n    Returns:\n        dict: Information about the user (e.g., email).\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.get(\"user/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get user info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get user info: {res.status_code} {res.text}\")\n    return User.from_json(res.json())\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.list_focoos_models","title":"<code>list_focoos_models()</code>","text":"<p>Lists models specific to Focoos.</p> <p>Returns:</p> Type Description <code>list[ModelPreview]</code> <p>list[ModelPreview]: List of Focoos models.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def list_focoos_models(self) -&gt; list[ModelPreview]:\n    \"\"\"\n    Lists models specific to Focoos.\n\n    Returns:\n        list[ModelPreview]: List of Focoos models.\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.get(\"models/focoos-models\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n    return [ModelPreview.from_json(r) for r in res.json()]\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.list_models","title":"<code>list_models()</code>","text":"<p>Lists all available models.</p> <p>Returns:</p> Type Description <code>list[ModelPreview]</code> <p>list[ModelPreview]: List of model previews.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def list_models(self) -&gt; list[ModelPreview]:\n    \"\"\"\n    Lists all available models.\n\n    Returns:\n        list[ModelPreview]: List of model previews.\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.get(\"models/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list models: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list models: {res.status_code} {res.text}\")\n    return [ModelPreview.from_json(r) for r in res.json()]\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.list_shared_datasets","title":"<code>list_shared_datasets()</code>","text":"<p>Lists datasets shared with the user.</p> <p>Returns:</p> Type Description <code>list[DatasetMetadata]</code> <p>list[DatasetMetadata]: List of shared datasets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def list_shared_datasets(self) -&gt; list[DatasetMetadata]:\n    \"\"\"\n    Lists datasets shared with the user.\n\n    Returns:\n        list[DatasetMetadata]: List of shared datasets.\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.get(\"datasets/shared\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n    return [DatasetMetadata.from_json(dataset) for dataset in res.json()]\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.new_model","title":"<code>new_model(name, focoos_model, description)</code>","text":"<p>Creates a new model in the Focoos system.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the new model.</p> required <code>focoos_model</code> <code>str</code> <p>Reference to the base Focoos model.</p> required <code>description</code> <code>str</code> <p>Description of the new model.</p> required <p>Returns:</p> Type Description <code>RemoteModel</code> <p>Optional[RemoteModel]: The created model instance, or None if creation fails.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def new_model(self, name: str, focoos_model: str, description: str) -&gt; RemoteModel:\n    \"\"\"\n    Creates a new model in the Focoos system.\n\n    Args:\n        name (str): Name of the new model.\n        focoos_model (str): Reference to the base Focoos model.\n        description (str): Description of the new model.\n\n    Returns:\n        Optional[RemoteModel]: The created model instance, or None if creation fails.\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.post(\n        \"models/\",\n        data={\n            \"name\": name,\n            \"focoos_model\": focoos_model,\n            \"description\": description,\n        },\n    )\n    if res.status_code in [200, 201]:\n        return RemoteModel(res.json()[\"ref\"], self.http_client)\n    if res.status_code == 409:\n        logger.warning(f\"Model already exists: {name}\")\n        return self.get_model_by_name(name, remote=True)\n    logger.warning(f\"Failed to create new model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/local_model/","title":"local model","text":"<p>LocalModel Module</p> <p>This module provides the <code>LocalModel</code> class that allows loading, inference, and benchmark testing of models in a local environment. It supports detection and segmentation tasks, and utilizes ONNXRuntime for model execution.</p> <p>Classes:</p> Name Description <code>LocalModel</code> <p>A class for managing and interacting with local models.</p> <p>Functions:</p> Name Description <code>__init__</code> <p>Initializes the LocalModel instance, loading the model, metadata,       and setting up the runtime.</p> <code>_read_metadata</code> <p>Reads the model metadata from a JSON file.</p> <code>_annotate</code> <p>Annotates the input image with detection or segmentation results.</p> <code>infer</code> <p>Runs inference on an input image, with optional annotation.</p> <code>benchmark</code> <p>Benchmarks the model's inference performance over a specified        number of iterations and input size.</p>"},{"location":"api/local_model/#focoos.local_model.LocalModel","title":"<code>LocalModel</code>","text":"Source code in <code>focoos/local_model.py</code> <pre><code>class LocalModel:\n    def __init__(\n        self,\n        model_dir: Union[str, Path],\n        runtime_type: Optional[RuntimeTypes] = None,\n    ):\n        \"\"\"\n        Initialize a LocalModel instance.\n\n        This class sets up a local model for inference by initializing the runtime environment,\n        loading metadata, and preparing annotation utilities.\n\n        Args:\n            model_dir (Union[str, Path]): The path to the directory containing the model files.\n            runtime_type (Optional[RuntimeTypes]): Specifies the runtime type to use for inference.\n                Defaults to the value of `FOCOOS_CONFIG.runtime_type` if not provided.\n\n        Raises:\n            ValueError: If no runtime type is provided and `FOCOOS_CONFIG.runtime_type` is not set.\n            FileNotFoundError: If the specified model directory does not exist.\n\n        Attributes:\n            model_dir (Union[str, Path]): Path to the model directory.\n            metadata (ModelMetadata): Metadata information for the model.\n            model_ref: Reference identifier for the model obtained from metadata.\n            label_annotator (sv.LabelAnnotator): Utility for adding labels to the output,\n                initialized with text padding and border radius.\n            box_annotator (sv.BoxAnnotator): Utility for annotating bounding boxes.\n            mask_annotator (sv.MaskAnnotator): Utility for annotating masks.\n            runtime (ONNXRuntime): Inference runtime initialized with the specified runtime type,\n                model path, metadata, and warmup iterations.\n\n        The method verifies the existence of the model directory, reads the model metadata,\n        and initializes the runtime for inference using the provided runtime type. Annotation\n        utilities are also prepared for visualizing model outputs.\n        \"\"\"\n        runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n\n        logger.debug(f\"Runtime type: {runtime_type}, Loading model from {model_dir},\")\n        if not os.path.exists(model_dir):\n            raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n        self.model_dir: Union[str, Path] = model_dir\n        self.metadata: ModelMetadata = self._read_metadata()\n        self.model_ref = self.metadata.ref\n        self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n        self.box_annotator = sv.BoxAnnotator()\n        self.mask_annotator = sv.MaskAnnotator()\n        self.runtime: ONNXRuntime = get_runtime(\n            runtime_type,\n            str(os.path.join(model_dir, \"model.onnx\")),\n            self.metadata,\n            FOCOOS_CONFIG.warmup_iter,\n        )\n\n    def _read_metadata(self) -&gt; ModelMetadata:\n        \"\"\"\n        Reads the model metadata from a JSON file.\n\n        Returns:\n            ModelMetadata: Metadata for the model.\n\n        Raises:\n            FileNotFoundError: If the metadata file does not exist in the model directory.\n        \"\"\"\n        metadata_path = os.path.join(self.model_dir, \"focoos_metadata.json\")\n        return ModelMetadata.from_json(metadata_path)\n\n    def _annotate(self, im: np.ndarray, detections: sv.Detections) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the input image with detection or segmentation results.\n\n        Args:\n            im (np.ndarray): The input image to annotate.\n            detections (sv.Detections): Detected objects or segmented regions.\n\n        Returns:\n            np.ndarray: The annotated image with bounding boxes or masks.\n        \"\"\"\n        classes = self.metadata.classes\n        labels = [\n            f\"{classes[int(class_id)] if classes is not None else str(class_id)}: {confid * 100:.0f}%\"\n            for class_id, confid in zip(detections.class_id, detections.confidence)  # type: ignore\n        ]\n        if self.metadata.task == FocoosTask.DETECTION:\n            annotated_im = self.box_annotator.annotate(scene=im.copy(), detections=detections)\n\n            annotated_im = self.label_annotator.annotate(scene=annotated_im, detections=detections, labels=labels)\n        elif self.metadata.task in [\n            FocoosTask.SEMSEG,\n            FocoosTask.INSTANCE_SEGMENTATION,\n        ]:\n            annotated_im = self.mask_annotator.annotate(scene=im.copy(), detections=detections)\n        return annotated_im\n\n    def infer(\n        self,\n        image: Union[bytes, str, Path, np.ndarray, Image.Image],\n        threshold: float = 0.5,\n        annotate: bool = False,\n    ) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n        \"\"\"\n        Run inference on an input image and optionally annotate the results.\n\n        Args:\n            image (Union[bytes, str, Path, np.ndarray, Image.Image]): The input image to infer on.\n                This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.\n            threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n                Detections with confidence scores below this threshold will be discarded.\n            annotate (bool, optional): Whether to annotate the image with detection results. Defaults to False.\n                If set to True, the method will return the image with bounding boxes or segmentation masks.\n\n        Returns:\n            Tuple[FocoosDetections, Optional[np.ndarray]]: A tuple containing:\n                - `FocoosDetections`: The detections from the inference, represented as a custom object (`FocoosDetections`).\n                This includes the details of the detected objects such as class, confidence score, and bounding box (if applicable).\n                - `Optional[np.ndarray]`: The annotated image, if `annotate=True`.\n                This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks.\n                If `annotate=False`, this value will be `None`.\n\n        Raises:\n            ValueError: If the model is not deployed locally (i.e., `self.runtime` is `None`).\n        \"\"\"\n        assert self.runtime is not None, \"Model is not deployed (locally)\"\n        resize = None  #!TODO  check for segmentation\n        if self.metadata.task == FocoosTask.DETECTION:\n            resize = 640 if not self.metadata.im_size else self.metadata.im_size\n        logger.debug(f\"Resize: {resize}\")\n        t0 = perf_counter()\n        im1, im0 = image_preprocess(image, resize=resize)\n        t1 = perf_counter()\n        detections = self.runtime(im1.astype(np.float32), threshold)\n        t2 = perf_counter()\n        if resize:\n            detections = scale_detections(detections, (resize, resize), (im0.shape[1], im0.shape[0]))\n        logger.debug(f\"Inference time: {t2 - t1:.3f} seconds\")\n        im = None\n        if annotate:\n            im = self._annotate(im0, detections)\n\n        out = sv_to_focoos_detections(detections, classes=self.metadata.classes)\n        t3 = perf_counter()\n        out.latency = {\n            \"inference\": round(t2 - t1, 3),\n            \"preprocess\": round(t1 - t0, 3),\n            \"postprocess\": round(t3 - t2, 3),\n        }\n        return out, im\n\n    def benchmark(self, iterations: int, size: int) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model's inference performance over multiple iterations.\n\n        Args:\n            iterations (int): Number of iterations to run for benchmarking.\n            size (int): The input size for each benchmark iteration.\n\n        Returns:\n            LatencyMetrics: Latency metrics including time taken for inference.\n        \"\"\"\n        return self.runtime.benchmark(iterations, size)\n</code></pre>"},{"location":"api/local_model/#focoos.local_model.LocalModel.__init__","title":"<code>__init__(model_dir, runtime_type=None)</code>","text":"<p>Initialize a LocalModel instance.</p> <p>This class sets up a local model for inference by initializing the runtime environment, loading metadata, and preparing annotation utilities.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Union[str, Path]</code> <p>The path to the directory containing the model files.</p> required <code>runtime_type</code> <code>Optional[RuntimeTypes]</code> <p>Specifies the runtime type to use for inference. Defaults to the value of <code>FOCOOS_CONFIG.runtime_type</code> if not provided.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no runtime type is provided and <code>FOCOOS_CONFIG.runtime_type</code> is not set.</p> <code>FileNotFoundError</code> <p>If the specified model directory does not exist.</p> <p>Attributes:</p> Name Type Description <code>model_dir</code> <code>Union[str, Path]</code> <p>Path to the model directory.</p> <code>metadata</code> <code>ModelMetadata</code> <p>Metadata information for the model.</p> <code>model_ref</code> <code>ModelMetadata</code> <p>Reference identifier for the model obtained from metadata.</p> <code>label_annotator</code> <code>LabelAnnotator</code> <p>Utility for adding labels to the output, initialized with text padding and border radius.</p> <code>box_annotator</code> <code>BoxAnnotator</code> <p>Utility for annotating bounding boxes.</p> <code>mask_annotator</code> <code>MaskAnnotator</code> <p>Utility for annotating masks.</p> <code>runtime</code> <code>ONNXRuntime</code> <p>Inference runtime initialized with the specified runtime type, model path, metadata, and warmup iterations.</p> <p>The method verifies the existence of the model directory, reads the model metadata, and initializes the runtime for inference using the provided runtime type. Annotation utilities are also prepared for visualizing model outputs.</p> Source code in <code>focoos/local_model.py</code> <pre><code>def __init__(\n    self,\n    model_dir: Union[str, Path],\n    runtime_type: Optional[RuntimeTypes] = None,\n):\n    \"\"\"\n    Initialize a LocalModel instance.\n\n    This class sets up a local model for inference by initializing the runtime environment,\n    loading metadata, and preparing annotation utilities.\n\n    Args:\n        model_dir (Union[str, Path]): The path to the directory containing the model files.\n        runtime_type (Optional[RuntimeTypes]): Specifies the runtime type to use for inference.\n            Defaults to the value of `FOCOOS_CONFIG.runtime_type` if not provided.\n\n    Raises:\n        ValueError: If no runtime type is provided and `FOCOOS_CONFIG.runtime_type` is not set.\n        FileNotFoundError: If the specified model directory does not exist.\n\n    Attributes:\n        model_dir (Union[str, Path]): Path to the model directory.\n        metadata (ModelMetadata): Metadata information for the model.\n        model_ref: Reference identifier for the model obtained from metadata.\n        label_annotator (sv.LabelAnnotator): Utility for adding labels to the output,\n            initialized with text padding and border radius.\n        box_annotator (sv.BoxAnnotator): Utility for annotating bounding boxes.\n        mask_annotator (sv.MaskAnnotator): Utility for annotating masks.\n        runtime (ONNXRuntime): Inference runtime initialized with the specified runtime type,\n            model path, metadata, and warmup iterations.\n\n    The method verifies the existence of the model directory, reads the model metadata,\n    and initializes the runtime for inference using the provided runtime type. Annotation\n    utilities are also prepared for visualizing model outputs.\n    \"\"\"\n    runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n\n    logger.debug(f\"Runtime type: {runtime_type}, Loading model from {model_dir},\")\n    if not os.path.exists(model_dir):\n        raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n    self.model_dir: Union[str, Path] = model_dir\n    self.metadata: ModelMetadata = self._read_metadata()\n    self.model_ref = self.metadata.ref\n    self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n    self.box_annotator = sv.BoxAnnotator()\n    self.mask_annotator = sv.MaskAnnotator()\n    self.runtime: ONNXRuntime = get_runtime(\n        runtime_type,\n        str(os.path.join(model_dir, \"model.onnx\")),\n        self.metadata,\n        FOCOOS_CONFIG.warmup_iter,\n    )\n</code></pre>"},{"location":"api/local_model/#focoos.local_model.LocalModel.benchmark","title":"<code>benchmark(iterations, size)</code>","text":"<p>Benchmark the model's inference performance over multiple iterations.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of iterations to run for benchmarking.</p> required <code>size</code> <code>int</code> <p>The input size for each benchmark iteration.</p> required <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Latency metrics including time taken for inference.</p> Source code in <code>focoos/local_model.py</code> <pre><code>def benchmark(self, iterations: int, size: int) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model's inference performance over multiple iterations.\n\n    Args:\n        iterations (int): Number of iterations to run for benchmarking.\n        size (int): The input size for each benchmark iteration.\n\n    Returns:\n        LatencyMetrics: Latency metrics including time taken for inference.\n    \"\"\"\n    return self.runtime.benchmark(iterations, size)\n</code></pre>"},{"location":"api/local_model/#focoos.local_model.LocalModel.infer","title":"<code>infer(image, threshold=0.5, annotate=False)</code>","text":"<p>Run inference on an input image and optionally annotate the results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[bytes, str, Path, ndarray, Image]</code> <p>The input image to infer on. This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.</p> required <code>threshold</code> <code>float</code> <p>The confidence threshold for detections. Defaults to 0.5. Detections with confidence scores below this threshold will be discarded.</p> <code>0.5</code> <code>annotate</code> <code>bool</code> <p>Whether to annotate the image with detection results. Defaults to False. If set to True, the method will return the image with bounding boxes or segmentation masks.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[FocoosDetections, Optional[ndarray]]</code> <p>Tuple[FocoosDetections, Optional[np.ndarray]]: A tuple containing: - <code>FocoosDetections</code>: The detections from the inference, represented as a custom object (<code>FocoosDetections</code>). This includes the details of the detected objects such as class, confidence score, and bounding box (if applicable). - <code>Optional[np.ndarray]</code>: The annotated image, if <code>annotate=True</code>. This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks. If <code>annotate=False</code>, this value will be <code>None</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not deployed locally (i.e., <code>self.runtime</code> is <code>None</code>).</p> Source code in <code>focoos/local_model.py</code> <pre><code>def infer(\n    self,\n    image: Union[bytes, str, Path, np.ndarray, Image.Image],\n    threshold: float = 0.5,\n    annotate: bool = False,\n) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n    \"\"\"\n    Run inference on an input image and optionally annotate the results.\n\n    Args:\n        image (Union[bytes, str, Path, np.ndarray, Image.Image]): The input image to infer on.\n            This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.\n        threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n            Detections with confidence scores below this threshold will be discarded.\n        annotate (bool, optional): Whether to annotate the image with detection results. Defaults to False.\n            If set to True, the method will return the image with bounding boxes or segmentation masks.\n\n    Returns:\n        Tuple[FocoosDetections, Optional[np.ndarray]]: A tuple containing:\n            - `FocoosDetections`: The detections from the inference, represented as a custom object (`FocoosDetections`).\n            This includes the details of the detected objects such as class, confidence score, and bounding box (if applicable).\n            - `Optional[np.ndarray]`: The annotated image, if `annotate=True`.\n            This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks.\n            If `annotate=False`, this value will be `None`.\n\n    Raises:\n        ValueError: If the model is not deployed locally (i.e., `self.runtime` is `None`).\n    \"\"\"\n    assert self.runtime is not None, \"Model is not deployed (locally)\"\n    resize = None  #!TODO  check for segmentation\n    if self.metadata.task == FocoosTask.DETECTION:\n        resize = 640 if not self.metadata.im_size else self.metadata.im_size\n    logger.debug(f\"Resize: {resize}\")\n    t0 = perf_counter()\n    im1, im0 = image_preprocess(image, resize=resize)\n    t1 = perf_counter()\n    detections = self.runtime(im1.astype(np.float32), threshold)\n    t2 = perf_counter()\n    if resize:\n        detections = scale_detections(detections, (resize, resize), (im0.shape[1], im0.shape[0]))\n    logger.debug(f\"Inference time: {t2 - t1:.3f} seconds\")\n    im = None\n    if annotate:\n        im = self._annotate(im0, detections)\n\n    out = sv_to_focoos_detections(detections, classes=self.metadata.classes)\n    t3 = perf_counter()\n    out.latency = {\n        \"inference\": round(t2 - t1, 3),\n        \"preprocess\": round(t1 - t0, 3),\n        \"postprocess\": round(t3 - t2, 3),\n    }\n    return out, im\n</code></pre>"},{"location":"api/remote_model/","title":"remote model","text":"<p>RemoteModel Module</p> <p>This module provides a class to manage remote models in the Focoos ecosystem. It supports various functionalities including model training, deployment, inference, and monitoring.</p> <p>Classes:</p> Name Description <code>RemoteModel</code> <p>A class for interacting with remote models, managing their lifecycle,          and performing inference.</p> <p>Modules:</p> Name Description <code>HttpClient</code> <p>Handles HTTP requests.</p> <code>logger</code> <p>Logging utility.</p> <code>BoxAnnotator, LabelAnnotator, MaskAnnotator</code> <p>Annotation tools for visualizing          detections and segmentation tasks.</p> <code>FocoosDet, FocoosDetections</code> <p>Classes for representing and managing detections.</p> <code>FocoosTask</code> <p>Enum for defining supported tasks (e.g., DETECTION, SEMSEG).</p> <code>Hyperparameters</code> <p>Structure for training configuration parameters.</p> <code>ModelMetadata</code> <p>Contains metadata for the model.</p> <code>ModelStatus</code> <p>Enum for representing the current status of the model.</p> <code>TrainInstance</code> <p>Enum for defining available training instances.</p> <code>image_loader</code> <p>Utility function for loading images.</p> <code>focoos_detections_to_supervision</code> <p>Converter for Focoos detections to supervision format.</p>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel","title":"<code>RemoteModel</code>","text":"<p>Represents a remote model in the Focoos platform.</p> <p>Attributes:</p> Name Type Description <code>model_ref</code> <code>str</code> <p>Reference ID for the model.</p> <code>http_client</code> <code>HttpClient</code> <p>Client for making HTTP requests.</p> <code>max_deploy_wait</code> <code>int</code> <p>Maximum wait time for model deployment.</p> <code>metadata</code> <code>ModelMetadata</code> <p>Metadata of the model.</p> <code>label_annotator</code> <code>LabelAnnotator</code> <p>Annotator for adding labels to images.</p> <code>box_annotator</code> <code>BoxAnnotator</code> <p>Annotator for drawing bounding boxes.</p> <code>mask_annotator</code> <code>MaskAnnotator</code> <p>Annotator for drawing masks on images.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>class RemoteModel:\n    \"\"\"\n    Represents a remote model in the Focoos platform.\n\n    Attributes:\n        model_ref (str): Reference ID for the model.\n        http_client (HttpClient): Client for making HTTP requests.\n        max_deploy_wait (int): Maximum wait time for model deployment.\n        metadata (ModelMetadata): Metadata of the model.\n        label_annotator (LabelAnnotator): Annotator for adding labels to images.\n        box_annotator (sv.BoxAnnotator): Annotator for drawing bounding boxes.\n        mask_annotator (sv.MaskAnnotator): Annotator for drawing masks on images.\n    \"\"\"\n\n    def __init__(self, model_ref: str, http_client: HttpClient):\n        \"\"\"\n        Initialize the RemoteModel instance.\n\n        Args:\n            model_ref (str): Reference ID for the model.\n            http_client (HttpClient): HTTP client instance for communication.\n\n        Raises:\n            ValueError: If model metadata retrieval fails.\n        \"\"\"\n        self.model_ref = model_ref\n        self.http_client = http_client\n        self.max_deploy_wait = 10\n        self.metadata: ModelMetadata = self.get_info()\n\n        self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n        self.box_annotator = sv.BoxAnnotator()\n        self.mask_annotator = sv.MaskAnnotator()\n        logger.info(\n            f\"[RemoteModel]: ref: {self.model_ref} name: {self.metadata.name} description: {self.metadata.description} status: {self.metadata.status}\"\n        )\n\n    def get_info(self) -&gt; ModelMetadata:\n        \"\"\"\n        Retrieve model metadata.\n\n        Returns:\n            ModelMetadata: Metadata of the model.\n\n        Raises:\n            ValueError: If the request fails.\n        \"\"\"\n        res = self.http_client.get(f\"models/{self.model_ref}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n        self.metadata = ModelMetadata(**res.json())\n        return self.metadata\n\n    def train(\n        self,\n        dataset_ref: str,\n        hyperparameters: Hyperparameters,\n        instance_type: TrainInstance = TrainInstance.ML_G4DN_XLARGE,\n        volume_size: int = 50,\n        max_runtime_in_seconds: int = 36000,\n    ) -&gt; dict | None:\n        \"\"\"\n        Initiate the training of a remote model on the Focoos platform.\n\n        This method sends a request to the Focoos platform to start the training process for the model\n        referenced by `self.model_ref`. It requires a dataset reference and hyperparameters for training,\n        as well as optional configuration options for the instance type, volume size, and runtime.\n\n        Args:\n            dataset_ref (str): The reference ID of the dataset to be used for training.\n            hyperparameters (Hyperparameters): A structure containing the hyperparameters for the training process.\n            anyma_version (str, optional): The version of Anyma to use for training. Defaults to \"anyma-sagemaker-cu12-torch22-0111\".\n            instance_type (TrainInstance, optional): The type of training instance to use. Defaults to TrainInstance.ML_G4DN_XLARGE.\n            volume_size (int, optional): The size of the disk volume (in GB) for the training instance. Defaults to 50.\n            max_runtime_in_seconds (int, optional): The maximum runtime for training in seconds. Defaults to 36000.\n\n        Returns:\n            dict: A dictionary containing the response from the training initiation request. The content depends on the Focoos platform's response.\n\n        Raises:\n            ValueError: If the request to start training fails (e.g., due to incorrect parameters or server issues).\n        \"\"\"\n        res = self.http_client.post(\n            f\"models/{self.model_ref}/train\",\n            data={\n                \"dataset_ref\": dataset_ref,\n                \"instance_type\": instance_type,\n                \"volume_size\": volume_size,\n                \"max_runtime_in_seconds\": max_runtime_in_seconds,\n                \"hyperparameters\": hyperparameters.model_dump(),\n            },\n        )\n        if res.status_code != 200:\n            logger.warning(f\"Failed to train model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to train model: {res.status_code} {res.text}\")\n        return res.json()\n\n    def train_info(self) -&gt; Optional[TrainingInfo]:\n        \"\"\"\n        Retrieve the current status of the model training.\n\n        Sends a request to check the training status of the model referenced by `self.model_ref`.\n\n        Returns:\n            dict: A dictionary containing the training status information.\n\n        Raises:\n            ValueError: If the request to get training status fails.\n        \"\"\"\n        res = self.http_client.get(f\"models/{self.model_ref}/train/status\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get train status: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get train status: {res.status_code} {res.text}\")\n        return TrainingInfo(**res.json())\n\n    def train_logs(self) -&gt; list[str]:\n        \"\"\"\n        Retrieve the training logs for the model.\n\n        This method sends a request to fetch the logs of the model's training process. If the request\n        is successful (status code 200), it returns the logs as a list of strings. If the request fails,\n        it logs a warning and returns an empty list.\n\n        Returns:\n            list[str]: A list of training logs as strings.\n\n        Raises:\n            None: Returns an empty list if the request fails.\n        \"\"\"\n        res = self.http_client.get(f\"models/{self.model_ref}/train/logs\")\n        if res.status_code != 200:\n            logger.warning(f\"Failed to get train logs: {res.status_code} {res.text}\")\n            return []\n        return res.json()\n\n    def metrics(self) -&gt; Metrics:  # noqa: F821\n        \"\"\"\n        Retrieve the metrics of the model.\n\n        This method sends a request to fetch the metrics of the model identified by `model_ref`.\n        If the request is successful (status code 200), it returns the metrics as a `Metrics` object.\n        If the request fails, it logs a warning and returns an empty `Metrics` object.\n\n        Returns:\n            Metrics: An object containing the metrics of the model.\n\n        Raises:\n            None: Returns an empty `Metrics` object if the request fails.\n        \"\"\"\n        res = self.http_client.get(f\"models/{self.model_ref}/metrics\")\n        if res.status_code != 200:\n            logger.warning(f\"Failed to get metrics: {res.status_code} {res.text}\")\n            return Metrics()  # noqa: F821\n        return Metrics(**res.json())\n\n    def _annotate(self, im: np.ndarray, detections: sv.Detections) -&gt; np.ndarray:\n        \"\"\"\n        Annotate an image with detection results.\n\n        This method adds visual annotations to the provided image based on the model's detection results.\n        It handles different tasks (e.g., object detection, semantic segmentation, instance segmentation)\n        and uses the corresponding annotator (bounding box, label, or mask) to draw on the image.\n\n        Args:\n            im (np.ndarray): The image to be annotated, represented as a NumPy array.\n            detections (sv.Detections): The detection results to be annotated, including class IDs and confidence scores.\n\n        Returns:\n            np.ndarray: The annotated image as a NumPy array.\n        \"\"\"\n        classes = self.metadata.classes\n        if classes is not None:\n            labels = [\n                f\"{classes[int(class_id)]}: {confid * 100:.0f}%\"\n                for class_id, confid in zip(detections.class_id, detections.confidence)\n            ]\n        else:\n            labels = [\n                f\"{str(class_id)}: {confid * 100:.0f}%\"\n                for class_id, confid in zip(detections.class_id, detections.confidence)\n            ]\n        if self.metadata.task == FocoosTask.DETECTION:\n            annotated_im = self.box_annotator.annotate(scene=im.copy(), detections=detections)\n\n            annotated_im = self.label_annotator.annotate(scene=annotated_im, detections=detections, labels=labels)\n        elif self.metadata.task in [\n            FocoosTask.SEMSEG,\n            FocoosTask.INSTANCE_SEGMENTATION,\n        ]:\n            annotated_im = self.mask_annotator.annotate(scene=im.copy(), detections=detections)\n        return annotated_im\n\n    def infer(\n        self,\n        image: Union[str, Path, np.ndarray, bytes],\n        threshold: float = 0.5,\n        annotate: bool = False,\n    ) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n        \"\"\"\n        Perform inference on the provided image using the remote model.\n\n        This method sends an image to the remote model for inference and retrieves the detection results.\n        Optionally, it can annotate the image with the detection results.\n\n        Args:\n            image (Union[str, Path, bytes]): The image to infer on, which can be a file path, a string representing the path, or raw bytes.\n            threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n            annotate (bool, optional): Whether to annotate the image with the detection results. Defaults to False.\n\n        Returns:\n            Tuple[FocoosDetections, Optional[np.ndarray]]:\n                - FocoosDetections: The detection results including class IDs, confidence scores, etc.\n                - Optional[np.ndarray]: The annotated image if `annotate` is True, else None.\n\n        Raises:\n            FileNotFoundError: If the provided image file path is invalid.\n            ValueError: If the inference request fails.\n        \"\"\"\n        image_bytes = None\n        if isinstance(image, str) or isinstance(image, Path):\n            if not os.path.exists(image):\n                logger.error(f\"Image file not found: {image}\")\n                raise FileNotFoundError(f\"Image file not found: {image}\")\n            image_bytes = open(image, \"rb\").read()\n        elif isinstance(image, np.ndarray):\n            _, buffer = cv2.imencode(\".jpg\", image)\n            image_bytes = buffer.tobytes()\n        else:\n            image_bytes = image\n        files = {\"file\": image_bytes}\n        t0 = time.time()\n        res = self.http_client.post(\n            f\"models/{self.model_ref}/inference?confidence_threshold={threshold}\",\n            files=files,\n        )\n        t1 = time.time()\n        if res.status_code == 200:\n            logger.debug(f\"Inference time: {t1 - t0:.3f} seconds\")\n            detections = FocoosDetections(\n                detections=[FocoosDet.from_json(d) for d in res.json().get(\"detections\", [])],\n                latency=res.json().get(\"latency\", None),\n            )\n            preview = None\n            if annotate:\n                im0 = image_loader(image)\n                sv_detections = focoos_detections_to_supervision(detections)\n                preview = self._annotate(im0, sv_detections)\n            return detections, preview\n        else:\n            logger.error(f\"Failed to infer: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to infer: {res.status_code} {res.text}\")\n\n    def notebook_monitor_train(self, interval: int = 30, plot_metrics: bool = False, max_runtime: int = 36000) -&gt; None:\n        \"\"\"\n        Monitor the training process in a Jupyter notebook and display metrics.\n\n        Periodically checks the training status and displays metrics in a notebook cell.\n        Clears previous output to maintain a clean view.\n\n        Args:\n            interval (int): Time between status checks in seconds. Must be 30-240. Default: 30\n            plot_metrics (bool): Whether to plot metrics graphs. Default: False\n            max_runtime (int): Maximum monitoring time in seconds. Default: 36000 (10 hours)\n\n        Returns:\n            None\n        \"\"\"\n        from IPython.display import clear_output\n\n        if not 30 &lt;= interval &lt;= 240:\n            raise ValueError(\"Interval must be between 30 and 240 seconds\")\n\n        last_update = self.get_info().updated_at\n        start_time = time.time()\n        status_history = []\n\n        while True:\n            # Get current status\n            model_info = self.get_info()\n            status = model_info.status\n\n            # Clear and display status\n            clear_output(wait=True)\n            status_msg = f\"[Live Monitor {self.metadata.name}] {status.value}\"\n            status_history.append(status_msg)\n            for msg in status_history:\n                logger.info(msg)\n\n            # Show metrics if training completed\n            if status == ModelStatus.TRAINING_COMPLETED:\n                metrics = self.metrics()\n                if metrics.best_valid_metric:\n                    logger.info(f\"Best Checkpoint (iter: {metrics.best_valid_metric.get('iteration', 'N/A')}):\")\n                    for k, v in metrics.best_valid_metric.items():\n                        logger.info(f\"  {k}: {v}\")\n                    visualizer = MetricsVisualizer(metrics)\n                    visualizer.log_metrics()\n                    if plot_metrics:\n                        visualizer.notebook_plot_training_metrics()\n\n            # Update metrics during training\n            if status == ModelStatus.TRAINING_RUNNING and model_info.updated_at &gt; last_update:\n                last_update = model_info.updated_at\n                metrics = self.metrics()\n                visualizer = MetricsVisualizer(metrics)\n                visualizer.log_metrics()\n                if plot_metrics:\n                    visualizer.notebook_plot_training_metrics()\n\n            # Check exit conditions\n            if status not in [ModelStatus.CREATED, ModelStatus.TRAINING_RUNNING, ModelStatus.TRAINING_STARTING]:\n                return\n\n            if time.time() - start_time &gt; max_runtime:\n                logger.warning(f\"Monitoring exceeded {max_runtime} seconds limit\")\n                return\n\n            sleep(interval)\n\n    def stop_training(self) -&gt; None:\n        \"\"\"\n        Stop the training process of the model.\n\n        This method sends a request to stop the training of the model identified by `model_ref`.\n        If the request fails, an error is logged and a `ValueError` is raised.\n\n        Raises:\n            ValueError: If the stop training request fails.\n\n        Logs:\n            - Error message if the request to stop training fails, including the status code and response text.\n\n        Returns:\n            None: This method does not return any value.\n        \"\"\"\n        res = self.http_client.delete(f\"models/{self.model_ref}/train\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get stop training: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get stop training: {res.status_code} {res.text}\")\n\n    def delete_model(self) -&gt; None:\n        \"\"\"\n        Delete the model from the system.\n\n        This method sends a request to delete the model identified by `model_ref`.\n        If the request fails or the status code is not 204 (No Content), an error is logged\n        and a `ValueError` is raised.\n\n        Raises:\n            ValueError: If the delete model request fails or does not return a 204 status code.\n\n        Logs:\n            - Error message if the request to delete the model fails, including the status code and response text.\n\n        Returns:\n            None: This method does not return any value.\n        \"\"\"\n        res = self.http_client.delete(f\"models/{self.model_ref}\")\n        if res.status_code != 204:\n            logger.error(f\"Failed to delete model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to delete model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.__init__","title":"<code>__init__(model_ref, http_client)</code>","text":"<p>Initialize the RemoteModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference ID for the model.</p> required <code>http_client</code> <code>HttpClient</code> <p>HTTP client instance for communication.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If model metadata retrieval fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def __init__(self, model_ref: str, http_client: HttpClient):\n    \"\"\"\n    Initialize the RemoteModel instance.\n\n    Args:\n        model_ref (str): Reference ID for the model.\n        http_client (HttpClient): HTTP client instance for communication.\n\n    Raises:\n        ValueError: If model metadata retrieval fails.\n    \"\"\"\n    self.model_ref = model_ref\n    self.http_client = http_client\n    self.max_deploy_wait = 10\n    self.metadata: ModelMetadata = self.get_info()\n\n    self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n    self.box_annotator = sv.BoxAnnotator()\n    self.mask_annotator = sv.MaskAnnotator()\n    logger.info(\n        f\"[RemoteModel]: ref: {self.model_ref} name: {self.metadata.name} description: {self.metadata.description} status: {self.metadata.status}\"\n    )\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.delete_model","title":"<code>delete_model()</code>","text":"<p>Delete the model from the system.</p> <p>This method sends a request to delete the model identified by <code>model_ref</code>. If the request fails or the status code is not 204 (No Content), an error is logged and a <code>ValueError</code> is raised.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the delete model request fails or does not return a 204 status code.</p> Logs <ul> <li>Error message if the request to delete the model fails, including the status code and response text.</li> </ul> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>This method does not return any value.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def delete_model(self) -&gt; None:\n    \"\"\"\n    Delete the model from the system.\n\n    This method sends a request to delete the model identified by `model_ref`.\n    If the request fails or the status code is not 204 (No Content), an error is logged\n    and a `ValueError` is raised.\n\n    Raises:\n        ValueError: If the delete model request fails or does not return a 204 status code.\n\n    Logs:\n        - Error message if the request to delete the model fails, including the status code and response text.\n\n    Returns:\n        None: This method does not return any value.\n    \"\"\"\n    res = self.http_client.delete(f\"models/{self.model_ref}\")\n    if res.status_code != 204:\n        logger.error(f\"Failed to delete model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to delete model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.get_info","title":"<code>get_info()</code>","text":"<p>Retrieve model metadata.</p> <p>Returns:</p> Name Type Description <code>ModelMetadata</code> <code>ModelMetadata</code> <p>Metadata of the model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def get_info(self) -&gt; ModelMetadata:\n    \"\"\"\n    Retrieve model metadata.\n\n    Returns:\n        ModelMetadata: Metadata of the model.\n\n    Raises:\n        ValueError: If the request fails.\n    \"\"\"\n    res = self.http_client.get(f\"models/{self.model_ref}\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n    self.metadata = ModelMetadata(**res.json())\n    return self.metadata\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.infer","title":"<code>infer(image, threshold=0.5, annotate=False)</code>","text":"<p>Perform inference on the provided image using the remote model.</p> <p>This method sends an image to the remote model for inference and retrieves the detection results. Optionally, it can annotate the image with the detection results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Path, bytes]</code> <p>The image to infer on, which can be a file path, a string representing the path, or raw bytes.</p> required <code>threshold</code> <code>float</code> <p>The confidence threshold for detections. Defaults to 0.5.</p> <code>0.5</code> <code>annotate</code> <code>bool</code> <p>Whether to annotate the image with the detection results. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[FocoosDetections, Optional[ndarray]]</code> <p>Tuple[FocoosDetections, Optional[np.ndarray]]: - FocoosDetections: The detection results including class IDs, confidence scores, etc. - Optional[np.ndarray]: The annotated image if <code>annotate</code> is True, else None.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the provided image file path is invalid.</p> <code>ValueError</code> <p>If the inference request fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def infer(\n    self,\n    image: Union[str, Path, np.ndarray, bytes],\n    threshold: float = 0.5,\n    annotate: bool = False,\n) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n    \"\"\"\n    Perform inference on the provided image using the remote model.\n\n    This method sends an image to the remote model for inference and retrieves the detection results.\n    Optionally, it can annotate the image with the detection results.\n\n    Args:\n        image (Union[str, Path, bytes]): The image to infer on, which can be a file path, a string representing the path, or raw bytes.\n        threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n        annotate (bool, optional): Whether to annotate the image with the detection results. Defaults to False.\n\n    Returns:\n        Tuple[FocoosDetections, Optional[np.ndarray]]:\n            - FocoosDetections: The detection results including class IDs, confidence scores, etc.\n            - Optional[np.ndarray]: The annotated image if `annotate` is True, else None.\n\n    Raises:\n        FileNotFoundError: If the provided image file path is invalid.\n        ValueError: If the inference request fails.\n    \"\"\"\n    image_bytes = None\n    if isinstance(image, str) or isinstance(image, Path):\n        if not os.path.exists(image):\n            logger.error(f\"Image file not found: {image}\")\n            raise FileNotFoundError(f\"Image file not found: {image}\")\n        image_bytes = open(image, \"rb\").read()\n    elif isinstance(image, np.ndarray):\n        _, buffer = cv2.imencode(\".jpg\", image)\n        image_bytes = buffer.tobytes()\n    else:\n        image_bytes = image\n    files = {\"file\": image_bytes}\n    t0 = time.time()\n    res = self.http_client.post(\n        f\"models/{self.model_ref}/inference?confidence_threshold={threshold}\",\n        files=files,\n    )\n    t1 = time.time()\n    if res.status_code == 200:\n        logger.debug(f\"Inference time: {t1 - t0:.3f} seconds\")\n        detections = FocoosDetections(\n            detections=[FocoosDet.from_json(d) for d in res.json().get(\"detections\", [])],\n            latency=res.json().get(\"latency\", None),\n        )\n        preview = None\n        if annotate:\n            im0 = image_loader(image)\n            sv_detections = focoos_detections_to_supervision(detections)\n            preview = self._annotate(im0, sv_detections)\n        return detections, preview\n    else:\n        logger.error(f\"Failed to infer: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to infer: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.metrics","title":"<code>metrics()</code>","text":"<p>Retrieve the metrics of the model.</p> <p>This method sends a request to fetch the metrics of the model identified by <code>model_ref</code>. If the request is successful (status code 200), it returns the metrics as a <code>Metrics</code> object. If the request fails, it logs a warning and returns an empty <code>Metrics</code> object.</p> <p>Returns:</p> Name Type Description <code>Metrics</code> <code>Metrics</code> <p>An object containing the metrics of the model.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns an empty <code>Metrics</code> object if the request fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def metrics(self) -&gt; Metrics:  # noqa: F821\n    \"\"\"\n    Retrieve the metrics of the model.\n\n    This method sends a request to fetch the metrics of the model identified by `model_ref`.\n    If the request is successful (status code 200), it returns the metrics as a `Metrics` object.\n    If the request fails, it logs a warning and returns an empty `Metrics` object.\n\n    Returns:\n        Metrics: An object containing the metrics of the model.\n\n    Raises:\n        None: Returns an empty `Metrics` object if the request fails.\n    \"\"\"\n    res = self.http_client.get(f\"models/{self.model_ref}/metrics\")\n    if res.status_code != 200:\n        logger.warning(f\"Failed to get metrics: {res.status_code} {res.text}\")\n        return Metrics()  # noqa: F821\n    return Metrics(**res.json())\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.notebook_monitor_train","title":"<code>notebook_monitor_train(interval=30, plot_metrics=False, max_runtime=36000)</code>","text":"<p>Monitor the training process in a Jupyter notebook and display metrics.</p> <p>Periodically checks the training status and displays metrics in a notebook cell. Clears previous output to maintain a clean view.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>int</code> <p>Time between status checks in seconds. Must be 30-240. Default: 30</p> <code>30</code> <code>plot_metrics</code> <code>bool</code> <p>Whether to plot metrics graphs. Default: False</p> <code>False</code> <code>max_runtime</code> <code>int</code> <p>Maximum monitoring time in seconds. Default: 36000 (10 hours)</p> <code>36000</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def notebook_monitor_train(self, interval: int = 30, plot_metrics: bool = False, max_runtime: int = 36000) -&gt; None:\n    \"\"\"\n    Monitor the training process in a Jupyter notebook and display metrics.\n\n    Periodically checks the training status and displays metrics in a notebook cell.\n    Clears previous output to maintain a clean view.\n\n    Args:\n        interval (int): Time between status checks in seconds. Must be 30-240. Default: 30\n        plot_metrics (bool): Whether to plot metrics graphs. Default: False\n        max_runtime (int): Maximum monitoring time in seconds. Default: 36000 (10 hours)\n\n    Returns:\n        None\n    \"\"\"\n    from IPython.display import clear_output\n\n    if not 30 &lt;= interval &lt;= 240:\n        raise ValueError(\"Interval must be between 30 and 240 seconds\")\n\n    last_update = self.get_info().updated_at\n    start_time = time.time()\n    status_history = []\n\n    while True:\n        # Get current status\n        model_info = self.get_info()\n        status = model_info.status\n\n        # Clear and display status\n        clear_output(wait=True)\n        status_msg = f\"[Live Monitor {self.metadata.name}] {status.value}\"\n        status_history.append(status_msg)\n        for msg in status_history:\n            logger.info(msg)\n\n        # Show metrics if training completed\n        if status == ModelStatus.TRAINING_COMPLETED:\n            metrics = self.metrics()\n            if metrics.best_valid_metric:\n                logger.info(f\"Best Checkpoint (iter: {metrics.best_valid_metric.get('iteration', 'N/A')}):\")\n                for k, v in metrics.best_valid_metric.items():\n                    logger.info(f\"  {k}: {v}\")\n                visualizer = MetricsVisualizer(metrics)\n                visualizer.log_metrics()\n                if plot_metrics:\n                    visualizer.notebook_plot_training_metrics()\n\n        # Update metrics during training\n        if status == ModelStatus.TRAINING_RUNNING and model_info.updated_at &gt; last_update:\n            last_update = model_info.updated_at\n            metrics = self.metrics()\n            visualizer = MetricsVisualizer(metrics)\n            visualizer.log_metrics()\n            if plot_metrics:\n                visualizer.notebook_plot_training_metrics()\n\n        # Check exit conditions\n        if status not in [ModelStatus.CREATED, ModelStatus.TRAINING_RUNNING, ModelStatus.TRAINING_STARTING]:\n            return\n\n        if time.time() - start_time &gt; max_runtime:\n            logger.warning(f\"Monitoring exceeded {max_runtime} seconds limit\")\n            return\n\n        sleep(interval)\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.stop_training","title":"<code>stop_training()</code>","text":"<p>Stop the training process of the model.</p> <p>This method sends a request to stop the training of the model identified by <code>model_ref</code>. If the request fails, an error is logged and a <code>ValueError</code> is raised.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the stop training request fails.</p> Logs <ul> <li>Error message if the request to stop training fails, including the status code and response text.</li> </ul> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>This method does not return any value.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def stop_training(self) -&gt; None:\n    \"\"\"\n    Stop the training process of the model.\n\n    This method sends a request to stop the training of the model identified by `model_ref`.\n    If the request fails, an error is logged and a `ValueError` is raised.\n\n    Raises:\n        ValueError: If the stop training request fails.\n\n    Logs:\n        - Error message if the request to stop training fails, including the status code and response text.\n\n    Returns:\n        None: This method does not return any value.\n    \"\"\"\n    res = self.http_client.delete(f\"models/{self.model_ref}/train\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get stop training: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get stop training: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.train","title":"<code>train(dataset_ref, hyperparameters, instance_type=TrainInstance.ML_G4DN_XLARGE, volume_size=50, max_runtime_in_seconds=36000)</code>","text":"<p>Initiate the training of a remote model on the Focoos platform.</p> <p>This method sends a request to the Focoos platform to start the training process for the model referenced by <code>self.model_ref</code>. It requires a dataset reference and hyperparameters for training, as well as optional configuration options for the instance type, volume size, and runtime.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_ref</code> <code>str</code> <p>The reference ID of the dataset to be used for training.</p> required <code>hyperparameters</code> <code>Hyperparameters</code> <p>A structure containing the hyperparameters for the training process.</p> required <code>anyma_version</code> <code>str</code> <p>The version of Anyma to use for training. Defaults to \"anyma-sagemaker-cu12-torch22-0111\".</p> required <code>instance_type</code> <code>TrainInstance</code> <p>The type of training instance to use. Defaults to TrainInstance.ML_G4DN_XLARGE.</p> <code>ML_G4DN_XLARGE</code> <code>volume_size</code> <code>int</code> <p>The size of the disk volume (in GB) for the training instance. Defaults to 50.</p> <code>50</code> <code>max_runtime_in_seconds</code> <code>int</code> <p>The maximum runtime for training in seconds. Defaults to 36000.</p> <code>36000</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict | None</code> <p>A dictionary containing the response from the training initiation request. The content depends on the Focoos platform's response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request to start training fails (e.g., due to incorrect parameters or server issues).</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def train(\n    self,\n    dataset_ref: str,\n    hyperparameters: Hyperparameters,\n    instance_type: TrainInstance = TrainInstance.ML_G4DN_XLARGE,\n    volume_size: int = 50,\n    max_runtime_in_seconds: int = 36000,\n) -&gt; dict | None:\n    \"\"\"\n    Initiate the training of a remote model on the Focoos platform.\n\n    This method sends a request to the Focoos platform to start the training process for the model\n    referenced by `self.model_ref`. It requires a dataset reference and hyperparameters for training,\n    as well as optional configuration options for the instance type, volume size, and runtime.\n\n    Args:\n        dataset_ref (str): The reference ID of the dataset to be used for training.\n        hyperparameters (Hyperparameters): A structure containing the hyperparameters for the training process.\n        anyma_version (str, optional): The version of Anyma to use for training. Defaults to \"anyma-sagemaker-cu12-torch22-0111\".\n        instance_type (TrainInstance, optional): The type of training instance to use. Defaults to TrainInstance.ML_G4DN_XLARGE.\n        volume_size (int, optional): The size of the disk volume (in GB) for the training instance. Defaults to 50.\n        max_runtime_in_seconds (int, optional): The maximum runtime for training in seconds. Defaults to 36000.\n\n    Returns:\n        dict: A dictionary containing the response from the training initiation request. The content depends on the Focoos platform's response.\n\n    Raises:\n        ValueError: If the request to start training fails (e.g., due to incorrect parameters or server issues).\n    \"\"\"\n    res = self.http_client.post(\n        f\"models/{self.model_ref}/train\",\n        data={\n            \"dataset_ref\": dataset_ref,\n            \"instance_type\": instance_type,\n            \"volume_size\": volume_size,\n            \"max_runtime_in_seconds\": max_runtime_in_seconds,\n            \"hyperparameters\": hyperparameters.model_dump(),\n        },\n    )\n    if res.status_code != 200:\n        logger.warning(f\"Failed to train model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to train model: {res.status_code} {res.text}\")\n    return res.json()\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.train_info","title":"<code>train_info()</code>","text":"<p>Retrieve the current status of the model training.</p> <p>Sends a request to check the training status of the model referenced by <code>self.model_ref</code>.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[TrainingInfo]</code> <p>A dictionary containing the training status information.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request to get training status fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def train_info(self) -&gt; Optional[TrainingInfo]:\n    \"\"\"\n    Retrieve the current status of the model training.\n\n    Sends a request to check the training status of the model referenced by `self.model_ref`.\n\n    Returns:\n        dict: A dictionary containing the training status information.\n\n    Raises:\n        ValueError: If the request to get training status fails.\n    \"\"\"\n    res = self.http_client.get(f\"models/{self.model_ref}/train/status\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get train status: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get train status: {res.status_code} {res.text}\")\n    return TrainingInfo(**res.json())\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.train_logs","title":"<code>train_logs()</code>","text":"<p>Retrieve the training logs for the model.</p> <p>This method sends a request to fetch the logs of the model's training process. If the request is successful (status code 200), it returns the logs as a list of strings. If the request fails, it logs a warning and returns an empty list.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of training logs as strings.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns an empty list if the request fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def train_logs(self) -&gt; list[str]:\n    \"\"\"\n    Retrieve the training logs for the model.\n\n    This method sends a request to fetch the logs of the model's training process. If the request\n    is successful (status code 200), it returns the logs as a list of strings. If the request fails,\n    it logs a warning and returns an empty list.\n\n    Returns:\n        list[str]: A list of training logs as strings.\n\n    Raises:\n        None: Returns an empty list if the request fails.\n    \"\"\"\n    res = self.http_client.get(f\"models/{self.model_ref}/train/logs\")\n    if res.status_code != 200:\n        logger.warning(f\"Failed to get train logs: {res.status_code} {res.text}\")\n        return []\n    return res.json()\n</code></pre>"},{"location":"api/runtime/","title":"runtime","text":"<p>Runtime Module for ONNX-based Models</p> <p>This module provides the necessary functionality for loading, preprocessing, running inference, and benchmarking ONNX-based models using different execution providers such as CUDA, TensorRT, OpenVINO, and CPU. It includes utility functions for image preprocessing, postprocessing, and interfacing with the ONNXRuntime library.</p> <p>Functions:</p> Name Description <code>det_postprocess</code> <p>Postprocesses detection model outputs into sv.Detections.</p> <code>semseg_postprocess</code> <p>Postprocesses semantic segmentation model outputs into sv.Detections.</p> <code>get_runtime</code> <p>Returns an ONNXRuntime instance configured for the given runtime type.</p> <p>Classes:</p> Name Description <code>ONNXRuntime</code> <p>A class that interfaces with ONNX Runtime for model inference.</p>"},{"location":"api/runtime/#focoos.runtime.ONNXRuntime","title":"<code>ONNXRuntime</code>","text":"<p>A class that interfaces with ONNX Runtime for model inference using different execution providers (CUDA, TensorRT, OpenVINO, CoreML, etc.). It manages preprocessing, inference, and postprocessing of data, as well as benchmarking the performance of the model.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>Logger for the ONNXRuntime instance.</p> <code>name</code> <code>str</code> <p>The name of the model (derived from its path).</p> <code>opts</code> <code>OnnxEngineOpts</code> <p>Options used for configuring the ONNX Runtime.</p> <code>model_metadata</code> <code>ModelMetadata</code> <p>Metadata related to the model.</p> <code>postprocess_fn</code> <code>Callable</code> <p>The function used to postprocess the model's output.</p> <code>ort_sess</code> <code>InferenceSession</code> <p>The ONNXRuntime inference session.</p> <code>dtype</code> <code>dtype</code> <p>The data type for the model input.</p> <code>binding</code> <code>Optional[str]</code> <p>The binding type for the runtime (e.g., CUDA, CPU).</p> Source code in <code>focoos/runtime.py</code> <pre><code>class ONNXRuntime:\n    \"\"\"\n    A class that interfaces with ONNX Runtime for model inference using different execution providers\n    (CUDA, TensorRT, OpenVINO, CoreML, etc.). It manages preprocessing, inference, and postprocessing\n    of data, as well as benchmarking the performance of the model.\n\n    Attributes:\n        logger (Logger): Logger for the ONNXRuntime instance.\n        name (str): The name of the model (derived from its path).\n        opts (OnnxEngineOpts): Options used for configuring the ONNX Runtime.\n        model_metadata (ModelMetadata): Metadata related to the model.\n        postprocess_fn (Callable): The function used to postprocess the model's output.\n        ort_sess (InferenceSession): The ONNXRuntime inference session.\n        dtype (np.dtype): The data type for the model input.\n        binding (Optional[str]): The binding type for the runtime (e.g., CUDA, CPU).\n    \"\"\"\n\n    def __init__(self, model_path: str, opts: OnnxEngineOpts, model_metadata: ModelMetadata):\n        \"\"\"\n        Initializes the ONNXRuntime instance with the specified model and configuration options.\n\n        Args:\n            model_path (str): Path to the ONNX model file.\n            opts (OnnxEngineOpts): The configuration options for ONNX Runtime.\n            model_metadata (ModelMetadata): Metadata for the model (e.g., task type).\n        \"\"\"\n        self.logger = get_logger()\n        self.logger.debug(f\"[onnxruntime device] {ort.get_device()}\")\n        self.logger.debug(f\"[onnxruntime available providers] {ort.get_available_providers()}\")\n        self.name = Path(model_path).stem\n        self.opts = opts\n        self.model_metadata = model_metadata\n        self.postprocess_fn = det_postprocess if model_metadata.task == FocoosTask.DETECTION else semseg_postprocess\n        options = ort.SessionOptions()\n        if opts.verbose:\n            options.log_severity_level = 0\n        options.enable_profiling = opts.verbose\n        # options.intra_op_num_threads = 1\n        available_providers = ort.get_available_providers()\n        if opts.cuda and \"CUDAExecutionProvider\" not in available_providers:\n            self.logger.warning(\"CUDA ExecutionProvider not found.\")\n        if opts.trt and \"TensorrtExecutionProvider\" not in available_providers:\n            self.logger.warning(\"Tensorrt ExecutionProvider not found.\")\n        if opts.vino and \"OpenVINOExecutionProvider\" not in available_providers:\n            self.logger.warning(\"OpenVINO ExecutionProvider not found.\")\n        if opts.coreml and \"CoreMLExecutionProvider\" not in available_providers:\n            self.logger.warning(\"CoreML ExecutionProvider not found.\")\n        # Set providers\n        providers = []\n        dtype = np.float32\n        binding = None\n        if opts.trt and \"TensorrtExecutionProvider\" in available_providers:\n            providers.append(\n                (\n                    \"TensorrtExecutionProvider\",\n                    {\n                        \"device_id\": 0,\n                        # 'trt_max_workspace_size': 1073741824,  # 1 GB\n                        \"trt_fp16_enable\": opts.fp16,\n                        \"trt_force_sequential_engine_build\": False,\n                    },\n                )\n            )\n            dtype = np.float32\n        elif opts.vino and \"OpenVINOExecutionProvider\" in available_providers:\n            providers.append(\n                (\n                    \"OpenVINOExecutionProvider\",\n                    {\n                        \"device_type\": \"MYRIAD_FP16\",\n                        \"enable_vpu_fast_compile\": True,\n                        \"num_of_threads\": 1,\n                    },\n                    # 'use_compiled_network': False}\n                )\n            )\n            options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n            dtype = np.float32\n            binding = None\n        elif opts.cuda and \"CUDAExecutionProvider\" in available_providers:\n            binding = \"cuda\"\n            options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n            providers.append(\n                (\n                    \"CUDAExecutionProvider\",\n                    {\n                        \"device_id\": GPU_ID,\n                        \"arena_extend_strategy\": \"kSameAsRequested\",\n                        \"gpu_mem_limit\": 16 * 1024 * 1024 * 1024,\n                        \"cudnn_conv_algo_search\": \"EXHAUSTIVE\",\n                        \"do_copy_in_default_stream\": True,\n                    },\n                )\n            )\n        elif opts.coreml and \"CoreMLExecutionProvider\" in available_providers:\n            #     # options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n            providers.append(\"CoreMLExecutionProvider\")\n        else:\n            binding = None\n\n        binding = None  # TODO: remove this\n        providers.append(\"CPUExecutionProvider\")\n        self.dtype = dtype\n        self.binding = binding\n        self.ort_sess = ort.InferenceSession(model_path, options, providers=providers)\n        self.active_providers = self.ort_sess.get_providers()\n        self.logger.info(f\"[onnxruntime] Active providers:{self.ort_sess.get_providers()}\")\n        if self.ort_sess.get_inputs()[0].type == \"tensor(uint8)\":\n            self.dtype = np.uint8\n        else:\n            self.dtype = np.float32\n        if self.opts.warmup_iter &gt; 0:\n            self.logger.info(\"\u23f1\ufe0f [onnxruntime] Warming up model ..\")\n            for _ in range(self.opts.warmup_iter):\n                np_image = np.random.rand(1, 3, 640, 640).astype(self.dtype)\n                input_name = self.ort_sess.get_inputs()[0].name\n                out_name = [output.name for output in self.ort_sess.get_outputs()]\n                if self.binding is not None:\n                    io_binding = self.ort_sess.io_binding()\n                    io_binding.bind_input(\n                        input_name,\n                        self.binding,\n                        device_id=GPU_ID,\n                        element_type=self.dtype,\n                        shape=np_image.shape,\n                        buffer_ptr=np_image.ctypes.data,\n                    )\n                    io_binding.bind_cpu_input(input_name, np_image)\n                    io_binding.bind_output(out_name[0], self.binding)\n                    self.ort_sess.run_with_iobinding(io_binding)\n                    io_binding.copy_outputs_to_cpu()\n                else:\n                    self.ort_sess.run(out_name, {input_name: np_image})\n\n            self.logger.info(f\"\u23f1\ufe0f [onnxruntime] {self.name} WARMUP DONE\")\n\n    def __call__(self, im: np.ndarray, conf_threshold: float) -&gt; sv.Detections:\n        \"\"\"\n        Runs inference on the provided input image and returns the model's detections.\n\n        Args:\n            im (np.ndarray): The preprocessed input image.\n            conf_threshold (float): The confidence threshold for filtering results.\n\n        Returns:\n            sv.Detections: A sv.Detections object containing the model's output detections.\n        \"\"\"\n        out_name = None\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n        if self.binding is not None:\n            self.logger.info(f\"binding {self.binding}\")\n            io_binding = self.ort_sess.io_binding()\n\n            io_binding.bind_input(\n                input_name,\n                self.binding,\n                device_id=GPU_ID,\n                element_type=self.dtype,\n                shape=im.shape,\n                buffer_ptr=im.ctypes.data,\n            )\n\n            io_binding.bind_cpu_input(input_name, im)\n            io_binding.bind_output(out_name[0], self.binding)\n            self.ort_sess.run_with_iobinding(io_binding)\n            out = io_binding.copy_outputs_to_cpu()\n        else:\n            out = self.ort_sess.run(out_name, {input_name: im})\n\n        detections = self.postprocess_fn(out, (im.shape[2], im.shape[3]), conf_threshold)\n        return detections\n\n    def benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmarks the model by running multiple inference iterations and measuring the latency.\n\n        Args:\n            iterations (int, optional): Number of iterations to run for benchmarking. Defaults to 20.\n            size (int, optional): The input image size for benchmarking. Defaults to 640.\n\n        Returns:\n            LatencyMetrics: The latency metrics (e.g., FPS, mean, min, max, and standard deviation).\n        \"\"\"\n        self.logger.info(\"\u23f1\ufe0f [onnxruntime] Benchmarking latency..\")\n        size = size if isinstance(size, (tuple, list)) else (size, size)\n\n        durations = []\n        np_input = (255 * np.random.random((1, 3, size[0], size[1]))).astype(self.dtype)\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = self.ort_sess.get_outputs()[0].name\n        if self.binding:\n            io_binding = self.ort_sess.io_binding()\n\n            io_binding.bind_input(\n                input_name,\n                \"cuda\",\n                device_id=0,\n                element_type=self.dtype,\n                shape=np_input.shape,\n                buffer_ptr=np_input.ctypes.data,\n            )\n\n            io_binding.bind_cpu_input(input_name, np_input)\n            io_binding.bind_output(out_name, \"cuda\")\n        else:\n            out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n        for step in range(iterations + 5):\n            if self.binding:\n                start = perf_counter()\n                self.ort_sess.run_with_iobinding(io_binding)\n                end = perf_counter()\n            else:\n                start = perf_counter()\n                self.ort_sess.run(out_name, {input_name: np_input})\n                end = perf_counter()\n\n            if step &gt;= 5:\n                durations.append((end - start) * 1000)\n        durations = np.array(durations)\n        provider = self.active_providers[0]\n        if provider in [\"CUDAExecutionProvider\", \"TensorrtExecutionProvider\"]:\n            device = get_gpu_name()\n        else:\n            device = get_cpu_name()\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean()),\n            engine=f\"onnx.{provider}\",\n            mean=round(durations.mean(), 3),\n            max=round(durations.max(), 3),\n            min=round(durations.min(), 3),\n            std=round(durations.std(), 3),\n            im_size=size[0],\n            device=str(device),\n        )\n        self.logger.info(f\"\ud83d\udd25 FPS: {metrics.fps}\")\n        return metrics\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.ONNXRuntime.__call__","title":"<code>__call__(im, conf_threshold)</code>","text":"<p>Runs inference on the provided input image and returns the model's detections.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>The preprocessed input image.</p> required <code>conf_threshold</code> <code>float</code> <p>The confidence threshold for filtering results.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>sv.Detections: A sv.Detections object containing the model's output detections.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def __call__(self, im: np.ndarray, conf_threshold: float) -&gt; sv.Detections:\n    \"\"\"\n    Runs inference on the provided input image and returns the model's detections.\n\n    Args:\n        im (np.ndarray): The preprocessed input image.\n        conf_threshold (float): The confidence threshold for filtering results.\n\n    Returns:\n        sv.Detections: A sv.Detections object containing the model's output detections.\n    \"\"\"\n    out_name = None\n    input_name = self.ort_sess.get_inputs()[0].name\n    out_name = [output.name for output in self.ort_sess.get_outputs()]\n    if self.binding is not None:\n        self.logger.info(f\"binding {self.binding}\")\n        io_binding = self.ort_sess.io_binding()\n\n        io_binding.bind_input(\n            input_name,\n            self.binding,\n            device_id=GPU_ID,\n            element_type=self.dtype,\n            shape=im.shape,\n            buffer_ptr=im.ctypes.data,\n        )\n\n        io_binding.bind_cpu_input(input_name, im)\n        io_binding.bind_output(out_name[0], self.binding)\n        self.ort_sess.run_with_iobinding(io_binding)\n        out = io_binding.copy_outputs_to_cpu()\n    else:\n        out = self.ort_sess.run(out_name, {input_name: im})\n\n    detections = self.postprocess_fn(out, (im.shape[2], im.shape[3]), conf_threshold)\n    return detections\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.ONNXRuntime.__init__","title":"<code>__init__(model_path, opts, model_metadata)</code>","text":"<p>Initializes the ONNXRuntime instance with the specified model and configuration options.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the ONNX model file.</p> required <code>opts</code> <code>OnnxEngineOpts</code> <p>The configuration options for ONNX Runtime.</p> required <code>model_metadata</code> <code>ModelMetadata</code> <p>Metadata for the model (e.g., task type).</p> required Source code in <code>focoos/runtime.py</code> <pre><code>def __init__(self, model_path: str, opts: OnnxEngineOpts, model_metadata: ModelMetadata):\n    \"\"\"\n    Initializes the ONNXRuntime instance with the specified model and configuration options.\n\n    Args:\n        model_path (str): Path to the ONNX model file.\n        opts (OnnxEngineOpts): The configuration options for ONNX Runtime.\n        model_metadata (ModelMetadata): Metadata for the model (e.g., task type).\n    \"\"\"\n    self.logger = get_logger()\n    self.logger.debug(f\"[onnxruntime device] {ort.get_device()}\")\n    self.logger.debug(f\"[onnxruntime available providers] {ort.get_available_providers()}\")\n    self.name = Path(model_path).stem\n    self.opts = opts\n    self.model_metadata = model_metadata\n    self.postprocess_fn = det_postprocess if model_metadata.task == FocoosTask.DETECTION else semseg_postprocess\n    options = ort.SessionOptions()\n    if opts.verbose:\n        options.log_severity_level = 0\n    options.enable_profiling = opts.verbose\n    # options.intra_op_num_threads = 1\n    available_providers = ort.get_available_providers()\n    if opts.cuda and \"CUDAExecutionProvider\" not in available_providers:\n        self.logger.warning(\"CUDA ExecutionProvider not found.\")\n    if opts.trt and \"TensorrtExecutionProvider\" not in available_providers:\n        self.logger.warning(\"Tensorrt ExecutionProvider not found.\")\n    if opts.vino and \"OpenVINOExecutionProvider\" not in available_providers:\n        self.logger.warning(\"OpenVINO ExecutionProvider not found.\")\n    if opts.coreml and \"CoreMLExecutionProvider\" not in available_providers:\n        self.logger.warning(\"CoreML ExecutionProvider not found.\")\n    # Set providers\n    providers = []\n    dtype = np.float32\n    binding = None\n    if opts.trt and \"TensorrtExecutionProvider\" in available_providers:\n        providers.append(\n            (\n                \"TensorrtExecutionProvider\",\n                {\n                    \"device_id\": 0,\n                    # 'trt_max_workspace_size': 1073741824,  # 1 GB\n                    \"trt_fp16_enable\": opts.fp16,\n                    \"trt_force_sequential_engine_build\": False,\n                },\n            )\n        )\n        dtype = np.float32\n    elif opts.vino and \"OpenVINOExecutionProvider\" in available_providers:\n        providers.append(\n            (\n                \"OpenVINOExecutionProvider\",\n                {\n                    \"device_type\": \"MYRIAD_FP16\",\n                    \"enable_vpu_fast_compile\": True,\n                    \"num_of_threads\": 1,\n                },\n                # 'use_compiled_network': False}\n            )\n        )\n        options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n        dtype = np.float32\n        binding = None\n    elif opts.cuda and \"CUDAExecutionProvider\" in available_providers:\n        binding = \"cuda\"\n        options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n        providers.append(\n            (\n                \"CUDAExecutionProvider\",\n                {\n                    \"device_id\": GPU_ID,\n                    \"arena_extend_strategy\": \"kSameAsRequested\",\n                    \"gpu_mem_limit\": 16 * 1024 * 1024 * 1024,\n                    \"cudnn_conv_algo_search\": \"EXHAUSTIVE\",\n                    \"do_copy_in_default_stream\": True,\n                },\n            )\n        )\n    elif opts.coreml and \"CoreMLExecutionProvider\" in available_providers:\n        #     # options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n        providers.append(\"CoreMLExecutionProvider\")\n    else:\n        binding = None\n\n    binding = None  # TODO: remove this\n    providers.append(\"CPUExecutionProvider\")\n    self.dtype = dtype\n    self.binding = binding\n    self.ort_sess = ort.InferenceSession(model_path, options, providers=providers)\n    self.active_providers = self.ort_sess.get_providers()\n    self.logger.info(f\"[onnxruntime] Active providers:{self.ort_sess.get_providers()}\")\n    if self.ort_sess.get_inputs()[0].type == \"tensor(uint8)\":\n        self.dtype = np.uint8\n    else:\n        self.dtype = np.float32\n    if self.opts.warmup_iter &gt; 0:\n        self.logger.info(\"\u23f1\ufe0f [onnxruntime] Warming up model ..\")\n        for _ in range(self.opts.warmup_iter):\n            np_image = np.random.rand(1, 3, 640, 640).astype(self.dtype)\n            input_name = self.ort_sess.get_inputs()[0].name\n            out_name = [output.name for output in self.ort_sess.get_outputs()]\n            if self.binding is not None:\n                io_binding = self.ort_sess.io_binding()\n                io_binding.bind_input(\n                    input_name,\n                    self.binding,\n                    device_id=GPU_ID,\n                    element_type=self.dtype,\n                    shape=np_image.shape,\n                    buffer_ptr=np_image.ctypes.data,\n                )\n                io_binding.bind_cpu_input(input_name, np_image)\n                io_binding.bind_output(out_name[0], self.binding)\n                self.ort_sess.run_with_iobinding(io_binding)\n                io_binding.copy_outputs_to_cpu()\n            else:\n                self.ort_sess.run(out_name, {input_name: np_image})\n\n        self.logger.info(f\"\u23f1\ufe0f [onnxruntime] {self.name} WARMUP DONE\")\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.ONNXRuntime.benchmark","title":"<code>benchmark(iterations=20, size=640)</code>","text":"<p>Benchmarks the model by running multiple inference iterations and measuring the latency.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of iterations to run for benchmarking. Defaults to 20.</p> <code>20</code> <code>size</code> <code>int</code> <p>The input image size for benchmarking. Defaults to 640.</p> <code>640</code> <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>The latency metrics (e.g., FPS, mean, min, max, and standard deviation).</p> Source code in <code>focoos/runtime.py</code> <pre><code>def benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmarks the model by running multiple inference iterations and measuring the latency.\n\n    Args:\n        iterations (int, optional): Number of iterations to run for benchmarking. Defaults to 20.\n        size (int, optional): The input image size for benchmarking. Defaults to 640.\n\n    Returns:\n        LatencyMetrics: The latency metrics (e.g., FPS, mean, min, max, and standard deviation).\n    \"\"\"\n    self.logger.info(\"\u23f1\ufe0f [onnxruntime] Benchmarking latency..\")\n    size = size if isinstance(size, (tuple, list)) else (size, size)\n\n    durations = []\n    np_input = (255 * np.random.random((1, 3, size[0], size[1]))).astype(self.dtype)\n    input_name = self.ort_sess.get_inputs()[0].name\n    out_name = self.ort_sess.get_outputs()[0].name\n    if self.binding:\n        io_binding = self.ort_sess.io_binding()\n\n        io_binding.bind_input(\n            input_name,\n            \"cuda\",\n            device_id=0,\n            element_type=self.dtype,\n            shape=np_input.shape,\n            buffer_ptr=np_input.ctypes.data,\n        )\n\n        io_binding.bind_cpu_input(input_name, np_input)\n        io_binding.bind_output(out_name, \"cuda\")\n    else:\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n    for step in range(iterations + 5):\n        if self.binding:\n            start = perf_counter()\n            self.ort_sess.run_with_iobinding(io_binding)\n            end = perf_counter()\n        else:\n            start = perf_counter()\n            self.ort_sess.run(out_name, {input_name: np_input})\n            end = perf_counter()\n\n        if step &gt;= 5:\n            durations.append((end - start) * 1000)\n    durations = np.array(durations)\n    provider = self.active_providers[0]\n    if provider in [\"CUDAExecutionProvider\", \"TensorrtExecutionProvider\"]:\n        device = get_gpu_name()\n    else:\n        device = get_cpu_name()\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean()),\n        engine=f\"onnx.{provider}\",\n        mean=round(durations.mean(), 3),\n        max=round(durations.max(), 3),\n        min=round(durations.min(), 3),\n        std=round(durations.std(), 3),\n        im_size=size[0],\n        device=str(device),\n    )\n    self.logger.info(f\"\ud83d\udd25 FPS: {metrics.fps}\")\n    return metrics\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.det_postprocess","title":"<code>det_postprocess(out, im0_shape, conf_threshold)</code>","text":"<p>Postprocesses the output of an object detection model and filters detections based on a confidence threshold.</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>List[ndarray]</code> <p>The output of the detection model.</p> required <code>im0_shape</code> <code>Tuple[int, int]</code> <p>The original shape of the input image (height, width).</p> required <code>conf_threshold</code> <code>float</code> <p>The confidence threshold for filtering detections.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>sv.Detections: A sv.Detections object containing the filtered bounding boxes, class ids, and confidences.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def det_postprocess(out: List[np.ndarray], im0_shape: Tuple[int, int], conf_threshold: float) -&gt; sv.Detections:\n    \"\"\"\n    Postprocesses the output of an object detection model and filters detections\n    based on a confidence threshold.\n\n    Args:\n        out (List[np.ndarray]): The output of the detection model.\n        im0_shape (Tuple[int, int]): The original shape of the input image (height, width).\n        conf_threshold (float): The confidence threshold for filtering detections.\n\n    Returns:\n        sv.Detections: A sv.Detections object containing the filtered bounding boxes, class ids, and confidences.\n    \"\"\"\n    cls_ids, boxes, confs = out\n    boxes[:, 0::2] *= im0_shape[1]\n    boxes[:, 1::2] *= im0_shape[0]\n    high_conf_indices = (confs &gt; conf_threshold).nonzero()\n\n    return sv.Detections(\n        xyxy=boxes[high_conf_indices].astype(int),\n        class_id=cls_ids[high_conf_indices].astype(int),\n        confidence=confs[high_conf_indices].astype(float),\n    )\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.get_runtime","title":"<code>get_runtime(runtime_type, model_path, model_metadata, warmup_iter=0)</code>","text":"<p>Creates and returns an ONNXRuntime instance based on the specified runtime type and model path, with options for various execution providers (CUDA, TensorRT, CPU, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>runtime_type</code> <code>RuntimeTypes</code> <p>The type of runtime to use (e.g., ONNX_CUDA32, ONNX_TRT32).</p> required <code>model_path</code> <code>str</code> <p>The path to the ONNX model.</p> required <code>model_metadata</code> <code>ModelMetadata</code> <p>Metadata describing the model.</p> required <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations before benchmarking. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>ONNXRuntime</code> <code>ONNXRuntime</code> <p>A fully configured ONNXRuntime instance.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def get_runtime(\n    runtime_type: RuntimeTypes,\n    model_path: str,\n    model_metadata: ModelMetadata,\n    warmup_iter: int = 0,\n) -&gt; ONNXRuntime:\n    \"\"\"\n    Creates and returns an ONNXRuntime instance based on the specified runtime type\n    and model path, with options for various execution providers (CUDA, TensorRT, CPU, etc.).\n\n    Args:\n        runtime_type (RuntimeTypes): The type of runtime to use (e.g., ONNX_CUDA32, ONNX_TRT32).\n        model_path (str): The path to the ONNX model.\n        model_metadata (ModelMetadata): Metadata describing the model.\n        warmup_iter (int, optional): Number of warmup iterations before benchmarking. Defaults to 0.\n\n    Returns:\n        ONNXRuntime: A fully configured ONNXRuntime instance.\n    \"\"\"\n    opts = OnnxEngineOpts(\n        cuda=runtime_type == RuntimeTypes.ONNX_CUDA32,\n        trt=runtime_type in [RuntimeTypes.ONNX_TRT32, RuntimeTypes.ONNX_TRT16],\n        fp16=runtime_type == RuntimeTypes.ONNX_TRT16,\n        warmup_iter=warmup_iter,\n        coreml=runtime_type == RuntimeTypes.ONNX_COREML,\n        verbose=False,\n    )\n    return ONNXRuntime(model_path, opts, model_metadata)\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.semseg_postprocess","title":"<code>semseg_postprocess(out, im0_shape, conf_threshold)</code>","text":"<p>Postprocesses the output of a semantic segmentation model and filters based on a confidence threshold.</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>List[ndarray]</code> <p>The output of the semantic segmentation model.</p> required <code>im0_shape</code> <code>Tuple[int, int]</code> <p>The original shape of the input image (height, width).</p> required <code>conf_threshold</code> <code>float</code> <p>The confidence threshold for filtering detections.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>sv.Detections: A sv.Detections object containing the masks, class ids, and confidences.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def semseg_postprocess(out: List[np.ndarray], im0_shape: Tuple[int, int], conf_threshold: float) -&gt; sv.Detections:\n    \"\"\"\n    Postprocesses the output of a semantic segmentation model and filters based\n    on a confidence threshold.\n\n    Args:\n        out (List[np.ndarray]): The output of the semantic segmentation model.\n        im0_shape (Tuple[int, int]): The original shape of the input image (height, width).\n        conf_threshold (float): The confidence threshold for filtering detections.\n\n    Returns:\n        sv.Detections: A sv.Detections object containing the masks, class ids, and confidences.\n    \"\"\"\n    cls_ids, mask, confs = out[0][0], out[1][0], out[2][0]\n    masks = np.equal(mask, np.arange(len(cls_ids))[:, None, None])\n    high_conf_indices = np.where(confs &gt; conf_threshold)[0]\n    masks = masks[high_conf_indices].astype(bool)\n    cls_ids = cls_ids[high_conf_indices].astype(int)\n    confs = confs[high_conf_indices].astype(float)\n    return sv.Detections(\n        mask=masks,\n        # xyxy is required from supervision\n        xyxy=np.zeros(shape=(len(high_conf_indices), 4), dtype=np.uint8),\n        class_id=cls_ids,\n        confidence=confs,\n    )\n</code></pre>"},{"location":"development/changelog/","title":"Changelog","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"development/code_of_conduct/","title":"Code of Conduct","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"development/contributing/","title":"Contributing","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>The focoos SDK provides flexibility for installation based on the execution environment you plan to use. The package supports <code>CPU</code>, <code>NVIDIA GPU</code>, and <code>NVIDIA GPU with TensorRT</code> environments. Please note that only one execution environment should be selected during installation.</p>"},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<p>For local inference, ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1.</p> <p>To install cuDNN 9:</p> <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> <p>To perform inference using TensorRT, ensure you have TensorRT version 10.5 installed.</p>"},{"location":"getting_started/installation/#installation-options","title":"Installation Options","text":"<ul> <li>CPU Environment</li> </ul> <p>If you plan to run the SDK on a CPU-only environment:</p> <pre><code>pip install 'focoos[cpu] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <ul> <li>NVIDIA GPU Environment</li> </ul> <p>For execution using NVIDIA GPUs (with ONNX Runtime GPU support):</p> <pre><code>pip install 'focoos[gpu] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <ul> <li>NVIDIA GPU with TensorRT</li> </ul> <p>For optimized execution using NVIDIA GPUs with TensorRT:</p> <pre><code>pip install 'focoos[tensorrt] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <p>Note</p> <p>\ud83d\udee0\ufe0f Installation Tip: If you want to install a specific version, for example <code>v0.1.3</code>, use: <pre><code>pip install 'focoos[tensorrt] @ git+https://github.com/FocoosAI/focoos.git@v0.1.3'\n</code></pre> \ud83d\udccb Check Versions: Visit https://github.com/FocoosAI/focoos/tags for available versions.</p>"},{"location":"getting_started/introduction/","title":"Focoos Python SDK \ud83d\udce6","text":"<p>Unlock the full potential of Focoos AI with the Focoos Python SDK! \ud83d\ude80 This powerful SDK gives you seamless access to our cutting-edge computer vision models and tools, allowing you to effortlessly interact with the Focoos API. With just a few lines of code, you can easily select, customize, test, and deploy pre-trained models tailored to your specific needs. Whether you're deploying in the cloud or on edge devices, the Focoos Python SDK integrates smoothly into your workflow, speeding up your development process.</p> <p>Ready to dive in? Get started with the setup in just a few simple steps!</p> <p>\ud83d\ude80 Install the Focoos Python SDK</p>"},{"location":"getting_started/quickstart/","title":"Quickstart \ud83d\ude80","text":"<p>Getting started with Focoos AI has never been easier! In just a few steps, you can quickly set up remote inference using our built-in models. Here's a simple example of how to perform object detection with the focoos_object365 model:</p>"},{"location":"getting_started/quickstart/#step-1-install-the-sdk","title":"Step 1: Install the SDK","text":"<p>First, make sure you've installed the Focoos Python SDK by following the installation guide.</p>"},{"location":"getting_started/quickstart/#step-2-set-up-remote-inference","title":"Step 2: Set Up Remote Inference","text":"<p>With the SDK installed, you can start using the Focoos API to run inference remotely. Here's a basic code snippet to detect objects in an image using a pre-trained model:</p> <pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (focoos_object365) from Focoos API\nmodel = focoos.get_remote_model(\"focoos_object365\")\n\n# Run inference on an image\ndetections = model.infer(\"./image.jpg\", threshold=0.4)\n\n# Output the detections\nprint(detections)\n</code></pre>"},{"location":"helpers/wip/","title":"Wip","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"how_to/cloud_training/","title":"Cloud Training","text":"<p>This section covers the steps to train a model in the cloud using the <code>focoos</code> library. The following example demonstrates how to interact with the Focoos API to manage models, datasets, and training jobs.</p>"},{"location":"how_to/cloud_training/#listing-available-datasets","title":"Listing Available Datasets","text":"<p>Before training a model, you can list all available shared datasets:</p> <pre><code>from pprint import pprint\nimport os\nfrom focoos import Focoos\n\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\ndatasets = focoos.list_shared_datasets()\npprint(datasets)\n</code></pre>"},{"location":"how_to/cloud_training/#initiating-a-cloud-training-job","title":"Initiating a Cloud Training Job","text":"<p>To start training, configure the model, dataset, and training parameters as shown below:</p> <pre><code>from focoos.ports import Hyperparameters, TrainInstance\n\nmodel = focoos.get_remote_model(\"&lt;YOUR-MODEL-ID&gt;\")\n\nres = model.train(\n    dataset_ref=\"&lt;YOUR-DATASET-ID&gt;\",\n    instance_type=TrainInstance.ML_G4DN_XLARGE,\n    volume_size=50,\n    max_runtime_in_seconds=36000,\n    hyperparameters=Hyperparameters(\n        learning_rate=0.0001,\n        batch_size=16,\n        max_iters=1500,\n        eval_period=100,\n        resolution=640,\n    ),  # type: ignore\n)\n</code></pre>"},{"location":"how_to/cloud_training/#monitoring-training-progress-on-jupyter-notebook","title":"Monitoring Training Progress on jupyter notebook","text":"<p>Once the training job is initiated, monitor its progress by polling the training status. Use the following code:</p> <pre><code>from focoos import  Focoos\n\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\nmodel = focoos.get_remote_model(\"&lt;YOUR-MODEL-ID&gt;\")\nmodel.notebook_monitor_train(interval=30, plot_metrics=True)\n</code></pre>"},{"location":"how_to/cloud_training/#retrieving-training-logs","title":"Retrieving Training Logs","text":"<p>After the training process is complete, retrieve the logs for detailed insights:</p> <pre><code>logs = model.train_logs()\npprint(logs)\n</code></pre>"},{"location":"how_to/cloud_training/#retrieve-and-visualize-training-metrics","title":"Retrieve and Visualize Training Metrics","text":"<pre><code>from focoos.utils.metrics import MetricsVisualizer\n\nmetrics = model.metrics()\nvisualizer = MetricsVisualizer(metrics)\nvisualizer.log_metrics()\n</code></pre>"},{"location":"how_to/inference/","title":"Inferece","text":"<p>This section covers how to perform inference using the <code>focoos</code> library. You can deploy models to the cloud for predictions, integrate with Gradio for interactive demos, or run inference locally.</p>"},{"location":"how_to/inference/#cloud-inference","title":"\ud83e\udd16 Cloud Inference","text":"<pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\nmodel = focoos.get_remote_model(\"focoos_object365\")\ndetections = model.infer(\"./image.jpg\", threshold=0.4)\n</code></pre>"},{"location":"how_to/inference/#cloud-inference-with-gradio","title":"\ud83e\udd16 Cloud Inference with Gradio","text":"<p>setup <code>FOCOOS_API_KEY_GRADIO</code> environment variable with your Focoos API key</p> <pre><code>pip install '.[gradio]'\n</code></pre> <pre><code>python gradio/app.py\n</code></pre>"},{"location":"how_to/inference/#local-inference","title":"\ud83e\udd16 Local Inference","text":"<pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\nmodel = focoos.get_local_model(\"focoos_object365\")\n\ndetections = model.infer(\"./image.jpg\", threshold=0.4)\n</code></pre>"}]}