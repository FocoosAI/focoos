{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Focoos AI SDK","text":"<p>Focoos AI provides a powerful development platform designed to help developers and businesses deploy high-performance, cost-efficient computer vision models. Whether you're processing images in the cloud, running inference on edge devices, or training custom models on your dataset, the Focoos AI SDK makes it seamless.</p>"},{"location":"#why-choose-focoos-ai","title":"Why choose Focoos AI?","text":"<ul> <li>\ud83d\udd39 Blazing Fast Inference. Up to 10x faster than traditional methods.</li> <li>\ud83d\udcb0 Optimized Cost. Requires 4x less computation, reducing cloud and hardware expenses.</li> <li>\u26a1 Quick Deployment. Deploy and fine-tune models with minimal effort.</li> </ul>"},{"location":"#sdk-overview","title":"SDK Overview","text":"<p>The Focoos Python SDK provides seamless access to our state-of-the-art computer vision models. With just a few lines of code, you can easily select, customize, test, and deploy pre-trained models tailored to your specific needs. Whether you're deploying in the cloud or on edge devices, the Focoos Python SDK integrates smoothly into your workflow, speeding up your development process.</p>"},{"location":"#quickstart","title":"Quickstart \ud83d\ude80","text":"<p>Ready to dive in? Get started with the setup in just a few simple steps!</p> <p>Install the Focoos Python SDK (for more options, see setup)</p> uvpipconda <pre><code>uv pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <p>\u2699\ufe0f Customize the models to your specific needs by fine-tuning on your own dataset. <pre><code>from focoos import Focoos\nfrom focoos.ports import Hyperparameters\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\nmodel = focoos.new_model(name=\"awesome\",\n                         focoos_model=\"fai-rtdetr-m-obj365\",\n                         description=\"An awesome model\")\n\nres = model.train(\n    dataset_ref=\"&lt;YOUR-DATASET-ID&gt;\",\n    hyperparameters=Hyperparameters(\n        learning_rate=0.0001,\n        batch_size=16,\n        max_iters=1500,\n    )\n)\n</code></pre></p> <p>\ud83d\ude80 Use the model with just few lines of code.</p> <pre><code>from focoos import Focoos\nfrom PIL import Image\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n# Get the remote model from Focoos API\nmodel = focoos.get_remote_model(\"&lt;MODEL-REF&gt;\")\n\n# Run inference on an image\ndetections, preview = model.infer(image_path, threshold=0.5, annotate=True)\n\n# Output the detections\nImage.fromarray(preview)\n</code></pre>"},{"location":"#our-models","title":"Our Models \ud83e\udde0","text":"<p>Focoos AI offers the best models in object detection, semantic and instance segmentation, and more is coming soon.</p> <p>Using Focoos AI helps you save both time and money while delivering high-performance AI models \ud83d\udcaa:</p> <ul> <li>10x Faster \ud83d\ude80: our models are able to process images up to ten times faster than traditional methods.</li> <li>4x Cheaper \ud83d\udcb0: our models require up to 4x less computational power, letting you save on hardware or cloud bill while ensuring high-quality results.</li> <li>90% Time Saved \u23f1\ufe0f: our platform accelerates computer vision model development and deployment, enabling faster model training, seamless integration, and optimized performance with minimal effort.</li> </ul> <p>These are not empty promises, but the result of years of research and development by our team \ud83d\udd2c:</p> ADE-20k Semantic Segmentation Results COCO Object Detection Results <p>See the list of our models in the models section.</p>"},{"location":"#start-now","title":"Start now!","text":"<p>By choosing Focoos AI, you can save time, reduce costs, and achieve superior model performance, all while ensuring the privacy and efficiency of your deployments.</p> <p>Reach out to us to ask for your API key and power your computer vision projects.</p> <p>Otherwise Book A Demo now to access the platform and test by yourself Focoos AI in action.</p>"},{"location":"models/","title":"Focoos Models \ud83e\udde0","text":"<p>With the Focoos SDK, you can take advantage of a collection of foundational models that are optimized for a range of computer vision tasks. These pre-trained models, covering detection and semantic segmentation across various domains, provide an excellent starting point for your specific use case. Whether you need to fine-tune for custom requirements or adapt them to your application, these models offer a solid foundation to accelerate your development process.</p>"},{"location":"models/#semantic-segmentation","title":"Semantic Segmentation \ud83d\uddbc\ufe0f","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-m2f-l-ade Mask2Former (Resnet-101) Common Scene (150) ADE20K mIoU: 48.27mAcc: 62.15 73 fai-m2f-m-ade Mask2Former (STDC-2) Common Scene (150) ADE20K mIoU: 45.32mACC: 57.75 127 fai-m2f-s-ade Mask2Former (STDC-1) Common Scene (150) ADE20K mIoU: 41.23mAcc: 52.21 189 <p> mIoU = Intersection over Union averaged by class   mAcc = Pixel Accuracy averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"models/#object-detection","title":"Object Detection \ud83d\udd75\ufe0f\u200d\u2642\ufe0f","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-rtdetr-l-coco RT-DETR (Resnet-50) Common Objects (80) COCO bbox/AP: 53.06bbox/AP50: 70.91 87 fai-rtdetr-m-coco RT-DETR (STDC-2) Common Objects (80) COCO bbox/AP: 44.69bbox/AP50: 61.63 181 fai-rtdetr-s-coco RT-DETR (STDC-1) Common Objects (80) COCO bbox/AP: 42.58bbox/AP50: 59.22 220 fai-rtdetr-n-coco RT-DETR (STDC-1) Common Objects (80) COCO bbox/AP: 40.59bbox/AP50: 56.69 269 fai-rtdetr-m-obj365 RT-DETR (Resnet50) Common Objects (365) Objects365 bbox/AP: 34.60bbox/AP50: 45.81 87 <p> AP = Average Precision averaged by class   AP50 = Average Precision at IoU threshold 0.50 averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"models/#instance-segmentation","title":"Instance Segmentation \ud83c\udfad","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-m2f-l-coco-ins Mask2Former (Resnet-50) Common Objects (80) COCO segm/AP: 42.39segm/AP50: 66.12 54 <p> AP = Average Precision averaged by class   AP50 = Average Precision at IoU threshold 0.50 averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"setup/","title":"Python SDK Setup \ud83d\udc0d","text":"<p>Focoos models support multiple inference runtimes. To keep the library lightweight and to allow users to use their environment, optional dependencies (e.g., torch, onnxruntime, tensorrt) are not installed by default.</p> <p>Focoos is shipped with the following local inference runtimes that requires to install additional dependencies. If you intend to use only Focoos AI servers for inference, you don't need to install any of the following dependencies.</p> RuntimeType Extra Runtime Compatible Devices Available ExecutionProvider ONNX_CUDA32 <code>[cuda]</code> onnxruntime CUDA NVIDIA GPUs CUDAExecutionProvider ONNX_TRT32 <code>[tensorrt]</code> onnxruntime TRT NVIDIA GPUs (Optimized) CUDAExecutionProvider, TensorrtExecutionProvider ONNX_TRT16 <code>[tensorrt]</code> onnxruntime TRT NVIDIA GPUs (Optimized) CUDAExecutionProvider, TensorrtExecutionProvider ONNX_CPU <code>[cpu]</code> onnxruntime CPU CPU (x86, ARM), M1, M2, M3 (Apple Silicon) CPUExecutionProvider, CoreMLExecutionProvider, AzureExecutionProvider ONNX_COREML <code>[cpu]</code> onnxruntime CPU M1, M2, M3 (Apple Silicon) CoreMLExecutionProvider, CPUExecutionProvider TORCHSCRIPT_32 <code>[torch]</code> torchscript CPU, NVIDIA GPUs -"},{"location":"setup/#install-the-focoos-sdk","title":"Install the Focoos SDK","text":"<p>The Focoos SDK can be installed with different package managers using python 3.10 and above.</p> uvpipconda <p>We recommend using UV (how to install uv) as a package manager and environment manager for a streamlined dependency management experience.</p> <p>You can easily create a new virtual environment with UV using the following command: <pre><code>uv venv --python 3.12\nsource .venv/bin/activate\n</code></pre></p> Cloud RuntimeCPU ONNX RuntimeTorchscript RuntimeNVIDIA GPU ONNX RuntimeNVIDIA GPU ONNX Runtime with TensorRT <pre><code>uv pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>uv pip install 'focoos[cpu] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <p>To run the models using the torchscript runtime, you need to install the torch package. <pre><code>uv pip install 'focoos[torch] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre></p> <pre><code>uv pip install 'focoos[cuda] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> To perform inference using TensorRT, ensure you have TensorRT version 10.5 installed. <pre><code>uv pip install 'focoos[tensorrt] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Create and activate a new virtual environment using pip with the following commands: <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> Cloud RuntimeCPU ONNX RuntimeTorchscript RuntimeNVIDIA GPU ONNX RuntimeNVIDIA GPU ONNX Runtime with TensorRT <pre><code>pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos[cpu] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos[torch] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> <pre><code>pip install 'focoos[cuda] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> To perform inference using TensorRT, ensure you have TensorRT version 10.5 installed. <pre><code>pip install 'focoos[tensorrt] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Create and activate a new conda (how to install conda) environment with Python 3.10 or higher: <pre><code>conda create -n focoos python=3.12\nconda activate focoos\nconda install pip\n</code></pre></p> Cloud RuntimeCPU ONNX RuntimeTorchscript RuntimeNVIDIA GPU ONNX RuntimeNVIDIA GPU ONNX Runtime with TensorRT <pre><code>pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos[cpu] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos[torch] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> <pre><code>pip install 'focoos[cuda] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> To perform inference using TensorRT, ensure you have TensorRT version 10.5 installed. <pre><code>pip install 'focoos[tensorrt] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Note</p> <p>\ud83e\udd16 Multiple Runtimes: You can install multiple extras by running <code>pip install .[torch,cuda,tensorrt]</code>. Anyway you can't use <code>cpu</code> and <code>cuda</code> or <code>tensorrt</code> at the same time.</p> <p>Note</p> <p>\ud83d\udee0\ufe0f Installation Tip: If you want to install a specific version, for example <code>v0.1.3</code>, use: <pre><code>pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git@v0.1.3'\n</code></pre> \ud83d\udccb Check Versions: Visit https://github.com/FocoosAI/focoos/tags for available versions.</p>"},{"location":"setup/#docker-and-devcontainers","title":"Docker and Devcontainers","text":"<p>For container support, Focoos offers four different Docker images:</p> <ul> <li><code>focoos-cpu</code>: only CPU</li> <li><code>focoos-cuda</code>: Includes ONNX (CUDA) support</li> <li><code>focoos-torch</code>: Includes ONNX and Torchscript (CUDA) support</li> <li><code>focoos-tensorrt</code>: Includes ONNX, Torchscript, and TensorRT  support</li> </ul> <p>to use the docker images, you can run the following command:</p> <pre><code>docker run -it . --target=focoos-cpu\n</code></pre> <p>This repository also includes a devcontainer configuration for each of the above images. You can launch these devcontainers in Visual Studio Code for a seamless development experience.</p>"},{"location":"api/config/","title":"Config","text":"<p>Configuration module for Focoos AI SDK.</p> <p>This module defines the configuration settings for the Focoos AI SDK, including API credentials, logging levels, default endpoints, and runtime preferences.</p> <p>Classes:</p> Name Description <code>FocoosConfig</code> <p>Pydantic settings class for Focoos SDK configuration.</p> Constants <p>LogLevel: Type definition for supported logging levels. FOCOOS_CONFIG: Global configuration instance.</p>"},{"location":"api/config/#focoos.config.FocoosConfig","title":"<code>FocoosConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration settings for the Focoos AI SDK.</p> <p>This class uses pydantic_settings to manage configuration values, supporting environment variable overrides and validation. All attributes can be configured via environment variables with the same name.</p> <p>Attributes:</p> Name Type Description <code>focoos_api_key</code> <code>Optional[str]</code> <p>API key for authenticating with Focoos services. Defaults to None.</p> <code>focoos_log_level</code> <code>LogLevel</code> <p>Logging level for the SDK. Defaults to \"DEBUG\".</p> <code>default_host_url</code> <code>str</code> <p>Default API endpoint URL. Defaults to the production API URL.</p> <code>runtime_type</code> <code>RuntimeTypes</code> <p>Default runtime type for model inference. Defaults to ONNX_CUDA32 for NVIDIA GPU acceleration.</p> <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations for model initialization. Defaults to 2.</p> Example <p>Setting configuration via environment variables in bash: <pre><code># Set API key and change log level\nexport FOCOOS_API_KEY=\"your-api-key-here\"\nexport FOCOOS_LOG_LEVEL=\"INFO\"\nexport DEFAULT_HOST_URL=\"https://custom-api-endpoint.com\"\nexport RUNTIME_TYPE=\"ONNX_CUDA32\"\nexport WARMUP_ITER=\"3\"\n\n# Then run your Python application\npython your_app.py\n</code></pre></p> Source code in <code>focoos/config.py</code> <pre><code>class FocoosConfig(BaseSettings):\n    \"\"\"\n    Configuration settings for the Focoos AI SDK.\n\n    This class uses pydantic_settings to manage configuration values,\n    supporting environment variable overrides and validation. All attributes\n    can be configured via environment variables with the same name.\n\n    Attributes:\n        focoos_api_key (Optional[str]): API key for authenticating with Focoos services.\n            Defaults to None.\n        focoos_log_level (LogLevel): Logging level for the SDK.\n            Defaults to \"DEBUG\".\n        default_host_url (str): Default API endpoint URL.\n            Defaults to the production API URL.\n        runtime_type (RuntimeTypes): Default runtime type for model inference.\n            Defaults to ONNX_CUDA32 for NVIDIA GPU acceleration.\n        warmup_iter (int): Number of warmup iterations for model initialization.\n            Defaults to 2.\n\n    Example:\n        Setting configuration via environment variables in bash:\n        ```bash\n        # Set API key and change log level\n        export FOCOOS_API_KEY=\"your-api-key-here\"\n        export FOCOOS_LOG_LEVEL=\"INFO\"\n        export DEFAULT_HOST_URL=\"https://custom-api-endpoint.com\"\n        export RUNTIME_TYPE=\"ONNX_CUDA32\"\n        export WARMUP_ITER=\"3\"\n\n        # Then run your Python application\n        python your_app.py\n        ```\n    \"\"\"\n\n    focoos_api_key: Optional[str] = None\n    focoos_log_level: LogLevel = \"DEBUG\"\n    default_host_url: str = PROD_API_URL\n    runtime_type: RuntimeTypes = RuntimeTypes.ONNX_CUDA32\n    warmup_iter: int = 2\n</code></pre>"},{"location":"api/focoos/","title":"Focoos","text":"<p>Focoos Module</p> <p>This module provides a Python interface for interacting with Focoos APIs, allowing users to manage machine learning models and datasets in the Focoos ecosystem. The module supports operations such as retrieving model metadata, downloading models, and listing shared datasets.</p> <p>Classes:</p> Name Description <code>Focoos</code> <p>Main class to interface with Focoos APIs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised for invalid API responses or missing parameters.</p>"},{"location":"api/focoos/#focoos.focoos.Focoos","title":"<code>Focoos</code>","text":"<p>Main class to interface with Focoos APIs.</p> <p>This class provides methods to interact with Focoos-hosted models and datasets. It supports functionalities such as listing models, retrieving model metadata, downloading models, and creating new models.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key for authentication.</p> <code>api_client</code> <code>ApiClient</code> <p>HTTP client for making API requests.</p> <code>user_info</code> <code>dict</code> <p>Information about the currently authenticated user.</p> <code>cache_dir</code> <code>str</code> <p>Local directory for caching downloaded models.</p> Source code in <code>focoos/focoos.py</code> <pre><code>class Focoos:\n    \"\"\"\n    Main class to interface with Focoos APIs.\n\n    This class provides methods to interact with Focoos-hosted models and datasets.\n    It supports functionalities such as listing models, retrieving model metadata,\n    downloading models, and creating new models.\n\n    Attributes:\n        api_key (str): The API key for authentication.\n        api_client (ApiClient): HTTP client for making API requests.\n        user_info (dict): Information about the currently authenticated user.\n        cache_dir (str): Local directory for caching downloaded models.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        host_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes the Focoos API client.\n\n        This client provides authenticated access to the Focoos API, enabling various operations\n        through the configured HTTP client. It retrieves user information upon initialization and\n        logs the environment details.\n\n        Args:\n            api_key (Optional[str]): API key for authentication. Defaults to the `focoos_api_key`\n                specified in the FOCOOS_CONFIG.\n            host_url (Optional[str]): Base URL for the Focoos API. Defaults to the `default_host_url`\n                specified in the FOCOOS_CONFIG.\n\n        Raises:\n            ValueError: If the API key is not provided, or if the host URL is not specified in the\n                arguments or the configuration.\n\n        Attributes:\n            api_key (str): The API key used for authentication.\n            api_client (ApiClient): An HTTP client instance configured with the API key and host URL.\n            user_info (dict): Information about the authenticated user retrieved from the API.\n            cache_dir (str): Path to the cache directory used by the client.\n\n        Logs:\n            - Error if the API key or host URL is missing.\n            - Info about the authenticated user and environment upon successful initialization.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            ```\n        \"\"\"\n        self.api_key = api_key or FOCOOS_CONFIG.focoos_api_key\n        if not self.api_key:\n            logger.error(\"API key is required \ud83e\udd16\")\n            raise ValueError(\"API key is required \ud83e\udd16\")\n\n        self.host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n        self.api_client = ApiClient(api_key=self.api_key, host_url=self.host_url)\n        self.user_info = self.get_user_info()\n        self.cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"focoos\")\n        logger.info(f\"Currently logged as: {self.user_info.email} environment: {self.host_url}\")\n\n    def get_user_info(self) -&gt; User:\n        \"\"\"\n        Retrieves information about the authenticated user.\n\n        Returns:\n            User: User object containing account information and usage quotas.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            user_info = focoos.get_user_info()\n\n            # Access user info fields\n            print(f\"Email: {user_info.email}\")\n            print(f\"Created at: {user_info.created_at}\")\n            print(f\"Updated at: {user_info.updated_at}\")\n            print(f\"Company: {user_info.company}\")\n            print(f\"API key: {user_info.api_key.key}\")\n\n            # Access quotas\n            quotas = user_info.quotas\n            print(f\"Total inferences: {quotas.total_inferences}\")\n            print(f\"Max inferences: {quotas.max_inferences}\")\n            print(f\"Used storage (GB): {quotas.used_storage_gb}\")\n            print(f\"Max storage (GB): {quotas.max_storage_gb}\")\n            print(f\"Active training jobs: {quotas.active_training_jobs}\")\n            print(f\"Max active training jobs: {quotas.max_active_training_jobs}\")\n            print(f\"Used MLG4DNXLarge training jobs hours: {quotas.used_mlg4dnxlarge_training_jobs_hours}\")\n            print(f\"Max MLG4DNXLarge training jobs hours: {quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n            ```\n        \"\"\"\n        res = self.api_client.get(\"user/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get user info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get user info: {res.status_code} {res.text}\")\n        return User.from_json(res.json())\n\n    def get_model_info(self, model_ref: str) -&gt; ModelMetadata:\n        \"\"\"\n        Retrieves metadata for a specific model.\n\n        Args:\n            model_ref (str): Name of the model.\n\n        Returns:\n            ModelMetadata: Metadata of the specified model.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            model_info = focoos.get_model_info(model_ref=&lt;user-or-fai-model-ref&gt;)\n            ```\n        \"\"\"\n        res = self.api_client.get(f\"models/{model_ref}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n        return ModelMetadata.from_json(res.json())\n\n    def list_models(self) -&gt; list[ModelPreview]:\n        \"\"\"\n        Lists all User Models.\n\n        Returns:\n            list[ModelPreview]: List of model previews.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            models = focoos.list_models()\n            ```\n        \"\"\"\n        res = self.api_client.get(\"models/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list models: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list models: {res.status_code} {res.text}\")\n        return [ModelPreview.from_json(r) for r in res.json()]\n\n    def list_focoos_models(self) -&gt; list[ModelPreview]:\n        \"\"\"\n        Lists FAI shared models.\n\n        Returns:\n            list[ModelPreview]: List of Focoos models.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            focoos_models = focoos.list_focoos_models()\n            ```\n        \"\"\"\n        res = self.api_client.get(\"models/focoos-models\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n        return [ModelPreview.from_json(r) for r in res.json()]\n\n    def get_local_model(\n        self,\n        model_ref: str,\n        runtime_type: Optional[RuntimeTypes] = RuntimeTypes.ONNX_CUDA32,\n    ) -&gt; LocalModel:\n        \"\"\"\n        Retrieves a local model for the specified reference.\n\n        Downloads the model if it does not already exist in the local cache.\n\n        Args:\n            model_ref (str): Reference identifier for the model.\n            runtime_type (Optional[RuntimeTypes]): Runtime type for the model. Defaults to the\n                `runtime_type` specified in FOCOOS_CONFIG.\n\n        Returns:\n            LocalModel: An instance of the local model.\n\n        Raises:\n            ValueError: If the runtime type is not specified.\n\n        Notes:\n            The model is cached in the directory specified by `self.cache_dir`.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            model = focoos.get_local_model(model_ref=&lt;user-or-fai-model-ref&gt;)\n            results, annotated_image = model.infer(\"image.jpg\", threshold=0.5, annotate=True) # inference is local!\n            ```\n        \"\"\"\n        runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n        model_dir = os.path.join(self.cache_dir, model_ref)\n        format = ModelFormat.from_runtime_type(runtime_type)\n        if not os.path.exists(os.path.join(model_dir, f\"model.{format.value}\")):\n            self._download_model(\n                model_ref,\n                format=format,\n            )\n        return LocalModel(model_dir, runtime_type)\n\n    def get_remote_model(self, model_ref: str) -&gt; RemoteModel:\n        \"\"\"\n        Retrieves a remote model instance.\n\n        Args:\n            model_ref (str): Reference name of the model.\n\n        Returns:\n            RemoteModel: The remote model instance.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            model = focoos.get_remote_model(model_ref=&lt;fai-model-ref&gt;)\n            results, annotated_image = model.infer(\"image.jpg\", threshold=0.5, annotate=True) # inference is remote!\n            ```\n        \"\"\"\n        return RemoteModel(model_ref, self.api_client)\n\n    def new_model(self, name: str, focoos_model: str, description: str) -&gt; RemoteModel:\n        \"\"\"\n        Creates a new model in the Focoos platform.\n\n        Args:\n            name (str): Name of the new model.\n            focoos_model (str): Reference to the base Focoos model.\n            description (str): Description of the new model.\n\n        Returns:\n            Optional[RemoteModel]: The created model instance, or None if creation fails.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            model = focoos.new_model(name=\"my-model\", focoos_model=\"fai-model-ref\", description=\"my-model-description\")\n            ```\n        \"\"\"\n        res = self.api_client.post(\n            \"models/\",\n            data={\n                \"name\": name,\n                \"focoos_model\": focoos_model,\n                \"description\": description,\n            },\n        )\n        if res.status_code in [200, 201]:\n            return RemoteModel(res.json()[\"ref\"], self.api_client)\n        if res.status_code == 409:\n            logger.warning(f\"Model already exists: {name}\")\n            return self.get_model_by_name(name, remote=True)\n        logger.warning(f\"Failed to create new model: {res.status_code} {res.text}\")\n\n    def list_shared_datasets(self) -&gt; list[DatasetPreview]:\n        \"\"\"\n        Lists datasets shared with the user.\n\n        Returns:\n            list[DatasetPreview]: List of shared datasets.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            datasets = focoos.list_shared_datasets()\n            ```\n        \"\"\"\n        res = self.api_client.get(\"datasets/shared\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        return [DatasetPreview.from_json(dataset) for dataset in res.json()]\n\n    def _download_model(self, model_ref: str, format: ModelFormat = ModelFormat.ONNX) -&gt; str:\n        \"\"\"\n        Downloads a model from the Focoos API.\n\n        Args:\n            model_ref (str): Reference name of the model.\n\n        Returns:\n            str: Path to the downloaded model.\n\n        Raises:\n            ValueError: If the API request fails or the download fails.\n        \"\"\"\n        model_dir = os.path.join(self.cache_dir, model_ref)\n        model_path = os.path.join(model_dir, f\"model.{format.value}\")\n        metadata_path = os.path.join(model_dir, \"focoos_metadata.json\")\n        if os.path.exists(model_path) and os.path.exists(metadata_path):\n            logger.info(\"\ud83d\udce5 Model already downloaded\")\n            return model_path\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir)\n        ## download model metadata\n        res = self.api_client.get(f\"models/{model_ref}/download?format={format.value}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n\n        download_data = res.json()\n\n        download_uri = download_data[\"download_uri\"]\n\n        ## download model from Focoos Cloud\n        logger.debug(f\"Model URI: {download_uri}\")\n        logger.info(\"\ud83d\udce5 Downloading model from Focoos Cloud.. \")\n        try:\n            model_path = self.api_client.download_file(download_uri, model_dir)\n            metadata = ModelMetadata.from_json(download_data[\"model_metadata\"])\n            with open(metadata_path, \"w\") as f:\n                f.write(metadata.model_dump_json())\n            logger.debug(f\"Dumped metadata to {metadata_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to download model: {e}\")\n            raise ValueError(f\"Failed to download model: {e}\")\n        if model_path is None:\n            logger.error(f\"Failed to download model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to download model: {res.status_code} {res.text}\")\n\n        return model_path\n\n    def get_model_by_name(self, name: str, remote: bool = True) -&gt; Union[RemoteModel, LocalModel]:\n        \"\"\"\n        Retrieves a model by its name.\n\n        Args:\n            name (str): Name of the model.\n            remote (bool): If True, retrieve as a RemoteModel. Otherwise, as a LocalModel. Defaults to True.\n\n        Returns:\n            Optional[Union[RemoteModel, LocalModel]]: The model instance if found, or None otherwise.\n        \"\"\"\n        models = self.list_models()\n        name_lower = name.lower()\n        for model in models:\n            if name_lower == model.name.lower():\n                if remote:\n                    return self.get_remote_model(model.ref)\n                else:\n                    return self.get_local_model(model.ref)\n        raise ModelNotFound(f\"Model not found: {name}\")\n\n    def list_datasets(self, include_shared: bool = False) -&gt; list[DatasetPreview]:\n        \"\"\"\n        Lists all datasets available to the user.\n\n        This method retrieves all datasets owned by the user and optionally includes\n        shared datasets as well.\n\n        Args:\n            include_shared (bool): If True, includes datasets shared with the user.\n                Defaults to False.\n\n        Returns:\n            list[DatasetPreview]: A list of DatasetPreview objects representing the available datasets.\n\n        Raises:\n            ValueError: If the API request to list datasets fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n\n            # List only user's datasets\n            datasets = focoos.list_datasets()\n\n            # List user's datasets and shared datasets\n            all_datasets = focoos.list_datasets(include_shared=True)\n\n            for dataset in all_datasets:\n                print(f\"Dataset: {dataset.name}, Task: {dataset.task}\")\n            ```\n        \"\"\"\n        res = self.api_client.get(\"datasets/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        datasets = [DatasetPreview.from_json(r) for r in res.json()]\n        if include_shared:\n            res = self.api_client.get(\"datasets/shared\")\n            if res.status_code != 200:\n                logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n                raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            datasets.extend([DatasetPreview.from_json(sh_dataset) for sh_dataset in res.json()])\n        return datasets\n\n    def add_remote_dataset(self, name: str, description: str, layout: DatasetLayout, task: FocoosTask) -&gt; RemoteDataset:\n        \"\"\"\n        Creates a new user dataset with the specified parameters.\n\n        Args:\n            name (str): The name of the dataset.\n            description (str): A description of the dataset.\n            layout (DatasetLayout): The layout structure of the dataset.\n            task (FocoosTask): The task type associated with the dataset.\n\n        Returns:\n            RemoteDataset: A RemoteDataset instance representing the newly created dataset.\n\n        Raises:\n            ValueError: If the dataset creation fails due to API errors.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            dataset = focoos.add_remote_dataset(name=\"my-dataset\", description=\"my-dataset-description\", layout=DatasetLayout.ROBOFLOW_COCO, task=FocoosTask.DETECTION)\n            ```\n        \"\"\"\n        res = self.api_client.post(\n            \"datasets/\", data={\"name\": name, \"description\": description, \"layout\": layout.value, \"task\": task.value}\n        )\n        if res.status_code != 200:\n            logger.error(f\"Failed to add dataset: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to add dataset: {res.status_code} {res.text}\")\n        logger.info(f\"Remote Dataset created: {res.json()['ref']}\")\n        return RemoteDataset(res.json()[\"ref\"], self.api_client)\n\n    def get_remote_dataset(self, ref: str) -&gt; RemoteDataset:\n        \"\"\"\n        Retrieves a remote dataset by its reference ID.\n\n        Args:\n            ref (str): The reference ID of the dataset to retrieve.\n\n        Returns:\n            RemoteDataset: A RemoteDataset instance for the specified reference.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            dataset = focoos.get_remote_dataset(ref=\"my-dataset-ref\")\n            ```\n        \"\"\"\n        return RemoteDataset(ref, self.api_client)\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.__init__","title":"<code>__init__(api_key=None, host_url=None)</code>","text":"<p>Initializes the Focoos API client.</p> <p>This client provides authenticated access to the Focoos API, enabling various operations through the configured HTTP client. It retrieves user information upon initialization and logs the environment details.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>API key for authentication. Defaults to the <code>focoos_api_key</code> specified in the FOCOOS_CONFIG.</p> <code>None</code> <code>host_url</code> <code>Optional[str]</code> <p>Base URL for the Focoos API. Defaults to the <code>default_host_url</code> specified in the FOCOOS_CONFIG.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API key is not provided, or if the host URL is not specified in the arguments or the configuration.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key used for authentication.</p> <code>api_client</code> <code>ApiClient</code> <p>An HTTP client instance configured with the API key and host URL.</p> <code>user_info</code> <code>dict</code> <p>Information about the authenticated user retrieved from the API.</p> <code>cache_dir</code> <code>str</code> <p>Path to the cache directory used by the client.</p> Logs <ul> <li>Error if the API key or host URL is missing.</li> <li>Info about the authenticated user and environment upon successful initialization.</li> </ul> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = None,\n    host_url: Optional[str] = None,\n):\n    \"\"\"\n    Initializes the Focoos API client.\n\n    This client provides authenticated access to the Focoos API, enabling various operations\n    through the configured HTTP client. It retrieves user information upon initialization and\n    logs the environment details.\n\n    Args:\n        api_key (Optional[str]): API key for authentication. Defaults to the `focoos_api_key`\n            specified in the FOCOOS_CONFIG.\n        host_url (Optional[str]): Base URL for the Focoos API. Defaults to the `default_host_url`\n            specified in the FOCOOS_CONFIG.\n\n    Raises:\n        ValueError: If the API key is not provided, or if the host URL is not specified in the\n            arguments or the configuration.\n\n    Attributes:\n        api_key (str): The API key used for authentication.\n        api_client (ApiClient): An HTTP client instance configured with the API key and host URL.\n        user_info (dict): Information about the authenticated user retrieved from the API.\n        cache_dir (str): Path to the cache directory used by the client.\n\n    Logs:\n        - Error if the API key or host URL is missing.\n        - Info about the authenticated user and environment upon successful initialization.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        ```\n    \"\"\"\n    self.api_key = api_key or FOCOOS_CONFIG.focoos_api_key\n    if not self.api_key:\n        logger.error(\"API key is required \ud83e\udd16\")\n        raise ValueError(\"API key is required \ud83e\udd16\")\n\n    self.host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n    self.api_client = ApiClient(api_key=self.api_key, host_url=self.host_url)\n    self.user_info = self.get_user_info()\n    self.cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"focoos\")\n    logger.info(f\"Currently logged as: {self.user_info.email} environment: {self.host_url}\")\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.add_remote_dataset","title":"<code>add_remote_dataset(name, description, layout, task)</code>","text":"<p>Creates a new user dataset with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the dataset.</p> required <code>description</code> <code>str</code> <p>A description of the dataset.</p> required <code>layout</code> <code>DatasetLayout</code> <p>The layout structure of the dataset.</p> required <code>task</code> <code>FocoosTask</code> <p>The task type associated with the dataset.</p> required <p>Returns:</p> Name Type Description <code>RemoteDataset</code> <code>RemoteDataset</code> <p>A RemoteDataset instance representing the newly created dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataset creation fails due to API errors.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\ndataset = focoos.add_remote_dataset(name=\"my-dataset\", description=\"my-dataset-description\", layout=DatasetLayout.ROBOFLOW_COCO, task=FocoosTask.DETECTION)\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def add_remote_dataset(self, name: str, description: str, layout: DatasetLayout, task: FocoosTask) -&gt; RemoteDataset:\n    \"\"\"\n    Creates a new user dataset with the specified parameters.\n\n    Args:\n        name (str): The name of the dataset.\n        description (str): A description of the dataset.\n        layout (DatasetLayout): The layout structure of the dataset.\n        task (FocoosTask): The task type associated with the dataset.\n\n    Returns:\n        RemoteDataset: A RemoteDataset instance representing the newly created dataset.\n\n    Raises:\n        ValueError: If the dataset creation fails due to API errors.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        dataset = focoos.add_remote_dataset(name=\"my-dataset\", description=\"my-dataset-description\", layout=DatasetLayout.ROBOFLOW_COCO, task=FocoosTask.DETECTION)\n        ```\n    \"\"\"\n    res = self.api_client.post(\n        \"datasets/\", data={\"name\": name, \"description\": description, \"layout\": layout.value, \"task\": task.value}\n    )\n    if res.status_code != 200:\n        logger.error(f\"Failed to add dataset: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to add dataset: {res.status_code} {res.text}\")\n    logger.info(f\"Remote Dataset created: {res.json()['ref']}\")\n    return RemoteDataset(res.json()[\"ref\"], self.api_client)\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_local_model","title":"<code>get_local_model(model_ref, runtime_type=RuntimeTypes.ONNX_CUDA32)</code>","text":"<p>Retrieves a local model for the specified reference.</p> <p>Downloads the model if it does not already exist in the local cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference identifier for the model.</p> required <code>runtime_type</code> <code>Optional[RuntimeTypes]</code> <p>Runtime type for the model. Defaults to the <code>runtime_type</code> specified in FOCOOS_CONFIG.</p> <code>ONNX_CUDA32</code> <p>Returns:</p> Name Type Description <code>LocalModel</code> <code>LocalModel</code> <p>An instance of the local model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the runtime type is not specified.</p> Notes <p>The model is cached in the directory specified by <code>self.cache_dir</code>.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\nmodel = focoos.get_local_model(model_ref=&lt;user-or-fai-model-ref&gt;)\nresults, annotated_image = model.infer(\"image.jpg\", threshold=0.5, annotate=True) # inference is local!\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def get_local_model(\n    self,\n    model_ref: str,\n    runtime_type: Optional[RuntimeTypes] = RuntimeTypes.ONNX_CUDA32,\n) -&gt; LocalModel:\n    \"\"\"\n    Retrieves a local model for the specified reference.\n\n    Downloads the model if it does not already exist in the local cache.\n\n    Args:\n        model_ref (str): Reference identifier for the model.\n        runtime_type (Optional[RuntimeTypes]): Runtime type for the model. Defaults to the\n            `runtime_type` specified in FOCOOS_CONFIG.\n\n    Returns:\n        LocalModel: An instance of the local model.\n\n    Raises:\n        ValueError: If the runtime type is not specified.\n\n    Notes:\n        The model is cached in the directory specified by `self.cache_dir`.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        model = focoos.get_local_model(model_ref=&lt;user-or-fai-model-ref&gt;)\n        results, annotated_image = model.infer(\"image.jpg\", threshold=0.5, annotate=True) # inference is local!\n        ```\n    \"\"\"\n    runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n    model_dir = os.path.join(self.cache_dir, model_ref)\n    format = ModelFormat.from_runtime_type(runtime_type)\n    if not os.path.exists(os.path.join(model_dir, f\"model.{format.value}\")):\n        self._download_model(\n            model_ref,\n            format=format,\n        )\n    return LocalModel(model_dir, runtime_type)\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_model_by_name","title":"<code>get_model_by_name(name, remote=True)</code>","text":"<p>Retrieves a model by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>remote</code> <code>bool</code> <p>If True, retrieve as a RemoteModel. Otherwise, as a LocalModel. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[RemoteModel, LocalModel]</code> <p>Optional[Union[RemoteModel, LocalModel]]: The model instance if found, or None otherwise.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_model_by_name(self, name: str, remote: bool = True) -&gt; Union[RemoteModel, LocalModel]:\n    \"\"\"\n    Retrieves a model by its name.\n\n    Args:\n        name (str): Name of the model.\n        remote (bool): If True, retrieve as a RemoteModel. Otherwise, as a LocalModel. Defaults to True.\n\n    Returns:\n        Optional[Union[RemoteModel, LocalModel]]: The model instance if found, or None otherwise.\n    \"\"\"\n    models = self.list_models()\n    name_lower = name.lower()\n    for model in models:\n        if name_lower == model.name.lower():\n            if remote:\n                return self.get_remote_model(model.ref)\n            else:\n                return self.get_local_model(model.ref)\n    raise ModelNotFound(f\"Model not found: {name}\")\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_model_info","title":"<code>get_model_info(model_ref)</code>","text":"<p>Retrieves metadata for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Name of the model.</p> required <p>Returns:</p> Name Type Description <code>ModelMetadata</code> <code>ModelMetadata</code> <p>Metadata of the specified model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\nmodel_info = focoos.get_model_info(model_ref=&lt;user-or-fai-model-ref&gt;)\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def get_model_info(self, model_ref: str) -&gt; ModelMetadata:\n    \"\"\"\n    Retrieves metadata for a specific model.\n\n    Args:\n        model_ref (str): Name of the model.\n\n    Returns:\n        ModelMetadata: Metadata of the specified model.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        model_info = focoos.get_model_info(model_ref=&lt;user-or-fai-model-ref&gt;)\n        ```\n    \"\"\"\n    res = self.api_client.get(f\"models/{model_ref}\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n    return ModelMetadata.from_json(res.json())\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_remote_dataset","title":"<code>get_remote_dataset(ref)</code>","text":"<p>Retrieves a remote dataset by its reference ID.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>The reference ID of the dataset to retrieve.</p> required <p>Returns:</p> Name Type Description <code>RemoteDataset</code> <code>RemoteDataset</code> <p>A RemoteDataset instance for the specified reference.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\ndataset = focoos.get_remote_dataset(ref=\"my-dataset-ref\")\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def get_remote_dataset(self, ref: str) -&gt; RemoteDataset:\n    \"\"\"\n    Retrieves a remote dataset by its reference ID.\n\n    Args:\n        ref (str): The reference ID of the dataset to retrieve.\n\n    Returns:\n        RemoteDataset: A RemoteDataset instance for the specified reference.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        dataset = focoos.get_remote_dataset(ref=\"my-dataset-ref\")\n        ```\n    \"\"\"\n    return RemoteDataset(ref, self.api_client)\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_remote_model","title":"<code>get_remote_model(model_ref)</code>","text":"<p>Retrieves a remote model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference name of the model.</p> required <p>Returns:</p> Name Type Description <code>RemoteModel</code> <code>RemoteModel</code> <p>The remote model instance.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\nmodel = focoos.get_remote_model(model_ref=&lt;fai-model-ref&gt;)\nresults, annotated_image = model.infer(\"image.jpg\", threshold=0.5, annotate=True) # inference is remote!\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def get_remote_model(self, model_ref: str) -&gt; RemoteModel:\n    \"\"\"\n    Retrieves a remote model instance.\n\n    Args:\n        model_ref (str): Reference name of the model.\n\n    Returns:\n        RemoteModel: The remote model instance.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        model = focoos.get_remote_model(model_ref=&lt;fai-model-ref&gt;)\n        results, annotated_image = model.infer(\"image.jpg\", threshold=0.5, annotate=True) # inference is remote!\n        ```\n    \"\"\"\n    return RemoteModel(model_ref, self.api_client)\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_user_info","title":"<code>get_user_info()</code>","text":"<p>Retrieves information about the authenticated user.</p> <p>Returns:</p> Name Type Description <code>User</code> <code>User</code> <p>User object containing account information and usage quotas.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\nuser_info = focoos.get_user_info()\n\n# Access user info fields\nprint(f\"Email: {user_info.email}\")\nprint(f\"Created at: {user_info.created_at}\")\nprint(f\"Updated at: {user_info.updated_at}\")\nprint(f\"Company: {user_info.company}\")\nprint(f\"API key: {user_info.api_key.key}\")\n\n# Access quotas\nquotas = user_info.quotas\nprint(f\"Total inferences: {quotas.total_inferences}\")\nprint(f\"Max inferences: {quotas.max_inferences}\")\nprint(f\"Used storage (GB): {quotas.used_storage_gb}\")\nprint(f\"Max storage (GB): {quotas.max_storage_gb}\")\nprint(f\"Active training jobs: {quotas.active_training_jobs}\")\nprint(f\"Max active training jobs: {quotas.max_active_training_jobs}\")\nprint(f\"Used MLG4DNXLarge training jobs hours: {quotas.used_mlg4dnxlarge_training_jobs_hours}\")\nprint(f\"Max MLG4DNXLarge training jobs hours: {quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def get_user_info(self) -&gt; User:\n    \"\"\"\n    Retrieves information about the authenticated user.\n\n    Returns:\n        User: User object containing account information and usage quotas.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        user_info = focoos.get_user_info()\n\n        # Access user info fields\n        print(f\"Email: {user_info.email}\")\n        print(f\"Created at: {user_info.created_at}\")\n        print(f\"Updated at: {user_info.updated_at}\")\n        print(f\"Company: {user_info.company}\")\n        print(f\"API key: {user_info.api_key.key}\")\n\n        # Access quotas\n        quotas = user_info.quotas\n        print(f\"Total inferences: {quotas.total_inferences}\")\n        print(f\"Max inferences: {quotas.max_inferences}\")\n        print(f\"Used storage (GB): {quotas.used_storage_gb}\")\n        print(f\"Max storage (GB): {quotas.max_storage_gb}\")\n        print(f\"Active training jobs: {quotas.active_training_jobs}\")\n        print(f\"Max active training jobs: {quotas.max_active_training_jobs}\")\n        print(f\"Used MLG4DNXLarge training jobs hours: {quotas.used_mlg4dnxlarge_training_jobs_hours}\")\n        print(f\"Max MLG4DNXLarge training jobs hours: {quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n        ```\n    \"\"\"\n    res = self.api_client.get(\"user/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get user info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get user info: {res.status_code} {res.text}\")\n    return User.from_json(res.json())\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.list_datasets","title":"<code>list_datasets(include_shared=False)</code>","text":"<p>Lists all datasets available to the user.</p> <p>This method retrieves all datasets owned by the user and optionally includes shared datasets as well.</p> <p>Parameters:</p> Name Type Description Default <code>include_shared</code> <code>bool</code> <p>If True, includes datasets shared with the user. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[DatasetPreview]</code> <p>list[DatasetPreview]: A list of DatasetPreview objects representing the available datasets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request to list datasets fails.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\n\n# List only user's datasets\ndatasets = focoos.list_datasets()\n\n# List user's datasets and shared datasets\nall_datasets = focoos.list_datasets(include_shared=True)\n\nfor dataset in all_datasets:\n    print(f\"Dataset: {dataset.name}, Task: {dataset.task}\")\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def list_datasets(self, include_shared: bool = False) -&gt; list[DatasetPreview]:\n    \"\"\"\n    Lists all datasets available to the user.\n\n    This method retrieves all datasets owned by the user and optionally includes\n    shared datasets as well.\n\n    Args:\n        include_shared (bool): If True, includes datasets shared with the user.\n            Defaults to False.\n\n    Returns:\n        list[DatasetPreview]: A list of DatasetPreview objects representing the available datasets.\n\n    Raises:\n        ValueError: If the API request to list datasets fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n\n        # List only user's datasets\n        datasets = focoos.list_datasets()\n\n        # List user's datasets and shared datasets\n        all_datasets = focoos.list_datasets(include_shared=True)\n\n        for dataset in all_datasets:\n            print(f\"Dataset: {dataset.name}, Task: {dataset.task}\")\n        ```\n    \"\"\"\n    res = self.api_client.get(\"datasets/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n    datasets = [DatasetPreview.from_json(r) for r in res.json()]\n    if include_shared:\n        res = self.api_client.get(\"datasets/shared\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        datasets.extend([DatasetPreview.from_json(sh_dataset) for sh_dataset in res.json()])\n    return datasets\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.list_focoos_models","title":"<code>list_focoos_models()</code>","text":"<p>Lists FAI shared models.</p> <p>Returns:</p> Type Description <code>list[ModelPreview]</code> <p>list[ModelPreview]: List of Focoos models.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\nfocoos_models = focoos.list_focoos_models()\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def list_focoos_models(self) -&gt; list[ModelPreview]:\n    \"\"\"\n    Lists FAI shared models.\n\n    Returns:\n        list[ModelPreview]: List of Focoos models.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        focoos_models = focoos.list_focoos_models()\n        ```\n    \"\"\"\n    res = self.api_client.get(\"models/focoos-models\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n    return [ModelPreview.from_json(r) for r in res.json()]\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.list_models","title":"<code>list_models()</code>","text":"<p>Lists all User Models.</p> <p>Returns:</p> Type Description <code>list[ModelPreview]</code> <p>list[ModelPreview]: List of model previews.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\nmodels = focoos.list_models()\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def list_models(self) -&gt; list[ModelPreview]:\n    \"\"\"\n    Lists all User Models.\n\n    Returns:\n        list[ModelPreview]: List of model previews.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        models = focoos.list_models()\n        ```\n    \"\"\"\n    res = self.api_client.get(\"models/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list models: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list models: {res.status_code} {res.text}\")\n    return [ModelPreview.from_json(r) for r in res.json()]\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.list_shared_datasets","title":"<code>list_shared_datasets()</code>","text":"<p>Lists datasets shared with the user.</p> <p>Returns:</p> Type Description <code>list[DatasetPreview]</code> <p>list[DatasetPreview]: List of shared datasets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\ndatasets = focoos.list_shared_datasets()\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def list_shared_datasets(self) -&gt; list[DatasetPreview]:\n    \"\"\"\n    Lists datasets shared with the user.\n\n    Returns:\n        list[DatasetPreview]: List of shared datasets.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        datasets = focoos.list_shared_datasets()\n        ```\n    \"\"\"\n    res = self.api_client.get(\"datasets/shared\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n    return [DatasetPreview.from_json(dataset) for dataset in res.json()]\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.new_model","title":"<code>new_model(name, focoos_model, description)</code>","text":"<p>Creates a new model in the Focoos platform.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the new model.</p> required <code>focoos_model</code> <code>str</code> <p>Reference to the base Focoos model.</p> required <code>description</code> <code>str</code> <p>Description of the new model.</p> required <p>Returns:</p> Type Description <code>RemoteModel</code> <p>Optional[RemoteModel]: The created model instance, or None if creation fails.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\nmodel = focoos.new_model(name=\"my-model\", focoos_model=\"fai-model-ref\", description=\"my-model-description\")\n</code></pre> Source code in <code>focoos/focoos.py</code> <pre><code>def new_model(self, name: str, focoos_model: str, description: str) -&gt; RemoteModel:\n    \"\"\"\n    Creates a new model in the Focoos platform.\n\n    Args:\n        name (str): Name of the new model.\n        focoos_model (str): Reference to the base Focoos model.\n        description (str): Description of the new model.\n\n    Returns:\n        Optional[RemoteModel]: The created model instance, or None if creation fails.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        model = focoos.new_model(name=\"my-model\", focoos_model=\"fai-model-ref\", description=\"my-model-description\")\n        ```\n    \"\"\"\n    res = self.api_client.post(\n        \"models/\",\n        data={\n            \"name\": name,\n            \"focoos_model\": focoos_model,\n            \"description\": description,\n        },\n    )\n    if res.status_code in [200, 201]:\n        return RemoteModel(res.json()[\"ref\"], self.api_client)\n    if res.status_code == 409:\n        logger.warning(f\"Model already exists: {name}\")\n        return self.get_model_by_name(name, remote=True)\n    logger.warning(f\"Failed to create new model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/local_model/","title":"LocalModel","text":"<p>LocalModel Module</p> <p>This module provides the <code>LocalModel</code> class that allows loading, inference, and benchmark testing of models in a local environment. It supports detection and segmentation tasks, and utilizes ONNXRuntime for model execution.</p> <p>Classes:</p> Name Description <code>LocalModel</code> <p>A class for managing and interacting with local models.</p> <p>Functions:</p> Name Description <code>__init__</code> <p>Initializes the LocalModel instance, loading the model, metadata,       and setting up the runtime.</p> <code>_read_metadata</code> <p>Reads the model metadata from a JSON file.</p> <code>_annotate</code> <p>Annotates the input image with detection or segmentation results.</p> <code>infer</code> <p>Runs inference on an input image, with optional annotation.</p> <code>benchmark</code> <p>Benchmarks the model's inference performance over a specified        number of iterations and input size.</p>"},{"location":"api/local_model/#focoos.local_model.LocalModel","title":"<code>LocalModel</code>","text":"Source code in <code>focoos/local_model.py</code> <pre><code>class LocalModel:\n    def __init__(\n        self,\n        model_dir: Union[str, Path],\n        runtime_type: Optional[RuntimeTypes] = None,\n    ):\n        \"\"\"\n        Initialize a LocalModel instance.\n\n        This class sets up a local model for inference by initializing the runtime environment,\n        loading metadata, and preparing annotation utilities.\n\n        Args:\n            model_dir (Union[str, Path]): The path to the directory containing the model files.\n            runtime_type (Optional[RuntimeTypes]): Specifies the runtime type to use for inference.\n                Defaults to the value of `FOCOOS_CONFIG.runtime_type` if not provided.\n\n        Raises:\n            ValueError: If no runtime type is provided and `FOCOOS_CONFIG.runtime_type` is not set.\n            FileNotFoundError: If the specified model directory does not exist.\n\n        Attributes:\n            model_dir (Union[str, Path]): Path to the model directory.\n            metadata (ModelMetadata): Metadata information for the model.\n            model_ref: Reference identifier for the model obtained from metadata.\n            label_annotator (sv.LabelAnnotator): Utility for adding labels to the output,\n                initialized with text padding and border radius.\n            box_annotator (sv.BoxAnnotator): Utility for annotating bounding boxes.\n            mask_annotator (sv.MaskAnnotator): Utility for annotating masks.\n            runtime (ONNXRuntime): Inference runtime initialized with the specified runtime type,\n                model path, metadata, and warmup iterations.\n\n        The method verifies the existence of the model directory, reads the model metadata,\n        and initializes the runtime for inference using the provided runtime type. Annotation\n        utilities are also prepared for visualizing model outputs.\n        \"\"\"\n        # Determine runtime type and model format\n        runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n        model_format = ModelFormat.from_runtime_type(runtime_type)\n\n        # Set model directory and path\n        self.model_dir: Union[str, Path] = model_dir\n        self.model_path = os.path.join(model_dir, f\"model.{model_format.value}\")\n        logger.debug(f\"Runtime type: {runtime_type}, Loading model from {self.model_path}..\")\n\n        # Check if model path exists\n        if not os.path.exists(self.model_path):\n            raise FileNotFoundError(f\"Model path not found: {self.model_path}\")\n\n        # Load metadata and set model reference\n        self.metadata: ModelMetadata = self._read_metadata()\n        self.model_ref = self.metadata.ref\n        self.postprocess_fn = get_postprocess_fn(self.metadata.task)\n\n        # Initialize annotation utilities\n        self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n        self.box_annotator = sv.BoxAnnotator()\n        self.mask_annotator = sv.MaskAnnotator()\n\n        # Load runtime for inference\n        self.runtime: BaseRuntime = load_runtime(\n            runtime_type,\n            str(self.model_path),\n            self.metadata,\n            FOCOOS_CONFIG.warmup_iter,\n        )\n\n    def _read_metadata(self) -&gt; ModelMetadata:\n        \"\"\"\n        Reads the model metadata from a JSON file.\n\n        Returns:\n            ModelMetadata: Metadata for the model.\n\n        Raises:\n            FileNotFoundError: If the metadata file does not exist in the model directory.\n        \"\"\"\n        metadata_path = os.path.join(self.model_dir, \"focoos_metadata.json\")\n        return ModelMetadata.from_json(metadata_path)\n\n    def _annotate(self, im: np.ndarray, detections: sv.Detections) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the input image with detection or segmentation results.\n\n        Args:\n            im (np.ndarray): The input image to annotate.\n            detections (sv.Detections): Detected objects or segmented regions.\n\n        Returns:\n            np.ndarray: The annotated image with bounding boxes or masks.\n        \"\"\"\n        if len(detections.xyxy) == 0:\n            logger.warning(\"No detections found, skipping annotation\")\n            return im\n        classes = self.metadata.classes\n        labels = [\n            f\"{classes[int(class_id)] if classes is not None else str(class_id)}: {confid * 100:.0f}%\"\n            for class_id, confid in zip(detections.class_id, detections.confidence)  # type: ignore\n        ]\n        if self.metadata.task == FocoosTask.DETECTION:\n            annotated_im = self.box_annotator.annotate(scene=im.copy(), detections=detections)\n\n            annotated_im = self.label_annotator.annotate(scene=annotated_im, detections=detections, labels=labels)\n        elif self.metadata.task in [\n            FocoosTask.SEMSEG,\n            FocoosTask.INSTANCE_SEGMENTATION,\n        ]:\n            annotated_im = self.mask_annotator.annotate(scene=im.copy(), detections=detections)\n        return annotated_im\n\n    def infer(\n        self,\n        image: Union[bytes, str, Path, np.ndarray, Image.Image],\n        threshold: float = 0.5,\n        annotate: bool = False,\n    ) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n        \"\"\"\n        Run inference on an input image and optionally annotate the results.\n\n        Args:\n            image (Union[bytes, str, Path, np.ndarray, Image.Image]): The input image to infer on.\n                This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.\n            threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n                Detections with confidence scores below this threshold will be discarded.\n            annotate (bool, optional): Whether to annotate the image with detection results. Defaults to False.\n                If set to True, the method will return the image with bounding boxes or segmentation masks.\n\n        Returns:\n            Tuple[FocoosDetections, Optional[np.ndarray]]: A tuple containing:\n                - `FocoosDetections`: The detections from the inference, represented as a custom object (`FocoosDetections`).\n                This includes the details of the detected objects such as class, confidence score, and bounding box (if applicable).\n                - `Optional[np.ndarray]`: The annotated image, if `annotate=True`.\n                This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks.\n                If `annotate=False`, this value will be `None`.\n\n        Raises:\n            ValueError: If the model is not deployed locally (i.e., `self.runtime` is `None`).\n\n        Example:\n            ```python\n            from focoos import Focoos, LocalModel\n\n            focoos = Focoos()\n            model = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\n            detections, annotated_image = model.infer(image, threshold=0.5, annotate=True)\n            ```\n        \"\"\"\n        assert self.runtime is not None, \"Model is not deployed (locally)\"\n        resize = None  #!TODO  check for segmentation\n        if self.metadata.task == FocoosTask.DETECTION:\n            resize = 640 if not self.metadata.im_size else self.metadata.im_size\n\n        t0 = perf_counter()\n        im1, im0 = image_preprocess(image, resize=resize)\n        logger.debug(f\"Input image size: {im0.shape}, Resize to: {resize}\")\n        t1 = perf_counter()\n        detections = self.runtime(im1.astype(np.float32))\n\n        t2 = perf_counter()\n\n        detections = self.postprocess_fn(\n            out=detections, im0_shape=(im0.shape[0], im0.shape[1]), conf_threshold=threshold\n        )\n        out = sv_to_fai_detections(detections, classes=self.metadata.classes)\n        t3 = perf_counter()\n        latency = {\n            \"inference\": round(t2 - t1, 3),\n            \"preprocess\": round(t1 - t0, 3),\n            \"postprocess\": round(t3 - t2, 3),\n        }\n        im = None\n        if annotate:\n            im = self._annotate(im0, detections)\n\n        logger.debug(\n            f\"Found {len(detections)} detections. Inference time: {(t2 - t1) * 1000:.0f}ms, preprocess: {(t1 - t0) * 1000:.0f}ms, postprocess: {(t3 - t2) * 1000:.0f}ms\"\n        )\n        return FocoosDetections(detections=out, latency=latency), im\n\n    def benchmark(self, iterations: int, size: int) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model's inference performance over multiple iterations.\n\n        Args:\n            iterations (int): Number of iterations to run for benchmarking.\n            size (int): The input size for each benchmark iteration.\n\n        Returns:\n            LatencyMetrics: Latency metrics including time taken for inference.\n\n        Example:\n            ```python\n            from focoos import Focoos, LocalModel\n\n            focoos = Focoos()\n            model = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\n            metrics = model.benchmark(iterations=10, size=640)\n\n            # Access latency metrics\n            print(f\"FPS: {metrics.fps}\")\n            print(f\"Mean latency: {metrics.mean} ms\")\n            print(f\"Engine: {metrics.engine}\")\n            print(f\"Device: {metrics.device}\")\n            print(f\"Input size: {metrics.im_size}x{metrics.im_size}\")\n            ```\n        \"\"\"\n        return self.runtime.benchmark(iterations, size)\n</code></pre>"},{"location":"api/local_model/#focoos.local_model.LocalModel.__init__","title":"<code>__init__(model_dir, runtime_type=None)</code>","text":"<p>Initialize a LocalModel instance.</p> <p>This class sets up a local model for inference by initializing the runtime environment, loading metadata, and preparing annotation utilities.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Union[str, Path]</code> <p>The path to the directory containing the model files.</p> required <code>runtime_type</code> <code>Optional[RuntimeTypes]</code> <p>Specifies the runtime type to use for inference. Defaults to the value of <code>FOCOOS_CONFIG.runtime_type</code> if not provided.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no runtime type is provided and <code>FOCOOS_CONFIG.runtime_type</code> is not set.</p> <code>FileNotFoundError</code> <p>If the specified model directory does not exist.</p> <p>Attributes:</p> Name Type Description <code>model_dir</code> <code>Union[str, Path]</code> <p>Path to the model directory.</p> <code>metadata</code> <code>ModelMetadata</code> <p>Metadata information for the model.</p> <code>model_ref</code> <code>ModelMetadata</code> <p>Reference identifier for the model obtained from metadata.</p> <code>label_annotator</code> <code>LabelAnnotator</code> <p>Utility for adding labels to the output, initialized with text padding and border radius.</p> <code>box_annotator</code> <code>BoxAnnotator</code> <p>Utility for annotating bounding boxes.</p> <code>mask_annotator</code> <code>MaskAnnotator</code> <p>Utility for annotating masks.</p> <code>runtime</code> <code>ONNXRuntime</code> <p>Inference runtime initialized with the specified runtime type, model path, metadata, and warmup iterations.</p> <p>The method verifies the existence of the model directory, reads the model metadata, and initializes the runtime for inference using the provided runtime type. Annotation utilities are also prepared for visualizing model outputs.</p> Source code in <code>focoos/local_model.py</code> <pre><code>def __init__(\n    self,\n    model_dir: Union[str, Path],\n    runtime_type: Optional[RuntimeTypes] = None,\n):\n    \"\"\"\n    Initialize a LocalModel instance.\n\n    This class sets up a local model for inference by initializing the runtime environment,\n    loading metadata, and preparing annotation utilities.\n\n    Args:\n        model_dir (Union[str, Path]): The path to the directory containing the model files.\n        runtime_type (Optional[RuntimeTypes]): Specifies the runtime type to use for inference.\n            Defaults to the value of `FOCOOS_CONFIG.runtime_type` if not provided.\n\n    Raises:\n        ValueError: If no runtime type is provided and `FOCOOS_CONFIG.runtime_type` is not set.\n        FileNotFoundError: If the specified model directory does not exist.\n\n    Attributes:\n        model_dir (Union[str, Path]): Path to the model directory.\n        metadata (ModelMetadata): Metadata information for the model.\n        model_ref: Reference identifier for the model obtained from metadata.\n        label_annotator (sv.LabelAnnotator): Utility for adding labels to the output,\n            initialized with text padding and border radius.\n        box_annotator (sv.BoxAnnotator): Utility for annotating bounding boxes.\n        mask_annotator (sv.MaskAnnotator): Utility for annotating masks.\n        runtime (ONNXRuntime): Inference runtime initialized with the specified runtime type,\n            model path, metadata, and warmup iterations.\n\n    The method verifies the existence of the model directory, reads the model metadata,\n    and initializes the runtime for inference using the provided runtime type. Annotation\n    utilities are also prepared for visualizing model outputs.\n    \"\"\"\n    # Determine runtime type and model format\n    runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n    model_format = ModelFormat.from_runtime_type(runtime_type)\n\n    # Set model directory and path\n    self.model_dir: Union[str, Path] = model_dir\n    self.model_path = os.path.join(model_dir, f\"model.{model_format.value}\")\n    logger.debug(f\"Runtime type: {runtime_type}, Loading model from {self.model_path}..\")\n\n    # Check if model path exists\n    if not os.path.exists(self.model_path):\n        raise FileNotFoundError(f\"Model path not found: {self.model_path}\")\n\n    # Load metadata and set model reference\n    self.metadata: ModelMetadata = self._read_metadata()\n    self.model_ref = self.metadata.ref\n    self.postprocess_fn = get_postprocess_fn(self.metadata.task)\n\n    # Initialize annotation utilities\n    self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n    self.box_annotator = sv.BoxAnnotator()\n    self.mask_annotator = sv.MaskAnnotator()\n\n    # Load runtime for inference\n    self.runtime: BaseRuntime = load_runtime(\n        runtime_type,\n        str(self.model_path),\n        self.metadata,\n        FOCOOS_CONFIG.warmup_iter,\n    )\n</code></pre>"},{"location":"api/local_model/#focoos.local_model.LocalModel.benchmark","title":"<code>benchmark(iterations, size)</code>","text":"<p>Benchmark the model's inference performance over multiple iterations.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of iterations to run for benchmarking.</p> required <code>size</code> <code>int</code> <p>The input size for each benchmark iteration.</p> required <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Latency metrics including time taken for inference.</p> Example <pre><code>from focoos import Focoos, LocalModel\n\nfocoos = Focoos()\nmodel = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\nmetrics = model.benchmark(iterations=10, size=640)\n\n# Access latency metrics\nprint(f\"FPS: {metrics.fps}\")\nprint(f\"Mean latency: {metrics.mean} ms\")\nprint(f\"Engine: {metrics.engine}\")\nprint(f\"Device: {metrics.device}\")\nprint(f\"Input size: {metrics.im_size}x{metrics.im_size}\")\n</code></pre> Source code in <code>focoos/local_model.py</code> <pre><code>def benchmark(self, iterations: int, size: int) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model's inference performance over multiple iterations.\n\n    Args:\n        iterations (int): Number of iterations to run for benchmarking.\n        size (int): The input size for each benchmark iteration.\n\n    Returns:\n        LatencyMetrics: Latency metrics including time taken for inference.\n\n    Example:\n        ```python\n        from focoos import Focoos, LocalModel\n\n        focoos = Focoos()\n        model = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\n        metrics = model.benchmark(iterations=10, size=640)\n\n        # Access latency metrics\n        print(f\"FPS: {metrics.fps}\")\n        print(f\"Mean latency: {metrics.mean} ms\")\n        print(f\"Engine: {metrics.engine}\")\n        print(f\"Device: {metrics.device}\")\n        print(f\"Input size: {metrics.im_size}x{metrics.im_size}\")\n        ```\n    \"\"\"\n    return self.runtime.benchmark(iterations, size)\n</code></pre>"},{"location":"api/local_model/#focoos.local_model.LocalModel.infer","title":"<code>infer(image, threshold=0.5, annotate=False)</code>","text":"<p>Run inference on an input image and optionally annotate the results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[bytes, str, Path, ndarray, Image]</code> <p>The input image to infer on. This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.</p> required <code>threshold</code> <code>float</code> <p>The confidence threshold for detections. Defaults to 0.5. Detections with confidence scores below this threshold will be discarded.</p> <code>0.5</code> <code>annotate</code> <code>bool</code> <p>Whether to annotate the image with detection results. Defaults to False. If set to True, the method will return the image with bounding boxes or segmentation masks.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[FocoosDetections, Optional[ndarray]]</code> <p>Tuple[FocoosDetections, Optional[np.ndarray]]: A tuple containing: - <code>FocoosDetections</code>: The detections from the inference, represented as a custom object (<code>FocoosDetections</code>). This includes the details of the detected objects such as class, confidence score, and bounding box (if applicable). - <code>Optional[np.ndarray]</code>: The annotated image, if <code>annotate=True</code>. This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks. If <code>annotate=False</code>, this value will be <code>None</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not deployed locally (i.e., <code>self.runtime</code> is <code>None</code>).</p> Example <pre><code>from focoos import Focoos, LocalModel\n\nfocoos = Focoos()\nmodel = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\ndetections, annotated_image = model.infer(image, threshold=0.5, annotate=True)\n</code></pre> Source code in <code>focoos/local_model.py</code> <pre><code>def infer(\n    self,\n    image: Union[bytes, str, Path, np.ndarray, Image.Image],\n    threshold: float = 0.5,\n    annotate: bool = False,\n) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n    \"\"\"\n    Run inference on an input image and optionally annotate the results.\n\n    Args:\n        image (Union[bytes, str, Path, np.ndarray, Image.Image]): The input image to infer on.\n            This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.\n        threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n            Detections with confidence scores below this threshold will be discarded.\n        annotate (bool, optional): Whether to annotate the image with detection results. Defaults to False.\n            If set to True, the method will return the image with bounding boxes or segmentation masks.\n\n    Returns:\n        Tuple[FocoosDetections, Optional[np.ndarray]]: A tuple containing:\n            - `FocoosDetections`: The detections from the inference, represented as a custom object (`FocoosDetections`).\n            This includes the details of the detected objects such as class, confidence score, and bounding box (if applicable).\n            - `Optional[np.ndarray]`: The annotated image, if `annotate=True`.\n            This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks.\n            If `annotate=False`, this value will be `None`.\n\n    Raises:\n        ValueError: If the model is not deployed locally (i.e., `self.runtime` is `None`).\n\n    Example:\n        ```python\n        from focoos import Focoos, LocalModel\n\n        focoos = Focoos()\n        model = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\n        detections, annotated_image = model.infer(image, threshold=0.5, annotate=True)\n        ```\n    \"\"\"\n    assert self.runtime is not None, \"Model is not deployed (locally)\"\n    resize = None  #!TODO  check for segmentation\n    if self.metadata.task == FocoosTask.DETECTION:\n        resize = 640 if not self.metadata.im_size else self.metadata.im_size\n\n    t0 = perf_counter()\n    im1, im0 = image_preprocess(image, resize=resize)\n    logger.debug(f\"Input image size: {im0.shape}, Resize to: {resize}\")\n    t1 = perf_counter()\n    detections = self.runtime(im1.astype(np.float32))\n\n    t2 = perf_counter()\n\n    detections = self.postprocess_fn(\n        out=detections, im0_shape=(im0.shape[0], im0.shape[1]), conf_threshold=threshold\n    )\n    out = sv_to_fai_detections(detections, classes=self.metadata.classes)\n    t3 = perf_counter()\n    latency = {\n        \"inference\": round(t2 - t1, 3),\n        \"preprocess\": round(t1 - t0, 3),\n        \"postprocess\": round(t3 - t2, 3),\n    }\n    im = None\n    if annotate:\n        im = self._annotate(im0, detections)\n\n    logger.debug(\n        f\"Found {len(detections)} detections. Inference time: {(t2 - t1) * 1000:.0f}ms, preprocess: {(t1 - t0) * 1000:.0f}ms, postprocess: {(t3 - t2) * 1000:.0f}ms\"\n    )\n    return FocoosDetections(detections=out, latency=latency), im\n</code></pre>"},{"location":"api/ports/","title":"Ports","text":""},{"location":"api/ports/#focoos.ports.ApiKey","title":"<code>ApiKey</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>API key for authentication.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ApiKey(FocoosBaseModel):\n    \"\"\"API key for authentication.\"\"\"\n\n    key: str  # type: ignore\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetLayout","title":"<code>DatasetLayout</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported dataset formats in Focoos.</p> Values <ul> <li>ROBOFLOW_COCO: (Detection,Instance Segmentation)</li> <li>ROBOFLOW_SEG: (Semantic Segmentation)</li> <li>SUPERVISELY: (Semantic Segmentation)</li> </ul> <p>Example:     <pre><code>- ROBOFLOW_COCO: (Detection,Instance Segmentation) Roboflow COCO format:\n    root/\n        train/\n            - _annotations.coco.json\n            - img_1.jpg\n            - img_2.jpg\n        valid/\n            - _annotations.coco.json\n            - img_3.jpg\n            - img_4.jpg\n- ROBOFLOW_SEG: (Semantic Segmentation) Roboflow segmentation format:\n    root/\n        train/\n            - _classes.csv (comma separated csv)\n            - img_1.jpg\n            - img_2.jpg\n        valid/\n            - _classes.csv (comma separated csv)\n            - img_3_mask.png\n            - img_4_mask.png\n\n- SUPERVISELY: (Semantic Segmentation) format:\n    root/\n        train/\n            meta.json\n            img/\n            ann/\n            mask/\n        valid/\n            meta.json\n            img/\n            ann/\n            mask/\n</code></pre></p> Source code in <code>focoos/ports.py</code> <pre><code>class DatasetLayout(str, Enum):\n    \"\"\"Supported dataset formats in Focoos.\n\n    Values:\n        - ROBOFLOW_COCO: (Detection,Instance Segmentation)\n        - ROBOFLOW_SEG: (Semantic Segmentation)\n        - SUPERVISELY: (Semantic Segmentation)\n    Example:\n        ```python\n        - ROBOFLOW_COCO: (Detection,Instance Segmentation) Roboflow COCO format:\n            root/\n                train/\n                    - _annotations.coco.json\n                    - img_1.jpg\n                    - img_2.jpg\n                valid/\n                    - _annotations.coco.json\n                    - img_3.jpg\n                    - img_4.jpg\n        - ROBOFLOW_SEG: (Semantic Segmentation) Roboflow segmentation format:\n            root/\n                train/\n                    - _classes.csv (comma separated csv)\n                    - img_1.jpg\n                    - img_2.jpg\n                valid/\n                    - _classes.csv (comma separated csv)\n                    - img_3_mask.png\n                    - img_4_mask.png\n\n        - SUPERVISELY: (Semantic Segmentation) format:\n            root/\n                train/\n                    meta.json\n                    img/\n                    ann/\n                    mask/\n                valid/\n                    meta.json\n                    img/\n                    ann/\n                    mask/\n        ```\n    \"\"\"\n\n    ROBOFLOW_COCO = \"roboflow_coco\"\n    ROBOFLOW_SEG = \"roboflow_seg\"\n    CATALOG = \"catalog\"\n    SUPERVISELY = \"supervisely\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetPreview","title":"<code>DatasetPreview</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Preview information for a Focoos dataset.</p> <p>This class provides metadata about a dataset in the Focoos platform, including its identification, task type, and layout format.</p> <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>Unique reference ID for the dataset.</p> <code>name</code> <code>str</code> <p>Human-readable name of the dataset.</p> <code>task</code> <code>FocoosTask</code> <p>The computer vision task this dataset is designed for.</p> <code>layout</code> <code>DatasetLayout</code> <p>The structural format of the dataset (e.g., ROBOFLOW_COCO, ROBOFLOW_SEG, SUPERVISELY).</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the dataset's purpose or contents.</p> <code>spec</code> <code>Optional[DatasetSpec]</code> <p>Detailed specifications about the dataset's composition and size.</p> Source code in <code>focoos/ports.py</code> <pre><code>class DatasetPreview(FocoosBaseModel):\n    \"\"\"Preview information for a Focoos dataset.\n\n    This class provides metadata about a dataset in the Focoos platform,\n    including its identification, task type, and layout format.\n\n    Attributes:\n        ref (str): Unique reference ID for the dataset.\n        name (str): Human-readable name of the dataset.\n        task (FocoosTask): The computer vision task this dataset is designed for.\n        layout (DatasetLayout): The structural format of the dataset (e.g., ROBOFLOW_COCO, ROBOFLOW_SEG, SUPERVISELY).\n        description (Optional[str]): Optional description of the dataset's purpose or contents.\n        spec (Optional[DatasetSpec]): Detailed specifications about the dataset's composition and size.\n    \"\"\"\n\n    ref: str\n    name: str\n    task: FocoosTask\n    layout: DatasetLayout\n    description: Optional[str] = None\n    spec: Optional[DatasetSpec] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetSpec","title":"<code>DatasetSpec</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Specification details for a dataset in the Focoos platform.</p> <p>This class provides information about the dataset's size and composition, including the number of samples in training and validation sets and the total size.</p> <p>Attributes:</p> Name Type Description <code>train_length</code> <code>int</code> <p>Number of samples in the training set.</p> <code>valid_length</code> <code>int</code> <p>Number of samples in the validation set.</p> <code>size_mb</code> <code>float</code> <p>Total size of the dataset in megabytes.</p> Source code in <code>focoos/ports.py</code> <pre><code>class DatasetSpec(FocoosBaseModel):\n    \"\"\"Specification details for a dataset in the Focoos platform.\n\n    This class provides information about the dataset's size and composition,\n    including the number of samples in training and validation sets and the total size.\n\n    Attributes:\n        train_length (int): Number of samples in the training set.\n        valid_length (int): Number of samples in the validation set.\n        size_mb (float): Total size of the dataset in megabytes.\n    \"\"\"\n\n    train_length: int\n    valid_length: int\n    size_mb: float\n</code></pre>"},{"location":"api/ports/#focoos.ports.FocoosDet","title":"<code>FocoosDet</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Single detection result from a model.</p> <p>This class represents a single detection or segmentation result from a Focoos model. It contains information about the detected object including its position, class, confidence score, and optional segmentation mask.</p> <p>Attributes:</p> Name Type Description <code>bbox</code> <code>Optional[list[int]]</code> <p>Bounding box coordinates in [x1, y1, x2, y2] format, where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.</p> <code>conf</code> <code>Optional[float]</code> <p>Confidence score of the detection, ranging from 0 to 1.</p> <code>cls_id</code> <code>Optional[int]</code> <p>Class ID of the detected object, corresponding to the index in the model's class list.</p> <code>label</code> <code>Optional[str]</code> <p>Human-readable label of the detected object.</p> <code>mask</code> <code>Optional[str]</code> <p>Base64-encoded PNG image representing the segmentation mask. Note that the mask is cropped to the bounding box coordinates and does not have the same shape as the input image.</p> <p>Note</p> <p>The mask is only present if the model is an instance segmentation or semantic segmentation model. The mask is a base64 encoded string having origin in the top left corner of bbox and the same width and height of the bbox.</p> Source code in <code>focoos/ports.py</code> <pre><code>class FocoosDet(FocoosBaseModel):\n    \"\"\"Single detection result from a model.\n\n    This class represents a single detection or segmentation result from a Focoos model.\n    It contains information about the detected object including its position, class,\n    confidence score, and optional segmentation mask.\n\n    Attributes:\n        bbox (Optional[list[int]]): Bounding box coordinates in [x1, y1, x2, y2] format,\n            where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.\n        conf (Optional[float]): Confidence score of the detection, ranging from 0 to 1.\n        cls_id (Optional[int]): Class ID of the detected object, corresponding to the index\n            in the model's class list.\n        label (Optional[str]): Human-readable label of the detected object.\n        mask (Optional[str]): Base64-encoded PNG image representing the segmentation mask.\n            Note that the mask is cropped to the bounding box coordinates and does not\n            have the same shape as the input image.\n\n    !!! Note\n        The mask is only present if the model is an instance segmentation or semantic segmentation model.\n        The mask is a base64 encoded string having origin in the top left corner of bbox and the same width and height of the bbox.\n\n    \"\"\"\n\n    bbox: Optional[list[int]] = None\n    conf: Optional[float] = None\n    cls_id: Optional[int] = None\n    label: Optional[str] = None\n    mask: Optional[str] = None\n\n    @classmethod\n    def from_json(cls, data: Union[str, dict]):\n        if isinstance(data, str):\n            with open(data, encoding=\"utf-8\") as f:\n                data_dict = json.load(f)\n        else:\n            data_dict = data\n\n        bbox = data_dict.get(\"bbox\")\n        if bbox is not None:  # Retrocompatibility fix for remote results with float bbox, !TODO remove asap\n            data_dict[\"bbox\"] = list(map(int, bbox))\n\n        return cls.model_validate(data_dict)\n</code></pre>"},{"location":"api/ports/#focoos.ports.FocoosDetections","title":"<code>FocoosDetections</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Collection of detection results from a model.</p> <p>This class represents a collection of detection or segmentation results from a Focoos model. It contains a list of individual detections and optional latency information.</p> <p>Attributes:</p> Name Type Description <code>detections</code> <code>list[FocoosDet]</code> <p>List of detection results, where each detection contains information about a detected object including its position, class, confidence score, and optional segmentation mask.</p> <code>latency</code> <code>Optional[dict]</code> <p>Dictionary containing latency information for the inference process. Typically includes keys like 'inference', 'preprocess', and 'postprocess' with values representing the time taken in seconds for each step.</p> Source code in <code>focoos/ports.py</code> <pre><code>class FocoosDetections(FocoosBaseModel):\n    \"\"\"Collection of detection results from a model.\n\n    This class represents a collection of detection or segmentation results from a Focoos model.\n    It contains a list of individual detections and optional latency information.\n\n    Attributes:\n        detections (list[FocoosDet]): List of detection results, where each detection contains\n            information about a detected object including its position, class, confidence score,\n            and optional segmentation mask.\n        latency (Optional[dict]): Dictionary containing latency information for the inference process.\n            Typically includes keys like 'inference', 'preprocess', and 'postprocess' with values\n            representing the time taken in seconds for each step.\n    \"\"\"\n\n    detections: list[FocoosDet]\n    latency: Optional[dict] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.FocoosTask","title":"<code>FocoosTask</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of computer vision tasks supported by Focoos.</p> Values <ul> <li>DETECTION: Object detection</li> <li>SEMSEG: Semantic segmentation</li> <li>INSTANCE_SEGMENTATION: Instance segmentation</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class FocoosTask(str, Enum):\n    \"\"\"Types of computer vision tasks supported by Focoos.\n\n    Values:\n        - DETECTION: Object detection\n        - SEMSEG: Semantic segmentation\n        - INSTANCE_SEGMENTATION: Instance segmentation\n    \"\"\"\n\n    DETECTION = \"detection\"\n    SEMSEG = \"semseg\"\n    INSTANCE_SEGMENTATION = \"instseg\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.GPUDevice","title":"<code>GPUDevice</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Information about a GPU device.</p> Source code in <code>focoos/ports.py</code> <pre><code>class GPUDevice(FocoosBaseModel):\n    \"\"\"Information about a GPU device.\"\"\"\n\n    gpu_id: Optional[int] = None\n    gpu_name: Optional[str] = None\n    gpu_memory_total_gb: Optional[float] = None\n    gpu_memory_used_percentage: Optional[float] = None\n    gpu_temperature: Optional[float] = None\n    gpu_load_percentage: Optional[float] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.GPUInfo","title":"<code>GPUInfo</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Information about a GPU driver.</p> Source code in <code>focoos/ports.py</code> <pre><code>class GPUInfo(FocoosBaseModel):\n    \"\"\"Information about a GPU driver.\"\"\"\n\n    gpu_count: Optional[int] = None\n    gpu_driver: Optional[str] = None\n    gpu_cuda_version: Optional[str] = None\n    devices: Optional[list[GPUDevice]] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.Hyperparameters","title":"<code>Hyperparameters</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Model training hyperparameters configuration.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Number of images processed in each training iteration. Range: 1-32. Larger batch sizes require more GPU memory but can speed up training.</p> <code>eval_period</code> <code>int</code> <p>Number of iterations between model evaluations. Range: 50-2000. Controls how frequently validation is performed during training.</p> <code>max_iters</code> <code>int</code> <p>Maximum number of training iterations. Range: 100-100,000. Total number of times the model will see batches of training data.</p> <code>resolution</code> <code>int</code> <p>Input image resolution for the model. Range: 128-6400 pixels. Higher resolutions can improve accuracy but require more compute.</p> <code>wandb_project</code> <code>Optional[str]</code> <p>Weights &amp; Biases project name in format \"ORG_ID/PROJECT_NAME\". Used for experiment tracking and visualization.</p> <code>wandb_apikey</code> <code>Optional[str]</code> <p>API key for Weights &amp; Biases integration. Required if using wandb_project.</p> <code>learning_rate</code> <code>float</code> <p>Step size for model weight updates. Range: 0.00001-0.1. Controls how quickly the model learns. Too high can cause instability.</p> <code>decoder_multiplier</code> <code>float</code> <p>Multiplier for decoder learning rate. Allows different learning rates for decoder vs backbone.</p> <code>backbone_multiplier</code> <code>float</code> <p>Multiplier for backbone learning rate. Default 0.1 means backbone learns 10x slower than decoder.</p> <code>amp_enabled</code> <code>bool</code> <p>Whether to use automatic mixed precision training. Can speed up training and reduce memory usage with minimal accuracy impact.</p> <code>weight_decay</code> <code>float</code> <p>L2 regularization factor to prevent overfitting. Higher values = stronger regularization.</p> <code>ema_enabled</code> <code>bool</code> <p>Whether to use Exponential Moving Average of model weights. Can improve model stability and final performance.</p> <code>ema_decay</code> <code>float</code> <p>Decay rate for EMA. Higher = slower but more stable updates. Only used if ema_enabled=True.</p> <code>ema_warmup</code> <code>int</code> <p>Number of iterations before starting EMA. Only used if ema_enabled=True.</p> <code>freeze_bn</code> <code>bool</code> <p>Whether to freeze all batch normalization layers. Useful for fine-tuning with small batch sizes.</p> <code>freeze_bn_bkb</code> <code>bool</code> <p>Whether to freeze backbone batch normalization layers. Default True to preserve pretrained backbone statistics.</p> <code>optimizer</code> <code>str</code> <p>Optimization algorithm. Options: \"ADAMW\", \"SGD\", \"RMSPROP\". ADAMW generally works best for vision tasks.</p> <code>scheduler</code> <code>str</code> <p>Learning rate schedule. Options: \"POLY\", \"FIXED\", \"COSINE\", \"MULTISTEP\". Controls how learning rate changes during training.</p> <code>early_stop</code> <code>bool</code> <p>Whether to stop training early if validation metrics plateau. Can prevent overfitting and save compute time.</p> <code>patience</code> <code>int</code> <p>Number of evaluations to wait for improvement before early stopping. Only used if early_stop=True.</p> Source code in <code>focoos/ports.py</code> <pre><code>class Hyperparameters(FocoosBaseModel):\n    \"\"\"Model training hyperparameters configuration.\n\n    Attributes:\n        batch_size (int): Number of images processed in each training iteration. Range: 1-32.\n            Larger batch sizes require more GPU memory but can speed up training.\n\n        eval_period (int): Number of iterations between model evaluations. Range: 50-2000.\n            Controls how frequently validation is performed during training.\n\n        max_iters (int): Maximum number of training iterations. Range: 100-100,000.\n            Total number of times the model will see batches of training data.\n\n        resolution (int): Input image resolution for the model. Range: 128-6400 pixels.\n            Higher resolutions can improve accuracy but require more compute.\n\n        wandb_project (Optional[str]): Weights &amp; Biases project name in format \"ORG_ID/PROJECT_NAME\".\n            Used for experiment tracking and visualization.\n\n        wandb_apikey (Optional[str]): API key for Weights &amp; Biases integration.\n            Required if using wandb_project.\n\n        learning_rate (float): Step size for model weight updates. Range: 0.00001-0.1.\n            Controls how quickly the model learns. Too high can cause instability.\n\n        decoder_multiplier (float): Multiplier for decoder learning rate.\n            Allows different learning rates for decoder vs backbone.\n\n        backbone_multiplier (float): Multiplier for backbone learning rate.\n            Default 0.1 means backbone learns 10x slower than decoder.\n\n        amp_enabled (bool): Whether to use automatic mixed precision training.\n            Can speed up training and reduce memory usage with minimal accuracy impact.\n\n        weight_decay (float): L2 regularization factor to prevent overfitting.\n            Higher values = stronger regularization.\n\n        ema_enabled (bool): Whether to use Exponential Moving Average of model weights.\n            Can improve model stability and final performance.\n\n        ema_decay (float): Decay rate for EMA. Higher = slower but more stable updates.\n            Only used if ema_enabled=True.\n\n        ema_warmup (int): Number of iterations before starting EMA.\n            Only used if ema_enabled=True.\n\n        freeze_bn (bool): Whether to freeze all batch normalization layers.\n            Useful for fine-tuning with small batch sizes.\n\n        freeze_bn_bkb (bool): Whether to freeze backbone batch normalization layers.\n            Default True to preserve pretrained backbone statistics.\n\n        optimizer (str): Optimization algorithm. Options: \"ADAMW\", \"SGD\", \"RMSPROP\".\n            ADAMW generally works best for vision tasks.\n\n        scheduler (str): Learning rate schedule. Options: \"POLY\", \"FIXED\", \"COSINE\", \"MULTISTEP\".\n            Controls how learning rate changes during training.\n\n        early_stop (bool): Whether to stop training early if validation metrics plateau.\n            Can prevent overfitting and save compute time.\n\n        patience (int): Number of evaluations to wait for improvement before early stopping.\n            Only used if early_stop=True.\n\n\n    \"\"\"\n\n    batch_size: Annotated[\n        int,\n        Field(\n            ge=1,\n            le=32,\n            description=\"Batch size, how many images are processed at every iteration\",\n        ),\n    ] = 16\n    eval_period: Annotated[\n        int,\n        Field(ge=50, le=2000, description=\"How often iterations to evaluate the model\"),\n    ] = 500\n    max_iters: Annotated[\n        int,\n        Field(1500, ge=100, le=100000, description=\"Maximum number of training iterations\"),\n    ] = 1500\n    resolution: Annotated[int, Field(640, description=\"Model expected resolution\", ge=128, le=6400)] = 640\n    wandb_project: Annotated[\n        Optional[str],\n        Field(description=\"Wandb project name must be like ORG_ID/PROJECT_NAME\"),\n    ] = None\n    wandb_apikey: Annotated[Optional[str], Field(description=\"Wandb API key\")] = None\n    learning_rate: Annotated[\n        float,\n        Field(gt=0.00001, lt=0.1, description=\"Learning rate\"),\n    ] = 5e-4\n    decoder_multiplier: Annotated[float, Field(description=\"Backbone multiplier\")] = 1\n    backbone_multiplier: float = 0.1\n    amp_enabled: Annotated[bool, Field(description=\"Enable automatic mixed precision\")] = True\n    weight_decay: Annotated[float, Field(description=\"Weight decay\")] = 0.02\n    ema_enabled: Annotated[bool, Field(description=\"Enable EMA (exponential moving average)\")] = False\n    ema_decay: Annotated[float, Field(description=\"EMA decay rate\")] = 0.999\n    ema_warmup: Annotated[int, Field(description=\"EMA warmup\")] = 100\n    freeze_bn: Annotated[bool, Field(description=\"Freeze batch normalization layers\")] = False\n    freeze_bn_bkb: Annotated[bool, Field(description=\"Freeze backbone batch normalization layers\")] = True\n    optimizer: Literal[\"ADAMW\", \"SGD\", \"RMSPROP\"] = \"ADAMW\"\n    scheduler: Literal[\"POLY\", \"FIXED\", \"COSINE\", \"MULTISTEP\"] = \"MULTISTEP\"\n\n    early_stop: Annotated[bool, Field(description=\"Enable early stopping\")] = True\n    patience: Annotated[\n        int,\n        Field(\n            description=\"(Only with early_stop=True) Validation cycles after which the train is stopped if there's no improvement in accuracy.\"\n        ),\n    ] = 5\n\n    @field_validator(\"wandb_project\")\n    def validate_wandb_project(cls, value):\n        if value is not None:\n            # Define a regex pattern to match valid characters\n            if not re.match(r\"^[\\w.-/]+$\", value):\n                raise ValueError(\"Wandb project name must only contain characters, dashes, underscores, and dots.\")\n        return value\n</code></pre>"},{"location":"api/ports/#focoos.ports.LatencyMetrics","title":"<code>LatencyMetrics</code>  <code>dataclass</code>","text":"<p>Performance metrics for model inference.</p> <p>This class provides performance metrics for model inference, including frames per second (FPS), engine used, minimum latency, maximum latency, mean latency, standard deviation of latency, input image size, and device type.</p> <p>Attributes:</p> Name Type Description <code>fps</code> <code>int</code> <p>Frames per second (FPS) of the inference process.</p> <code>engine</code> <code>str</code> <p>The inference engine used (e.g., \"onnx\", \"torchscript\").</p> <code>min</code> <code>float</code> <p>Minimum latency in milliseconds.</p> <code>max</code> <code>float</code> <p>Maximum latency in milliseconds.</p> <code>mean</code> <code>float</code> <p>Mean latency in milliseconds.</p> <code>std</code> <code>float</code> <p>Standard deviation of latency in milliseconds.</p> <code>im_size</code> <code>int</code> <p>Input image size.</p> <code>device</code> <code>str</code> <p>Device type.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass LatencyMetrics:\n    \"\"\"Performance metrics for model inference.\n\n    This class provides performance metrics for model inference, including frames per second (FPS),\n    engine used, minimum latency, maximum latency, mean latency, standard deviation of latency,\n    input image size, and device type.\n\n    Attributes:\n        fps (int): Frames per second (FPS) of the inference process.\n        engine (str): The inference engine used (e.g., \"onnx\", \"torchscript\").\n        min (float): Minimum latency in milliseconds.\n        max (float): Maximum latency in milliseconds.\n        mean (float): Mean latency in milliseconds.\n        std (float): Standard deviation of latency in milliseconds.\n        im_size (int): Input image size.\n        device (str): Device type.\n    \"\"\"\n\n    fps: int\n    engine: str\n    min: float\n    max: float\n    mean: float\n    std: float\n    im_size: int\n    device: str\n</code></pre>"},{"location":"api/ports/#focoos.ports.Metrics","title":"<code>Metrics</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Collection of training and inference metrics.</p> Source code in <code>focoos/ports.py</code> <pre><code>class Metrics(FocoosBaseModel):\n    \"\"\"\n    Collection of training and inference metrics.\n    \"\"\"\n\n    infer_metrics: list[dict] = []\n    valid_metrics: list[dict] = []\n    train_metrics: list[dict] = []\n    iterations: Optional[int] = None\n    best_valid_metric: Optional[dict] = None\n    updated_at: Optional[datetime] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelFormat","title":"<code>ModelFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported model formats.</p> Values <ul> <li>ONNX: ONNX format</li> <li>TORCHSCRIPT: TorchScript format</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class ModelFormat(str, Enum):\n    \"\"\"Supported model formats.\n\n    Values:\n        - ONNX: ONNX format\n        - TORCHSCRIPT: TorchScript format\n\n    \"\"\"\n\n    ONNX = \"onnx\"\n    TORCHSCRIPT = \"pt\"\n\n    @classmethod\n    def from_runtime_type(cls, runtime_type: RuntimeTypes):\n        if runtime_type in [\n            RuntimeTypes.ONNX_CUDA32,\n            RuntimeTypes.ONNX_TRT32,\n            RuntimeTypes.ONNX_TRT16,\n            RuntimeTypes.ONNX_CPU,\n            RuntimeTypes.ONNX_COREML,\n        ]:\n            return cls.ONNX\n        elif runtime_type == RuntimeTypes.TORCHSCRIPT_32:\n            return cls.TORCHSCRIPT\n        else:\n            raise ValueError(f\"Invalid runtime type: {runtime_type}\")\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelMetadata","title":"<code>ModelMetadata</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Complete metadata for a Focoos model.</p> <p>This class contains comprehensive information about a model in the Focoos platform, including its identification, configuration, performance metrics, and training details.</p> <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>Unique reference ID for the model.</p> <code>name</code> <code>str</code> <p>Human-readable name of the model.</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the model's purpose or capabilities.</p> <code>owner_ref</code> <code>str</code> <p>Reference ID of the model owner.</p> <code>focoos_model</code> <code>str</code> <p>The base model architecture used.</p> <code>task</code> <code>FocoosTask</code> <p>The task type the model is designed for (e.g., DETECTION, SEMSEG).</p> <code>created_at</code> <code>datetime</code> <p>Timestamp when the model was created.</p> <code>updated_at</code> <code>datetime</code> <p>Timestamp when the model was last updated.</p> <code>status</code> <code>ModelStatus</code> <p>Current status of the model (e.g., TRAINING, DEPLOYED).</p> <code>metrics</code> <code>Optional[dict]</code> <p>Performance metrics of the model (e.g., mAP, accuracy).</p> <code>latencies</code> <code>Optional[list[dict]]</code> <p>Inference latency measurements across different configurations.</p> <code>classes</code> <code>Optional[list[str]]</code> <p>List of class names the model can detect or segment.</p> <code>im_size</code> <code>Optional[int]</code> <p>Input image size the model expects.</p> <code>hyperparameters</code> <code>Optional[Hyperparameters]</code> <p>Training hyperparameters used.</p> <code>training_info</code> <code>Optional[TrainingInfo]</code> <p>Information about the training process.</p> <code>location</code> <code>Optional[str]</code> <p>Storage location of the model.</p> <code>dataset</code> <code>Optional[DatasetPreview]</code> <p>Information about the dataset used for training.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ModelMetadata(FocoosBaseModel):\n    \"\"\"Complete metadata for a Focoos model.\n\n    This class contains comprehensive information about a model in the Focoos platform,\n    including its identification, configuration, performance metrics, and training details.\n\n    Attributes:\n        ref (str): Unique reference ID for the model.\n        name (str): Human-readable name of the model.\n        description (Optional[str]): Optional description of the model's purpose or capabilities.\n        owner_ref (str): Reference ID of the model owner.\n        focoos_model (str): The base model architecture used.\n        task (FocoosTask): The task type the model is designed for (e.g., DETECTION, SEMSEG).\n        created_at (datetime): Timestamp when the model was created.\n        updated_at (datetime): Timestamp when the model was last updated.\n        status (ModelStatus): Current status of the model (e.g., TRAINING, DEPLOYED).\n        metrics (Optional[dict]): Performance metrics of the model (e.g., mAP, accuracy).\n        latencies (Optional[list[dict]]): Inference latency measurements across different configurations.\n        classes (Optional[list[str]]): List of class names the model can detect or segment.\n        im_size (Optional[int]): Input image size the model expects.\n        hyperparameters (Optional[Hyperparameters]): Training hyperparameters used.\n        training_info (Optional[TrainingInfo]): Information about the training process.\n        location (Optional[str]): Storage location of the model.\n        dataset (Optional[DatasetPreview]): Information about the dataset used for training.\n    \"\"\"\n\n    ref: str\n    name: str\n    description: Optional[str] = None\n    owner_ref: str\n    focoos_model: str\n    task: FocoosTask\n    created_at: datetime\n    updated_at: datetime\n    status: ModelStatus\n    metrics: Optional[dict] = None\n    latencies: Optional[list[dict]] = None\n    classes: Optional[list[str]] = None\n    im_size: Optional[int] = None\n    hyperparameters: Optional[Hyperparameters] = None\n    training_info: Optional[TrainingInfo] = None\n    location: Optional[str] = None\n    dataset: Optional[DatasetPreview] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelNotFound","title":"<code>ModelNotFound</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when a requested model is not found.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ModelNotFound(Exception):\n    \"\"\"Exception raised when a requested model is not found.\"\"\"\n\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelPreview","title":"<code>ModelPreview</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Preview information for a Focoos model.</p> <p>This class provides a lightweight preview of model information in the Focoos platform, containing essential details like reference ID, name, task type, and status.</p> <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>Unique reference ID for the model.</p> <code>name</code> <code>str</code> <p>Human-readable name of the model.</p> <code>task</code> <code>FocoosTask</code> <p>The computer vision task this model is designed for.</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the model's purpose or capabilities.</p> <code>status</code> <code>ModelStatus</code> <p>Current status of the model (e.g., training, ready, failed).</p> <code>focoos_model</code> <code>str</code> <p>The base model architecture identifier.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ModelPreview(FocoosBaseModel):\n    \"\"\"Preview information for a Focoos model.\n\n    This class provides a lightweight preview of model information in the Focoos platform,\n    containing essential details like reference ID, name, task type, and status.\n\n    Attributes:\n        ref (str): Unique reference ID for the model.\n        name (str): Human-readable name of the model.\n        task (FocoosTask): The computer vision task this model is designed for.\n        description (Optional[str]): Optional description of the model's purpose or capabilities.\n        status (ModelStatus): Current status of the model (e.g., training, ready, failed).\n        focoos_model (str): The base model architecture identifier.\n    \"\"\"\n\n    ref: str\n    name: str\n    task: FocoosTask\n    description: Optional[str] = None\n    status: ModelStatus\n    focoos_model: str\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelStatus","title":"<code>ModelStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of a Focoos model during its lifecycle.</p> Values <ul> <li>CREATED: Model has been created</li> <li>TRAINING_STARTING: Training is about to start</li> <li>TRAINING_RUNNING: Training is in progress</li> <li>TRAINING_ERROR: Training encountered an error</li> <li>TRAINING_COMPLETED: Training finished successfully</li> <li>TRAINING_STOPPED: Training was stopped</li> <li>DEPLOYED: Model is deployed</li> <li>DEPLOY_ERROR: Deployment encountered an error</li> </ul> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\nmodel = focoos.get_remote_model(\"my-model\")\n\nif model.status == ModelStatus.DEPLOYED:\n    print(\"Model is deployed and ready for inference\")\nelif model.status == ModelStatus.TRAINING_RUNNING:\n    print(\"Model is currently training\")\nelif model.status == ModelStatus.TRAINING_ERROR:\n    print(\"Model training encountered an error\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class ModelStatus(str, Enum):\n    \"\"\"Status of a Focoos model during its lifecycle.\n\n    Values:\n        - CREATED: Model has been created\n        - TRAINING_STARTING: Training is about to start\n        - TRAINING_RUNNING: Training is in progress\n        - TRAINING_ERROR: Training encountered an error\n        - TRAINING_COMPLETED: Training finished successfully\n        - TRAINING_STOPPED: Training was stopped\n        - DEPLOYED: Model is deployed\n        - DEPLOY_ERROR: Deployment encountered an error\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n        model = focoos.get_remote_model(\"my-model\")\n\n        if model.status == ModelStatus.DEPLOYED:\n            print(\"Model is deployed and ready for inference\")\n        elif model.status == ModelStatus.TRAINING_RUNNING:\n            print(\"Model is currently training\")\n        elif model.status == ModelStatus.TRAINING_ERROR:\n            print(\"Model training encountered an error\")\n        ```\n    \"\"\"\n\n    CREATED = \"CREATED\"\n    TRAINING_STARTING = \"TRAINING_STARTING\"\n    TRAINING_RUNNING = \"TRAINING_RUNNING\"\n    TRAINING_ERROR = \"TRAINING_ERROR\"\n    TRAINING_COMPLETED = \"TRAINING_COMPLETED\"\n    TRAINING_STOPPED = \"TRAINING_STOPPED\"\n    DEPLOYED = \"DEPLOYED\"\n    DEPLOY_ERROR = \"DEPLOY_ERROR\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.OnnxRuntimeOpts","title":"<code>OnnxRuntimeOpts</code>  <code>dataclass</code>","text":"<p>ONNX runtime configuration options.</p> <p>This class provides configuration options for the ONNX runtime used for model inference.</p> <p>Attributes:</p> Name Type Description <code>fp16</code> <code>Optional[bool]</code> <p>Enable FP16 precision. Default is False.</p> <code>cuda</code> <code>Optional[bool]</code> <p>Enable CUDA acceleration for GPU inference. Default is False.</p> <code>vino</code> <code>Optional[bool]</code> <p>Enable OpenVINO acceleration for Intel hardware. Default is False.</p> <code>verbose</code> <code>Optional[bool]</code> <p>Enable verbose logging during inference. Default is False.</p> <code>trt</code> <code>Optional[bool]</code> <p>Enable TensorRT acceleration for NVIDIA GPUs. Default is False.</p> <code>coreml</code> <code>Optional[bool]</code> <p>Enable CoreML acceleration for Apple hardware. Default is False.</p> <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations to run before benchmarking. Default is 0.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass OnnxRuntimeOpts:\n    \"\"\"ONNX runtime configuration options.\n\n    This class provides configuration options for the ONNX runtime used for model inference.\n\n    Attributes:\n        fp16 (Optional[bool]): Enable FP16 precision. Default is False.\n        cuda (Optional[bool]): Enable CUDA acceleration for GPU inference. Default is False.\n        vino (Optional[bool]): Enable OpenVINO acceleration for Intel hardware. Default is False.\n        verbose (Optional[bool]): Enable verbose logging during inference. Default is False.\n        trt (Optional[bool]): Enable TensorRT acceleration for NVIDIA GPUs. Default is False.\n        coreml (Optional[bool]): Enable CoreML acceleration for Apple hardware. Default is False.\n        warmup_iter (int): Number of warmup iterations to run before benchmarking. Default is 0.\n\n    \"\"\"\n\n    fp16: Optional[bool] = False\n    cuda: Optional[bool] = False\n    vino: Optional[bool] = False\n    verbose: Optional[bool] = False\n    trt: Optional[bool] = False\n    coreml: Optional[bool] = False\n    warmup_iter: int = 0\n</code></pre>"},{"location":"api/ports/#focoos.ports.Quotas","title":"<code>Quotas</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Usage quotas and limits for a user account.</p> <p>Attributes:</p> Name Type Description <code>total_inferences</code> <code>int</code> <p>Total number of inferences allowed.</p> <code>max_inferences</code> <code>int</code> <p>Maximum number of inferences allowed.</p> <code>used_storage_gb</code> <code>float</code> <p>Used storage in gigabytes.</p> <code>max_storage_gb</code> <code>float</code> <p>Maximum storage in gigabytes.</p> <code>active_training_jobs</code> <code>list[str]</code> <p>List of active training job IDs.</p> <code>max_active_training_jobs</code> <code>int</code> <p>Maximum number of active training jobs allowed.</p> Source code in <code>focoos/ports.py</code> <pre><code>class Quotas(FocoosBaseModel):\n    \"\"\"Usage quotas and limits for a user account.\n\n    Attributes:\n        total_inferences (int): Total number of inferences allowed.\n        max_inferences (int): Maximum number of inferences allowed.\n        used_storage_gb (float): Used storage in gigabytes.\n        max_storage_gb (float): Maximum storage in gigabytes.\n        active_training_jobs (list[str]): List of active training job IDs.\n        max_active_training_jobs (int): Maximum number of active training jobs allowed.\n    \"\"\"\n\n    # INFERENCE\n    total_inferences: int\n    max_inferences: int\n    # STORAGE\n    used_storage_gb: float\n    max_storage_gb: float\n    # TRAINING\n    active_training_jobs: list[str]\n    max_active_training_jobs: int\n\n    # ML_G4DN_XLARGE TRAINING HOURS\n    used_mlg4dnxlarge_training_jobs_hours: float\n    max_mlg4dnxlarge_training_jobs_hours: float\n</code></pre>"},{"location":"api/ports/#focoos.ports.RuntimeTypes","title":"<code>RuntimeTypes</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Available runtime configurations for model inference.</p> Values <ul> <li>ONNX_CUDA32: ONNX with CUDA FP32</li> <li>ONNX_TRT32: ONNX with TensorRT FP32</li> <li>ONNX_TRT16: ONNX with TensorRT FP16</li> <li>ONNX_CPU: ONNX on CPU</li> <li>ONNX_COREML: ONNX with CoreML</li> <li>TORCHSCRIPT_32: TorchScript FP32</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class RuntimeTypes(str, Enum):\n    \"\"\"Available runtime configurations for model inference.\n\n    Values:\n        - ONNX_CUDA32: ONNX with CUDA FP32\n        - ONNX_TRT32: ONNX with TensorRT FP32\n        - ONNX_TRT16: ONNX with TensorRT FP16\n        - ONNX_CPU: ONNX on CPU\n        - ONNX_COREML: ONNX with CoreML\n        - TORCHSCRIPT_32: TorchScript FP32\n\n    \"\"\"\n\n    ONNX_CUDA32 = \"onnx_cuda32\"\n    ONNX_TRT32 = \"onnx_trt32\"\n    ONNX_TRT16 = \"onnx_trt16\"\n    ONNX_CPU = \"onnx_cpu\"\n    ONNX_COREML = \"onnx_coreml\"\n    TORCHSCRIPT_32 = \"torchscript_32\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.SystemInfo","title":"<code>SystemInfo</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>System information including hardware and software details.</p> Source code in <code>focoos/ports.py</code> <pre><code>class SystemInfo(FocoosBaseModel):\n    \"\"\"System information including hardware and software details.\"\"\"\n\n    focoos_host: Optional[str] = None\n    system: Optional[str] = None\n    system_name: Optional[str] = None\n    cpu_type: Optional[str] = None\n    cpu_cores: Optional[int] = None\n    memory_gb: Optional[float] = None\n    memory_used_percentage: Optional[float] = None\n    available_providers: Optional[list[str]] = None\n    disk_space_total_gb: Optional[float] = None\n    disk_space_used_percentage: Optional[float] = None\n    gpu_info: Optional[GPUInfo] = None\n    packages_versions: Optional[dict[str, str]] = None\n    environment: Optional[dict[str, str]] = None\n\n    def pretty_print(self):\n        print(\"================ SYSTEM INFO ====================\")\n        for key, value in self.model_dump().items():\n            if key == \"gpu_info\" and value is not None:\n                print(f\"{key}:\")\n                print(f\"  - gpu_count: {value.get('gpu_count')}\")\n                print(f\"  - gpu_driver: {value.get('gpu_driver')}\")\n                print(f\"  - gpu_cuda_version: {value.get('gpu_cuda_version')}\")\n                if value.get(\"devices\"):\n                    print(\"  - devices:\")\n                    for device in value.get(\"devices\", []):\n                        print(f\"    - GPU {device.get('gpu_id')}:\")\n                        for device_key, device_value in device.items():\n                            if device_key != \"gpu_id\":\n                                print(f\"      - {device_key}: {device_value}\")\n            elif isinstance(value, list):\n                print(f\"{key}:\")\n                for item in value:\n                    print(f\"  - {item}\")\n            elif isinstance(value, dict) and key == \"packages_versions\":  # Special formatting for packages_versions\n                print(f\"{key}:\")\n                for pkg_name, pkg_version in value.items():\n                    print(f\"  - {pkg_name}: {pkg_version}\")\n            elif isinstance(value, dict) and key == \"environment\":  # Special formatting for environment\n                print(f\"{key}:\")\n                for env_key, env_value in value.items():\n                    print(f\"  - {env_key}: {env_value}\")\n            else:\n                print(f\"{key}: {value}\")\n        print(\"================================================\")\n</code></pre>"},{"location":"api/ports/#focoos.ports.TorchscriptRuntimeOpts","title":"<code>TorchscriptRuntimeOpts</code>  <code>dataclass</code>","text":"<p>TorchScript runtime configuration options.</p> <p>This class provides configuration options for the TorchScript runtime used for model inference.</p> <p>Attributes:</p> Name Type Description <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations to run before benchmarking. Default is 0.</p> <code>optimize_for_inference</code> <code>bool</code> <p>Enable inference optimizations. Default is True.</p> <code>set_fusion_strategy</code> <code>bool</code> <p>Enable operator fusion. Default is True.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass TorchscriptRuntimeOpts:\n    \"\"\"TorchScript runtime configuration options.\n\n    This class provides configuration options for the TorchScript runtime used for model inference.\n\n    Attributes:\n        warmup_iter (int): Number of warmup iterations to run before benchmarking. Default is 0.\n        optimize_for_inference (bool): Enable inference optimizations. Default is True.\n        set_fusion_strategy (bool): Enable operator fusion. Default is True.\n    \"\"\"\n\n    warmup_iter: int = 0\n    optimize_for_inference: bool = True\n    set_fusion_strategy: bool = True\n</code></pre>"},{"location":"api/ports/#focoos.ports.TrainInstance","title":"<code>TrainInstance</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Available training instance types.</p> Values <ul> <li>ML_G4DN_XLARGE: ml.g4dn.xlarge instance, Nvidia Tesla T4, 16GB RAM, 4vCPU</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class TrainInstance(str, Enum):\n    \"\"\"Available training instance types.\n\n    Values:\n        - ML_G4DN_XLARGE: ml.g4dn.xlarge instance, Nvidia Tesla T4, 16GB RAM, 4vCPU\n    \"\"\"\n\n    ML_G4DN_XLARGE = \"ml.g4dn.xlarge\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.TrainingInfo","title":"<code>TrainingInfo</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Information about a model's training process.</p> <p>This class contains details about the training job configuration, status, and timing.</p> <p>Attributes:</p> Name Type Description <code>algorithm_name</code> <code>str</code> <p>The name of the training algorithm used.</p> <code>instance_type</code> <code>Optional[str]</code> <p>The compute instance type used for training.</p> <code>volume_size</code> <code>Optional[int]</code> <p>The storage volume size in GB allocated for the training job.</p> <code>max_runtime_in_seconds</code> <code>Optional[int]</code> <p>Maximum allowed runtime for the training job in seconds.</p> <code>main_status</code> <code>Optional[str]</code> <p>The primary status of the training job (e.g., \"InProgress\", \"Completed\").</p> <code>secondary_status</code> <code>Optional[str]</code> <p>Additional status information about the training job.</p> <code>failure_reason</code> <code>Optional[str]</code> <p>Description of why the training job failed, if applicable.</p> <code>elapsed_time</code> <code>Optional[int]</code> <p>Time elapsed since the start of the training job in seconds.</p> <code>status_transitions</code> <code>list[dict]</code> <p>List of status change events during the training process.</p> <code>start_time</code> <code>Optional[datetime]</code> <p>Timestamp when the training job started.</p> <code>end_time</code> <code>Optional[datetime]</code> <p>Timestamp when the training job completed or failed.</p> <code>artifact_location</code> <code>Optional[str]</code> <p>Storage location of the training artifacts and model outputs.</p> Source code in <code>focoos/ports.py</code> <pre><code>class TrainingInfo(FocoosBaseModel):\n    \"\"\"Information about a model's training process.\n\n    This class contains details about the training job configuration, status, and timing.\n\n    Attributes:\n        algorithm_name: The name of the training algorithm used.\n        instance_type: The compute instance type used for training.\n        volume_size: The storage volume size in GB allocated for the training job.\n        max_runtime_in_seconds: Maximum allowed runtime for the training job in seconds.\n        main_status: The primary status of the training job (e.g., \"InProgress\", \"Completed\").\n        secondary_status: Additional status information about the training job.\n        failure_reason: Description of why the training job failed, if applicable.\n        elapsed_time: Time elapsed since the start of the training job in seconds.\n        status_transitions: List of status change events during the training process.\n        start_time: Timestamp when the training job started.\n        end_time: Timestamp when the training job completed or failed.\n        artifact_location: Storage location of the training artifacts and model outputs.\n    \"\"\"\n\n    algorithm_name: str\n    instance_type: Optional[str] = None\n    volume_size: Optional[int] = 100\n    max_runtime_in_seconds: Optional[int] = 36000\n    main_status: Optional[str] = None\n    secondary_status: Optional[str] = None\n    failure_reason: Optional[str] = None\n    elapsed_time: Optional[int] = None\n    status_transitions: list[dict] = []\n    start_time: Optional[datetime] = None\n    end_time: Optional[datetime] = None\n    artifact_location: Optional[str] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.User","title":"<code>User</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>User account information.</p> <p>This class represents a user account in the Focoos platform, containing personal information, API key, and usage quotas.</p> <p>Attributes:</p> Name Type Description <code>email</code> <code>str</code> <p>The user's email address.</p> <code>created_at</code> <code>datetime</code> <p>When the user account was created.</p> <code>updated_at</code> <code>datetime</code> <p>When the user account was last updated.</p> <code>company</code> <code>Optional[str]</code> <p>The user's company name, if provided.</p> <code>api_key</code> <code>ApiKey</code> <p>The API key associated with the user account.</p> <code>quotas</code> <code>Quotas</code> <p>Usage quotas and limits for the user account.</p> Source code in <code>focoos/ports.py</code> <pre><code>class User(FocoosBaseModel):\n    \"\"\"User account information.\n\n    This class represents a user account in the Focoos platform, containing\n    personal information, API key, and usage quotas.\n\n    Attributes:\n        email (str): The user's email address.\n        created_at (datetime): When the user account was created.\n        updated_at (datetime): When the user account was last updated.\n        company (Optional[str]): The user's company name, if provided.\n        api_key (ApiKey): The API key associated with the user account.\n        quotas (Quotas): Usage quotas and limits for the user account.\n    \"\"\"\n\n    email: str\n    created_at: datetime\n    updated_at: datetime\n    company: Optional[str] = None\n    api_key: ApiKey\n    quotas: Quotas\n</code></pre>"},{"location":"api/remote_dataset/","title":"RemoteDataset","text":"<p>A class to manage remote datasets through the Focoos API.</p> <p>This class provides functionality to interact with datasets stored remotely, including uploading, downloading, and managing dataset data.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>The reference identifier for the dataset.</p> required <code>api_client</code> <code>ApiClient</code> <p>The API client instance for making requests.</p> required <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>The dataset reference identifier.</p> <code>api_client</code> <code>ApiClient</code> <p>The API client instance.</p> <code>metadata</code> <code>DatasetPreview</code> <p>The dataset metadata.</p> Source code in <code>focoos/remote_dataset.py</code> <pre><code>class RemoteDataset:\n    \"\"\"\n    A class to manage remote datasets through the Focoos API.\n\n    This class provides functionality to interact with datasets stored remotely,\n    including uploading, downloading, and managing dataset data.\n\n    Args:\n        ref (str): The reference identifier for the dataset.\n        api_client (ApiClient): The API client instance for making requests.\n\n    Attributes:\n        ref (str): The dataset reference identifier.\n        api_client (ApiClient): The API client instance.\n        metadata (DatasetPreview): The dataset metadata.\n    \"\"\"\n\n    def __init__(self, ref: str, api_client: ApiClient):\n        self.ref = ref\n        self.api_client = api_client\n        self.metadata: DatasetPreview = self.get_info()\n\n    def get_info(self) -&gt; DatasetPreview:\n        \"\"\"\n        Retrieves the dataset information from the API.\n\n        Returns:\n            DatasetPreview: The dataset preview information.\n        \"\"\"\n        res = self.api_client.get(f\"datasets/{self.ref}\")\n        return DatasetPreview.from_json(res.json())\n\n    def upload_data(self, path: str) -&gt; Optional[DatasetSpec]:\n        \"\"\"\n        Uploads dataset data from a local zip file to the remote storage.\n\n        Args:\n            path (str): Local path to the zip file containing dataset data.\n\n        Returns:\n            Optional[DatasetSpec]: The dataset specification after successful upload.\n\n        Raises:\n            FileNotFoundError: If the specified file does not exist.\n            ValueError: If the file is not a zip file or upload fails.\n        \"\"\"\n        if not path.endswith(\".zip\"):\n            raise ValueError(\"Dataset must be .zip compressed\")\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"File not found: {path}\")\n\n        file_name = os.path.basename(path)\n        file_size = os.path.getsize(path)\n        file_size_mb = file_size / (1024 * 1024)\n        logger.info(f\"\ud83d\udd17 Requesting upload url for {file_name} of size {file_size_mb:.2f} MB\")\n        presigned_url = self.api_client.post(\n            f\"datasets/{self.ref}/generate-upload-url\",\n            data={\"file_size_bytes\": file_size, \"file_name\": file_name},\n        )\n        if presigned_url.status_code != 200:\n            raise ValueError(f\"Failed to generate upload url: {presigned_url.status_code} {presigned_url.text}\")\n        presigned_url = presigned_url.json()\n        fields = {k: v for k, v in presigned_url[\"fields\"].items()}\n        logger.info(f\"\ud83d\udce4 Uploading file {file_name}..\")\n        fields[\"file\"] = (file_name, open(path, \"rb\"), \"application/zip\")\n\n        res = self.api_client.external_post(\n            presigned_url[\"url\"],\n            files=fields,\n            data=presigned_url[\"fields\"],\n            stream=True,\n        )\n        logger.info(\"\u2705 Upload file done.\")\n        if res.status_code not in [200, 201, 204]:\n            raise ValueError(f\"Failed to upload dataset: {res.status_code} {res.text}\")\n\n        logger.info(\"\ud83d\udd17 Validating dataset..\")\n        complete_upload = self.api_client.post(\n            f\"datasets/{self.ref}/complete-upload\",\n        )\n        if complete_upload.status_code not in [200, 201, 204]:\n            raise ValueError(f\"Failed to validate dataset: {complete_upload.status_code} {complete_upload.text}\")\n        self.metadata = self.get_info()\n        logger.info(f\"\u2705 Dataset validated! =&gt; {self.metadata.spec}\")\n        return self.metadata.spec\n\n    def download_data(self, path: str):\n        \"\"\"\n        Downloads the dataset data to a local path.\n\n        Args:\n            path (str): Local path where the dataset should be downloaded.\n\n        Returns:\n            str: The path where the file was downloaded.\n\n        Raises:\n            ValueError: If the download fails.\n        \"\"\"\n        res = self.api_client.get(f\"datasets/{self.ref}/download\")\n        if res.status_code != 200:\n            raise ValueError(f\"Failed to download dataset data: {res.status_code} {res.text}\")\n        logger.info(f\"\ud83d\udce5 Downloading dataset data to {path}\")\n        url = res.json()[\"download_uri\"]\n        path = self.api_client.download_file(url, path)\n        logger.info(f\"\u2705 Dataset data downloaded to {path}\")\n        return path\n\n    def delete(self):\n        \"\"\"\n        Deletes the entire dataset from the remote storage.\n\n        Raises:\n            Exception: If the deletion fails.\n        \"\"\"\n        try:\n            res = self.api_client.delete(f\"datasets/{self.ref}\")\n            res.raise_for_status()\n            logger.warning(f\"Deleted dataset {self.ref}\")\n        except Exception as e:\n            logger.error(f\"Failed to delete dataset {self.ref}: {e}\")\n            raise e\n\n    def delete_data(self):\n        \"\"\"\n        Deletes only the data content of the dataset while preserving metadata.\n\n        Updates the metadata after successful deletion.\n        \"\"\"\n        try:\n            res = self.api_client.delete(f\"datasets/{self.ref}/data\")\n\n            res.raise_for_status()\n            new_metadata = DatasetPreview.from_json(res.json())\n            self.metadata = new_metadata\n            logger.warning(f\"Deleted dataset data {self.ref}\")\n        except Exception as e:\n            logger.error(f\"Failed to delete dataset data {self.ref}: {e}\")\n</code></pre>"},{"location":"api/remote_dataset/#focoos.remote_dataset.RemoteDataset.delete","title":"<code>delete()</code>","text":"<p>Deletes the entire dataset from the remote storage.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the deletion fails.</p> Source code in <code>focoos/remote_dataset.py</code> <pre><code>def delete(self):\n    \"\"\"\n    Deletes the entire dataset from the remote storage.\n\n    Raises:\n        Exception: If the deletion fails.\n    \"\"\"\n    try:\n        res = self.api_client.delete(f\"datasets/{self.ref}\")\n        res.raise_for_status()\n        logger.warning(f\"Deleted dataset {self.ref}\")\n    except Exception as e:\n        logger.error(f\"Failed to delete dataset {self.ref}: {e}\")\n        raise e\n</code></pre>"},{"location":"api/remote_dataset/#focoos.remote_dataset.RemoteDataset.delete_data","title":"<code>delete_data()</code>","text":"<p>Deletes only the data content of the dataset while preserving metadata.</p> <p>Updates the metadata after successful deletion.</p> Source code in <code>focoos/remote_dataset.py</code> <pre><code>def delete_data(self):\n    \"\"\"\n    Deletes only the data content of the dataset while preserving metadata.\n\n    Updates the metadata after successful deletion.\n    \"\"\"\n    try:\n        res = self.api_client.delete(f\"datasets/{self.ref}/data\")\n\n        res.raise_for_status()\n        new_metadata = DatasetPreview.from_json(res.json())\n        self.metadata = new_metadata\n        logger.warning(f\"Deleted dataset data {self.ref}\")\n    except Exception as e:\n        logger.error(f\"Failed to delete dataset data {self.ref}: {e}\")\n</code></pre>"},{"location":"api/remote_dataset/#focoos.remote_dataset.RemoteDataset.download_data","title":"<code>download_data(path)</code>","text":"<p>Downloads the dataset data to a local path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Local path where the dataset should be downloaded.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The path where the file was downloaded.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the download fails.</p> Source code in <code>focoos/remote_dataset.py</code> <pre><code>def download_data(self, path: str):\n    \"\"\"\n    Downloads the dataset data to a local path.\n\n    Args:\n        path (str): Local path where the dataset should be downloaded.\n\n    Returns:\n        str: The path where the file was downloaded.\n\n    Raises:\n        ValueError: If the download fails.\n    \"\"\"\n    res = self.api_client.get(f\"datasets/{self.ref}/download\")\n    if res.status_code != 200:\n        raise ValueError(f\"Failed to download dataset data: {res.status_code} {res.text}\")\n    logger.info(f\"\ud83d\udce5 Downloading dataset data to {path}\")\n    url = res.json()[\"download_uri\"]\n    path = self.api_client.download_file(url, path)\n    logger.info(f\"\u2705 Dataset data downloaded to {path}\")\n    return path\n</code></pre>"},{"location":"api/remote_dataset/#focoos.remote_dataset.RemoteDataset.get_info","title":"<code>get_info()</code>","text":"<p>Retrieves the dataset information from the API.</p> <p>Returns:</p> Name Type Description <code>DatasetPreview</code> <code>DatasetPreview</code> <p>The dataset preview information.</p> Source code in <code>focoos/remote_dataset.py</code> <pre><code>def get_info(self) -&gt; DatasetPreview:\n    \"\"\"\n    Retrieves the dataset information from the API.\n\n    Returns:\n        DatasetPreview: The dataset preview information.\n    \"\"\"\n    res = self.api_client.get(f\"datasets/{self.ref}\")\n    return DatasetPreview.from_json(res.json())\n</code></pre>"},{"location":"api/remote_dataset/#focoos.remote_dataset.RemoteDataset.upload_data","title":"<code>upload_data(path)</code>","text":"<p>Uploads dataset data from a local zip file to the remote storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Local path to the zip file containing dataset data.</p> required <p>Returns:</p> Type Description <code>Optional[DatasetSpec]</code> <p>Optional[DatasetSpec]: The dataset specification after successful upload.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>ValueError</code> <p>If the file is not a zip file or upload fails.</p> Source code in <code>focoos/remote_dataset.py</code> <pre><code>def upload_data(self, path: str) -&gt; Optional[DatasetSpec]:\n    \"\"\"\n    Uploads dataset data from a local zip file to the remote storage.\n\n    Args:\n        path (str): Local path to the zip file containing dataset data.\n\n    Returns:\n        Optional[DatasetSpec]: The dataset specification after successful upload.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        ValueError: If the file is not a zip file or upload fails.\n    \"\"\"\n    if not path.endswith(\".zip\"):\n        raise ValueError(\"Dataset must be .zip compressed\")\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File not found: {path}\")\n\n    file_name = os.path.basename(path)\n    file_size = os.path.getsize(path)\n    file_size_mb = file_size / (1024 * 1024)\n    logger.info(f\"\ud83d\udd17 Requesting upload url for {file_name} of size {file_size_mb:.2f} MB\")\n    presigned_url = self.api_client.post(\n        f\"datasets/{self.ref}/generate-upload-url\",\n        data={\"file_size_bytes\": file_size, \"file_name\": file_name},\n    )\n    if presigned_url.status_code != 200:\n        raise ValueError(f\"Failed to generate upload url: {presigned_url.status_code} {presigned_url.text}\")\n    presigned_url = presigned_url.json()\n    fields = {k: v for k, v in presigned_url[\"fields\"].items()}\n    logger.info(f\"\ud83d\udce4 Uploading file {file_name}..\")\n    fields[\"file\"] = (file_name, open(path, \"rb\"), \"application/zip\")\n\n    res = self.api_client.external_post(\n        presigned_url[\"url\"],\n        files=fields,\n        data=presigned_url[\"fields\"],\n        stream=True,\n    )\n    logger.info(\"\u2705 Upload file done.\")\n    if res.status_code not in [200, 201, 204]:\n        raise ValueError(f\"Failed to upload dataset: {res.status_code} {res.text}\")\n\n    logger.info(\"\ud83d\udd17 Validating dataset..\")\n    complete_upload = self.api_client.post(\n        f\"datasets/{self.ref}/complete-upload\",\n    )\n    if complete_upload.status_code not in [200, 201, 204]:\n        raise ValueError(f\"Failed to validate dataset: {complete_upload.status_code} {complete_upload.text}\")\n    self.metadata = self.get_info()\n    logger.info(f\"\u2705 Dataset validated! =&gt; {self.metadata.spec}\")\n    return self.metadata.spec\n</code></pre>"},{"location":"api/remote_model/","title":"RemoteModel","text":"<p>RemoteModel Module</p> <p>This module provides a class to manage remote models in the Focoos ecosystem. It supports various functionalities including model training, deployment, inference, and monitoring.</p> <p>Classes:</p> Name Description <code>RemoteModel</code> <p>A class for interacting with remote models, managing their lifecycle,          and performing inference.</p> <p>Functions:</p> Name Description <code>__init__</code> <p>Initializes the RemoteModel instance.</p> <code>get_info</code> <p>Retrieves model metadata.</p> <code>train</code> <p>Initiates model training.</p> <code>train_info</code> <p>Retrieves training status.</p> <code>train_logs</code> <p>Retrieves training logs.</p> <code>metrics</code> <p>Retrieves model metrics.</p>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel","title":"<code>RemoteModel</code>","text":"<p>Represents a remote model in the Focoos platform.</p> <p>Attributes:</p> Name Type Description <code>model_ref</code> <code>str</code> <p>Reference ID for the model.</p> <code>api_client</code> <code>ApiClient</code> <p>Client for making HTTP requests.</p> <code>max_deploy_wait</code> <code>int</code> <p>Maximum wait time for model deployment.</p> <code>metadata</code> <code>ModelMetadata</code> <p>Metadata of the model.</p> <code>label_annotator</code> <code>LabelAnnotator</code> <p>Annotator for adding labels to images.</p> <code>box_annotator</code> <code>BoxAnnotator</code> <p>Annotator for drawing bounding boxes.</p> <code>mask_annotator</code> <code>MaskAnnotator</code> <p>Annotator for drawing masks on images.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>class RemoteModel:\n    \"\"\"\n    Represents a remote model in the Focoos platform.\n\n    Attributes:\n        model_ref (str): Reference ID for the model.\n        api_client (ApiClient): Client for making HTTP requests.\n        max_deploy_wait (int): Maximum wait time for model deployment.\n        metadata (ModelMetadata): Metadata of the model.\n        label_annotator (LabelAnnotator): Annotator for adding labels to images.\n        box_annotator (sv.BoxAnnotator): Annotator for drawing bounding boxes.\n        mask_annotator (sv.MaskAnnotator): Annotator for drawing masks on images.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_ref: str,\n        api_client: ApiClient,\n    ):\n        \"\"\"\n        Initialize the RemoteModel instance.\n\n        Args:\n            model_ref (str): Reference ID for the model.\n            api_client (ApiClient): HTTP client instance for communication.\n\n        Raises:\n            ValueError: If model metadata retrieval fails.\n        \"\"\"\n        self.model_ref = model_ref\n        self.api_client = api_client\n        self.metadata: ModelMetadata = self.get_info()\n\n        self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n        self.box_annotator = sv.BoxAnnotator()\n        self.mask_annotator = sv.MaskAnnotator()\n        logger.info(\n            f\"[RemoteModel]: ref: {self.model_ref} name: {self.metadata.name} description: {self.metadata.description} status: {self.metadata.status}\"\n        )\n\n    def get_info(self) -&gt; ModelMetadata:\n        \"\"\"\n        Retrieve model metadata.\n\n        Returns:\n            ModelMetadata: Metadata of the model.\n\n        Raises:\n            ValueError: If the request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos, RemoteModel\n\n            focoos = Focoos()\n            model = focoos.get_remote_model(model_ref=\"&lt;model_ref&gt;\")\n            model_info = model.get_info()\n            ```\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n        self.metadata = ModelMetadata(**res.json())\n        return self.metadata\n\n    def train(\n        self,\n        dataset_ref: str,\n        hyperparameters: Hyperparameters,\n        instance_type: TrainInstance = TrainInstance.ML_G4DN_XLARGE,\n        volume_size: int = 50,\n        max_runtime_in_seconds: int = 36000,\n    ) -&gt; dict | None:\n        \"\"\"\n        Initiate the training of a remote model on the Focoos platform.\n\n        This method sends a request to the Focoos platform to start the training process for the model\n        referenced by `self.model_ref`. It requires a dataset reference and hyperparameters for training,\n        as well as optional configuration options for the instance type, volume size, and runtime.\n\n        Args:\n            dataset_ref (str): The reference ID of the dataset to be used for training.\n            hyperparameters (Hyperparameters): A structure containing the hyperparameters for the training process.\n            instance_type (TrainInstance, optional): The type of training instance to use. Defaults to TrainInstance.ML_G4DN_XLARGE.\n            volume_size (int, optional): The size of the disk volume (in GB) for the training instance. Defaults to 50.\n            max_runtime_in_seconds (int, optional): The maximum runtime for training in seconds. Defaults to 36000.\n\n        Returns:\n            dict: A dictionary containing the response from the training initiation request. The content depends on the Focoos platform's response.\n\n        Raises:\n            ValueError: If the request to start training fails (e.g., due to incorrect parameters or server issues).\n        \"\"\"\n        res = self.api_client.post(\n            f\"models/{self.model_ref}/train\",\n            data={\n                \"dataset_ref\": dataset_ref,\n                \"instance_type\": instance_type,\n                \"volume_size\": volume_size,\n                \"max_runtime_in_seconds\": max_runtime_in_seconds,\n                \"hyperparameters\": hyperparameters.model_dump(),\n            },\n        )\n        if res.status_code != 200:\n            logger.warning(f\"Failed to train model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to train model: {res.status_code} {res.text}\")\n        return res.json()\n\n    def train_info(self) -&gt; Optional[TrainingInfo]:\n        \"\"\"\n        Retrieve the current status of the model training.\n\n        Sends a request to check the training status of the model referenced by `self.model_ref`.\n\n        Returns:\n            dict: A dictionary containing the training status information.\n\n        Raises:\n            ValueError: If the request to get training status fails.\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}/train/status\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get train status: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get train status: {res.status_code} {res.text}\")\n        return TrainingInfo(**res.json())\n\n    def train_logs(self) -&gt; list[str]:\n        \"\"\"\n        Retrieve the training logs for the model.\n\n        This method sends a request to fetch the logs of the model's training process. If the request\n        is successful (status code 200), it returns the logs as a list of strings. If the request fails,\n        it logs a warning and returns an empty list.\n\n        Returns:\n            list[str]: A list of training logs as strings.\n\n        Raises:\n            None: Returns an empty list if the request fails.\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}/train/logs\")\n        if res.status_code != 200:\n            logger.warning(f\"Failed to get train logs: {res.status_code} {res.text}\")\n            return []\n        return res.json()\n\n    def metrics(self) -&gt; Metrics:  # noqa: F821\n        \"\"\"\n        Retrieve the metrics of the model.\n\n        This method sends a request to fetch the metrics of the model identified by `model_ref`.\n        If the request is successful (status code 200), it returns the metrics as a `Metrics` object.\n        If the request fails, it logs a warning and returns an empty `Metrics` object.\n\n        Returns:\n            Metrics: An object containing the metrics of the model.\n\n        Raises:\n            None: Returns an empty `Metrics` object if the request fails.\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}/metrics\")\n        if res.status_code != 200:\n            logger.warning(f\"Failed to get metrics: {res.status_code} {res.text}\")\n            return Metrics()  # noqa: F821\n        return Metrics(**res.json())\n\n    def _annotate(self, im: np.ndarray, detections: sv.Detections) -&gt; np.ndarray:\n        \"\"\"\n        Annotate an image with detection results.\n\n        This method adds visual annotations to the provided image based on the model's detection results.\n        It handles different tasks (e.g., object detection, semantic segmentation, instance segmentation)\n        and uses the corresponding annotator (bounding box, label, or mask) to draw on the image.\n\n        Args:\n            im (np.ndarray): The image to be annotated, represented as a NumPy array.\n            detections (sv.Detections): The detection results to be annotated, including class IDs and confidence scores.\n\n        Returns:\n            np.ndarray: The annotated image as a NumPy array.\n        \"\"\"\n\n        if len(detections.xyxy) == 0:\n            logger.warning(\"No detections found, skipping annotation\")\n            return im\n        classes = self.metadata.classes\n        if classes is not None:\n            labels = [\n                f\"{classes[int(class_id)]}: {confid * 100:.0f}%\"\n                for class_id, confid in zip(detections.class_id, detections.confidence)\n            ]\n        else:\n            labels = [\n                f\"{str(class_id)}: {confid * 100:.0f}%\"\n                for class_id, confid in zip(detections.class_id, detections.confidence)\n            ]\n        if self.metadata.task == FocoosTask.DETECTION:\n            annotated_im = self.box_annotator.annotate(scene=im.copy(), detections=detections)\n\n            annotated_im = self.label_annotator.annotate(scene=annotated_im, detections=detections, labels=labels)\n        elif self.metadata.task in [\n            FocoosTask.SEMSEG,\n            FocoosTask.INSTANCE_SEGMENTATION,\n        ]:\n            annotated_im = self.mask_annotator.annotate(scene=im.copy(), detections=detections)\n        return annotated_im\n\n    def infer(\n        self,\n        image: Union[str, Path, np.ndarray, bytes],\n        threshold: float = 0.5,\n        annotate: bool = False,\n    ) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n        \"\"\"\n        Perform inference on the provided image using the remote model.\n\n        This method sends an image to the remote model for inference and retrieves the detection results.\n        Optionally, it can annotate the image with the detection results.\n\n        Args:\n            image (Union[str, Path, np.ndarray, bytes]): The image to infer on, which can be a file path,\n                a string representing the path, a NumPy array, or raw bytes.\n            threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n                Detections with confidence scores below this threshold will be discarded.\n            annotate (bool, optional): Whether to annotate the image with the detection results. Defaults to False.\n                If set to True, the method will return the image with bounding boxes or segmentation masks.\n\n        Returns:\n            Tuple[FocoosDetections, Optional[np.ndarray]]:\n                - FocoosDetections: The detection results including class IDs, confidence scores, bounding boxes,\n                  and segmentation masks (if applicable).\n                - Optional[np.ndarray]: The annotated image if `annotate` is True, else None.\n                  This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks.\n\n        Raises:\n            FileNotFoundError: If the provided image file path is invalid.\n            ValueError: If the inference request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n\n            model = focoos.get_remote_model(\"my-model\")\n            results, annotated_image = model.infer(\"image.jpg\", threshold=0.5, annotate=True)\n\n            # Print detection results\n            for det in results.detections:\n                print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n                print(f\"Bounding box: {det.bbox}\")\n                if det.mask:\n                    print(\"Instance segmentation mask included\")\n            ```\n        \"\"\"\n        image_bytes = None\n        if isinstance(image, str) or isinstance(image, Path):\n            if not os.path.exists(image):\n                logger.error(f\"Image file not found: {image}\")\n                raise FileNotFoundError(f\"Image file not found: {image}\")\n            image_bytes = open(image, \"rb\").read()\n        elif isinstance(image, np.ndarray):\n            _, buffer = cv2.imencode(\".jpg\", image)\n            image_bytes = buffer.tobytes()\n        else:\n            image_bytes = image\n        files = {\"file\": (\"image\", image_bytes, \"image/jpeg\")}\n        t0 = time.time()\n        res = self.api_client.post(\n            f\"models/{self.model_ref}/inference?confidence_threshold={threshold}\",\n            files=files,\n        )\n        t1 = time.time()\n        if res.status_code == 200:\n            detections = FocoosDetections(\n                detections=[FocoosDet.from_json(d) for d in res.json().get(\"detections\", [])],\n                latency=res.json().get(\"latency\", None),\n            )\n            logger.debug(\n                f\"Found {len(detections.detections)} detections. Inference Request time: {(t1 - t0) * 1000:.0f}ms\"\n            )\n            preview = None\n            if annotate:\n                im0 = image_loader(image)\n                sv_detections = fai_detections_to_sv(detections, im0.shape[:-1])\n                preview = self._annotate(im0, sv_detections)\n            return detections, preview\n        else:\n            logger.error(f\"Failed to infer: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to infer: {res.status_code} {res.text}\")\n\n    def notebook_monitor_train(self, interval: int = 30, plot_metrics: bool = False, max_runtime: int = 36000) -&gt; None:\n        \"\"\"\n        Monitor the training process in a Jupyter notebook and display metrics.\n\n        Periodically checks the training status and displays metrics in a notebook cell.\n        Clears previous output to maintain a clean view.\n\n        Args:\n            interval (int): Time between status checks in seconds. Must be 30-240. Default: 30\n            plot_metrics (bool): Whether to plot metrics graphs. Default: False\n            max_runtime (int): Maximum monitoring time in seconds. Default: 36000 (10 hours)\n\n        Returns:\n            None\n        \"\"\"\n        from IPython.display import clear_output\n\n        if not 30 &lt;= interval &lt;= 240:\n            raise ValueError(\"Interval must be between 30 and 240 seconds\")\n\n        last_update = self.get_info().updated_at\n        start_time = time.time()\n        status_history = []\n\n        while True:\n            # Get current status\n            model_info = self.get_info()\n            status = model_info.status\n\n            # Clear and display status\n            clear_output(wait=True)\n            status_msg = f\"[Live Monitor {self.metadata.name}] {status.value}\"\n            status_history.append(status_msg)\n            for msg in status_history:\n                logger.info(msg)\n\n            # Show metrics if training completed\n            if status == ModelStatus.TRAINING_COMPLETED:\n                metrics = self.metrics()\n                if metrics.best_valid_metric:\n                    logger.info(f\"Best Checkpoint (iter: {metrics.best_valid_metric.get('iteration', 'N/A')}):\")\n                    for k, v in metrics.best_valid_metric.items():\n                        logger.info(f\"  {k}: {v}\")\n                    visualizer = MetricsVisualizer(metrics)\n                    visualizer.log_metrics()\n                    if plot_metrics:\n                        visualizer.notebook_plot_training_metrics()\n\n            # Update metrics during training\n            if status == ModelStatus.TRAINING_RUNNING and model_info.updated_at &gt; last_update:\n                last_update = model_info.updated_at\n                metrics = self.metrics()\n                visualizer = MetricsVisualizer(metrics)\n                visualizer.log_metrics()\n                if plot_metrics:\n                    visualizer.notebook_plot_training_metrics()\n\n            # Check exit conditions\n            if status not in [ModelStatus.CREATED, ModelStatus.TRAINING_RUNNING, ModelStatus.TRAINING_STARTING]:\n                return\n\n            if time.time() - start_time &gt; max_runtime:\n                logger.warning(f\"Monitoring exceeded {max_runtime} seconds limit\")\n                return\n\n            sleep(interval)\n\n    def stop_training(self) -&gt; None:\n        \"\"\"\n        Stop the training process of the model.\n\n        This method sends a request to stop the training of the model identified by `model_ref`.\n        If the request fails, an error is logged and a `ValueError` is raised.\n\n        Raises:\n            ValueError: If the stop training request fails.\n\n        Logs:\n            - Error message if the request to stop training fails, including the status code and response text.\n\n        Returns:\n            None: This method does not return any value.\n        \"\"\"\n        res = self.api_client.delete(f\"models/{self.model_ref}/train\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get stop training: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get stop training: {res.status_code} {res.text}\")\n\n    def delete_model(self) -&gt; None:\n        \"\"\"\n        Delete the model from the system.\n\n        This method sends a request to delete the model identified by `model_ref`.\n        If the request fails or the status code is not 204 (No Content), an error is logged\n        and a `ValueError` is raised.\n\n        Raises:\n            ValueError: If the delete model request fails or does not return a 204 status code.\n\n        Logs:\n            - Error message if the request to delete the model fails, including the status code and response text.\n\n        Returns:\n            None: This method does not return any value.\n        \"\"\"\n        res = self.api_client.delete(f\"models/{self.model_ref}\")\n        if res.status_code != 204:\n            logger.error(f\"Failed to delete model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to delete model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.__init__","title":"<code>__init__(model_ref, api_client)</code>","text":"<p>Initialize the RemoteModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference ID for the model.</p> required <code>api_client</code> <code>ApiClient</code> <p>HTTP client instance for communication.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If model metadata retrieval fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def __init__(\n    self,\n    model_ref: str,\n    api_client: ApiClient,\n):\n    \"\"\"\n    Initialize the RemoteModel instance.\n\n    Args:\n        model_ref (str): Reference ID for the model.\n        api_client (ApiClient): HTTP client instance for communication.\n\n    Raises:\n        ValueError: If model metadata retrieval fails.\n    \"\"\"\n    self.model_ref = model_ref\n    self.api_client = api_client\n    self.metadata: ModelMetadata = self.get_info()\n\n    self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n    self.box_annotator = sv.BoxAnnotator()\n    self.mask_annotator = sv.MaskAnnotator()\n    logger.info(\n        f\"[RemoteModel]: ref: {self.model_ref} name: {self.metadata.name} description: {self.metadata.description} status: {self.metadata.status}\"\n    )\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.delete_model","title":"<code>delete_model()</code>","text":"<p>Delete the model from the system.</p> <p>This method sends a request to delete the model identified by <code>model_ref</code>. If the request fails or the status code is not 204 (No Content), an error is logged and a <code>ValueError</code> is raised.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the delete model request fails or does not return a 204 status code.</p> Logs <ul> <li>Error message if the request to delete the model fails, including the status code and response text.</li> </ul> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>This method does not return any value.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def delete_model(self) -&gt; None:\n    \"\"\"\n    Delete the model from the system.\n\n    This method sends a request to delete the model identified by `model_ref`.\n    If the request fails or the status code is not 204 (No Content), an error is logged\n    and a `ValueError` is raised.\n\n    Raises:\n        ValueError: If the delete model request fails or does not return a 204 status code.\n\n    Logs:\n        - Error message if the request to delete the model fails, including the status code and response text.\n\n    Returns:\n        None: This method does not return any value.\n    \"\"\"\n    res = self.api_client.delete(f\"models/{self.model_ref}\")\n    if res.status_code != 204:\n        logger.error(f\"Failed to delete model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to delete model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.get_info","title":"<code>get_info()</code>","text":"<p>Retrieve model metadata.</p> <p>Returns:</p> Name Type Description <code>ModelMetadata</code> <code>ModelMetadata</code> <p>Metadata of the model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request fails.</p> Example <pre><code>from focoos import Focoos, RemoteModel\n\nfocoos = Focoos()\nmodel = focoos.get_remote_model(model_ref=\"&lt;model_ref&gt;\")\nmodel_info = model.get_info()\n</code></pre> Source code in <code>focoos/remote_model.py</code> <pre><code>def get_info(self) -&gt; ModelMetadata:\n    \"\"\"\n    Retrieve model metadata.\n\n    Returns:\n        ModelMetadata: Metadata of the model.\n\n    Raises:\n        ValueError: If the request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos, RemoteModel\n\n        focoos = Focoos()\n        model = focoos.get_remote_model(model_ref=\"&lt;model_ref&gt;\")\n        model_info = model.get_info()\n        ```\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n    self.metadata = ModelMetadata(**res.json())\n    return self.metadata\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.infer","title":"<code>infer(image, threshold=0.5, annotate=False)</code>","text":"<p>Perform inference on the provided image using the remote model.</p> <p>This method sends an image to the remote model for inference and retrieves the detection results. Optionally, it can annotate the image with the detection results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Path, ndarray, bytes]</code> <p>The image to infer on, which can be a file path, a string representing the path, a NumPy array, or raw bytes.</p> required <code>threshold</code> <code>float</code> <p>The confidence threshold for detections. Defaults to 0.5. Detections with confidence scores below this threshold will be discarded.</p> <code>0.5</code> <code>annotate</code> <code>bool</code> <p>Whether to annotate the image with the detection results. Defaults to False. If set to True, the method will return the image with bounding boxes or segmentation masks.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[FocoosDetections, Optional[ndarray]]</code> <p>Tuple[FocoosDetections, Optional[np.ndarray]]: - FocoosDetections: The detection results including class IDs, confidence scores, bounding boxes,   and segmentation masks (if applicable). - Optional[np.ndarray]: The annotated image if <code>annotate</code> is True, else None.   This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the provided image file path is invalid.</p> <code>ValueError</code> <p>If the inference request fails.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\n\nmodel = focoos.get_remote_model(\"my-model\")\nresults, annotated_image = model.infer(\"image.jpg\", threshold=0.5, annotate=True)\n\n# Print detection results\nfor det in results.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n</code></pre> Source code in <code>focoos/remote_model.py</code> <pre><code>def infer(\n    self,\n    image: Union[str, Path, np.ndarray, bytes],\n    threshold: float = 0.5,\n    annotate: bool = False,\n) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n    \"\"\"\n    Perform inference on the provided image using the remote model.\n\n    This method sends an image to the remote model for inference and retrieves the detection results.\n    Optionally, it can annotate the image with the detection results.\n\n    Args:\n        image (Union[str, Path, np.ndarray, bytes]): The image to infer on, which can be a file path,\n            a string representing the path, a NumPy array, or raw bytes.\n        threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n            Detections with confidence scores below this threshold will be discarded.\n        annotate (bool, optional): Whether to annotate the image with the detection results. Defaults to False.\n            If set to True, the method will return the image with bounding boxes or segmentation masks.\n\n    Returns:\n        Tuple[FocoosDetections, Optional[np.ndarray]]:\n            - FocoosDetections: The detection results including class IDs, confidence scores, bounding boxes,\n              and segmentation masks (if applicable).\n            - Optional[np.ndarray]: The annotated image if `annotate` is True, else None.\n              This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks.\n\n    Raises:\n        FileNotFoundError: If the provided image file path is invalid.\n        ValueError: If the inference request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n\n        model = focoos.get_remote_model(\"my-model\")\n        results, annotated_image = model.infer(\"image.jpg\", threshold=0.5, annotate=True)\n\n        # Print detection results\n        for det in results.detections:\n            print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n            print(f\"Bounding box: {det.bbox}\")\n            if det.mask:\n                print(\"Instance segmentation mask included\")\n        ```\n    \"\"\"\n    image_bytes = None\n    if isinstance(image, str) or isinstance(image, Path):\n        if not os.path.exists(image):\n            logger.error(f\"Image file not found: {image}\")\n            raise FileNotFoundError(f\"Image file not found: {image}\")\n        image_bytes = open(image, \"rb\").read()\n    elif isinstance(image, np.ndarray):\n        _, buffer = cv2.imencode(\".jpg\", image)\n        image_bytes = buffer.tobytes()\n    else:\n        image_bytes = image\n    files = {\"file\": (\"image\", image_bytes, \"image/jpeg\")}\n    t0 = time.time()\n    res = self.api_client.post(\n        f\"models/{self.model_ref}/inference?confidence_threshold={threshold}\",\n        files=files,\n    )\n    t1 = time.time()\n    if res.status_code == 200:\n        detections = FocoosDetections(\n            detections=[FocoosDet.from_json(d) for d in res.json().get(\"detections\", [])],\n            latency=res.json().get(\"latency\", None),\n        )\n        logger.debug(\n            f\"Found {len(detections.detections)} detections. Inference Request time: {(t1 - t0) * 1000:.0f}ms\"\n        )\n        preview = None\n        if annotate:\n            im0 = image_loader(image)\n            sv_detections = fai_detections_to_sv(detections, im0.shape[:-1])\n            preview = self._annotate(im0, sv_detections)\n        return detections, preview\n    else:\n        logger.error(f\"Failed to infer: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to infer: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.metrics","title":"<code>metrics()</code>","text":"<p>Retrieve the metrics of the model.</p> <p>This method sends a request to fetch the metrics of the model identified by <code>model_ref</code>. If the request is successful (status code 200), it returns the metrics as a <code>Metrics</code> object. If the request fails, it logs a warning and returns an empty <code>Metrics</code> object.</p> <p>Returns:</p> Name Type Description <code>Metrics</code> <code>Metrics</code> <p>An object containing the metrics of the model.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns an empty <code>Metrics</code> object if the request fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def metrics(self) -&gt; Metrics:  # noqa: F821\n    \"\"\"\n    Retrieve the metrics of the model.\n\n    This method sends a request to fetch the metrics of the model identified by `model_ref`.\n    If the request is successful (status code 200), it returns the metrics as a `Metrics` object.\n    If the request fails, it logs a warning and returns an empty `Metrics` object.\n\n    Returns:\n        Metrics: An object containing the metrics of the model.\n\n    Raises:\n        None: Returns an empty `Metrics` object if the request fails.\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}/metrics\")\n    if res.status_code != 200:\n        logger.warning(f\"Failed to get metrics: {res.status_code} {res.text}\")\n        return Metrics()  # noqa: F821\n    return Metrics(**res.json())\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.notebook_monitor_train","title":"<code>notebook_monitor_train(interval=30, plot_metrics=False, max_runtime=36000)</code>","text":"<p>Monitor the training process in a Jupyter notebook and display metrics.</p> <p>Periodically checks the training status and displays metrics in a notebook cell. Clears previous output to maintain a clean view.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>int</code> <p>Time between status checks in seconds. Must be 30-240. Default: 30</p> <code>30</code> <code>plot_metrics</code> <code>bool</code> <p>Whether to plot metrics graphs. Default: False</p> <code>False</code> <code>max_runtime</code> <code>int</code> <p>Maximum monitoring time in seconds. Default: 36000 (10 hours)</p> <code>36000</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def notebook_monitor_train(self, interval: int = 30, plot_metrics: bool = False, max_runtime: int = 36000) -&gt; None:\n    \"\"\"\n    Monitor the training process in a Jupyter notebook and display metrics.\n\n    Periodically checks the training status and displays metrics in a notebook cell.\n    Clears previous output to maintain a clean view.\n\n    Args:\n        interval (int): Time between status checks in seconds. Must be 30-240. Default: 30\n        plot_metrics (bool): Whether to plot metrics graphs. Default: False\n        max_runtime (int): Maximum monitoring time in seconds. Default: 36000 (10 hours)\n\n    Returns:\n        None\n    \"\"\"\n    from IPython.display import clear_output\n\n    if not 30 &lt;= interval &lt;= 240:\n        raise ValueError(\"Interval must be between 30 and 240 seconds\")\n\n    last_update = self.get_info().updated_at\n    start_time = time.time()\n    status_history = []\n\n    while True:\n        # Get current status\n        model_info = self.get_info()\n        status = model_info.status\n\n        # Clear and display status\n        clear_output(wait=True)\n        status_msg = f\"[Live Monitor {self.metadata.name}] {status.value}\"\n        status_history.append(status_msg)\n        for msg in status_history:\n            logger.info(msg)\n\n        # Show metrics if training completed\n        if status == ModelStatus.TRAINING_COMPLETED:\n            metrics = self.metrics()\n            if metrics.best_valid_metric:\n                logger.info(f\"Best Checkpoint (iter: {metrics.best_valid_metric.get('iteration', 'N/A')}):\")\n                for k, v in metrics.best_valid_metric.items():\n                    logger.info(f\"  {k}: {v}\")\n                visualizer = MetricsVisualizer(metrics)\n                visualizer.log_metrics()\n                if plot_metrics:\n                    visualizer.notebook_plot_training_metrics()\n\n        # Update metrics during training\n        if status == ModelStatus.TRAINING_RUNNING and model_info.updated_at &gt; last_update:\n            last_update = model_info.updated_at\n            metrics = self.metrics()\n            visualizer = MetricsVisualizer(metrics)\n            visualizer.log_metrics()\n            if plot_metrics:\n                visualizer.notebook_plot_training_metrics()\n\n        # Check exit conditions\n        if status not in [ModelStatus.CREATED, ModelStatus.TRAINING_RUNNING, ModelStatus.TRAINING_STARTING]:\n            return\n\n        if time.time() - start_time &gt; max_runtime:\n            logger.warning(f\"Monitoring exceeded {max_runtime} seconds limit\")\n            return\n\n        sleep(interval)\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.stop_training","title":"<code>stop_training()</code>","text":"<p>Stop the training process of the model.</p> <p>This method sends a request to stop the training of the model identified by <code>model_ref</code>. If the request fails, an error is logged and a <code>ValueError</code> is raised.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the stop training request fails.</p> Logs <ul> <li>Error message if the request to stop training fails, including the status code and response text.</li> </ul> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>This method does not return any value.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def stop_training(self) -&gt; None:\n    \"\"\"\n    Stop the training process of the model.\n\n    This method sends a request to stop the training of the model identified by `model_ref`.\n    If the request fails, an error is logged and a `ValueError` is raised.\n\n    Raises:\n        ValueError: If the stop training request fails.\n\n    Logs:\n        - Error message if the request to stop training fails, including the status code and response text.\n\n    Returns:\n        None: This method does not return any value.\n    \"\"\"\n    res = self.api_client.delete(f\"models/{self.model_ref}/train\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get stop training: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get stop training: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.train","title":"<code>train(dataset_ref, hyperparameters, instance_type=TrainInstance.ML_G4DN_XLARGE, volume_size=50, max_runtime_in_seconds=36000)</code>","text":"<p>Initiate the training of a remote model on the Focoos platform.</p> <p>This method sends a request to the Focoos platform to start the training process for the model referenced by <code>self.model_ref</code>. It requires a dataset reference and hyperparameters for training, as well as optional configuration options for the instance type, volume size, and runtime.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_ref</code> <code>str</code> <p>The reference ID of the dataset to be used for training.</p> required <code>hyperparameters</code> <code>Hyperparameters</code> <p>A structure containing the hyperparameters for the training process.</p> required <code>instance_type</code> <code>TrainInstance</code> <p>The type of training instance to use. Defaults to TrainInstance.ML_G4DN_XLARGE.</p> <code>ML_G4DN_XLARGE</code> <code>volume_size</code> <code>int</code> <p>The size of the disk volume (in GB) for the training instance. Defaults to 50.</p> <code>50</code> <code>max_runtime_in_seconds</code> <code>int</code> <p>The maximum runtime for training in seconds. Defaults to 36000.</p> <code>36000</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict | None</code> <p>A dictionary containing the response from the training initiation request. The content depends on the Focoos platform's response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request to start training fails (e.g., due to incorrect parameters or server issues).</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def train(\n    self,\n    dataset_ref: str,\n    hyperparameters: Hyperparameters,\n    instance_type: TrainInstance = TrainInstance.ML_G4DN_XLARGE,\n    volume_size: int = 50,\n    max_runtime_in_seconds: int = 36000,\n) -&gt; dict | None:\n    \"\"\"\n    Initiate the training of a remote model on the Focoos platform.\n\n    This method sends a request to the Focoos platform to start the training process for the model\n    referenced by `self.model_ref`. It requires a dataset reference and hyperparameters for training,\n    as well as optional configuration options for the instance type, volume size, and runtime.\n\n    Args:\n        dataset_ref (str): The reference ID of the dataset to be used for training.\n        hyperparameters (Hyperparameters): A structure containing the hyperparameters for the training process.\n        instance_type (TrainInstance, optional): The type of training instance to use. Defaults to TrainInstance.ML_G4DN_XLARGE.\n        volume_size (int, optional): The size of the disk volume (in GB) for the training instance. Defaults to 50.\n        max_runtime_in_seconds (int, optional): The maximum runtime for training in seconds. Defaults to 36000.\n\n    Returns:\n        dict: A dictionary containing the response from the training initiation request. The content depends on the Focoos platform's response.\n\n    Raises:\n        ValueError: If the request to start training fails (e.g., due to incorrect parameters or server issues).\n    \"\"\"\n    res = self.api_client.post(\n        f\"models/{self.model_ref}/train\",\n        data={\n            \"dataset_ref\": dataset_ref,\n            \"instance_type\": instance_type,\n            \"volume_size\": volume_size,\n            \"max_runtime_in_seconds\": max_runtime_in_seconds,\n            \"hyperparameters\": hyperparameters.model_dump(),\n        },\n    )\n    if res.status_code != 200:\n        logger.warning(f\"Failed to train model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to train model: {res.status_code} {res.text}\")\n    return res.json()\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.train_info","title":"<code>train_info()</code>","text":"<p>Retrieve the current status of the model training.</p> <p>Sends a request to check the training status of the model referenced by <code>self.model_ref</code>.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[TrainingInfo]</code> <p>A dictionary containing the training status information.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request to get training status fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def train_info(self) -&gt; Optional[TrainingInfo]:\n    \"\"\"\n    Retrieve the current status of the model training.\n\n    Sends a request to check the training status of the model referenced by `self.model_ref`.\n\n    Returns:\n        dict: A dictionary containing the training status information.\n\n    Raises:\n        ValueError: If the request to get training status fails.\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}/train/status\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get train status: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get train status: {res.status_code} {res.text}\")\n    return TrainingInfo(**res.json())\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.train_logs","title":"<code>train_logs()</code>","text":"<p>Retrieve the training logs for the model.</p> <p>This method sends a request to fetch the logs of the model's training process. If the request is successful (status code 200), it returns the logs as a list of strings. If the request fails, it logs a warning and returns an empty list.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of training logs as strings.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns an empty list if the request fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def train_logs(self) -&gt; list[str]:\n    \"\"\"\n    Retrieve the training logs for the model.\n\n    This method sends a request to fetch the logs of the model's training process. If the request\n    is successful (status code 200), it returns the logs as a list of strings. If the request fails,\n    it logs a warning and returns an empty list.\n\n    Returns:\n        list[str]: A list of training logs as strings.\n\n    Raises:\n        None: Returns an empty list if the request fails.\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}/train/logs\")\n    if res.status_code != 200:\n        logger.warning(f\"Failed to get train logs: {res.status_code} {res.text}\")\n        return []\n    return res.json()\n</code></pre>"},{"location":"api/runtime/","title":"Runtime","text":"<p>Runtime Module for the models</p> <p>This module provides the necessary functionality for loading, preprocessing, running inference, and benchmarking ONNX and TorchScript models using different execution providers such as CUDA, TensorRT, and CPU. It includes utility functions for image preprocessing, postprocessing, and interfacing with the ONNXRuntime and TorchScript libraries.</p> <p>Functions:</p> Name Description <code>det_postprocess</code> <p>Postprocesses detection model outputs into sv.Detections.</p> <code>semseg_postprocess</code> <p>Postprocesses semantic segmentation model outputs into sv.Detections.</p> <code>load_runtime</code> <p>Returns an ONNXRuntime or TorchscriptRuntime instance configured for the given runtime type.</p> <p>Classes:</p> Name Description <code>RuntimeTypes</code> <p>Enum for the different runtime types.</p> <code>ONNXRuntime</code> <p>A class that interfaces with ONNX Runtime for model inference.</p> <code>TorchscriptRuntime</code> <p>A class that interfaces with TorchScript for model inference.</p>"},{"location":"api/runtime/#focoos.runtime.BaseRuntime","title":"<code>BaseRuntime</code>","text":"<p>Abstract base class for runtime implementations.</p> <p>This class defines the interface that all runtime implementations must follow. It provides methods for model initialization, inference, and performance benchmarking.</p> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>str</code> <p>Path to the model file.</p> <code>opts</code> <code>Any</code> <p>Runtime-specific options.</p> <code>model_metadata</code> <code>ModelMetadata</code> <p>Metadata about the model.</p> Source code in <code>focoos/runtime.py</code> <pre><code>class BaseRuntime:\n    \"\"\"\n    Abstract base class for runtime implementations.\n\n    This class defines the interface that all runtime implementations must follow.\n    It provides methods for model initialization, inference, and performance benchmarking.\n\n    Attributes:\n        model_path (str): Path to the model file.\n        opts (Any): Runtime-specific options.\n        model_metadata (ModelMetadata): Metadata about the model.\n    \"\"\"\n\n    def __init__(self, model_path: str, opts: Any, model_metadata: ModelMetadata):\n        \"\"\"\n        Initialize the runtime with model path, options and metadata.\n\n        Args:\n            model_path (str): Path to the model file.\n            opts (Any): Runtime-specific configuration options.\n            model_metadata (ModelMetadata): Metadata about the model.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __call__(self, im: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Run inference on the input image.\n\n        Args:\n            im (np.ndarray): Input image as a numpy array.\n\n        Returns:\n            np.ndarray: Model output as a numpy array.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model performance.\n\n        Args:\n            iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n            size (int, optional): Input image size for benchmarking. Defaults to 640.\n\n        Returns:\n            LatencyMetrics: Performance metrics including mean, median, and percentile latencies.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.BaseRuntime.__call__","title":"<code>__call__(im)</code>  <code>abstractmethod</code>","text":"<p>Run inference on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>Input image as a numpy array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Model output as a numpy array.</p> Source code in <code>focoos/runtime.py</code> <pre><code>@abstractmethod\ndef __call__(self, im: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Run inference on the input image.\n\n    Args:\n        im (np.ndarray): Input image as a numpy array.\n\n    Returns:\n        np.ndarray: Model output as a numpy array.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.BaseRuntime.__init__","title":"<code>__init__(model_path, opts, model_metadata)</code>","text":"<p>Initialize the runtime with model path, options and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model file.</p> required <code>opts</code> <code>Any</code> <p>Runtime-specific configuration options.</p> required <code>model_metadata</code> <code>ModelMetadata</code> <p>Metadata about the model.</p> required Source code in <code>focoos/runtime.py</code> <pre><code>def __init__(self, model_path: str, opts: Any, model_metadata: ModelMetadata):\n    \"\"\"\n    Initialize the runtime with model path, options and metadata.\n\n    Args:\n        model_path (str): Path to the model file.\n        opts (Any): Runtime-specific configuration options.\n        model_metadata (ModelMetadata): Metadata about the model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.BaseRuntime.benchmark","title":"<code>benchmark(iterations=20, size=640)</code>  <code>abstractmethod</code>","text":"<p>Benchmark the model performance.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of inference iterations to run. Defaults to 20.</p> <code>20</code> <code>size</code> <code>int</code> <p>Input image size for benchmarking. Defaults to 640.</p> <code>640</code> <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Performance metrics including mean, median, and percentile latencies.</p> Source code in <code>focoos/runtime.py</code> <pre><code>@abstractmethod\ndef benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model performance.\n\n    Args:\n        iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n        size (int, optional): Input image size for benchmarking. Defaults to 640.\n\n    Returns:\n        LatencyMetrics: Performance metrics including mean, median, and percentile latencies.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.ONNXRuntime","title":"<code>ONNXRuntime</code>","text":"<p>               Bases: <code>BaseRuntime</code></p> <p>ONNX Runtime wrapper for model inference with different execution providers.</p> <p>This class implements the BaseRuntime interface for ONNX models, supporting various execution providers like CUDA, TensorRT, OpenVINO, and CoreML. It handles model initialization, provider configuration, warmup, inference, and performance benchmarking.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the model derived from the model path.</p> <code>opts</code> <code>OnnxRuntimeOpts</code> <p>Configuration options for the ONNX runtime.</p> <code>model_metadata</code> <code>ModelMetadata</code> <p>Metadata about the model.</p> <code>ort_sess</code> <code>InferenceSession</code> <p>ONNX Runtime inference session.</p> <code>active_providers</code> <code>list</code> <p>List of active execution providers.</p> <code>dtype</code> <code>dtype</code> <p>Input data type for the model.</p> Source code in <code>focoos/runtime.py</code> <pre><code>class ONNXRuntime(BaseRuntime):\n    \"\"\"\n    ONNX Runtime wrapper for model inference with different execution providers.\n\n    This class implements the BaseRuntime interface for ONNX models, supporting\n    various execution providers like CUDA, TensorRT, OpenVINO, and CoreML.\n    It handles model initialization, provider configuration, warmup, inference,\n    and performance benchmarking.\n\n    Attributes:\n        name (str): Name of the model derived from the model path.\n        opts (OnnxRuntimeOpts): Configuration options for the ONNX runtime.\n        model_metadata (ModelMetadata): Metadata about the model.\n        ort_sess (ort.InferenceSession): ONNX Runtime inference session.\n        active_providers (list): List of active execution providers.\n        dtype (np.dtype): Input data type for the model.\n    \"\"\"\n\n    def __init__(self, model_path: str, opts: OnnxRuntimeOpts, model_metadata: ModelMetadata):\n        self.logger = get_logger()\n\n        self.logger.debug(f\"\ud83d\udd27 [onnxruntime device] {ort.get_device()}\")\n\n        self.name = Path(model_path).stem\n        self.opts = opts\n        self.model_metadata = model_metadata\n\n        # Setup session options\n        options = ort.SessionOptions()\n        options.log_severity_level = 0 if opts.verbose else 2\n        options.enable_profiling = opts.verbose\n\n        # Setup providers\n        self.providers = self._setup_providers(model_dir=Path(model_path).parent)\n        self.active_provider = self.providers[0][0]\n        self.logger.info(f\"[onnxruntime] using: {self.active_provider}\")\n        # Create session\n        self.ort_sess = ort.InferenceSession(model_path, options, providers=self.providers)\n\n        if self.opts.trt and self.providers[0][0] == \"TensorrtExecutionProvider\":\n            self.logger.info(\n                \"\ud83d\udfe2 [onnxruntime] TensorRT enabled. First execution may take longer as it builds the TRT engine.\"\n            )\n        # Set input type\n        self.dtype = np.uint8 if self.ort_sess.get_inputs()[0].type == \"tensor(uint8)\" else np.float32\n\n        # Warmup\n        if self.opts.warmup_iter &gt; 0:\n            self._warmup()\n\n    def _setup_providers(self, model_dir: str):\n        providers = []\n        available = ort.get_available_providers()\n        self.logger.info(f\"[onnxruntime] available providers:{available}\")\n        _dir = Path(model_dir)\n        models_root = _dir.parent\n        # Check and add providers in order of preference\n        provider_configs = [\n            (\n                \"TensorrtExecutionProvider\",\n                self.opts.trt,\n                {\n                    \"device_id\": GPU_ID,\n                    \"trt_fp16_enable\": self.opts.fp16,\n                    \"trt_force_sequential_engine_build\": False,\n                    \"trt_engine_cache_enable\": True,\n                    \"trt_engine_cache_path\": str(_dir / \".trt_cache\"),\n                    \"trt_ep_context_file_path\": str(_dir),\n                    \"trt_timing_cache_enable\": True,  # Timing cache can be shared across multiple models if layers are the same\n                    \"trt_builder_optimization_level\": 3,\n                    \"trt_timing_cache_path\": str(models_root / \".trt_timing_cache\"),\n                },\n            ),\n            (\n                \"OpenVINOExecutionProvider\",\n                self.opts.vino,\n                {\"device_type\": \"MYRIAD_FP16\", \"enable_vpu_fast_compile\": True, \"num_of_threads\": 1},\n            ),\n            (\n                \"CUDAExecutionProvider\",\n                self.opts.cuda,\n                {\n                    \"device_id\": GPU_ID,\n                    \"arena_extend_strategy\": \"kSameAsRequested\",\n                    \"gpu_mem_limit\": 16 * 1024 * 1024 * 1024,\n                    \"cudnn_conv_algo_search\": \"EXHAUSTIVE\",\n                    \"do_copy_in_default_stream\": True,\n                },\n            ),\n            (\"CoreMLExecutionProvider\", self.opts.coreml, {}),\n        ]\n\n        for provider, enabled, config in provider_configs:\n            if enabled and provider in available:\n                providers.append((provider, config))\n            elif enabled:\n                self.logger.warning(f\"{provider} not found.\")\n\n        providers.append((\"CPUExecutionProvider\", {}))\n        return providers\n\n    def _warmup(self):\n        self.logger.info(\"\u23f1\ufe0f [onnxruntime] Warming up model ..\")\n        np_image = np.random.rand(1, 3, 640, 640).astype(self.dtype)\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n        for _ in range(self.opts.warmup_iter):\n            self.ort_sess.run(out_name, {input_name: np_image})\n\n        self.logger.info(\"\u23f1\ufe0f [onnxruntime] Warmup done\")\n\n    def __call__(self, im: np.ndarray) -&gt; list[np.ndarray]:\n        \"\"\"\n        Run inference on the input image.\n\n        Args:\n            im (np.ndarray): Input image as a numpy array.\n\n        Returns:\n            list[np.ndarray]: Model outputs as a list of numpy arrays.\n        \"\"\"\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n        out = self.ort_sess.run(out_name, {input_name: im})\n        return out\n\n    def benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model performance.\n\n        Runs multiple inference iterations and measures execution time to calculate\n        performance metrics like FPS, mean latency, and other statistics.\n\n        Args:\n            iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n            size (int or tuple, optional): Input image size for benchmarking. Defaults to 640.\n\n        Returns:\n            LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n        \"\"\"\n        gpu_info = get_gpu_info()\n        device_name = \"CPU\"\n        if gpu_info.devices is not None and len(gpu_info.devices) &gt; 0:\n            device_name = gpu_info.devices[0].gpu_name\n        else:\n            device_name = get_cpu_name()\n            self.logger.warning(f\"No GPU found, using CPU {device_name}.\")\n\n        self.logger.info(f\"\u23f1\ufe0f [onnxruntime] Benchmarking latency on {device_name}..\")\n        size = size if isinstance(size, (tuple, list)) else (size, size)\n\n        np_input = (255 * np.random.random((1, 3, size[0], size[1]))).astype(self.dtype)\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n        durations = []\n        for step in range(iterations + 5):\n            start = perf_counter()\n            self.ort_sess.run(out_name, {input_name: np_input})\n            end = perf_counter()\n\n            if step &gt;= 5:  # Skip first 5 iterations\n                durations.append((end - start) * 1000)\n\n        durations = np.array(durations)\n\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean()),\n            engine=f\"onnx.{self.active_provider}\",\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size[0],\n            device=str(device_name),\n        )\n        self.logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.ONNXRuntime.__call__","title":"<code>__call__(im)</code>","text":"<p>Run inference on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>Input image as a numpy array.</p> required <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>list[np.ndarray]: Model outputs as a list of numpy arrays.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def __call__(self, im: np.ndarray) -&gt; list[np.ndarray]:\n    \"\"\"\n    Run inference on the input image.\n\n    Args:\n        im (np.ndarray): Input image as a numpy array.\n\n    Returns:\n        list[np.ndarray]: Model outputs as a list of numpy arrays.\n    \"\"\"\n    input_name = self.ort_sess.get_inputs()[0].name\n    out_name = [output.name for output in self.ort_sess.get_outputs()]\n    out = self.ort_sess.run(out_name, {input_name: im})\n    return out\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.ONNXRuntime.benchmark","title":"<code>benchmark(iterations=20, size=640)</code>","text":"<p>Benchmark the model performance.</p> <p>Runs multiple inference iterations and measures execution time to calculate performance metrics like FPS, mean latency, and other statistics.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of inference iterations to run. Defaults to 20.</p> <code>20</code> <code>size</code> <code>int or tuple</code> <p>Input image size for benchmarking. Defaults to 640.</p> <code>640</code> <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Performance metrics including FPS, mean, min, max, and std latencies.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model performance.\n\n    Runs multiple inference iterations and measures execution time to calculate\n    performance metrics like FPS, mean latency, and other statistics.\n\n    Args:\n        iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n        size (int or tuple, optional): Input image size for benchmarking. Defaults to 640.\n\n    Returns:\n        LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n    \"\"\"\n    gpu_info = get_gpu_info()\n    device_name = \"CPU\"\n    if gpu_info.devices is not None and len(gpu_info.devices) &gt; 0:\n        device_name = gpu_info.devices[0].gpu_name\n    else:\n        device_name = get_cpu_name()\n        self.logger.warning(f\"No GPU found, using CPU {device_name}.\")\n\n    self.logger.info(f\"\u23f1\ufe0f [onnxruntime] Benchmarking latency on {device_name}..\")\n    size = size if isinstance(size, (tuple, list)) else (size, size)\n\n    np_input = (255 * np.random.random((1, 3, size[0], size[1]))).astype(self.dtype)\n    input_name = self.ort_sess.get_inputs()[0].name\n    out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n    durations = []\n    for step in range(iterations + 5):\n        start = perf_counter()\n        self.ort_sess.run(out_name, {input_name: np_input})\n        end = perf_counter()\n\n        if step &gt;= 5:  # Skip first 5 iterations\n            durations.append((end - start) * 1000)\n\n    durations = np.array(durations)\n\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean()),\n        engine=f\"onnx.{self.active_provider}\",\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size[0],\n        device=str(device_name),\n    )\n    self.logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.TorchscriptRuntime","title":"<code>TorchscriptRuntime</code>","text":"<p>               Bases: <code>BaseRuntime</code></p> <p>TorchScript Runtime wrapper for model inference.</p> <p>This class implements the BaseRuntime interface for TorchScript models, supporting both CPU and CUDA devices. It handles model initialization, device placement, warmup, inference, and performance benchmarking.</p> <p>Attributes:</p> Name Type Description <code>device</code> <code>device</code> <p>Device to run inference on (CPU or CUDA).</p> <code>opts</code> <code>TorchscriptRuntimeOpts</code> <p>Configuration options for the TorchScript runtime.</p> <code>model</code> <code>ScriptModule</code> <p>Loaded TorchScript model.</p> Source code in <code>focoos/runtime.py</code> <pre><code>class TorchscriptRuntime(BaseRuntime):\n    \"\"\"\n    TorchScript Runtime wrapper for model inference.\n\n    This class implements the BaseRuntime interface for TorchScript models,\n    supporting both CPU and CUDA devices. It handles model initialization,\n    device placement, warmup, inference, and performance benchmarking.\n\n    Attributes:\n        device (torch.device): Device to run inference on (CPU or CUDA).\n        opts (TorchscriptRuntimeOpts): Configuration options for the TorchScript runtime.\n        model (torch.jit.ScriptModule): Loaded TorchScript model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        opts: TorchscriptRuntimeOpts,\n        model_metadata: ModelMetadata,\n    ):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.logger = get_logger(name=\"TorchscriptEngine\")\n        self.logger.info(f\"\ud83d\udd27 [torchscript] Device: {self.device}\")\n        self.opts = opts\n\n        map_location = None if torch.cuda.is_available() else \"cpu\"\n\n        self.model = torch.jit.load(model_path, map_location=map_location)\n        self.model = self.model.to(self.device)\n\n        if self.opts.warmup_iter &gt; 0:\n            self.logger.info(\"\u23f1\ufe0f [torchscript] Warming up model..\")\n            with torch.no_grad():\n                np_image = torch.rand(1, 3, 640, 640, device=self.device)\n                for _ in range(self.opts.warmup_iter):\n                    self.model(np_image)\n            self.logger.info(\"\u23f1\ufe0f [torchscript] WARMUP DONE\")\n\n    def __call__(self, im: np.ndarray) -&gt; list[np.ndarray]:\n        \"\"\"\n        Run inference on the input image.\n\n        Args:\n            im (np.ndarray): Input image as a numpy array.\n\n        Returns:\n            list[np.ndarray]: Model outputs as a list of numpy arrays.\n        \"\"\"\n        with torch.no_grad():\n            torch_image = torch.from_numpy(im).to(self.device, dtype=torch.float32)\n            res = self.model(torch_image)\n            return [r.cpu().numpy() for r in res]\n\n    def benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model performance.\n\n        Runs multiple inference iterations and measures execution time to calculate\n        performance metrics like FPS, mean latency, and other statistics.\n\n        Args:\n            iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n            size (int or tuple, optional): Input image size for benchmarking. Defaults to 640.\n\n        Returns:\n            LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n        \"\"\"\n        gpu_info = get_gpu_info()\n        device_name = \"CPU\"\n        if gpu_info.devices is not None and len(gpu_info.devices) &gt; 0:\n            device_name = gpu_info.devices[0].gpu_name\n        else:\n            device_name = get_cpu_name()\n            self.logger.warning(f\"No GPU found, using CPU {device_name}.\")\n        self.logger.info(\"\u23f1\ufe0f [torchscript] Benchmarking latency..\")\n        size = size if isinstance(size, (tuple, list)) else (size, size)\n\n        torch_input = torch.rand(1, 3, size[0], size[1], device=self.device)\n        durations = []\n\n        with torch.no_grad():\n            for step in range(iterations + 5):\n                start = perf_counter()\n                self.model(torch_input)\n                end = perf_counter()\n\n                if step &gt;= 5:  # Skip first 5 iterations\n                    durations.append((end - start) * 1000)\n\n        durations = np.array(durations)\n\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean().astype(float)),\n            engine=\"torchscript\",\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size[0],\n            device=str(device_name),\n        )\n        self.logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.TorchscriptRuntime.__call__","title":"<code>__call__(im)</code>","text":"<p>Run inference on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>Input image as a numpy array.</p> required <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>list[np.ndarray]: Model outputs as a list of numpy arrays.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def __call__(self, im: np.ndarray) -&gt; list[np.ndarray]:\n    \"\"\"\n    Run inference on the input image.\n\n    Args:\n        im (np.ndarray): Input image as a numpy array.\n\n    Returns:\n        list[np.ndarray]: Model outputs as a list of numpy arrays.\n    \"\"\"\n    with torch.no_grad():\n        torch_image = torch.from_numpy(im).to(self.device, dtype=torch.float32)\n        res = self.model(torch_image)\n        return [r.cpu().numpy() for r in res]\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.TorchscriptRuntime.benchmark","title":"<code>benchmark(iterations=20, size=640)</code>","text":"<p>Benchmark the model performance.</p> <p>Runs multiple inference iterations and measures execution time to calculate performance metrics like FPS, mean latency, and other statistics.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of inference iterations to run. Defaults to 20.</p> <code>20</code> <code>size</code> <code>int or tuple</code> <p>Input image size for benchmarking. Defaults to 640.</p> <code>640</code> <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Performance metrics including FPS, mean, min, max, and std latencies.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model performance.\n\n    Runs multiple inference iterations and measures execution time to calculate\n    performance metrics like FPS, mean latency, and other statistics.\n\n    Args:\n        iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n        size (int or tuple, optional): Input image size for benchmarking. Defaults to 640.\n\n    Returns:\n        LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n    \"\"\"\n    gpu_info = get_gpu_info()\n    device_name = \"CPU\"\n    if gpu_info.devices is not None and len(gpu_info.devices) &gt; 0:\n        device_name = gpu_info.devices[0].gpu_name\n    else:\n        device_name = get_cpu_name()\n        self.logger.warning(f\"No GPU found, using CPU {device_name}.\")\n    self.logger.info(\"\u23f1\ufe0f [torchscript] Benchmarking latency..\")\n    size = size if isinstance(size, (tuple, list)) else (size, size)\n\n    torch_input = torch.rand(1, 3, size[0], size[1], device=self.device)\n    durations = []\n\n    with torch.no_grad():\n        for step in range(iterations + 5):\n            start = perf_counter()\n            self.model(torch_input)\n            end = perf_counter()\n\n            if step &gt;= 5:  # Skip first 5 iterations\n                durations.append((end - start) * 1000)\n\n    durations = np.array(durations)\n\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean().astype(float)),\n        engine=\"torchscript\",\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size[0],\n        device=str(device_name),\n    )\n    self.logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.load_runtime","title":"<code>load_runtime(runtime_type, model_path, model_metadata, warmup_iter=0)</code>","text":"<p>Creates and returns a runtime instance based on the specified runtime type. Supports both ONNX and TorchScript runtimes with various execution providers.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_type</code> <code>RuntimeTypes</code> <p>The type of runtime to use. Can be one of: - ONNX_CUDA32: ONNX runtime with CUDA FP32 - ONNX_TRT32: ONNX runtime with TensorRT FP32 - ONNX_TRT16: ONNX runtime with TensorRT FP16 - ONNX_CPU: ONNX runtime with CPU - ONNX_COREML: ONNX runtime with CoreML - TORCHSCRIPT_32: TorchScript runtime with FP32</p> required <code>model_path</code> <code>str</code> <p>Path to the model file (.onnx or .pt)</p> required <code>model_metadata</code> <code>ModelMetadata</code> <p>Model metadata containing task type, classes etc.</p> required <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations before inference. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>BaseRuntime</code> <code>BaseRuntime</code> <p>A configured runtime instance (ONNXRuntime or TorchscriptRuntime)</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required dependencies (torch/onnxruntime) are not installed</p> Source code in <code>focoos/runtime.py</code> <pre><code>def load_runtime(\n    runtime_type: RuntimeTypes,\n    model_path: str,\n    model_metadata: ModelMetadata,\n    warmup_iter: int = 0,\n) -&gt; BaseRuntime:\n    \"\"\"\n    Creates and returns a runtime instance based on the specified runtime type.\n    Supports both ONNX and TorchScript runtimes with various execution providers.\n\n    Args:\n        runtime_type (RuntimeTypes): The type of runtime to use. Can be one of:\n            - ONNX_CUDA32: ONNX runtime with CUDA FP32\n            - ONNX_TRT32: ONNX runtime with TensorRT FP32\n            - ONNX_TRT16: ONNX runtime with TensorRT FP16\n            - ONNX_CPU: ONNX runtime with CPU\n            - ONNX_COREML: ONNX runtime with CoreML\n            - TORCHSCRIPT_32: TorchScript runtime with FP32\n        model_path (str): Path to the model file (.onnx or .pt)\n        model_metadata (ModelMetadata): Model metadata containing task type, classes etc.\n        warmup_iter (int, optional): Number of warmup iterations before inference. Defaults to 0.\n\n    Returns:\n        BaseRuntime: A configured runtime instance (ONNXRuntime or TorchscriptRuntime)\n\n    Raises:\n        ImportError: If required dependencies (torch/onnxruntime) are not installed\n    \"\"\"\n    if runtime_type == RuntimeTypes.TORCHSCRIPT_32:\n        if not TORCH_AVAILABLE:\n            logger.error(\n                \"\u26a0\ufe0f Pytorch not found =(  please install focoos with ['torch'] extra. See https://focoosai.github.io/focoos/setup/ for more details\"\n            )\n            raise ImportError(\"Pytorch not found\")\n        opts = TorchscriptRuntimeOpts(warmup_iter=warmup_iter)\n        return TorchscriptRuntime(model_path, opts, model_metadata)\n    else:\n        if not ORT_AVAILABLE:\n            logger.error(\n                \"\u26a0\ufe0f onnxruntime not found =(  please install focoos with one of 'cpu', 'cuda', 'tensorrt' extra. See https://focoosai.github.io/focoos/setup/ for more details\"\n            )\n            raise ImportError(\"onnxruntime not found\")\n        opts = OnnxRuntimeOpts(\n            cuda=runtime_type == RuntimeTypes.ONNX_CUDA32,\n            trt=runtime_type in [RuntimeTypes.ONNX_TRT32, RuntimeTypes.ONNX_TRT16],\n            fp16=runtime_type == RuntimeTypes.ONNX_TRT16,\n            warmup_iter=warmup_iter,\n            coreml=runtime_type == RuntimeTypes.ONNX_COREML,\n            verbose=False,\n        )\n    return ONNXRuntime(model_path, opts, model_metadata)\n</code></pre>"},{"location":"development/changelog/","title":"Changelog","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"development/code_of_conduct/","title":"Code of conduct","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"development/contributing/","title":"Contributing","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"helpers/wip/","title":"Wip","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"howto/create_dataset/","title":"Dataset Management","text":"<p>This section covers the steps to create, upload, and manage datasets in Focoos using the SDK. The <code>focoos</code> library supports multiple dataset formats, making it flexible for various machine learning tasks.</p> <p></p> <p>In this guide, we will show the following steps:</p> <ol> <li>\ud83e\uddec Dataset format</li> <li>\ud83d\udcf8 Create dataset</li> <li>\ud83d\udce4 Upload data</li> <li>\ud83d\udce5 Download your own dataset from Focoos</li> <li>\ud83c\udf0d Download dataset from external sources</li> <li>\ud83d\uddd1\ufe0f Delete data</li> <li>\ud83d\udeae Delete dataset</li> </ol>"},{"location":"howto/create_dataset/#1-dataset-format","title":"1. Dataset format","text":"<p>The <code>focoos</code> library currently supports three distinct dataset layouts, providing seamless compatibility with various machine learning workflows. Below are the supported formats along with their respective folder structures:</p> <ul> <li>ROBOFLOW_COCO (Detection, Instance Segmentation): <pre><code>root/\n    train/\n        - _annotations.coco.json\n        - img_1.jpg\n        - img_2.jpg\n    valid/\n        - _annotations.coco.json\n        - img_3.jpg\n        - img_4.jpg\n</code></pre></li> <li>ROBOFLOW_SEG (Semantic Segmentation): <pre><code>root/\n    train/\n        - _classes.csv (comma separated csv)\n        - img_1.jpg\n        - img_2.jpg\n    valid/\n        - _classes.csv (comma separated csv)\n        - img_3_mask.png\n        - img_4_mask.png\n</code></pre></li> <li>SUPERVISELY (Semantic Segmentation): <pre><code>root/\n    train/\n        meta.json\n        img/\n        ann/\n        mask/\n    valid/\n        meta.json\n        img/\n        ann/\n        mask/\n</code></pre></li> </ul> <p>Note</p> <p>More dataset formats will be added soon. If you need support for a specific format, feel free to reach out via email at support@focoos.ai</p>"},{"location":"howto/create_dataset/#2-create-dataset","title":"2. Create dataset","text":"<p>The <code>focoos</code> library enables you to create datasets tailored for specific deep learning tasks, such as object detection and semantic segmentation. The available computer vision tasks are defined in the FocoosTask function. Each dataset must follow a specific structure to ensure compatibility with the Focoos platform. You can select the appropriate dataset format from the supported options detailed in Dataset Format.</p> <p>Use the following code to create a new dataset:</p> <pre><code>from focoos import DatasetLayout, Focoos, FocoosTask\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n# Create a new remote dataset\ndataset = focoos.add_remote_dataset(\n    name=\"my-dataset\",\n    description=\"My custom dataset for object detection\",\n    layout=DatasetLayout.ROBOFLOW_COCO,  # Choose dataset format\n    task=FocoosTask.DETECTION  # Specify the task type\n)\n</code></pre>"},{"location":"howto/create_dataset/#3-upload-data","title":"3. Upload data","text":"<p>Once you've created a dataset, you can upload your data as a ZIP archive from your local folder:</p> <pre><code>dataset.upload_data(\"./datasets/my_dataset.zip\")\n</code></pre> <p>After the upload, you can check dataset preview using:</p> <pre><code>dataset_info = dataset.get_info()\nprint(dataset_info)\n</code></pre> <p>Alternatively, you can list all available datasets (both personal and shared):</p> <pre><code>datasets = focoos.list_datasets()\nfor dataset in datasets:\n    print(f\"Name: {dataset.name}\")\n    print(f\"Reference: {dataset.ref}\")\n    print(f\"Task: {dataset.task}\")\n    print(f\"Description: {dataset.description}\")\n    print(f\"spec: {dataset.spec}\")\n    print(\"-\" * 50)\n</code></pre>"},{"location":"howto/create_dataset/#4-download-your-own-dataset-from-focoos-platform","title":"4. Download your own dataset from Focoos platform","text":"<p>If you have previously uploaded a dataset to Focoos platform, you can retrieve it by following these steps. First, list all your datasets to identify the dataset reference:</p> <pre><code>datasets = focoos.list_datasets()\n\nfor dataset in datasets:\n    print(f\"Name: {dataset.name}\")\n    print(f\"Reference: {dataset.ref}\")\n</code></pre> <p>Once you have the dataset reference, use the following code to download the associated data to a predefined local folder:</p> <pre><code>dataset_ref = \"&lt;YOUR-DATASET-REFERENCE&gt;\"\ndataset = focoos.get_remote_dataset(dataset_ref)\n\ndataset.download_data(\"./&lt;YOUR-DATA-FOLDER&gt;/\")\n</code></pre>"},{"location":"howto/create_dataset/#5-download-dataset-from-external-sources","title":"5. Download dataset from external sources","text":"<p>You can also download datasets from external sources like Dataset-Ninja (Supervisely) and Roboflow Universe, then upload them to the Focoos platform for use in your projects.</p> pip <pre><code>pip install dataset-tools roboflow\npip install setuptools\n</code></pre> <ul> <li> <p>Dataset Ninja: <pre><code>import dataset_tools as dtools\n\ndtools.download(dataset=\"dacl10k\", dst_dir=\"./datasets/dataset-ninja/\")\n</code></pre></p> </li> <li> <p>Roboflow: <pre><code>import os\n\nfrom roboflow import Roboflow\n\nrf = Roboflow(api_key=os.getenv(\"ROBOFLOW_API_KEY\"))\nproject = rf.workspace(\"roboflow-58fyf\").project(\"rock-paper-scissors-sxsw\")\nversion = project.version(14)\ndataset = version.download(\"coco\")\n</code></pre></p> </li> </ul>"},{"location":"howto/create_dataset/#6-delete-data","title":"6. Delete data","text":"<p>If you need to remove specific files from an existing dataset without deleting the entire dataset, you can do so by specifying the filename. This is useful when updating or refining your dataset.</p> <p>Use the following command: <pre><code>dataset_ref = \"&lt;YOUR-DATASET-REFERENCE&gt;\"\ndataset = focoos.get_remote_dataset(dataset_ref)\ndataset.delete_data()\n</code></pre></p> <p>Warning</p> <p>This will permanently remove the specified file from your dataset in Focoos platform. Be sure to double-check the filename before executing the command, as deleted data cannot be recovered.</p>"},{"location":"howto/create_dataset/#7-delete-dataset","title":"7. Delete dataset","text":"<p>If you want to remove an entire dataset from the Focoos platform, use the following command:</p> <pre><code>dataset_ref = \"&lt;YOUR-DATASET-REFERENCE&gt;\"\ndataset = focoos.get_remote_dataset(dataset_ref)\ndataset.delete()\n</code></pre> <p>Warning</p> <p>Deleting a dataset is irreversible. Once deleted, all data associated with the dataset is permanently lost and cannot be recovered.</p>"},{"location":"howto/create_dataset/#_1","title":"Manage Dataset","text":""},{"location":"howto/manage_models/","title":"Model Management","text":"<p>This section covers the steps to monitor the status of your models on the FocoosAI platform.</p> <p></p> <p>In this guide, we will cover the following topics:</p> <ol> <li>\ud83d\udccb List available Focoos models</li> <li>\ud83d\udcdc List all your models</li> <li>\ud83d\udcc8 Retrieve model metrics</li> <li>\ud83d\uddd1\ufe0f Delete a model</li> </ol>"},{"location":"howto/manage_models/#how-to-list-the-focoos-models","title":"How to list the Focoos models","text":"<p>To list all the models available on the Focoos AI platform, you can use the following code: <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodels = focoos.list_focoos_models()\nfor model in models:\n    print(f\"Name: {model.name}\")\n    print(f\"Reference: {model.ref}\")\n    print(f\"Status: {model.status}\")\n    print(f\"Task: {model.task}\")\n    print(f\"Description: {model.description}\")\n    print(\"-\" * 50)\n</code></pre> <code>models</code> is a list of <code>ModelPreview</code> objects that contains the following information:</p> <ul> <li><code>name</code>: The name of the model.</li> <li><code>ref</code>: The reference of the model.</li> <li><code>status</code>: The status of the model.</li> <li><code>task</code>: The task of the model.</li> <li><code>description</code>: The description of the model.</li> <li><code>status</code>: The status of the model, which indicates its current state (e.g. CREATED, TRAINING_RUNNING, TRAINING_COMPLETED - see <code>ModelStatus</code>).</li> </ul>"},{"location":"howto/manage_models/#how-to-list-all-your-models","title":"How to list all your models","text":"<p>To list all your models, the library provides a <code>list_models</code> function. This function will return a list of <code>Model</code> objects.</p> <p><pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodels = focoos.list_models()\nfor model in models:\n    print(f\"Name: {model.name}\")\n    print(f\"Reference: {model.ref}\")\n    print(f\"Status: {model.status}\")\n    print(f\"Task: {model.task}\")\n    print(f\"Description: {model.description}\")\n    print(f\"Focoos Model: {model.focoos_model}\")\n    print(\"-\" * 50)\n</code></pre> <code>models</code> is a list of <code>ModelPreview</code> objects that contains the following information:</p> <ul> <li><code>name</code>: The name of the model.</li> <li><code>ref</code>: The reference of the model.</li> <li><code>status</code>: The status of the model.</li> <li><code>task</code>: The task of the model.</li> <li><code>description</code>: The description of the model.</li> <li><code>focoos_model</code>: The starting Focoos Model used for training.</li> <li><code>status</code>: The status of the model, which indicates its current state (e.g. CREATED, TRAINING_RUNNING, TRAINING_COMPLETED - see <code>ModelStatus</code>).</li> </ul>"},{"location":"howto/manage_models/#see-the-metrics-for-a-model","title":"See the metrics for a model","text":"<p>To see the validation metrics of a model, you can use the <code>metrics</code> method on the model object.</p> <p><pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodel = focoos.get_remote_model(\"my-model\")\nmetrics = model.metrics()\n\nif metrics.best_valid_metric:\n    print(f\"Best validation metrics:\")\n    for k, v in metrics.best_valid_metric.items():\n        print(f\"  {k}: {v}\")\n\nif metrics.valid_metrics:\n    print(f\"Last iteration validation metrics:\")\n    for k, v in metrics.valid_metrics[-1].items():\n        print(f\"  {k}: {v}\")\n\nif metrics.train_metrics:\n    print(f\"Last iteration training metrics:\")\n    for k, v in metrics.train_metrics[-1].items():\n        print(f\"  {k}: {v}\")\n</code></pre> <code>metrics</code> is a <code>Metrics</code> object that contains the validation metrics of the model.</p>"},{"location":"howto/manage_models/#delete-a-model","title":"Delete a model","text":"<p>To delete a model, you can use the <code>delete_model</code> method on the model object.</p> <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodel = focoos.get_remote_model(\"my-model\")\nmodel.delete_model()\n</code></pre> <p>Warning</p> <p>This action is irreversible. Ensure you double-check before executing this command, as once deleted, the model cannot be recovered.</p>"},{"location":"howto/manage_user/","title":"User Management","text":"<p>Managing your user information is essential for tracking account details, platform usage, and resource quotas. The Focoos library provides built-in methods to retrieve your user information, including email, API key details, company affiliation, and allocated usage quotas.</p> <p></p> <p>In this guide, we will cover the following topics:</p> <ol> <li>\ud83d\udcc4 Retrieve User Information</li> <li>\ud83d\udcca Monitor Quota Usage</li> </ol>"},{"location":"howto/manage_user/#retrieve-user-information","title":"Retrieve user information","text":"<p>To access your user details, you can use the <code>get_user_info</code> function provided by the Focoos library. This function returns a <code>User</code> object containing key account information.</p> <p><pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nuser_info = focoos.get_user_info()\n\nprint(f\"Email: {user_info.email}\")\nprint(f\"Created at: {user_info.created_at}\")\nprint(f\"Updated at: {user_info.updated_at}\")\nif user_info.company:\n    print(f\"Company: {user_info.company}\")\n</code></pre> The <code>user_info</code> object contains the following fields:</p> <ul> <li><code>email</code>: The email address associated with your account</li> <li><code>created_at</code>: Timestamp of when the account was created</li> <li><code>updated_at</code>: Timestamp of the last account update</li> <li><code>company</code>: The company affiliated with your account (if applicable)</li> <li><code>api_key</code>: Your API key details used for authentication</li> <li><code>quotas</code>: Your allocated platform usage quotas (see <code>Quotas</code> allocated to the user).</li> </ul>"},{"location":"howto/manage_user/#monitor-your-quota-usage","title":"Monitor your quota usage","text":"<p>The Focoos platform enforces usage quotas to manage resources efficiently. You can retrieve your current quota limits using the <code>get_user_info</code> function like this:</p> <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nuser_info = focoos.get_user_info()\n\nprint(\"\\nQuotas:\")\nprint(f\"Total inferences: {user_info.quotas.total_inferences}\")\nprint(f\"Max inferences: {user_info.quotas.max_inferences}\")\nprint(f\"Used storage (GB): {user_info.quotas.used_storage_gb}\")\nprint(f\"Max storage (GB): {user_info.quotas.max_storage_gb}\")\nprint(f\"Active training jobs: {user_info.quotas.active_training_jobs}\")\nprint(f\"Max active training jobs: {user_info.quotas.max_active_training_jobs}\")\nprint(f\"Used training jobs hours: {user_info.quotas.used_mlg4dnxlarge_training_jobs_hours}\")\nprint(f\"Max training jobs hours: {user_info.quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n</code></pre> <p>Note</p> <p>If you need to increase your quotas, please contact us at support.</p>"},{"location":"howto/personalize_model/","title":"Create and Train Model","text":"<p>This section covers the steps to create a model and train it in the cloud using the <code>focoos</code> library. The following example demonstrates how to interact with the Focoos API to manage models, datasets, and training jobs.</p> <p></p> <p>In this guide, we will perform the following steps:</p> <ol> <li>\ud83d\udce6 Select dataset</li> <li>\ud83c\udfaf Create model</li> <li>\ud83c\udfc3\u200d\u2642\ufe0f Train model</li> <li>\ud83d\udcca Visualize training metrics</li> <li>\ud83e\uddea Test model</li> </ol>"},{"location":"howto/personalize_model/#1-select-dataset","title":"1. Select dataset","text":"<p>You can list publicly shared datasets using the following code:</p> <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\ndatasets = focoos.list_shared_datasets()\nfor dataset in datasets:\n    print(f\"Name: {dataset.name}\")\n    print(f\"Reference: {dataset.ref}\")\n    print(f\"Task: {dataset.task}\")\n    print(f\"Description: {dataset.description}\")\n    print(\"-\" * 50)\n</code></pre> <p>To view only your personal datasets, use the following code: <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\ndatasets = focoos.list_datasets(include_shared=False)\nfor dataset in datasets:\n    print(f\"Name: {dataset.name}\")\n    print(f\"Reference: {dataset.ref}\")\n    print(f\"Task: {dataset.task}\")\n    print(f\"Description: {dataset.description}\")\n    print(\"-\" * 50)\n</code></pre></p> <p>Note</p> <p>If you haven\u2019t uploaded a dataset yet, you can follow this guide: How to load a dataset</p> <p>Once you've identified the dataset you want to use, you\u2019ll need its reference <code>dataset_ref</code> to train your model. You can either copy it or store it in a variable like this: <pre><code>dataset_ref = \"&lt;YOUR-DATASET-REFERENCE&gt;\"\n</code></pre></p>"},{"location":"howto/personalize_model/#2-create-model","title":"2. Create model","text":"<p>The first step to personalize your model is to create a model. You can create a model by calling the <code>new_model</code> method on the <code>Focoos</code> object. You can choose the model you want to personalize from the list of Focoos Models available on the platform. Make sure to select the correct model for your task.</p> <p><pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodel = focoos.new_model(\n    name=\"&lt;YOUR-MODEL-NAME&gt;\",\n    description=\"&lt;YOUR-MODEL-DESCRIPTION&gt;\",\n    focoos_model=\"&lt;FOCOOS-MODEL-NAME&gt;\",\n)\n</code></pre> An example of how to create a model is the following: <pre><code>model = focoos.new_model(\n    name=\"my-model\",\n    description=\"my-model-description\",\n    focoos_model=\"fai-rtdetr-m-obj365\",\n)\n</code></pre> This function will return a new <code>RemoteModel</code> object that you can use to train the model and to perform remote inference.</p>"},{"location":"howto/personalize_model/#3-train-model","title":"3. Train model","text":"<p>Once the model is created, you can start the training process by calling the <code>train</code> method on the model object.</p> <p><pre><code>from focoos.ports import Hyperparameters\n\nres = model.train(\n    dataset_ref=dataset_ref,\n    hyperparameters=Hyperparameters(\n        learning_rate=0.0001, # custom learning rate\n        batch_size=16, # custom batch size\n        max_iters=1500, # custom max iterations\n    ),\n)\n</code></pre> For selecting the <code>dataset_ref</code> see the step 2. You can further customize the training process by passing additional parameters to the <code>train</code> method (such as the instance type, the volume size, the maximum runtime, etc.) or use additional hyperparameters (see the list available hyperparameters).</p> <p>Futhermore, you can monitor the training progress by polling the training status. Use the <code>notebook_monitor_train</code> method on a jupyter notebook: <pre><code>model.notebook_monitor_train(interval=30, plot_metrics=True)\n</code></pre></p> <p>You can also get the training logs by calling the <code>train_logs</code> method: <pre><code>logs = model.train_logs()\npprint(logs)\n</code></pre></p> <p>Finally, if for some reason you need to cancel the training, you can do so by calling the <code>stop_training</code> method: <pre><code>model.stop_training()\n</code></pre></p>"},{"location":"howto/personalize_model/#4-visualize-training-metrics","title":"4. Visualize training metrics","text":"<p>You can visualize the training metrics by calling the <code>metrics</code> method: <pre><code>metrics = model.metrics()\nvisualizer = MetricsVisualizer(metrics)\nvisualizer.log_metrics()\n</code></pre> The function will return an object of type <code>Metrics</code> that you can use to visualize the training metrics using a <code>MetricsVisualizer</code> object.</p> <p>On notebooks, you can also plot the metrics by calling the <code>notebook_plot_training_metrics</code> method: <pre><code>visualizer.notebook_plot_training_metrics()\n</code></pre></p>"},{"location":"howto/personalize_model/#5-test-model","title":"5. Test model","text":""},{"location":"howto/personalize_model/#remote-inference","title":"Remote inference","text":"<p>Once the training is over, you can test your model using remote inference by calling the <code>infer</code> method on the model object.</p> <p><pre><code>image_path = \"&lt;PATH-TO-YOUR-IMAGE&gt;\"\nresult, _ = model.infer(image_path, threshold=0.5, annotate=False)\n\nfor det in result.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n</code></pre> <code>result</code> is a FocoosDetections object, containing a list of FocoosDet objects and optionally a dict of information about the latency of the inference.</p> <p>The <code>threshold</code> parameter is optional and defines the minimum confidence score for a detection to be considered valid (predictions with a confidence score lower than the threshold are discarded).</p> <p>Optionally, you can preview the results by passing the <code>annotate</code> parameter to the <code>infer</code> method. <pre><code>from PIL import Image\n\noutput, preview = model.infer(image_path, threshold=0.5, annotate=True)\npreview = Image.fromarray(preview[:,:,[2,1,0]]) # invert to make it RGB\n</code></pre></p>"},{"location":"howto/personalize_model/#local-inference","title":"Local inference","text":"<p>Note</p> <p>To perform local inference, you need to install the package with one of the extra modules (<code>[cpu]</code>, <code>[torch]</code>, <code>[cuda]</code>, <code>[tensorrt]</code>). See the installation page for more details.</p> <p>You can perform inference locally by getting the <code>LocalModel</code> you already trained and calling the <code>infer</code> method on your image. If it's the first time you run the model locally, the model will be downloaded from the cloud and saved on your machine. Additionally, if you use CUDA or TensorRT, the model will be optimized for your GPU before running the inference (it can take few seconds, especially for TensorRT).</p> <p><pre><code>model = focoos.get_local_model(model.model_ref) # get the local model\n\nimage_path = \"&lt;PATH-TO-YOUR-IMAGE&gt;\"\nresult, _ = model.infer(image_path, threshold=0.5, annotate=False)\n\nfor det in result.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n</code></pre> As for remote inference, you can pass the <code>annotate</code> parameter to return a preview of the prediction and play with the <code>threshold</code> parameter to change the minimum confidence score for a detection to be considered valid.</p>"},{"location":"howto/use_model/","title":"Select and Inference with Focoos Models","text":"<p>This section covers how to perform inference using the Focoos Models on the cloud or locally using the <code>focoos</code> library.</p> <p>As a reference, the following example demonstrates how to perform inference using the <code>fai-rtdetr-m-obj365</code> model, but you can use any of the models listed in the models section.</p> <p></p> <p>In this guide, we will cover the following topics:</p> <ol> <li>\u2601\ufe0f Cloud Inference !</li> <li>\u2601\ufe0f Cloud Inference with Gradio</li> <li>\ud83c\udfe0 Local Inference</li> </ol>"},{"location":"howto/use_model/#cloud-inference-for-image-processing","title":"Cloud inference for image processing","text":"<p>Running inference on the cloud is simple and efficient. Select the model you want to use and call the <code>infer</code> method on your image. The image will be securely uploaded to the Focoos AI platform, where the selected model processes it and returns the results.</p> <p>To get the model reference you can refere to the Model Management section. Here the code to handle a single image inference:</p> <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nimage_path = \"&lt;PATH-TO-YOUR-IMAGE&gt;\"\nmodel = focoos.get_remote_model(\"&lt;MODEL-REF&gt;\")\nresult, _ = model.infer(image_path, threshold=0.5, annotate=True)\n\nfor det in result.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n        print(f\"Mask shape: {det.mask.shape}\")\n</code></pre> <p><code>result</code> is a FocoosDetections object, containing a list of FocoosDet objects and optionally a dict of information about the latency of the inference. The <code>FocoosDet</code> object contains the following attributes:</p> <ul> <li><code>bbox</code>: Bounding box coordinates in x1y1x2y2 absolute format.</li> <li><code>conf</code>: Confidence score (from 0 to 1).</li> <li><code>cls_id</code>: Class ID (0-indexed).</li> <li><code>label</code>: Label (name of the class).</li> <li><code>mask</code>: Mask (base64 encoded string having origin in the top left corner of bbox and the same width and height of the bbox).</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>threshold</code> (default: 0.5) \u2013 Sets the minimum confidence score required for prediction to be considered valid. Predictions below this threshold are discarded.</li> <li><code>annotate</code> (default: False) \u2013 If set to True, the method returns preview, an annotated image with the detected objects.</li> </ul>"},{"location":"howto/use_model/#image-preview","title":"Image Preview","text":"<p>You can preview the results by passing the <code>annotate</code> parameter to the <code>infer</code> method:</p> <pre><code>from PIL import Image\n\noutput, preview = model.infer(image_path, threshold=0.5, annotate=True)\npreview = Image.fromarray(preview)\n</code></pre>"},{"location":"howto/use_model/#cloud-inference-with-gradio","title":"Cloud inference with Gradio","text":"<p>You can easily create a web interface for your model using Gradio.</p> <p>First, install the required <code>dev</code> dependencies:</p> <pre><code>pip install '.[dev]'\n</code></pre> <p>Set your Focoos API key as an environment variable and start the application. The model selection will be available from the UI:</p> <p><pre><code>export FOCOOS_API_KEY_GRADIO=&lt;YOUR-API-KEY&gt; &amp;&amp; python gradio/app.py\n</code></pre> Now, your model is accessible through a user-friendly web interface! \ud83d\ude80</p>"},{"location":"howto/use_model/#local-inference","title":"Local inference","text":"<p>Note</p> <p>To perform local inference, you need to install the package with one of the extra modules (<code>[cpu]</code>, <code>[torch]</code>, <code>[cuda]</code>, <code>[tensorrt]</code>). See the installation page for more details.</p> <p>You can run inference locally by selecting a model and calling the <code>infer</code> method on your image. If this is the first time you are running the model locally, it will be downloaded from the cloud and stored on your machine. If you are using CUDA or TensorRT, the model will be optimized for your GPU before inference. This process may take a few seconds, especially for TensorRT.</p> <p>Example code: <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodel = focoos.get_local_model(\"&lt;MODEL-REF&gt;\")\n\nimage_path = \"&lt;PATH-TO-YOUR-IMAGE&gt;\"\nresult, _ = model.infer(image_path, threshold=0.5, annotate=True)\n\nfor det in result.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n        print(f\"Mask shape: {det.mask.shape}\")\n</code></pre></p> <p><code>result</code> is a FocoosDetections object, containing a list of FocoosDet objects and optionally a dict of information about the latency of the inference. The <code>FocoosDet</code> object contains the following attributes:</p> <ul> <li><code>bbox</code>: Bounding box coordinates in x1y1x2y2 absolute format.</li> <li><code>conf</code>: Confidence score (from 0 to 1).</li> <li><code>cls_id</code>: Class ID (0-indexed).</li> <li><code>label</code>: Label (name of the class).</li> <li><code>mask</code>: Mask (base64 encoded string having origin in the top left corner of bbox and the same width and height of the bbox).</li> </ul> <p>As for remote inference, you can use the <code>annotate</code> parameter to return a preview of the prediction. Local inference provides faster results and reduces cloud dependency, making it ideal for real-time applications and edge deployments.</p>"},{"location":"models/fai-m2f-l-ade/","title":"fai-m2f-l-ade","text":""},{"location":"models/fai-m2f-l-ade/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the ADE20K dataset. It is a semantic segmentation model able to segment 150 classes, comprising both stuff (sky, road, etc.) and thing (dog, cat, car, etc.).</p>"},{"location":"models/fai-m2f-l-ade/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-m2f-l-ade/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a transformer-based encoder-decoder architecture. Differently from traditional segmentation models (such as DeepLab), Mask2Former uses a mask-classification approach, where the prediction is made by a set of segmentation mask with associated class probabilities.</p>"},{"location":"models/fai-m2f-l-ade/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is a Resnet-101,that guarantees a good performance while having good efficiency.</li> <li>the pixel decoder is a FPN getting the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. Differently from the original paper, for the sake of portability, we removed the deformable attention modules in the pixel decoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 6 decoder layers (instead of 9) and 100 learnable queries.</li> </ul>"},{"location":"models/fai-m2f-l-ade/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the 6 transformer decoder layers. Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-m2f-l-ade/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-m2f-l-ade/#classes","title":"Classes","text":"<p>The model is pretrained on the ADE20K dataset with 150 classes.</p> Class ID Class Name mIoU 1wall77.284 2building81.396 3sky94.337 4floor81.584 5tree74.103 6ceiling83.073 7road, route83.013 8bed88.120 9window61.048 10grass69.099 11cabinet56.303 12sidewalk, pavement62.300 13person82.073 14earth, ground35.094 15door45.140 16table59.436 17mountain, mount60.538 18plant51.829 19curtain71.510 20chair56.219 21car83.766 22water49.028 23painting, picture70.214 24sofa68.081 25shelf35.453 26house45.656 27sea51.205 28mirror61.611 29rug64.144 30field30.577 31armchair45.761 32seat61.850 33fence40.992 34desk41.814 35rock, stone47.600 36wardrobe, closet, press39.846 37lamp64.062 38tub74.760 39rail24.105 40cushion56.811 41base, pedestal, stand27.777 42box24.670 43column, pillar40.094 44signboard, sign33.495 45chest of drawers, chest, bureau, dresser41.847 46counter21.387 47sand29.763 48sink74.092 49skyscraper37.613 50fireplace65.037 51refrigerator, icebox57.648 52grandstand, covered stand46.626 53path24.543 54stairs28.681 55runway73.779 56case, display case, showcase, vitrine38.437 57pool table, billiard table, snooker table91.825 58pillow49.388 59screen door, screen59.058 60stairway, staircase32.832 61river18.597 62bridge, span56.011 63bookcase28.848 64blind, screen43.934 65coffee table59.869 66toilet, can, commode, crapper, pot, potty, stool, throne86.346 67flower38.141 68book42.528 69hill6.905 70bench45.494 71countertop49.007 72stove73.973 73palm, palm tree49.478 74kitchen island42.603 75computer72.142 76swivel chair44.262 77boat73.689 78bar37.749 79arcade machine78.733 80hovel, hut, hutch, shack, shanty30.537 81bus90.808 82towel58.158 83light57.444 84truck31.745 85tower32.058 86chandelier67.524 87awning, sunshade, sunblind28.566 88street lamp30.507 89booth39.696 90tv76.194 91plane50.005 92dirt track18.268 93clothes37.748 94pole23.343 95land, ground, soil0.001 96bannister, banister, balustrade, balusters, handrail16.222 97escalator, moving staircase, moving stairway54.888 98ottoman, pouf, pouffe, puff, hassock32.444 99bottle22.166 100buffet, counter, sideboard48.994 101poster, posting, placard, notice, bill, card31.773 102stage18.731 103van46.747 104ship79.937 105fountain21.205 106conveyer belt, conveyor belt, conveyer, conveyor, transporter62.591 107canopy23.719 108washer, automatic washer, washing machine66.458 109plaything, toy35.377 110pool34.297 111stool41.199 112barrel, cask61.803 113basket, handbasket34.313 114falls57.149 115tent94.077 116bag19.126 117minibike, motorbike71.207 118cradle85.775 119oven50.996 120ball32.601 121food, solid food58.662 122step, stair16.474 123tank, storage tank37.627 124trade name20.788 125microwave37.998 126pot53.411 127animal57.360 128bicycle58.772 129lake41.597 130dishwasher74.543 131screen79.757 132blanket, cover15.202 133sculpture53.537 134hood, exhaust hood52.684 135sconce48.160 136vase45.300 137traffic light35.375 138tray14.093 139trash can30.699 140fan56.574 141pier10.286 142crt screen0.936 143plate53.268 144monitor9.358 145bulletin board29.970 146shower8.978 147radiator59.763 148glass, drinking glass18.246 149clock29.088 150flag37.727"},{"location":"models/fai-m2f-l-ade/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-m2f-l-ade) from Focoos API\nmodel = focoos.get_remote_model(\"fai-m2f-l-ade\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-m2f-l-coco-ins/","title":"fai-m2f-l-coco-ins","text":""},{"location":"models/fai-m2f-l-coco-ins/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the COCO dataset. It is an instance segmentation model able to segment 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-m2f-l-coco-ins/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a mask-classification approach and a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-m2f-l-coco-ins/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is Resnet-50 that show an amazing trade-off between performance and efficiency.</li> <li>the pixel decoder is a transformer-augmented FPN. It gets the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. It first uses a transformer encoder to process the features at the lowest resolution (stage 5) and then uses a feature pyramid network to upsample the features. This part is different from the original implementation using deformable attention modules.</li> <li>the transformer decoder is implemented as in the original paper, having 9 decoder layers and 100 learnable queries.</li> </ul>"},{"location":"models/fai-m2f-l-coco-ins/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the 3 transformer decoder layers. Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-m2f-l-coco-ins/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-m2f-l-coco-ins/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class Segmentation AP 1 person 48.9 2 bicycle 22.2 3 car 41.3 4 motorcycle 40.0 5 airplane 55.6 6 bus 68.2 7 train 69.6 8 truck 40.5 9 boat 26.2 10 traffic light 27.4 11 fire hydrant 69.2 12 stop sign 65.0 13 parking meter 45.4 14 bench 23.4 15 bird 33.8 16 cat 77.7 17 dog 68.9 18 horse 50.1 19 sheep 54.0 20 cow 51.0 21 elephant 63.4 22 bear 81.1 23 zebra 66.0 24 giraffe 60.5 25 backpack 22.7 26 umbrella 52.6 27 handbag 23.3 28 tie 33.2 29 suitcase 45.3 30 frisbee 66.4 31 skis 7.4 32 snowboard 28.2 33 sports ball 42.8 34 kite 30.3 35 baseball bat 32.1 36 baseball glove 42.3 37 skateboard 36.8 38 surfboard 37.3 39 tennis racket 58.7 40 bottle 39.2 41 wine glass 36.9 42 cup 46.0 43 fork 22.2 44 knife 17.8 45 spoon 18.0 46 bowl 44.3 47 banana 26.5 48 apple 23.9 49 sandwich 43.0 50 orange 33.8 51 broccoli 24.4 52 carrot 22.7 53 hot dog 36.3 54 pizza 55.1 55 donut 51.1 56 cake 44.6 57 chair 25.0 58 couch 47.7 59 potted plant 25.0 60 bed 45.0 61 dining table 22.9 62 toilet 67.6 63 tv 64.3 64 laptop 67.2 65 mouse 60.1 66 remote 36.1 67 keyboard 52.6 68 cell phone 42.0 69 microwave 60.7 70 oven 33.8 71 toaster 35.9 72 sink 39.9 73 refrigerator 64.0 74 book 12.0 75 clock 52.5 76 vase 37.7 77 scissors 26.8 78 teddy bear 55.1 79 hair drier 16.8 80 toothbrush 22.4"},{"location":"models/fai-m2f-l-coco-ins/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-m2f-l-coco-ins) from Focoos API\nmodel = focoos.get_remote_model(\"fai-m2f-l-coco-ins\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-m2f-m-ade/","title":"fai-m2f-m-ade","text":""},{"location":"models/fai-m2f-m-ade/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the ADE20K dataset. It is a semantic segmentation model able to segment 150 classes, comprising both stuff (sky, road, etc.) and thing (dog, cat, car, etc.).</p>"},{"location":"models/fai-m2f-m-ade/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-m2f-m-ade/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a transformer-based encoder-decoder architecture. Differently from traditional segmentation models (such as DeepLab), Mask2Former uses a mask-classification approach, where the prediction is made by a set of segmentation mask with associated class probabilities.</p>"},{"location":"models/fai-m2f-m-ade/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-2 that show an amazing trade-off between performance and efficiency.</li> <li>the pixel decoder is a FPN getting the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. Differently from the original paper, for the sake of portability, we removed the deformable attention modules in the pixel decoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 3 decoder layers (instead of 9) and 100 learnable queries.</li> </ul>"},{"location":"models/fai-m2f-m-ade/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the 3 transformer decoder layers. Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-m2f-m-ade/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-m2f-m-ade/#classes","title":"Classes","text":"<p>The model is pretrained on the ADE20K dataset with 150 classes.</p> Class mIoU 1 wall 75.369549 2 building 79.835995 3 sky 94.176995 4 floor 79.620841 5 tree 73.204506 6 ceiling 82.303035 7 road, route 80.822591 8 bed 87.573840 9 window 57.452584 10 grass 70.099493 11 cabinet 56.903790 12 sidewalk, pavement 62.247267 13 person 79.460606 14 earth, ground 38.537802 15 door 43.930878 16 table 56.753292 17 mountain, mount 61.160462 18 plant 48.995487 19 curtain 71.951930 20 chair 52.852125 21 car 80.725703 22 water 51.233498 23 painting, picture 66.989493 24 sofa 58.103663 25 shelf 34.979205 26 house 36.828611 27 sea 51.219096 28 mirror 58.572852 29 rug 54.897799 30 field 29.053876 31 armchair 39.565663 32 seat 53.113668 33 fence 41.113128 34 desk 37.930189 35 rock, stone 44.940982 36 wardrobe, closet, press 39.897858 37 lamp 60.921356 38 tub 78.041637 39 rail 31.893878 40 cushion 53.029316 41 base, pedestal, stand 20.233620 42 box 18.276924 43 column, pillar 42.655306 44 signboard, sign 35.959448 45 chest of drawers, chest, bureau, dresser 36.521600 46 counter 29.353667 47 sand 38.729599 48 sink 72.303141 49 skyscraper 44.122387 50 fireplace 66.614683 51 refrigerator, icebox 72.137179 52 grandstand, covered stand 29.061628 53 path 26.629478 54 stairs 31.833328 55 runway 76.017706 56 case, display case, showcase, vitrine 37.452627 57 pool table, billiard table, snooker table 93.246039 58 pillow 54.689591 59 screen door, screen 58.096890 60 stairway, staircase 29.962829 61 river 15.010211 62 bridge, span 66.617580 63 bookcase 31.383789 64 blind, screen 39.221180 65 coffee table 63.300795 66 toilet, can, commode, crapper, pot, potty, stool, throne 84.038177 67 flower 35.994798 68 book 43.252042 69 hill 6.240850 70 bench 35.007473 71 countertop 56.592858 72 stove 74.866261 73 palm, palm tree 49.092486 74 kitchen island 32.353614 75 computer 57.673329 76 swivel chair 43.202283 77 boat 48.170742 78 bar 24.034261 79 arcade machine 11.467819 80 hovel, hut, hutch, shack, shanty 10.258017 81 bus 81.375072 82 towel 54.954106 83 light 53.256340 84 truck 29.656645 85 tower 36.864496 86 chandelier 63.787459 87 awning, sunshade, sunblind 23.610311 88 street lamp 29.944617 89 booth 29.360433 90 tv 61.512572 91 plane 53.270513 92 dirt track 4.206758 93 clothes 35.342074 94 pole 20.678348 95 land, ground, soil 3.195710 96 bannister, banister, balustrade, balusters, handrail 17.522631 97 escalator, moving staircase, moving stairway 20.889345 98 ottoman, pouf, pouffe, puff, hassock 47.003450 99 bottle 15.504667 100 buffet, counter, sideboard 26.077572 101 poster, posting, placard, notice, bill, card 30.691103 102 stage 11.744151 103 van 40.161822 104 ship 79.300311 105 fountain 0.112958 106 conveyer belt, conveyor belt, conveyer, conveyor, transporter 60.552373 107 canopy 25.086350 108 washer, automatic washer, washing machine 63.550537 109 plaything, toy 18.290597 110 pool 32.873865 111 stool 39.256308 112 barrel, cask 6.358771 113 basket, handbasket 29.850719 114 falls 57.657161 115 tent 93.717152 116 bag 10.629695 117 minibike, motorbike 56.217901 118 cradle 69.441302 119 oven 38.940583 120 ball 45.543376 121 food, solid food 52.779065 122 step, stair 10.843115 123 tank, storage tank 30.871163 124 trade name 27.908376 125 microwave 32.381977 126 pot 41.040635 127 animal 55.882266 128 bicycle 50.185374 129 lake 0.007605 130 dishwasher 58.970317 131 screen 60.016197 132 blanket, cover 26.963189 133 sculpture 27.667732 134 hood, exhaust hood 58.025458 135 sconce 39.341998 136 vase 31.185747 137 traffic light 23.810429 138 tray 7.244281 139 trash can 30.072544 140 fan 52.113861 141 pier 56.678802 142 crt screen 9.133357 143 plate 38.900407 144 monitor 3.323130 145 bulletin board 52.337659 146 shower 4.692180 147 radiator 43.811464 148 glass, drinking glass 14.036491 149 clock 25.044316 150 flag 40.007933"},{"location":"models/fai-m2f-m-ade/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-m2f-m-ade) from Focoos API\nmodel = focoos.get_remote_model(\"fai-m2f-m-ade\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-m2f-s-ade/","title":"fai-m2f-l-ade","text":""},{"location":"models/fai-m2f-s-ade/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the ADE20K dataset. It is a semantic segmentation model able to segment 150 classes, comprising both stuff (sky, road, etc.) and thing (dog, cat, car, etc.).</p>"},{"location":"models/fai-m2f-s-ade/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-m2f-s-ade/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a transformer-based encoder-decoder architecture. Differently from traditional segmentation models (such as DeepLab), Mask2Former uses a mask-classification approach, where the prediction is made by a set of segmentation mask with associated class probabilities.</p>"},{"location":"models/fai-m2f-s-ade/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-1 that shows a trade-off tending to be more efficient.</li> <li>the pixel decoder is a FPN getting the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. Differently from the original paper, for the sake of portability, we removed the deformable attention modules in the pixel decoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a extremely light version of the original, having only 1 decoder layer (instead of 9) and 100 learnable queries.</li> </ul>"},{"location":"models/fai-m2f-s-ade/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-m2f-s-ade/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-m2f-s-ade/#classes","title":"Classes","text":"<p>The model is pretrained on the ADE20K dataset with 150 classes.</p> Class mIoU 1 wall 69.973850 2 building 78.431035 3 sky 91.401107 4 floor 73.162280 5 tree 70.535439 6 ceiling 77.258595 7 road, route 78.314172 8 bed 77.755793 9 window 53.012898 10 grass 64.432303 11 cabinet 51.032268 12 sidewalk, pavement 55.642697 13 person 70.461440 14 eartd, ground 30.454824 15 door 36.431782 16 table 43.096636 17 mountain, mount 54.971609 18 plant 45.115711 19 curtain 64.930372 20 chair 40.465565 21 car 76.888113 22 water 41.666148 23 painting, picture 60.099652 24 sofa 49.840449 25 shelf 31.991519 26 house 45.338182 27 sea 51.614250 28 mirror 55.731406 29 rug 51.858072 30 field 23.065903 31 armchair 30.602317 32 seat 50.277596 33 fence 34.439293 34 desk 35.494495 35 rock, stone 39.573617 36 wardrobe, closet, press 51.343586 37 lamp 47.754304 38 tub 71.511291 39 rail 23.280869 40 cushion 39.251768 41 base, pedestal, stand 28.472143 42 box 16.070477 43 column, pillar 37.924454 44 signboard, sign 29.057276 45 chest of drawers, chest, bureau, dresser 36.343963 46 counter 19.595326 47 sand 31.296151 48 sink 54.413180 49 skyscraper 47.583224 50 fireplace 62.204434 51 refrigerator, icebox 54.270643 52 grandstand, covered stand 31.345801 53 patd 22.330369 54 stairs 20.323718 55 runway 63.892811 56 case, display case, showcase, vitrine 34.649422 57 pool table, billiard table, snooker table 85.365581 58 pillow 46.426184 59 screen door, screen 57.292321 60 stairway, staircase 28.904954 61 river 16.681450 62 bridge, span 52.791513 63 bookcase 26.722881 64 blind, screen 36.787453 65 coffee table 41.603442 66 toilet, can, commode, crapper, pot, potty, stool, tdrone 75.753455 67 flower 30.200230 68 book 37.602484 69 hill 5.509057 70 bench 29.331054 71 countertop 46.661677 72 stove 58.972851 73 palm, palm tree 48.317300 74 kitchen island 25.279206 75 computer 49.335666 76 swivel chair 34.845392 77 boat 48.521646 78 bar 30.174155 79 arcade machine 24.721694 80 hovel, hut, hutch, shack, shanty 32.843717 81 bus 82.174778 82 towel 46.050430 83 light 30.983118 84 truck 23.456256 85 tower 32.147803 86 chandelier 54.045160 87 awning, sunshade, sunblind 18.526182 88 street lamp 13.641714 89 bootd 60.471570 90 tv 55.530715 91 plane 42.894525 92 dirt track 0.001787 93 clotdes 30.124455 94 pole 11.280532 95 land, ground, soil 4.243296 96 bannister, banister, balustrade, balusters, handrail 9.922319 97 escalator, moving staircase, moving stairway 19.186240 98 ottoman, pouf, pouffe, puff, hassock 30.352586 99 bottle 11.872842 100 buffet, counter, sideboard 34.547476 101 poster, posting, placard, notice, bill, card 15.081001 102 stage 17.466091 103 van 39.027877 104 ship 66.778301 105 fountain 18.879113 106 conveyer belt, conveyor belt, conveyer, conveyor, transporter 67.580228 107 canopy 25.654567 108 washer, automatic washer, washing machine 60.187881 109 playtding, toy 13.836259 110 pool 28.796494 111 stool 26.432746 112 barrel, cask 43.777156 113 basket, handbasket 19.144369 114 falls 47.131198 115 tent 88.431441 116 bag 7.634387 117 minibike, motorbike 40.625528 118 cradle 54.247514 119 oven 33.695444 120 ball 36.066130 121 food, solid food 50.837348 122 step, stair 13.071184 123 tank, storage tank 43.042742 124 trade name 21.579095 125 microwave 32.179626 126 pot 27.438416 127 animal 55.993825 128 bicycle 38.273475 129 lake 35.704904 130 dishwasher 37.616793 131 screen 57.100955 132 blanket, cover 15.560568 133 sculpture 31.317035 134 hood, exhaust hood 49.290385 135 sconce 29.971644 136 vase 24.983318 137 traffic light 17.806663 138 tray 5.720345 139 trash can 28.621136 140 fan 39.083851 141 pier 51.310956 142 crt screen 0.858346 143 plate 35.344330 144 monitor 1.994270 145 bulletin board 35.468027 146 shower 1.090403 147 radiator 42.574652 148 glass, drinking glass 8.510381 149 clock 14.128872 150 flag 24.098100"},{"location":"models/fai-m2f-s-ade/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-m2f-s-ade) from Focoos API\nmodel = focoos.get_remote_model(\"fai-m2f-s-ade\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-rtdetr-l-coco/","title":"fai-rtdetr-l-coco","text":""},{"location":"models/fai-rtdetr-l-coco/#overview","title":"Overview","text":"<p>The models is the reimplementation of the RT-DETR model by FocoosAI for the COCO dataset. It is a object detection model able to detect 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-rtdetr-l-coco/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-rtdetr-l-coco/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-rtdetr-l-coco/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>This implementation is a reimplementation of the RT-DETR model by FocoosAI. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is a Resnet-50,that guarantees a good performance while having good efficiency.</li> <li>the encoder is the Hybrid Encoder, as proposed by the paper, and it is a bi-FPN (bilinear feature pyramid network) that includes a transformer encoder on the smaller feature resolution for improving efficiency.</li> <li>The query selection mechanism select the features of the pixels (aka queries) with the highest probability of containing an object and pass them to a transformer decoder head that will generate the final detection output. In this implementation, we select 300 queries and use 6 transformer decoder layers.</li> </ul>"},{"location":"models/fai-rtdetr-l-coco/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. For more details look at GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-rtdetr-l-coco/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-rtdetr-l-coco/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class AP 1 person 63.2 2 bicycle 40.5 3 car 52.3 4 motorcycle 55.0 5 airplane 76.3 6 bus 74.9 7 train 75.0 8 truck 47.9 9 boat 36.6 10 traffic light 32.6 11 fire hydrant 75.5 12 stop sign 71.2 13 parking meter 54.6 14 bench 34.9 15 bird 46.6 16 cat 79.8 17 dog 75.4 18 horse 69.7 19 sheep 63.0 20 cow 68.8 21 elephant 74.1 22 bear 83.2 23 zebra 78.3 24 giraffe 76.9 25 backpack 25.1 26 umbrella 53.8 27 handbag 24.3 28 tie 44.8 29 suitcase 52.6 30 frisbee 75.3 31 skis 37.2 32 snowboard 50.8 33 sports ball 53.9 34 kite 54.8 35 baseball bat 53.2 36 baseball glove 45.3 37 skateboard 63.7 38 surfboard 50.3 39 tennis racket 61.1 40 bottle 48.8 41 wine glass 44.1 42 cup 53.4 43 fork 51.3 44 knife 34.1 45 spoon 33.5 46 bowl 52.1 47 banana 33.0 48 apple 27.1 49 sandwich 48.1 50 orange 37.9 51 broccoli 28.9 52 carrot 28.2 53 hot dog 50.3 54 pizza 62.5 55 donut 62.3 56 cake 47.5 57 chair 41.2 58 couch 57.3 59 potted plant 36.0 60 bed 58.5 61 dining table 39.4 62 toilet 72.6 63 tv 65.8 64 laptop 73.1 65 mouse 67.1 66 remote 48.2 67 keyboard 63.0 68 cell phone 46.0 69 microwave 64.6 70 oven 44.9 71 toaster 50.4 72 sink 45.7 73 refrigerator 69.4 74 book 22.3 75 clock 59.2 76 vase 45.7 77 scissors 42.4 78 teddy bear 59.5 79 hair drier 35.1 80 tootdbrush 42.0"},{"location":"models/fai-rtdetr-l-coco/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-rtdetr-l-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-rtdetr-l-coco\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-rtdetr-m-coco/","title":"fai-rtdetr-m-coco","text":""},{"location":"models/fai-rtdetr-m-coco/#overview","title":"Overview","text":"<p>The models is a RT-DETR model otimized by FocoosAI for the COCO dataset. It is a object detection model able to detect 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-rtdetr-m-coco/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-rtdetr-m-coco/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-rtdetr-m-coco/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The RT-DETR FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-2 that show an amazing trade-off between performance and efficiency.</li> <li>the encoder is a bi-FPN (bilinear feature pyramid network). With respect to the original paper, we removed the attention modules in the encoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 3 decoder layers, instead of 6, and we select 300 queries.</li> </ul>"},{"location":"models/fai-rtdetr-m-coco/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. for more details look here: GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-rtdetr-m-coco/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-rtdetr-m-coco/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class ID Class Name AP 1 person 56.4 2 bicycle 32.8 3 car 44.2 4 motorcycle 47.9 5 airplane 72.5 6 bus 70.4 7 train 69.1 8 truck 39.5 9 boat 27.9 10 traffic light 26.6 11 fire hydrant 68.8 12 stop sign 64.4 13 parking meter 45.5 14 bench 26.0 15 bird 37.4 16 cat 73.4 17 dog 68.6 18 horse 59.5 19 sheep 56.2 20 cow 59.2 21 elephant 67.9 22 bear 77.9 23 zebra 71.4 24 giraffe 72.2 25 backpack 16.7 26 umbrella 43.4 27 handbag 16.6 28 tie 36.5 29 suitcase 44.4 30 frisbee 68.0 31 skis 27.5 32 snowboard 35.0 33 sports ball 46.5 34 kite 44.8 35 baseball bat 29.2 36 baseball glove 38.4 37 skateboard 56.2 38 surfboard 43.3 39 tennis racket 49.5 40 bottle 37.8 41 wine glass 35.7 42 cup 43.1 43 fork 39.0 44 knife 22.6 45 spoon 20.4 46 bowl 43.5 47 banana 27.1 48 apple 22.2 49 sandwich 38.8 50 orange 33.4 51 broccoli 24.7 52 carrot 23.8 53 hot dog 38.4 54 pizza 57.4 55 donut 50.6 56 cake 38.4 57 chair 30.9 58 couch 50.0 59 potted plant 28.9 60 bed 51.4 61 dining table 32.8 62 toilet 67.2 63 tv 59.4 64 laptop 62.7 65 mouse 64.7 66 remote 34.4 67 keyboard 55.8 68 cell phone 38.2 69 microwave 61.4 70 oven 41.8 71 toaster 48.3 72 sink 39.4 73 refrigerator 59.5 74 book 15.5 75 clock 49.0 76 vase 39.3 77 scissors 30.3 78 teddy bear 50.6 79 hair drier 4.8 80 toothbrush 30.2"},{"location":"models/fai-rtdetr-m-coco/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-rtdetr-m-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-rtdetr-m-coco\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-rtdetr-m-obj365/","title":"fai-rtdetr-m-obj365","text":""},{"location":"models/fai-rtdetr-m-obj365/#overview","title":"Overview","text":"<p>The models is a RT-DETR model otimized by FocoosAI for the Objects365. It is a object detection model able to detect 365 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-rtdetr-m-obj365/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-rtdetr-m-obj365/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>This implementation is a reimplementation of the RT-DETR model by FocoosAI. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is a Resnet-50,that guarantees a good performance while having good efficiency.</li> <li>the encoder is the Hybrid Encoder, as proposed by the paper, and it is a bi-FPN (bilinear feature pyramid network) that includes a transformer encoder on the smaller feature resolution for improving efficiency.</li> <li>The query selection mechanism select the features of the pixels (aka queries) with the highest probability of containing an object and pass them to a transformer decoder head that will generate the final detection output. In this implementation, we select 300 queries and use 6 transformer decoder layers.</li> </ul>"},{"location":"models/fai-rtdetr-m-obj365/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. For more details look at GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-rtdetr-m-obj365/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-rtdetr-m-obj365/#classes","title":"Classes","text":"<p>The model is pretrained on the Objects365 with 365 classes.</p> Class ID Class Name AP 1 Person 68.8 2 Sneakers 47.3 3 Chair 52.7 4 Other Shoes 14.8 5 Hat 56.4 6 Car 37.2 7 Lamp 43.7 8 Glasses 47.8 9 Bottle 40.6 10 Desk 48.2 11 Cup 53.9 12 Street Lights 38.9 13 Cabinet/shelf 46.8 14 Handbag/Satchel 29.5 15 Bracelet 27.8 16 Plate 70.5 17 Picture/Frame 66.0 18 Helmet 47.7 19 Book 23.0 20 Gloves 42.9 21 Storage box 24.1 22 Boat 31.1 23 Leather Shoes 29.3 24 Flower 39.6 25 Bench 26.6 26 Potted Plant 42.4 27 Bowl/Basin 57.7 28 Flag 38.6 29 Pillow 56.0 30 Boots 39.8 31 Vase 37.1 32 Microphone 39.1 33 Necklace 30.3 34 Ring 19.5 35 SUV 33.4 36 Wine Glass 68.6 37 Belt 35.8 38 Moniter/TV 75.1 39 Backpack 29.9 40 Umbrella 37.4 41 Traffic Light 39.0 42 Speaker 58.3 43 Watch 47.1 44 Tie 37.0 45 Trash bin Can 49.1 46 Slippers 43.9 47 Bicycle 46.8 48 Stool 48.9 49 Barrel/bucket 39.9 50 Van 30.7 51 Couch 62.5 52 Sandals 43.7 53 Bakset 40.1 54 Drum 54.8 55 Pen/Pencil 29.9 56 Bus 45.3 57 Wild Bird 12.6 58 High Heels 41.9 59 Motorcycle 31.8 60 Guitar 64.7 61 Carpet 59.8 62 Cell Phone 43.8 63 Bread 23.6 64 Camera 32.8 65 Canned 37.4 66 Truck 23.0 67 Traffic cone 44.5 68 Cymbal 55.7 69 Lifesaver 31.2 70 Towel 54.0 71 Stuffed Toy 40.7 72 Candle 30.5 73 Sailboat 55.5 74 Laptop 73.8 75 Awning 27.4 76 Bed 66.1 77 Faucet 41.8 78 Tent 30.8 79 Horse 46.3 80 Mirror 59.0 81 Power outlet 43.1 82 Sink 53.3 83 Apple 22.0 84 Air Conditioner 30.1 85 Knife 46.1 86 Hockey Stick 59.0 87 Paddle 25.5 88 Pickup Truck 45.2 89 Fork 57.7 90 Traffic Sign 30.0 91 Ballon 47.7 92 Tripod 29.7 93 Dog 58.5 94 Spoon 46.7 95 Clock 61.6 96 Pot 44.3 97 Cow 18.7 98 Cake 15.7 99 Dinning Table 46.2 100 Sheep 30.5 101 Hanger 9.9 102 Blackboard/Whiteboard 47.5 103 Napkin 35.5 104 Other Fish 30.4 105 Orange/Tangerine 10.8 106 Toiletry 30.1 107 Keyboard 71.8 108 Tomato 36.4 109 Lantern 47.7 110 Machinery Vehicle 30.5 111 Fan 49.4 112 Green Vegetables 13.2 113 Banana 30.2 114 Baseball Glove 38.8 115 Airplane 60.9 116 Mouse 61.5 117 Train 50.5 118 Pumpkin 53.4 119 Soccer 29.0 120 Skiboard 24.3 121 Luggage 32.5 122 Nightstand 62.3 123 Tea pot 31.7 124 Telephone 45.8 125 Trolley 36.8 126 Head Phone 40.9 127 Sports Car 67.8 128 Stop Sign 49.3 129 Dessert 28.7 130 Scooter 35.5 131 Stroller 42.7 132 Crane 46.0 133 Remote 47.1 134 Refrigerator 70.2 135 Oven 51.9 136 Lemon 33.4 137 Duck 43.3 138 Baseball Bat 40.4 139 Surveillance Camera 24.2 140 Cat 67.5 141 Jug 24.1 142 Broccoli 29.3 143 Piano 41.5 144 Pizza 50.9 145 Elephant 66.9 146 Skateboard 19.0 147 Surfboard 44.0 148 Gun 23.8 149 Skating and Skiing shoes 64.6 150 Gas stove 39.2 151 Donut 45.0 152 Bow Tie 28.8 153 Carrot 15.6 154 Toilet 73.2 155 Kite 44.1 156 Strawberry 24.2 157 Other Balls 36.1 158 Shovel 18.9 159 Pepper 18.5 160 Computer Box 49.3 161 Toilet Paper 39.0 162 Cleaning Products 21.1 163 Chopsticks 40.3 164 Microwave 68.0 165 Pigeon 48.1 166 Baseball 32.6 167 Cutting/chopping Board 40.9 168 Coffee Table 49.8 169 Side Table 34.6 170 Scissors 28.1 171 Marker 20.6 172 Pie 20.8 173 Ladder 34.4 174 Snowboard 36.8 175 Cookies 13.0 176 Radiator 50.3 177 Fire Hydrant 47.6 178 Basketball 32.9 179 Zebra 58.3 180 Grape 10.4 181 Giraffe 64.0 182 Potato 11.0 183 Sausage 25.1 184 Tricycle 27.4 185 Violin 39.6 186 Egg 34.7 187 Fire Extinguisher 47.3 188 Candy 1.9 189 Fire Truck 55.7 190 Billards 63.0 191 Converter 18.0 192 Bathtub 58.8 193 Wheelchair 62.9 194 Golf Club 33.8 195 Briefcase 35.1 196 Cucumber 26.6 197 Cigar/Cigarette 11.2 198 Paint Brush 15.0 199 Pear 5.5 200 Heavy Truck 35.3 201 Hamburger 40.5 202 Extractor 62.7 203 Extention Cord 19.1 204 Tong 16.4 205 Tennis Racket 51.4 206 Folder 8.8 207 American Football 22.6 208 earphone 7.6 209 Mask 36.9 210 Kettle 43.9 211 Tennis 37.3 212 Ship 49.0 213 Swing 48.4 214 Coffee Machine 50.3 215 Slide 46.7 216 Carriage 59.5 217 Onion 7.4 218 Green beans 3.6 219 Projector 48.6 220 Frisbee 33.0 221 Washing Machine/Drying Machine 53.6 222 Chicken 49.5 223 Printer 54.8 224 Watermelon 28.7 225 Saxophone 52.2 226 Tissue 31.6 227 Toothbrush 23.6 228 Ice cream 27.6 229 Hotair ballon 77.2 230 Cello 45.9 231 French Fries 42.7 232 Scale 28.3 233 Trophy 37.6 234 Cabbage 11.9 235 Hot dog 39.9 236 Blender 44.3 237 Peach 6.2 238 Rice 44.3 239 Wallet/Purse 30.4 240 Volleyball 51.2 241 Deer 45.0 242 Goose 17.5 243 Tape 24.0 244 Tablet 39.9 245 Cosmetics 18.8 246 Trumpet 36.7 247 Pineapple 19.1 248 Golf Ball 39.5 249 Ambulance 78.2 250 Parking meter 33.8 251 Mango 0.8 252 Key 3.0 253 Hurdle 33.9 254 Fishing Rod 29.1 255 Medal 22.8 256 Flute 31.9 257 Brush 8.2 258 Penguin 57.5 259 Megaphone 18.8 260 Corn 22.4 261 Lettuce 2.3 262 Garlic 16.8 263 Swan 46.1 264 Helicopter 42.7 265 Green Onion 0.5 266 Sandwich 29.5 267 Nuts 0.6 268 Speed Limit Sign 43.4 269 Induction Cooker 27.3 270 Broom 19.6 271 Trombone 33.0 272 Plum 3.7 273 Rickshaw 24.7 274 Goldfish 14.3 275 Kiwi fruit 8.3 276 Router/modem 14.5 277 Poker Card 31.2 278 Toaster 48.6 279 Shrimp 5.0 280 Sushi 22.3 281 Cheese 21.2 282 Notepaper 9.3 283 Cherry 4.3 284 Pliers 12.2 285 CD 21.1 286 Pasta 36.3 287 Hammer 20.6 288 Cue 46.2 289 Avocado 9.8 290 Hamimelon 2.9 291 Flask 31.2 292 Mushroon 3.1 293 Screwdriver 13.6 294 Soap 19.0 295 Recorder 32.9 296 Bear 46.2 297 Eggplant 8.8 298 Board Eraser 38.7 299 Coconut 10.0 300 Tape Measur/ Ruler 15.0 301 Pig 50.0 302 Showerhead 11.6 303 Globe 57.1 304 Chips 13.7 305 Steak 28.2 306 Crosswalk Sign 48.2 307 Stapler 27.2 308 Campel 55.3 309 Formula 1 59.1 310 Pomegranate 3.9 311 Dishwasher 53.1 312 Crab 11.6 313 Hoverboard 29.9 314 Meat ball 7.6 315 Rice Cooker 30.5 316 Tuba 24.6 317 Calculator 38.0 318 Papaya 4.3 319 Antelope 24.4 320 Parrot 34.1 321 Seal 41.1 322 Buttefly 36.4 323 Dumbbell 8.0 324 Donkey 42.0 325 Lion 33.8 326 Urinal 53.0 327 Dolphin 39.5 328 Electric Drill 24.1 329 Hair Dryer 10.0 330 Egg tart 5.1 331 Jellyfish 40.2 332 Treadmill 43.9 333 Lighter 12.6 334 Grapefruit 1.2 335 Game board 37.3 336 Mop 5.8 337 Radish 0.6 338 Baozi 40.2 339 Target 14.6 340 French 27.3 341 Spring Rolls 29.1 342 Monkey 37.8 343 Rabbit 36.4 344 Pencil Case 22.8 345 Yak 37.7 346 Red Cabbage 7.1 347 Binoculars 15.7 348 Asparagus 4.1 349 Barbell 17.1 350 Scallop 12.4 351 Noddles 21.1 352 Comb 14.1 353 Dumpling 5.2 354 Oyster 17.6 355 Table Teniis paddle 22.2 356 Cosmetics Brush/Eyeliner Pencil 40.3 357 Chainsaw 13.8 358 Eraser 16.4 359 Lobster 18.0 360 Durian 33.7 361 Okra 0.1 362 Lipstick 36.3 363 Cosmetics Mirror 8.2 364 Curling 44.7 365 Table Tennis 25.1"},{"location":"models/fai-rtdetr-m-obj365/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-rtdetr-s-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-rtdetr-m-obj365\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-rtdetr-n-coco/","title":"fai-rtdetr-n-coco","text":""},{"location":"models/fai-rtdetr-n-coco/#overview","title":"Overview","text":"<p>The models is a RT-DETR model otimized by FocoosAI for the COCO dataset. It is a object detection model able to detect 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-rtdetr-n-coco/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-rtdetr-n-coco/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-rtdetr-n-coco/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The RT-DETR FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-1 that shows an amazing speed while maintaining a satisfactory accuracy.</li> <li>the encoder is a bi-FPN (bilinear feature pyramid network). With respect to the original paper, we removed the attention modules in the encoder and we reduce the internal features dimension, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 3 decoder layers, instead of 6, and we select 300 queries.</li> </ul>"},{"location":"models/fai-rtdetr-n-coco/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. for more details look here: GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-rtdetr-n-coco/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-rtdetr-n-coco/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class ID Class Name AP 1 person 53.3 2 bicycle 28.0 3 car 40.5 4 motorcycle 42.6 5 airplane 67.8 6 bus 65.0 7 train 63.7 8 truck 34.7 9 boat 27.4 10 traffic light 25.0 11 fire hydrant 63.9 12 stop sign 62.1 13 parking meter 46.6 14 bench 23.1 15 bird 35.0 16 cat 70.6 17 dog 65.8 18 horse 54.2 19 sheep 52.7 20 cow 56.5 21 elephant 64.0 22 bear 72.9 23 zebra 69.7 24 giraffe 68.1 25 backpack 12.1 26 umbrella 37.1 27 handbag 11.9 28 tie 31.3 29 suitcase 40.2 30 frisbee 66.2 31 skis 22.4 32 snowboard 27.6 33 sports ball 42.7 34 kite 44.9 35 baseball bat 24.8 36 baseball glove 33.4 37 skateboard 49.1 38 surfboard 34.9 39 tennis racket 43.8 40 bottle 34.3 41 wine glass 30.7 42 cup 38.6 43 fork 32.2 44 knife 15.4 45 spoon 15.1 46 bowl 38.1 47 banana 26.0 48 apple 18.8 49 sandwich 36.6 50 orange 30.6 51 broccoli 23.6 52 carrot 22.2 53 hot dog 31.9 54 pizza 53.9 55 donut 45.7 56 cake 34.7 57 chair 26.0 58 couch 44.1 59 potted plant 24.5 60 bed 46.2 61 dining table 28.7 62 toilet 60.6 63 tv 56.0 64 laptop 58.3 65 mouse 58.4 66 remote 27.6 67 keyboard 51.6 68 cell phone 32.6 69 microwave 56.1 70 oven 34.4 71 toaster 45.6 72 sink 35.6 73 refrigerator 53.8 74 book 12.6 75 clock 48.9 76 vase 33.9 77 scissors 26.9 78 teddy bear 45.1 79 hair drier 10.0 80 toothbrush 26.3"},{"location":"models/fai-rtdetr-n-coco/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-rtdetr-n-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-rtdetr-n-coco\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-rtdetr-s-coco/","title":"fai-rtdetr-s-coco","text":""},{"location":"models/fai-rtdetr-s-coco/#overview","title":"Overview","text":"<p>The models is a RT-DETR model otimized by FocoosAI for the COCO dataset. It is a object detection model able to detect 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-rtdetr-s-coco/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-rtdetr-s-coco/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-rtdetr-s-coco/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The RT-DETR FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-2 that show an amazing trade-off between performance and efficiency.</li> <li>the encoder is a bi-FPN (bilinear feature pyramid network). With respect to the original paper, we removed the attention modules in the encoder and we reduce the internal features dimension, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 3 decoder layers, instead of 6, and we select 300 queries.</li> </ul>"},{"location":"models/fai-rtdetr-s-coco/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. for more details look here: GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-rtdetr-s-coco/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-rtdetr-s-coco/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class ID Class Name AP 1 person 54.7 2 bicycle 29.1 3 car 41.4 4 motorcycle 44.9 5 airplane 71.4 6 bus 67.8 7 train 68.9 8 truck 36.4 9 boat 26.8 10 traffic light 25.0 11 fire hydrant 66.0 12 stop sign 62.2 13 parking meter 46.1 14 bench 25.2 15 bird 36.5 16 cat 72.6 17 dog 68.5 18 horse 57.9 19 sheep 54.1 20 cow 56.6 21 elephant 66.2 22 bear 78.3 23 zebra 70.0 24 giraffe 70.0 25 backpack 14.9 26 umbrella 39.9 27 handbag 13.2 28 tie 32.6 29 suitcase 41.2 30 frisbee 66.3 31 skis 24.9 32 snowboard 31.6 33 sports ball 44.8 34 kite 45.1 35 baseball bat 29.7 36 baseball glove 35.2 37 skateboard 54.5 38 surfboard 39.9 39 tennis racket 46.1 40 bottle 35.8 41 wine glass 32.6 42 cup 41.1 43 fork 35.5 44 knife 18.9 45 spoon 18.0 46 bowl 42.2 47 banana 24.6 48 apple 18.6 49 sandwich 41.6 50 orange 33.1 51 broccoli 22.4 52 carrot 22.2 53 hot dog 37.6 54 pizza 55.2 55 donut 48.0 56 cake 36.7 57 chair 28.4 58 couch 47.8 59 potted plant 26.8 60 bed 49.0 61 dining table 30.5 62 toilet 60.1 63 tv 57.2 64 laptop 59.6 65 mouse 62.3 66 remote 27.7 67 keyboard 53.8 68 cell phone 33.2 69 microwave 60.7 70 oven 38.8 71 toaster 41.9 72 sink 37.0 73 refrigerator 57.6 74 book 13.8 75 clock 50.3 76 vase 35.5 77 scissors 31.8 78 teddy bear 44.7 79 hair drier 10.3 80 toothbrush 26.8"},{"location":"models/fai-rtdetr-s-coco/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-rtdetr-s-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-rtdetr-s-coco\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"}]}