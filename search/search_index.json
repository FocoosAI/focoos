{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Focoos AI","text":"<p>Focoos AI provides an advanced development platform designed to empower developers and businesses with efficient, customizable computer vision solutions. Whether you're working with data from cloud infrastructures or deploying on edge devices, Focoos AI enables you to select, fine-tune, and deploy state-of-the-art models optimized for your unique needs.</p>"},{"location":"#sdk-overview","title":"SDK Overview","text":"<p>This powerful SDK gives you seamless access to our cutting-edge computer vision models and tools, allowing you to effortlessly interact with the Focoos API. With just a few lines of code, you can easily select, customize, test, and deploy pre-trained models tailored to your specific needs.</p> <p>Whether you're deploying in the cloud or on edge devices, the Focoos Python SDK integrates smoothly into your workflow, speeding up your development process.</p>"},{"location":"#quickstart","title":"Quickstart \ud83d\ude80","text":"<p>Ready to dive in? Get started with the setup in just a few simple steps!</p> <p>Install the Focoos Python SDK (for more options, see setup)</p> uvpipconda <pre><code>uv pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <p>\ud83d\ude80 Directly use our Efficient Models, optimized for different data, applications, and hardware.</p> <pre><code>from focoos import Focoos\nfrom PIL import Image\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n# Get the remote model (fai-rtdetr-m-obj365) from Focoos API\nmodel = focoos.get_remote_model(\"fai-rtdetr-m-obj365\")\n\n# Run inference on an image\ndetections, preview = model.infer(image_path, threshold=0.4, annotations=True)\n\n# Output the detections\nImage.fromarray(preview[:, :, [2, 1, 0]])\n</code></pre> <p>\u2699\ufe0f Customize the models to your specific needs by fine-tuning on your own dataset.</p> <pre><code>from focoos import Focoos\nfrom focoos.ports import Hyperparameters\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\nmodel = focoos.new_model(name=\"awesome\",\n                         focoos_model=\"fai-rtdetr-m-obj365\",\n                         description=\"An awesome model\")\n\nres = model.train(\n    dataset_ref=\"&lt;YOUR-DATASET-ID&gt;\",\n    hyperparameters=Hyperparameters(\n        learning_rate=0.0001,\n        batch_size=16,\n        max_iters=1500,\n    )\n)\n</code></pre> <p>See more examples in the how to section.</p>"},{"location":"#our-models","title":"Our Models \ud83e\udde0","text":"<p>Focoos AI offers the best models in object detection, semantic and instance segmentation, and more is coming soon.</p> <p>Using Focoos AI helps you save both time and money while delivering high-performance AI models \ud83d\udcaa:</p> <ul> <li>10x Faster \u23f3: Our models are able to process images up to ten times faster than traditional methods.</li> <li>4x Cheaper \ud83d\udcb0: Our models require up to 4x less computational power, letting you save on hardware or cloud bill while ensuring high-quality results.</li> <li>Tons of CO2 saved annually per model \ud83c\udf31: Our models are energy-efficient, helping you reduce your carbon footprint by using less powerful hardware with respect to mainstream models.</li> </ul> <p>These are not empty promises, but the result of years of research and development by our team \ud83d\udd2c:</p> ADE-20k Semantic Segmentation Results COCO Object Detection Results <p>See the list of our models in the models section.</p>"},{"location":"#start-now","title":"Start now!","text":"<p>By choosing Focoos AI, you can save time, reduce costs, and achieve superior model performance, all while ensuring the privacy and efficiency of your deployments. Reach out to us to ask for your API key for free and power your computer vision projects. \ud83d\ude80</p>"},{"location":"datasets/","title":"Datasets","text":"<p>With the Focoos SDK, you can leverage a diverse collection of foundational datasets specifically tailored for computer vision tasks. These datasets, spanning tasks such as segmentation, detection, and instance segmentation, provide a strong foundation for building and optimizing models across a variety of domains.</p> <p>Datasets:</p> Name Task Description Layout Aeroscapes semseg A drone dataset to recognize many classes! supervisely Blister instseg A dataset to find blisters roboflow_coco Boxes detection Finding different boxes on the conveyor belt roboflow_coco Cable detection A dataset for detecting damages in cables (from Roboflow 100) - https://universe.roboflow.com/roboflow-100/cable-damage/dataset/2# roboflow_coco Circuit dataset detection A dataset with electronic circuits roboflow_coco Concrete instseg A dataset to find defect in concrete roboflow_coco Crack Segmentation instseg A dataset for segmenting cracks in buildings with 4k images. roboflow_coco Football-detection detection Football-detection by Roboflow roboflow_coco Peanuts detection Finding Molded or non Molded Peanuts roboflow_coco Strawberries instseg Finding defects on strawberries roboflow_coco aquarium detection aquarium roboflow_coco bottles detection bottles roboflow_coco chess_pieces detection A chess detector dataset by roboflow https://universe.roboflow.com/roboflow-100/chess-pieces-mjzgj roboflow_coco coco_2017_det detection COCO Detection catalog halo detection Halo fps by Roboflow roboflow_coco lettuce detection A dataset to find lettuce roboflow_coco safety detection From roboflow Universe: https://universe.roboflow.com/roboflow-100/construction-safety-gsnvb roboflow_coco screw detection Screw by Roboflow roboflow_coco"},{"location":"models/","title":"Focoos Models \ud83e\udde0","text":"<p>With the Focoos SDK, you can take advantage of a collection of foundational models that are optimized for a range of computer vision tasks. These pre-trained models, covering detection and semantic segmentation across various domains, provide an excellent starting point for your specific use case. Whether you need to fine-tune for custom requirements or adapt them to your application, these models offer a solid foundation to accelerate your development process.</p>"},{"location":"models/#semantic-segmentation","title":"Semantic Segmentation \ud83d\uddbc\ufe0f","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-m2f-l-ade Mask2Former (Resnet-101) Common Scene (150) ADE20K mIoU: 48.27mAcc: 62.15 73 fai-m2f-m-ade Mask2Former (STDC-2) Common Scene (150) ADE20K mIoU: 45.32mACC: 57.75 127 fai-m2f-s-ade Mask2Former (STDC-1) Common Scene (150) ADE20K mIoU: 41.23mAcc: 52.21 189 <p> mIoU = Intersection over Union averaged by class   mAcc = Pixel Accuracy averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"models/#object-detection","title":"Object Detection \ud83d\udd75\ufe0f\u200d\u2642\ufe0f","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-rtdetr-l-coco RT-DETR (Resnet-50) Common Objects (80) COCO bbox/AP: 53.06bbox/AP50: 70.91 87 fai-rtdetr-m-coco RT-DETR (STDC-2) Common Objects (80) COCO bbox/AP: 44.69bbox/AP50: 61.63 181 fai-rtdetr-s-coco RT-DETR (STDC-1) Common Objects (80) COCO bbox/AP: 42.58bbox/AP50: 59.22 220 fai-rtdetr-n-coco RT-DETR (STDC-1) Common Objects (80) COCO bbox/AP: 40.59bbox/AP50: 56.69 269 fai-rtdetr-m-obj365 RT-DETR (Resnet50) Common Objects (365) Objects365 bbox/AP: 34.60bbox/AP50: 45.81 87 <p> AP = Average Precision averaged by class   AP50 = Average Precision at IoU threshold 0.50 averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"models/#instance-segmentation","title":"Instance Segmentation \ud83c\udfad","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-m2f-l-coco-ins Mask2Former (Resnet-50) Common Objects (80) COCO segm/AP: 42.39segm/AP50: 66.12 54 <p> AP = Average Precision averaged by class   AP50 = Average Precision at IoU threshold 0.50 averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"setup/","title":"Python SDK Setup \ud83d\udc0d","text":"<p>Focoos models support multiple inference runtimes. To keep the library lightweight and to allow users to use their environment, optional dependencies (e.g., torch, onnxruntime, tensorrt) are not installed by default.</p> <p>Focoos is shipped with the following local inference runtimes that requires to install additional dependencies. If you intend to use only Focoos AI servers for inference, you don't need to install any of the following dependencies.</p> RuntimeType Extra Runtime Compatible Devices Available ExecutionProvider ONNX_CUDA32 <code>[cuda]</code> onnxruntime CUDA NVIDIA GPUs CUDAExecutionProvider ONNX_TRT32 <code>[tensorrt]</code> onnxruntime TRT NVIDIA GPUs (Optimized) CUDAExecutionProvider, TensorrtExecutionProvider ONNX_TRT16 <code>[tensorrt]</code> onnxruntime TRT NVIDIA GPUs (Optimized) CUDAExecutionProvider, TensorrtExecutionProvider ONNX_CPU <code>[cpu]</code> onnxruntime CPU CPU (x86, ARM), M1, M2, M3 (Apple Silicon) CPUExecutionProvider, CoreMLExecutionProvider, AzureExecutionProvider ONNX_COREML <code>[cpu]</code> onnxruntime CPU M1, M2, M3 (Apple Silicon) CoreMLExecutionProvider, CPUExecutionProvider TORCHSCRIPT_32 <code>[torch]</code> torchscript CPU, NVIDIA GPUs -"},{"location":"setup/#install-the-focoos-sdk","title":"Install the Focoos SDK","text":"<p>The Focoos SDK can be installed with different package managers using python 3.10 and above.</p> uvpipconda <p>We recommend using UV (how to install uv) as a package manager and environment manager for a streamlined dependency management experience.</p> <p>You can easily create a new virtual environment with UV using the following command: <pre><code>uv venv --python 3.12\nsource .venv/bin/activate\n</code></pre></p> Cloud RuntimeCPU ONNX RuntimeTorchscript RuntimeNVIDIA GPU ONNX RuntimeNVIDIA GPU ONNX Runtime with TensorRT <pre><code>uv pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>uv pip install 'focoos[cpu] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <p>To run the models using the torchscript runtime, you need to install the torch package. <pre><code>uv pip install 'focoos[torch] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre></p> <pre><code>uv pip install 'focoos[cuda] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> To perform inference using TensorRT, ensure you have TensorRT version 10.5 installed. <pre><code>uv pip install 'focoos[tensorrt] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Create and activate a new virtual environment using pip with the following commands: <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> Cloud RuntimeCPU ONNX RuntimeTorchscript RuntimeNVIDIA GPU ONNX RuntimeNVIDIA GPU ONNX Runtime with TensorRT <pre><code>pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos[cpu] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos[torch] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> <pre><code>pip install 'focoos[cuda] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> To perform inference using TensorRT, ensure you have TensorRT version 10.5 installed. <pre><code>pip install 'focoos[tensorrt] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Create and activate a new conda (how to install conda) environment with Python 3.10 or higher: <pre><code>conda create -n focoos python=3.12\nconda activate focoos\nconda install pip\n</code></pre></p> Cloud RuntimeCPU ONNX RuntimeTorchscript RuntimeNVIDIA GPU ONNX RuntimeNVIDIA GPU ONNX Runtime with TensorRT <pre><code>pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos[cpu] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <pre><code>pip install 'focoos[torch] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> <pre><code>pip install 'focoos[cuda] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.20.1. To install cuDNN 9: <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> To perform inference using TensorRT, ensure you have TensorRT version 10.5 installed. <pre><code>pip install 'focoos[tensorrt] @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre></p> <p>Note</p> <p>\ud83e\udd16 Multiple Runtimes: You can install multiple extras by running <code>pip install .[torch,cuda,tensorrt]</code>. Anyway you can't use <code>cpu</code> and <code>cuda</code> or <code>tensorrt</code> at the same time.</p> <p>Note</p> <p>\ud83d\udee0\ufe0f Installation Tip: If you want to install a specific version, for example <code>v0.1.3</code>, use: <pre><code>pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git@v0.1.3'\n</code></pre> \ud83d\udccb Check Versions: Visit https://github.com/FocoosAI/focoos/tags for available versions.</p>"},{"location":"setup/#docker-and-devcontainers","title":"Docker and Devcontainers","text":"<p>For container support, Focoos offers four different Docker images:</p> <ul> <li><code>focoos-cpu</code>: only CPU</li> <li><code>focoos-cuda</code>: Includes ONNX (CUDA) support</li> <li><code>focoos-torch</code>: Includes ONNX and Torchscript (CUDA) support</li> <li><code>focoos-tensorrt</code>: Includes ONNX, Torchscript, and TensorRT  support</li> </ul> <p>to use the docker images, you can run the following command:</p> <pre><code>docker run -it . --target=focoos-cpu\n</code></pre> <p>This repository also includes a devcontainer configuration for each of the above images. You can launch these devcontainers in Visual Studio Code for a seamless development experience.</p>"},{"location":"api/focoos/","title":"focoos","text":"<p>Focoos Module</p> <p>This module provides a Python interface for interacting with Focoos APIs, allowing users to manage machine learning models and datasets in the Focoos ecosystem. The module supports operations such as retrieving model metadata, downloading models, and listing shared datasets.</p> <p>Classes:</p> Name Description <code>Focoos</code> <p>Main class to interface with Focoos APIs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised for invalid API responses or missing parameters.</p>"},{"location":"api/focoos/#focoos.focoos.Focoos","title":"<code>Focoos</code>","text":"<p>Main class to interface with Focoos APIs.</p> <p>This class provides methods to interact with Focoos-hosted models and datasets. It supports functionalities such as listing models, retrieving model metadata, downloading models, and creating new models.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key for authentication.</p> <code>http_client</code> <code>HttpClient</code> <p>HTTP client for making API requests.</p> <code>user_info</code> <code>dict</code> <p>Information about the currently authenticated user.</p> <code>cache_dir</code> <code>str</code> <p>Local directory for caching downloaded models.</p> Source code in <code>focoos/focoos.py</code> <pre><code>class Focoos:\n    \"\"\"\n    Main class to interface with Focoos APIs.\n\n    This class provides methods to interact with Focoos-hosted models and datasets.\n    It supports functionalities such as listing models, retrieving model metadata,\n    downloading models, and creating new models.\n\n    Attributes:\n        api_key (str): The API key for authentication.\n        http_client (HttpClient): HTTP client for making API requests.\n        user_info (dict): Information about the currently authenticated user.\n        cache_dir (str): Local directory for caching downloaded models.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        host_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes the Focoos API client.\n\n        This client provides authenticated access to the Focoos API, enabling various operations\n        through the configured HTTP client. It retrieves user information upon initialization and\n        logs the environment details.\n\n        Args:\n            api_key (Optional[str]): API key for authentication. Defaults to the `focoos_api_key`\n                specified in the FOCOOS_CONFIG.\n            host_url (Optional[str]): Base URL for the Focoos API. Defaults to the `default_host_url`\n                specified in the FOCOOS_CONFIG.\n\n        Raises:\n            ValueError: If the API key is not provided, or if the host URL is not specified in the\n                arguments or the configuration.\n\n        Attributes:\n            api_key (str): The API key used for authentication.\n            http_client (HttpClient): An HTTP client instance configured with the API key and host URL.\n            user_info (dict): Information about the authenticated user retrieved from the API.\n            cache_dir (str): Path to the cache directory used by the client.\n\n        Logs:\n            - Error if the API key or host URL is missing.\n            - Info about the authenticated user and environment upon successful initialization.\n        \"\"\"\n        self.api_key = api_key or FOCOOS_CONFIG.focoos_api_key\n        if not self.api_key:\n            logger.error(\"API key is required \ud83e\udd16\")\n            raise ValueError(\"API key is required \ud83e\udd16\")\n\n        host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n        self.http_client = HttpClient(self.api_key, host_url)\n        self.user_info = self.get_user_info()\n        self.cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"focoos\")\n        logger.info(f\"Currently logged as: {self.user_info.email} environment: {host_url}\")\n\n    def get_user_info(self) -&gt; User:\n        \"\"\"\n        Retrieves information about the authenticated user.\n\n        Returns:\n            dict: Information about the user (e.g., email).\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.get(\"user/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get user info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get user info: {res.status_code} {res.text}\")\n        return User.from_json(res.json())\n\n    def get_model_info(self, model_name: str) -&gt; ModelMetadata:\n        \"\"\"\n        Retrieves metadata for a specific model.\n\n        Args:\n            model_name (str): Name of the model.\n\n        Returns:\n            ModelMetadata: Metadata of the specified model.\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.get(f\"models/{model_name}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n        return ModelMetadata.from_json(res.json())\n\n    def list_models(self) -&gt; list[ModelPreview]:\n        \"\"\"\n        Lists all available models.\n\n        Returns:\n            list[ModelPreview]: List of model previews.\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.get(\"models/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list models: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list models: {res.status_code} {res.text}\")\n        return [ModelPreview.from_json(r) for r in res.json()]\n\n    def list_focoos_models(self) -&gt; list[ModelPreview]:\n        \"\"\"\n        Lists models specific to Focoos.\n\n        Returns:\n            list[ModelPreview]: List of Focoos models.\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.get(\"models/focoos-models\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n        return [ModelPreview.from_json(r) for r in res.json()]\n\n    def get_local_model(\n        self,\n        model_ref: str,\n        runtime_type: Optional[RuntimeTypes] = RuntimeTypes.ONNX_CUDA32,\n    ) -&gt; LocalModel:\n        \"\"\"\n        Retrieves a local model for the specified reference.\n\n        Downloads the model if it does not already exist in the local cache.\n\n        Args:\n            model_ref (str): Reference identifier for the model.\n            runtime_type (Optional[RuntimeTypes]): Runtime type for the model. Defaults to the\n                `runtime_type` specified in FOCOOS_CONFIG.\n\n        Returns:\n            LocalModel: An instance of the local model.\n\n        Raises:\n            ValueError: If the runtime type is not specified.\n\n        Notes:\n            The model is cached in the directory specified by `self.cache_dir`.\n        \"\"\"\n        runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n        model_dir = os.path.join(self.cache_dir, model_ref)\n        format = ModelFormat.from_runtime_type(runtime_type)\n        if not os.path.exists(os.path.join(model_dir, f\"model.{format.value}\")):\n            self._download_model(\n                model_ref,\n                format=format,\n            )\n        return LocalModel(model_dir, runtime_type)\n\n    def get_remote_model(self, model_ref: str) -&gt; RemoteModel:\n        \"\"\"\n        Retrieves a remote model instance.\n\n        Args:\n            model_ref (str): Reference name of the model.\n\n        Returns:\n            RemoteModel: The remote model instance.\n        \"\"\"\n        return RemoteModel(model_ref, self.http_client)\n\n    def new_model(self, name: str, focoos_model: str, description: str) -&gt; RemoteModel:\n        \"\"\"\n        Creates a new model in the Focoos system.\n\n        Args:\n            name (str): Name of the new model.\n            focoos_model (str): Reference to the base Focoos model.\n            description (str): Description of the new model.\n\n        Returns:\n            Optional[RemoteModel]: The created model instance, or None if creation fails.\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.post(\n            \"models/\",\n            data={\n                \"name\": name,\n                \"focoos_model\": focoos_model,\n                \"description\": description,\n            },\n        )\n        if res.status_code in [200, 201]:\n            return RemoteModel(res.json()[\"ref\"], self.http_client)\n        if res.status_code == 409:\n            logger.warning(f\"Model already exists: {name}\")\n            return self.get_model_by_name(name, remote=True)\n        logger.warning(f\"Failed to create new model: {res.status_code} {res.text}\")\n\n    def list_shared_datasets(self) -&gt; list[DatasetMetadata]:\n        \"\"\"\n        Lists datasets shared with the user.\n\n        Returns:\n            list[DatasetMetadata]: List of shared datasets.\n\n        Raises:\n            ValueError: If the API request fails.\n        \"\"\"\n        res = self.http_client.get(\"datasets/shared\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        return [DatasetMetadata.from_json(dataset) for dataset in res.json()]\n\n    def _download_model(self, model_ref: str, format: ModelFormat = ModelFormat.ONNX) -&gt; str:\n        \"\"\"\n        Downloads a model from the Focoos API.\n\n        Args:\n            model_ref (str): Reference name of the model.\n\n        Returns:\n            str: Path to the downloaded model.\n\n        Raises:\n            ValueError: If the API request fails or the download fails.\n        \"\"\"\n        model_dir = os.path.join(self.cache_dir, model_ref)\n        model_path = os.path.join(model_dir, f\"model.{format.value}\")\n        metadata_path = os.path.join(model_dir, \"focoos_metadata.json\")\n        if os.path.exists(model_path) and os.path.exists(metadata_path):\n            logger.info(\"\ud83d\udce5 Model already downloaded\")\n            return model_path\n\n        ## download model metadata\n        res = self.http_client.get(f\"models/{model_ref}/download?format={format.value}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to download model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to download model: {res.status_code} {res.text}\")\n\n        download_data = res.json()\n        metadata = ModelMetadata.from_json(download_data[\"model_metadata\"])\n        download_uri = download_data[\"download_uri\"]\n\n        ## download model from Focoos Cloud\n        logger.debug(f\"Model URI: {download_uri}\")\n        logger.info(\"\ud83d\udce5 Downloading model from Focoos Cloud.. \")\n        response = self.http_client.get_external_url(download_uri, stream=True)\n        if response.status_code != 200:\n            logger.error(f\"Failed to download model: {response.status_code} {response.text}\")\n            raise ValueError(f\"Failed to download model: {response.status_code} {response.text}\")\n        total_size = int(response.headers.get(\"content-length\", 0))\n        logger.info(f\"\ud83d\udce5 Size: {total_size / (1024**2):.2f} MB\")\n\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir)\n\n        with open(metadata_path, \"w\") as f:\n            f.write(metadata.model_dump_json())\n        logger.debug(f\"Dumped metadata to {metadata_path}\")\n\n        with (\n            open(model_path, \"wb\") as f,\n            tqdm(\n                desc=str(model_path).split(\"/\")[-1],\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as bar,\n        ):\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n                bar.update(len(chunk))\n        logger.info(f\"\ud83d\udce5 File downloaded: {model_path}\")\n        return model_path\n\n    def get_dataset_by_name(self, name: str) -&gt; Optional[DatasetMetadata]:\n        \"\"\"\n        Retrieves a dataset by its name.\n\n        Args:\n            name (str): Name of the dataset.\n\n        Returns:\n            Optional[DatasetMetadata]: The dataset metadata if found, or None otherwise.\n        \"\"\"\n        datasets = self.list_shared_datasets()\n        name_lower = name.lower()\n        for dataset in datasets:\n            if name_lower == dataset.name.lower():\n                return dataset\n\n    def get_model_by_name(self, name: str, remote: bool = True) -&gt; Union[RemoteModel, LocalModel]:\n        \"\"\"\n        Retrieves a model by its name.\n\n        Args:\n            name (str): Name of the model.\n            remote (bool): If True, retrieve as a RemoteModel. Otherwise, as a LocalModel. Defaults to True.\n\n        Returns:\n            Optional[Union[RemoteModel, LocalModel]]: The model instance if found, or None otherwise.\n        \"\"\"\n        models = self.list_models()\n        name_lower = name.lower()\n        for model in models:\n            if name_lower == model.name.lower():\n                if remote:\n                    return self.get_remote_model(model.ref)\n                else:\n                    return self.get_local_model(model.ref)\n        raise ModelNotFound(f\"Model not found: {name}\")\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.__init__","title":"<code>__init__(api_key=None, host_url=None)</code>","text":"<p>Initializes the Focoos API client.</p> <p>This client provides authenticated access to the Focoos API, enabling various operations through the configured HTTP client. It retrieves user information upon initialization and logs the environment details.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>API key for authentication. Defaults to the <code>focoos_api_key</code> specified in the FOCOOS_CONFIG.</p> <code>None</code> <code>host_url</code> <code>Optional[str]</code> <p>Base URL for the Focoos API. Defaults to the <code>default_host_url</code> specified in the FOCOOS_CONFIG.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API key is not provided, or if the host URL is not specified in the arguments or the configuration.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key used for authentication.</p> <code>http_client</code> <code>HttpClient</code> <p>An HTTP client instance configured with the API key and host URL.</p> <code>user_info</code> <code>dict</code> <p>Information about the authenticated user retrieved from the API.</p> <code>cache_dir</code> <code>str</code> <p>Path to the cache directory used by the client.</p> Logs <ul> <li>Error if the API key or host URL is missing.</li> <li>Info about the authenticated user and environment upon successful initialization.</li> </ul> Source code in <code>focoos/focoos.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = None,\n    host_url: Optional[str] = None,\n):\n    \"\"\"\n    Initializes the Focoos API client.\n\n    This client provides authenticated access to the Focoos API, enabling various operations\n    through the configured HTTP client. It retrieves user information upon initialization and\n    logs the environment details.\n\n    Args:\n        api_key (Optional[str]): API key for authentication. Defaults to the `focoos_api_key`\n            specified in the FOCOOS_CONFIG.\n        host_url (Optional[str]): Base URL for the Focoos API. Defaults to the `default_host_url`\n            specified in the FOCOOS_CONFIG.\n\n    Raises:\n        ValueError: If the API key is not provided, or if the host URL is not specified in the\n            arguments or the configuration.\n\n    Attributes:\n        api_key (str): The API key used for authentication.\n        http_client (HttpClient): An HTTP client instance configured with the API key and host URL.\n        user_info (dict): Information about the authenticated user retrieved from the API.\n        cache_dir (str): Path to the cache directory used by the client.\n\n    Logs:\n        - Error if the API key or host URL is missing.\n        - Info about the authenticated user and environment upon successful initialization.\n    \"\"\"\n    self.api_key = api_key or FOCOOS_CONFIG.focoos_api_key\n    if not self.api_key:\n        logger.error(\"API key is required \ud83e\udd16\")\n        raise ValueError(\"API key is required \ud83e\udd16\")\n\n    host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n    self.http_client = HttpClient(self.api_key, host_url)\n    self.user_info = self.get_user_info()\n    self.cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"focoos\")\n    logger.info(f\"Currently logged as: {self.user_info.email} environment: {host_url}\")\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_dataset_by_name","title":"<code>get_dataset_by_name(name)</code>","text":"<p>Retrieves a dataset by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset.</p> required <p>Returns:</p> Type Description <code>Optional[DatasetMetadata]</code> <p>Optional[DatasetMetadata]: The dataset metadata if found, or None otherwise.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_dataset_by_name(self, name: str) -&gt; Optional[DatasetMetadata]:\n    \"\"\"\n    Retrieves a dataset by its name.\n\n    Args:\n        name (str): Name of the dataset.\n\n    Returns:\n        Optional[DatasetMetadata]: The dataset metadata if found, or None otherwise.\n    \"\"\"\n    datasets = self.list_shared_datasets()\n    name_lower = name.lower()\n    for dataset in datasets:\n        if name_lower == dataset.name.lower():\n            return dataset\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_local_model","title":"<code>get_local_model(model_ref, runtime_type=RuntimeTypes.ONNX_CUDA32)</code>","text":"<p>Retrieves a local model for the specified reference.</p> <p>Downloads the model if it does not already exist in the local cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference identifier for the model.</p> required <code>runtime_type</code> <code>Optional[RuntimeTypes]</code> <p>Runtime type for the model. Defaults to the <code>runtime_type</code> specified in FOCOOS_CONFIG.</p> <code>ONNX_CUDA32</code> <p>Returns:</p> Name Type Description <code>LocalModel</code> <code>LocalModel</code> <p>An instance of the local model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the runtime type is not specified.</p> Notes <p>The model is cached in the directory specified by <code>self.cache_dir</code>.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_local_model(\n    self,\n    model_ref: str,\n    runtime_type: Optional[RuntimeTypes] = RuntimeTypes.ONNX_CUDA32,\n) -&gt; LocalModel:\n    \"\"\"\n    Retrieves a local model for the specified reference.\n\n    Downloads the model if it does not already exist in the local cache.\n\n    Args:\n        model_ref (str): Reference identifier for the model.\n        runtime_type (Optional[RuntimeTypes]): Runtime type for the model. Defaults to the\n            `runtime_type` specified in FOCOOS_CONFIG.\n\n    Returns:\n        LocalModel: An instance of the local model.\n\n    Raises:\n        ValueError: If the runtime type is not specified.\n\n    Notes:\n        The model is cached in the directory specified by `self.cache_dir`.\n    \"\"\"\n    runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n    model_dir = os.path.join(self.cache_dir, model_ref)\n    format = ModelFormat.from_runtime_type(runtime_type)\n    if not os.path.exists(os.path.join(model_dir, f\"model.{format.value}\")):\n        self._download_model(\n            model_ref,\n            format=format,\n        )\n    return LocalModel(model_dir, runtime_type)\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_model_by_name","title":"<code>get_model_by_name(name, remote=True)</code>","text":"<p>Retrieves a model by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>remote</code> <code>bool</code> <p>If True, retrieve as a RemoteModel. Otherwise, as a LocalModel. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[RemoteModel, LocalModel]</code> <p>Optional[Union[RemoteModel, LocalModel]]: The model instance if found, or None otherwise.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_model_by_name(self, name: str, remote: bool = True) -&gt; Union[RemoteModel, LocalModel]:\n    \"\"\"\n    Retrieves a model by its name.\n\n    Args:\n        name (str): Name of the model.\n        remote (bool): If True, retrieve as a RemoteModel. Otherwise, as a LocalModel. Defaults to True.\n\n    Returns:\n        Optional[Union[RemoteModel, LocalModel]]: The model instance if found, or None otherwise.\n    \"\"\"\n    models = self.list_models()\n    name_lower = name.lower()\n    for model in models:\n        if name_lower == model.name.lower():\n            if remote:\n                return self.get_remote_model(model.ref)\n            else:\n                return self.get_local_model(model.ref)\n    raise ModelNotFound(f\"Model not found: {name}\")\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_model_info","title":"<code>get_model_info(model_name)</code>","text":"<p>Retrieves metadata for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <p>Returns:</p> Name Type Description <code>ModelMetadata</code> <code>ModelMetadata</code> <p>Metadata of the specified model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_model_info(self, model_name: str) -&gt; ModelMetadata:\n    \"\"\"\n    Retrieves metadata for a specific model.\n\n    Args:\n        model_name (str): Name of the model.\n\n    Returns:\n        ModelMetadata: Metadata of the specified model.\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.get(f\"models/{model_name}\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n    return ModelMetadata.from_json(res.json())\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_remote_model","title":"<code>get_remote_model(model_ref)</code>","text":"<p>Retrieves a remote model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference name of the model.</p> required <p>Returns:</p> Name Type Description <code>RemoteModel</code> <code>RemoteModel</code> <p>The remote model instance.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_remote_model(self, model_ref: str) -&gt; RemoteModel:\n    \"\"\"\n    Retrieves a remote model instance.\n\n    Args:\n        model_ref (str): Reference name of the model.\n\n    Returns:\n        RemoteModel: The remote model instance.\n    \"\"\"\n    return RemoteModel(model_ref, self.http_client)\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.get_user_info","title":"<code>get_user_info()</code>","text":"<p>Retrieves information about the authenticated user.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>User</code> <p>Information about the user (e.g., email).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def get_user_info(self) -&gt; User:\n    \"\"\"\n    Retrieves information about the authenticated user.\n\n    Returns:\n        dict: Information about the user (e.g., email).\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.get(\"user/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get user info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get user info: {res.status_code} {res.text}\")\n    return User.from_json(res.json())\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.list_focoos_models","title":"<code>list_focoos_models()</code>","text":"<p>Lists models specific to Focoos.</p> <p>Returns:</p> Type Description <code>list[ModelPreview]</code> <p>list[ModelPreview]: List of Focoos models.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def list_focoos_models(self) -&gt; list[ModelPreview]:\n    \"\"\"\n    Lists models specific to Focoos.\n\n    Returns:\n        list[ModelPreview]: List of Focoos models.\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.get(\"models/focoos-models\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list focoos models: {res.status_code} {res.text}\")\n    return [ModelPreview.from_json(r) for r in res.json()]\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.list_models","title":"<code>list_models()</code>","text":"<p>Lists all available models.</p> <p>Returns:</p> Type Description <code>list[ModelPreview]</code> <p>list[ModelPreview]: List of model previews.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def list_models(self) -&gt; list[ModelPreview]:\n    \"\"\"\n    Lists all available models.\n\n    Returns:\n        list[ModelPreview]: List of model previews.\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.get(\"models/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list models: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list models: {res.status_code} {res.text}\")\n    return [ModelPreview.from_json(r) for r in res.json()]\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.list_shared_datasets","title":"<code>list_shared_datasets()</code>","text":"<p>Lists datasets shared with the user.</p> <p>Returns:</p> Type Description <code>list[DatasetMetadata]</code> <p>list[DatasetMetadata]: List of shared datasets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def list_shared_datasets(self) -&gt; list[DatasetMetadata]:\n    \"\"\"\n    Lists datasets shared with the user.\n\n    Returns:\n        list[DatasetMetadata]: List of shared datasets.\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.get(\"datasets/shared\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n    return [DatasetMetadata.from_json(dataset) for dataset in res.json()]\n</code></pre>"},{"location":"api/focoos/#focoos.focoos.Focoos.new_model","title":"<code>new_model(name, focoos_model, description)</code>","text":"<p>Creates a new model in the Focoos system.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the new model.</p> required <code>focoos_model</code> <code>str</code> <p>Reference to the base Focoos model.</p> required <code>description</code> <code>str</code> <p>Description of the new model.</p> required <p>Returns:</p> Type Description <code>RemoteModel</code> <p>Optional[RemoteModel]: The created model instance, or None if creation fails.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Source code in <code>focoos/focoos.py</code> <pre><code>def new_model(self, name: str, focoos_model: str, description: str) -&gt; RemoteModel:\n    \"\"\"\n    Creates a new model in the Focoos system.\n\n    Args:\n        name (str): Name of the new model.\n        focoos_model (str): Reference to the base Focoos model.\n        description (str): Description of the new model.\n\n    Returns:\n        Optional[RemoteModel]: The created model instance, or None if creation fails.\n\n    Raises:\n        ValueError: If the API request fails.\n    \"\"\"\n    res = self.http_client.post(\n        \"models/\",\n        data={\n            \"name\": name,\n            \"focoos_model\": focoos_model,\n            \"description\": description,\n        },\n    )\n    if res.status_code in [200, 201]:\n        return RemoteModel(res.json()[\"ref\"], self.http_client)\n    if res.status_code == 409:\n        logger.warning(f\"Model already exists: {name}\")\n        return self.get_model_by_name(name, remote=True)\n    logger.warning(f\"Failed to create new model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/local_model/","title":"local model","text":"<p>LocalModel Module</p> <p>This module provides the <code>LocalModel</code> class that allows loading, inference, and benchmark testing of models in a local environment. It supports detection and segmentation tasks, and utilizes ONNXRuntime for model execution.</p> <p>Classes:</p> Name Description <code>LocalModel</code> <p>A class for managing and interacting with local models.</p> <p>Functions:</p> Name Description <code>__init__</code> <p>Initializes the LocalModel instance, loading the model, metadata,       and setting up the runtime.</p> <code>_read_metadata</code> <p>Reads the model metadata from a JSON file.</p> <code>_annotate</code> <p>Annotates the input image with detection or segmentation results.</p> <code>infer</code> <p>Runs inference on an input image, with optional annotation.</p> <code>benchmark</code> <p>Benchmarks the model's inference performance over a specified        number of iterations and input size.</p>"},{"location":"api/local_model/#focoos.local_model.LocalModel","title":"<code>LocalModel</code>","text":"Source code in <code>focoos/local_model.py</code> <pre><code>class LocalModel:\n    def __init__(\n        self,\n        model_dir: Union[str, Path],\n        runtime_type: Optional[RuntimeTypes] = None,\n    ):\n        \"\"\"\n        Initialize a LocalModel instance.\n\n        This class sets up a local model for inference by initializing the runtime environment,\n        loading metadata, and preparing annotation utilities.\n\n        Args:\n            model_dir (Union[str, Path]): The path to the directory containing the model files.\n            runtime_type (Optional[RuntimeTypes]): Specifies the runtime type to use for inference.\n                Defaults to the value of `FOCOOS_CONFIG.runtime_type` if not provided.\n\n        Raises:\n            ValueError: If no runtime type is provided and `FOCOOS_CONFIG.runtime_type` is not set.\n            FileNotFoundError: If the specified model directory does not exist.\n\n        Attributes:\n            model_dir (Union[str, Path]): Path to the model directory.\n            metadata (ModelMetadata): Metadata information for the model.\n            model_ref: Reference identifier for the model obtained from metadata.\n            label_annotator (sv.LabelAnnotator): Utility for adding labels to the output,\n                initialized with text padding and border radius.\n            box_annotator (sv.BoxAnnotator): Utility for annotating bounding boxes.\n            mask_annotator (sv.MaskAnnotator): Utility for annotating masks.\n            runtime (ONNXRuntime): Inference runtime initialized with the specified runtime type,\n                model path, metadata, and warmup iterations.\n\n        The method verifies the existence of the model directory, reads the model metadata,\n        and initializes the runtime for inference using the provided runtime type. Annotation\n        utilities are also prepared for visualizing model outputs.\n        \"\"\"\n        # Determine runtime type and model format\n        runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n        model_format = ModelFormat.from_runtime_type(runtime_type)\n\n        # Set model directory and path\n        self.model_dir: Union[str, Path] = model_dir\n        self.model_path = os.path.join(model_dir, f\"model.{model_format.value}\")\n        logger.debug(f\"Runtime type: {runtime_type}, Loading model from {self.model_path}..\")\n\n        # Check if model path exists\n        if not os.path.exists(self.model_path):\n            raise FileNotFoundError(f\"Model path not found: {self.model_path}\")\n\n        # Load metadata and set model reference\n        self.metadata: ModelMetadata = self._read_metadata()\n        self.model_ref = self.metadata.ref\n\n        # Initialize annotation utilities\n        self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n        self.box_annotator = sv.BoxAnnotator()\n        self.mask_annotator = sv.MaskAnnotator()\n\n        # Load runtime for inference\n        self.runtime: BaseRuntime = load_runtime(\n            runtime_type,\n            str(self.model_path),\n            self.metadata,\n            FOCOOS_CONFIG.warmup_iter,\n        )\n\n    def _read_metadata(self) -&gt; ModelMetadata:\n        \"\"\"\n        Reads the model metadata from a JSON file.\n\n        Returns:\n            ModelMetadata: Metadata for the model.\n\n        Raises:\n            FileNotFoundError: If the metadata file does not exist in the model directory.\n        \"\"\"\n        metadata_path = os.path.join(self.model_dir, \"focoos_metadata.json\")\n        return ModelMetadata.from_json(metadata_path)\n\n    def _annotate(self, im: np.ndarray, detections: sv.Detections) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the input image with detection or segmentation results.\n\n        Args:\n            im (np.ndarray): The input image to annotate.\n            detections (sv.Detections): Detected objects or segmented regions.\n\n        Returns:\n            np.ndarray: The annotated image with bounding boxes or masks.\n        \"\"\"\n        classes = self.metadata.classes\n        labels = [\n            f\"{classes[int(class_id)] if classes is not None else str(class_id)}: {confid * 100:.0f}%\"\n            for class_id, confid in zip(detections.class_id, detections.confidence)  # type: ignore\n        ]\n        if self.metadata.task == FocoosTask.DETECTION:\n            annotated_im = self.box_annotator.annotate(scene=im.copy(), detections=detections)\n\n            annotated_im = self.label_annotator.annotate(scene=annotated_im, detections=detections, labels=labels)\n        elif self.metadata.task in [\n            FocoosTask.SEMSEG,\n            FocoosTask.INSTANCE_SEGMENTATION,\n        ]:\n            annotated_im = self.mask_annotator.annotate(scene=im.copy(), detections=detections)\n        return annotated_im\n\n    def infer(\n        self,\n        image: Union[bytes, str, Path, np.ndarray, Image.Image],\n        threshold: float = 0.5,\n        annotate: bool = False,\n    ) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n        \"\"\"\n        Run inference on an input image and optionally annotate the results.\n\n        Args:\n            image (Union[bytes, str, Path, np.ndarray, Image.Image]): The input image to infer on.\n                This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.\n            threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n                Detections with confidence scores below this threshold will be discarded.\n            annotate (bool, optional): Whether to annotate the image with detection results. Defaults to False.\n                If set to True, the method will return the image with bounding boxes or segmentation masks.\n\n        Returns:\n            Tuple[FocoosDetections, Optional[np.ndarray]]: A tuple containing:\n                - `FocoosDetections`: The detections from the inference, represented as a custom object (`FocoosDetections`).\n                This includes the details of the detected objects such as class, confidence score, and bounding box (if applicable).\n                - `Optional[np.ndarray]`: The annotated image, if `annotate=True`.\n                This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks.\n                If `annotate=False`, this value will be `None`.\n\n        Raises:\n            ValueError: If the model is not deployed locally (i.e., `self.runtime` is `None`).\n        \"\"\"\n        assert self.runtime is not None, \"Model is not deployed (locally)\"\n        resize = None  #!TODO  check for segmentation\n        if self.metadata.task == FocoosTask.DETECTION:\n            resize = 640 if not self.metadata.im_size else self.metadata.im_size\n        logger.debug(f\"Resize: {resize}\")\n        t0 = perf_counter()\n        im1, im0 = image_preprocess(image, resize=resize)\n        t1 = perf_counter()\n        detections = self.runtime(im1.astype(np.float32), threshold)\n        t2 = perf_counter()\n        if resize:\n            detections = scale_detections(detections, (resize, resize), (im0.shape[1], im0.shape[0]))\n        logger.debug(f\"Inference time: {t2 - t1:.3f} seconds\")\n        im = None\n        if annotate:\n            im = self._annotate(im0, detections)\n\n        out = sv_to_focoos_detections(detections, classes=self.metadata.classes)\n        t3 = perf_counter()\n        out.latency = {\n            \"inference\": round(t2 - t1, 3),\n            \"preprocess\": round(t1 - t0, 3),\n            \"postprocess\": round(t3 - t2, 3),\n        }\n        return out, im\n\n    def benchmark(self, iterations: int, size: int) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model's inference performance over multiple iterations.\n\n        Args:\n            iterations (int): Number of iterations to run for benchmarking.\n            size (int): The input size for each benchmark iteration.\n\n        Returns:\n            LatencyMetrics: Latency metrics including time taken for inference.\n        \"\"\"\n        return self.runtime.benchmark(iterations, size)\n</code></pre>"},{"location":"api/local_model/#focoos.local_model.LocalModel.__init__","title":"<code>__init__(model_dir, runtime_type=None)</code>","text":"<p>Initialize a LocalModel instance.</p> <p>This class sets up a local model for inference by initializing the runtime environment, loading metadata, and preparing annotation utilities.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Union[str, Path]</code> <p>The path to the directory containing the model files.</p> required <code>runtime_type</code> <code>Optional[RuntimeTypes]</code> <p>Specifies the runtime type to use for inference. Defaults to the value of <code>FOCOOS_CONFIG.runtime_type</code> if not provided.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no runtime type is provided and <code>FOCOOS_CONFIG.runtime_type</code> is not set.</p> <code>FileNotFoundError</code> <p>If the specified model directory does not exist.</p> <p>Attributes:</p> Name Type Description <code>model_dir</code> <code>Union[str, Path]</code> <p>Path to the model directory.</p> <code>metadata</code> <code>ModelMetadata</code> <p>Metadata information for the model.</p> <code>model_ref</code> <code>ModelMetadata</code> <p>Reference identifier for the model obtained from metadata.</p> <code>label_annotator</code> <code>LabelAnnotator</code> <p>Utility for adding labels to the output, initialized with text padding and border radius.</p> <code>box_annotator</code> <code>BoxAnnotator</code> <p>Utility for annotating bounding boxes.</p> <code>mask_annotator</code> <code>MaskAnnotator</code> <p>Utility for annotating masks.</p> <code>runtime</code> <code>ONNXRuntime</code> <p>Inference runtime initialized with the specified runtime type, model path, metadata, and warmup iterations.</p> <p>The method verifies the existence of the model directory, reads the model metadata, and initializes the runtime for inference using the provided runtime type. Annotation utilities are also prepared for visualizing model outputs.</p> Source code in <code>focoos/local_model.py</code> <pre><code>def __init__(\n    self,\n    model_dir: Union[str, Path],\n    runtime_type: Optional[RuntimeTypes] = None,\n):\n    \"\"\"\n    Initialize a LocalModel instance.\n\n    This class sets up a local model for inference by initializing the runtime environment,\n    loading metadata, and preparing annotation utilities.\n\n    Args:\n        model_dir (Union[str, Path]): The path to the directory containing the model files.\n        runtime_type (Optional[RuntimeTypes]): Specifies the runtime type to use for inference.\n            Defaults to the value of `FOCOOS_CONFIG.runtime_type` if not provided.\n\n    Raises:\n        ValueError: If no runtime type is provided and `FOCOOS_CONFIG.runtime_type` is not set.\n        FileNotFoundError: If the specified model directory does not exist.\n\n    Attributes:\n        model_dir (Union[str, Path]): Path to the model directory.\n        metadata (ModelMetadata): Metadata information for the model.\n        model_ref: Reference identifier for the model obtained from metadata.\n        label_annotator (sv.LabelAnnotator): Utility for adding labels to the output,\n            initialized with text padding and border radius.\n        box_annotator (sv.BoxAnnotator): Utility for annotating bounding boxes.\n        mask_annotator (sv.MaskAnnotator): Utility for annotating masks.\n        runtime (ONNXRuntime): Inference runtime initialized with the specified runtime type,\n            model path, metadata, and warmup iterations.\n\n    The method verifies the existence of the model directory, reads the model metadata,\n    and initializes the runtime for inference using the provided runtime type. Annotation\n    utilities are also prepared for visualizing model outputs.\n    \"\"\"\n    # Determine runtime type and model format\n    runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n    model_format = ModelFormat.from_runtime_type(runtime_type)\n\n    # Set model directory and path\n    self.model_dir: Union[str, Path] = model_dir\n    self.model_path = os.path.join(model_dir, f\"model.{model_format.value}\")\n    logger.debug(f\"Runtime type: {runtime_type}, Loading model from {self.model_path}..\")\n\n    # Check if model path exists\n    if not os.path.exists(self.model_path):\n        raise FileNotFoundError(f\"Model path not found: {self.model_path}\")\n\n    # Load metadata and set model reference\n    self.metadata: ModelMetadata = self._read_metadata()\n    self.model_ref = self.metadata.ref\n\n    # Initialize annotation utilities\n    self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n    self.box_annotator = sv.BoxAnnotator()\n    self.mask_annotator = sv.MaskAnnotator()\n\n    # Load runtime for inference\n    self.runtime: BaseRuntime = load_runtime(\n        runtime_type,\n        str(self.model_path),\n        self.metadata,\n        FOCOOS_CONFIG.warmup_iter,\n    )\n</code></pre>"},{"location":"api/local_model/#focoos.local_model.LocalModel.benchmark","title":"<code>benchmark(iterations, size)</code>","text":"<p>Benchmark the model's inference performance over multiple iterations.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of iterations to run for benchmarking.</p> required <code>size</code> <code>int</code> <p>The input size for each benchmark iteration.</p> required <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Latency metrics including time taken for inference.</p> Source code in <code>focoos/local_model.py</code> <pre><code>def benchmark(self, iterations: int, size: int) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model's inference performance over multiple iterations.\n\n    Args:\n        iterations (int): Number of iterations to run for benchmarking.\n        size (int): The input size for each benchmark iteration.\n\n    Returns:\n        LatencyMetrics: Latency metrics including time taken for inference.\n    \"\"\"\n    return self.runtime.benchmark(iterations, size)\n</code></pre>"},{"location":"api/local_model/#focoos.local_model.LocalModel.infer","title":"<code>infer(image, threshold=0.5, annotate=False)</code>","text":"<p>Run inference on an input image and optionally annotate the results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[bytes, str, Path, ndarray, Image]</code> <p>The input image to infer on. This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.</p> required <code>threshold</code> <code>float</code> <p>The confidence threshold for detections. Defaults to 0.5. Detections with confidence scores below this threshold will be discarded.</p> <code>0.5</code> <code>annotate</code> <code>bool</code> <p>Whether to annotate the image with detection results. Defaults to False. If set to True, the method will return the image with bounding boxes or segmentation masks.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[FocoosDetections, Optional[ndarray]]</code> <p>Tuple[FocoosDetections, Optional[np.ndarray]]: A tuple containing: - <code>FocoosDetections</code>: The detections from the inference, represented as a custom object (<code>FocoosDetections</code>). This includes the details of the detected objects such as class, confidence score, and bounding box (if applicable). - <code>Optional[np.ndarray]</code>: The annotated image, if <code>annotate=True</code>. This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks. If <code>annotate=False</code>, this value will be <code>None</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not deployed locally (i.e., <code>self.runtime</code> is <code>None</code>).</p> Source code in <code>focoos/local_model.py</code> <pre><code>def infer(\n    self,\n    image: Union[bytes, str, Path, np.ndarray, Image.Image],\n    threshold: float = 0.5,\n    annotate: bool = False,\n) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n    \"\"\"\n    Run inference on an input image and optionally annotate the results.\n\n    Args:\n        image (Union[bytes, str, Path, np.ndarray, Image.Image]): The input image to infer on.\n            This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.\n        threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n            Detections with confidence scores below this threshold will be discarded.\n        annotate (bool, optional): Whether to annotate the image with detection results. Defaults to False.\n            If set to True, the method will return the image with bounding boxes or segmentation masks.\n\n    Returns:\n        Tuple[FocoosDetections, Optional[np.ndarray]]: A tuple containing:\n            - `FocoosDetections`: The detections from the inference, represented as a custom object (`FocoosDetections`).\n            This includes the details of the detected objects such as class, confidence score, and bounding box (if applicable).\n            - `Optional[np.ndarray]`: The annotated image, if `annotate=True`.\n            This will be a NumPy array representation of the image with drawn bounding boxes or segmentation masks.\n            If `annotate=False`, this value will be `None`.\n\n    Raises:\n        ValueError: If the model is not deployed locally (i.e., `self.runtime` is `None`).\n    \"\"\"\n    assert self.runtime is not None, \"Model is not deployed (locally)\"\n    resize = None  #!TODO  check for segmentation\n    if self.metadata.task == FocoosTask.DETECTION:\n        resize = 640 if not self.metadata.im_size else self.metadata.im_size\n    logger.debug(f\"Resize: {resize}\")\n    t0 = perf_counter()\n    im1, im0 = image_preprocess(image, resize=resize)\n    t1 = perf_counter()\n    detections = self.runtime(im1.astype(np.float32), threshold)\n    t2 = perf_counter()\n    if resize:\n        detections = scale_detections(detections, (resize, resize), (im0.shape[1], im0.shape[0]))\n    logger.debug(f\"Inference time: {t2 - t1:.3f} seconds\")\n    im = None\n    if annotate:\n        im = self._annotate(im0, detections)\n\n    out = sv_to_focoos_detections(detections, classes=self.metadata.classes)\n    t3 = perf_counter()\n    out.latency = {\n        \"inference\": round(t2 - t1, 3),\n        \"preprocess\": round(t1 - t0, 3),\n        \"postprocess\": round(t3 - t2, 3),\n    }\n    return out, im\n</code></pre>"},{"location":"api/ports/","title":"ports","text":""},{"location":"api/ports/#focoos.ports.ApiKey","title":"<code>ApiKey</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>API key for authentication.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ApiKey(FocoosBaseModel):\n    \"\"\"API key for authentication.\"\"\"\n\n    key: str  # type: ignore\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetLayout","title":"<code>DatasetLayout</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported dataset formats in Focoos.</p> Values <ul> <li>ROBOFLOW_COCO: Roboflow COCO format</li> <li>ROBOFLOW_SEG: Roboflow segmentation format</li> <li>CATALOG: Catalog format</li> <li>SUPERVISELY: Supervisely format</li> </ul> Example <pre><code>from focoos import Focoos\nfrom focoos.ports import DatasetLayout\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\ndatasets = focoos.list_shared_datasets()\n\nfor dataset in datasets:\n    if dataset.layout == DatasetLayout.ROBOFLOW_COCO:\n        print(f\"Dataset {dataset.name} uses ROBOFLOW_COCO format\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class DatasetLayout(str, Enum):\n    \"\"\"Supported dataset formats in Focoos.\n\n    Values:\n        - ROBOFLOW_COCO: Roboflow COCO format\n        - ROBOFLOW_SEG: Roboflow segmentation format\n        - CATALOG: Catalog format\n        - SUPERVISELY: Supervisely format\n\n    Example:\n        ```python\n        from focoos import Focoos\n        from focoos.ports import DatasetLayout\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n        datasets = focoos.list_shared_datasets()\n\n        for dataset in datasets:\n            if dataset.layout == DatasetLayout.ROBOFLOW_COCO:\n                print(f\"Dataset {dataset.name} uses ROBOFLOW_COCO format\")\n        ```\n    \"\"\"\n\n    ROBOFLOW_COCO = \"roboflow_coco\"\n    ROBOFLOW_SEG = \"roboflow_seg\"\n    CATALOG = \"catalog\"\n    SUPERVISELY = \"supervisely\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetMetadata","title":"<code>DatasetMetadata</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Metadata for a dataset.</p> <p>Example: <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n# List all shared datasets\ndatasets = focoos.list_shared_datasets()\n\n# Print info about each dataset\nfor dataset in datasets:\n    print(f\"Dataset: {dataset.name}\")\n    print(f\"Reference: {dataset.ref}\")\n    print(f\"Task: {dataset.task}\")\n    print(f\"Layout: {dataset.layout}\")\n    print(f\"Description: {dataset.description}\")\n    print(\"---\")\n</code></pre></p> Source code in <code>focoos/ports.py</code> <pre><code>class DatasetMetadata(FocoosBaseModel):\n    \"\"\"\n    Metadata for a dataset.\n\n    Example:\n    ```python\n    from focoos import Focoos\n\n    focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n    # List all shared datasets\n    datasets = focoos.list_shared_datasets()\n\n    # Print info about each dataset\n    for dataset in datasets:\n        print(f\"Dataset: {dataset.name}\")\n        print(f\"Reference: {dataset.ref}\")\n        print(f\"Task: {dataset.task}\")\n        print(f\"Layout: {dataset.layout}\")\n        print(f\"Description: {dataset.description}\")\n        print(\"---\")\n    ```\n    \"\"\"\n\n    ref: str\n    name: str\n    task: FocoosTask\n    layout: DatasetLayout\n    description: Optional[str] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.FocoosDet","title":"<code>FocoosDet</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Single detection result from a model.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n# Get a remote model instance\nmodel = focoos.get_remote_model(\"my-model\")\n\n# Run inference on an image\nimage_path = \"image.jpg\"\nresults, _ = model.infer(image_path)\n\n# Print detection results\nfor det in results.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class FocoosDet(FocoosBaseModel):\n    \"\"\"Single detection result from a model.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n        # Get a remote model instance\n        model = focoos.get_remote_model(\"my-model\")\n\n        # Run inference on an image\n        image_path = \"image.jpg\"\n        results, _ = model.infer(image_path)\n\n        # Print detection results\n        for det in results.detections:\n            print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n            print(f\"Bounding box: {det.bbox}\")\n            if det.mask:\n                print(\"Instance segmentation mask included\")\n        ```\n    \"\"\"\n\n    bbox: Optional[list[float]] = None\n    conf: Optional[float] = None\n    cls_id: Optional[int] = None\n    label: Optional[str] = None\n    mask: Optional[str] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.FocoosDetections","title":"<code>FocoosDetections</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Collection of detection results from a model.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n# Get a remote model instance\nmodel = focoos.get_remote_model(\"my-model\")\n\n# Run inference on an image\nimage_path = \"image.jpg\"\nresults, _ = model.infer(image_path)\n\n# Print detection results\nfor det in results.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class FocoosDetections(FocoosBaseModel):\n    \"\"\"Collection of detection results from a model.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n        # Get a remote model instance\n        model = focoos.get_remote_model(\"my-model\")\n\n        # Run inference on an image\n        image_path = \"image.jpg\"\n        results, _ = model.infer(image_path)\n\n        # Print detection results\n        for det in results.detections:\n            print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n            print(f\"Bounding box: {det.bbox}\")\n            if det.mask:\n                print(\"Instance segmentation mask included\")\n        ```\n    \"\"\"\n\n    detections: list[FocoosDet]\n    latency: Optional[dict] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.FocoosTask","title":"<code>FocoosTask</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of computer vision tasks supported by Focoos.</p> Values <ul> <li>DETECTION: Object detection</li> <li>SEMSEG: Semantic segmentation</li> <li>INSTANCE_SEGMENTATION: Instance segmentation</li> </ul> Example <pre><code>from focoos import Focoos, FocoosTask\n\n# Initialize Focoos client\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n# Get list of models\nmodels = focoos.list_models()\n\n# Print task type for each model\nfor model in models:\n    if model.task == FocoosTask.DETECTION:\n        print(f\"{model.name} is a detection model\")\n    elif model.task == FocoosTask.SEMSEG:\n        print(f\"{model.name} is a semantic segmentation model\")\n    elif model.task == FocoosTask.INSTANCE_SEGMENTATION:\n        print(f\"{model.name} is an instance segmentation model\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class FocoosTask(str, Enum):\n    \"\"\"Types of computer vision tasks supported by Focoos.\n\n    Values:\n        - DETECTION: Object detection\n        - SEMSEG: Semantic segmentation\n        - INSTANCE_SEGMENTATION: Instance segmentation\n\n    Example:\n        ```python\n        from focoos import Focoos, FocoosTask\n\n        # Initialize Focoos client\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n        # Get list of models\n        models = focoos.list_models()\n\n        # Print task type for each model\n        for model in models:\n            if model.task == FocoosTask.DETECTION:\n                print(f\"{model.name} is a detection model\")\n            elif model.task == FocoosTask.SEMSEG:\n                print(f\"{model.name} is a semantic segmentation model\")\n            elif model.task == FocoosTask.INSTANCE_SEGMENTATION:\n                print(f\"{model.name} is an instance segmentation model\")\n        ```\n    \"\"\"\n\n    DETECTION = \"detection\"\n    SEMSEG = \"semseg\"\n    INSTANCE_SEGMENTATION = \"instseg\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.GPUInfo","title":"<code>GPUInfo</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Information about a GPU device.</p> <p>```</p> Source code in <code>focoos/ports.py</code> <pre><code>class GPUInfo(FocoosBaseModel):\n    \"\"\"Information about a GPU device.\n\n    ```\n    \"\"\"\n\n    gpu_id: Optional[int] = None\n    gpu_name: Optional[str] = None\n    gpu_memory_total_gb: Optional[float] = None\n    gpu_memory_used_percentage: Optional[float] = None\n    gpu_temperature: Optional[float] = None\n    gpu_load_percentage: Optional[float] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.Hyperparameters","title":"<code>Hyperparameters</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Model training hyperparameters configuration.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Number of images processed in each training iteration. Range: 1-32. Larger batch sizes require more GPU memory but can speed up training.</p> <code>eval_period</code> <code>int</code> <p>Number of iterations between model evaluations. Range: 50-2000. Controls how frequently validation is performed during training.</p> <code>max_iters</code> <code>int</code> <p>Maximum number of training iterations. Range: 100-100,000. Total number of times the model will see batches of training data.</p> <code>resolution</code> <code>int</code> <p>Input image resolution for the model. Range: 128-6400 pixels. Higher resolutions can improve accuracy but require more compute.</p> <code>wandb_project</code> <code>Optional[str]</code> <p>Weights &amp; Biases project name in format \"ORG_ID/PROJECT_NAME\". Used for experiment tracking and visualization.</p> <code>wandb_apikey</code> <code>Optional[str]</code> <p>API key for Weights &amp; Biases integration. Required if using wandb_project.</p> <code>learning_rate</code> <code>float</code> <p>Step size for model weight updates. Range: 0.00001-0.1. Controls how quickly the model learns. Too high can cause instability.</p> <code>decoder_multiplier</code> <code>float</code> <p>Multiplier for decoder learning rate. Allows different learning rates for decoder vs backbone.</p> <code>backbone_multiplier</code> <code>float</code> <p>Multiplier for backbone learning rate. Default 0.1 means backbone learns 10x slower than decoder.</p> <code>amp_enabled</code> <code>bool</code> <p>Whether to use automatic mixed precision training. Can speed up training and reduce memory usage with minimal accuracy impact.</p> <code>weight_decay</code> <code>float</code> <p>L2 regularization factor to prevent overfitting. Higher values = stronger regularization.</p> <code>ema_enabled</code> <code>bool</code> <p>Whether to use Exponential Moving Average of model weights. Can improve model stability and final performance.</p> <code>ema_decay</code> <code>float</code> <p>Decay rate for EMA. Higher = slower but more stable updates. Only used if ema_enabled=True.</p> <code>ema_warmup</code> <code>int</code> <p>Number of iterations before starting EMA. Only used if ema_enabled=True.</p> <code>freeze_bn</code> <code>bool</code> <p>Whether to freeze all batch normalization layers. Useful for fine-tuning with small batch sizes.</p> <code>freeze_bn_bkb</code> <code>bool</code> <p>Whether to freeze backbone batch normalization layers. Default True to preserve pretrained backbone statistics.</p> <code>optimizer</code> <code>str</code> <p>Optimization algorithm. Options: \"ADAMW\", \"SGD\", \"RMSPROP\". ADAMW generally works best for vision tasks.</p> <code>scheduler</code> <code>str</code> <p>Learning rate schedule. Options: \"POLY\", \"FIXED\", \"COSINE\", \"MULTISTEP\". Controls how learning rate changes during training.</p> <code>early_stop</code> <code>bool</code> <p>Whether to stop training early if validation metrics plateau. Can prevent overfitting and save compute time.</p> <code>patience</code> <code>int</code> <p>Number of evaluations to wait for improvement before early stopping. Only used if early_stop=True.</p> <p>Example: <pre><code>from focoos import Focoos, Hyperparameters\n\n# Train with custom hyperparameters\nhyperparams = Hyperparameters(\n    batch_size=8,  # Smaller batch size for limited GPU memory\n    max_iters=2000,  # Train for more iterations\n    learning_rate=1e-4,  # Lower learning rate for more stable training\n    amp_enabled=True,  # Use mixed precision training\n    ema_enabled=True,  # Use EMA for better stability\n    early_stop=True,  # Enable early stopping\n    patience=3,  # Stop after 3 evaluations without improvement\n)\nmodel.train(dataset_ref, hyperparams)\n</code></pre></p> Source code in <code>focoos/ports.py</code> <pre><code>class Hyperparameters(FocoosBaseModel):\n    \"\"\"Model training hyperparameters configuration.\n\n    Attributes:\n        batch_size (int): Number of images processed in each training iteration. Range: 1-32.\n            Larger batch sizes require more GPU memory but can speed up training.\n\n        eval_period (int): Number of iterations between model evaluations. Range: 50-2000.\n            Controls how frequently validation is performed during training.\n\n        max_iters (int): Maximum number of training iterations. Range: 100-100,000.\n            Total number of times the model will see batches of training data.\n\n        resolution (int): Input image resolution for the model. Range: 128-6400 pixels.\n            Higher resolutions can improve accuracy but require more compute.\n\n        wandb_project (Optional[str]): Weights &amp; Biases project name in format \"ORG_ID/PROJECT_NAME\".\n            Used for experiment tracking and visualization.\n\n        wandb_apikey (Optional[str]): API key for Weights &amp; Biases integration.\n            Required if using wandb_project.\n\n        learning_rate (float): Step size for model weight updates. Range: 0.00001-0.1.\n            Controls how quickly the model learns. Too high can cause instability.\n\n        decoder_multiplier (float): Multiplier for decoder learning rate.\n            Allows different learning rates for decoder vs backbone.\n\n        backbone_multiplier (float): Multiplier for backbone learning rate.\n            Default 0.1 means backbone learns 10x slower than decoder.\n\n        amp_enabled (bool): Whether to use automatic mixed precision training.\n            Can speed up training and reduce memory usage with minimal accuracy impact.\n\n        weight_decay (float): L2 regularization factor to prevent overfitting.\n            Higher values = stronger regularization.\n\n        ema_enabled (bool): Whether to use Exponential Moving Average of model weights.\n            Can improve model stability and final performance.\n\n        ema_decay (float): Decay rate for EMA. Higher = slower but more stable updates.\n            Only used if ema_enabled=True.\n\n        ema_warmup (int): Number of iterations before starting EMA.\n            Only used if ema_enabled=True.\n\n        freeze_bn (bool): Whether to freeze all batch normalization layers.\n            Useful for fine-tuning with small batch sizes.\n\n        freeze_bn_bkb (bool): Whether to freeze backbone batch normalization layers.\n            Default True to preserve pretrained backbone statistics.\n\n        optimizer (str): Optimization algorithm. Options: \"ADAMW\", \"SGD\", \"RMSPROP\".\n            ADAMW generally works best for vision tasks.\n\n        scheduler (str): Learning rate schedule. Options: \"POLY\", \"FIXED\", \"COSINE\", \"MULTISTEP\".\n            Controls how learning rate changes during training.\n\n        early_stop (bool): Whether to stop training early if validation metrics plateau.\n            Can prevent overfitting and save compute time.\n\n        patience (int): Number of evaluations to wait for improvement before early stopping.\n            Only used if early_stop=True.\n\n    Example:\n    ```python\n    from focoos import Focoos, Hyperparameters\n\n    # Train with custom hyperparameters\n    hyperparams = Hyperparameters(\n        batch_size=8,  # Smaller batch size for limited GPU memory\n        max_iters=2000,  # Train for more iterations\n        learning_rate=1e-4,  # Lower learning rate for more stable training\n        amp_enabled=True,  # Use mixed precision training\n        ema_enabled=True,  # Use EMA for better stability\n        early_stop=True,  # Enable early stopping\n        patience=3,  # Stop after 3 evaluations without improvement\n    )\n    model.train(dataset_ref, hyperparams)\n    ```\n\n    \"\"\"\n\n    batch_size: Annotated[\n        int,\n        Field(\n            ge=1,\n            le=32,\n            description=\"Batch size, how many images are processed at every iteration\",\n        ),\n    ] = 16\n    eval_period: Annotated[\n        int,\n        Field(ge=50, le=2000, description=\"How often iterations to evaluate the model\"),\n    ] = 500\n    max_iters: Annotated[\n        int,\n        Field(1500, ge=100, le=100000, description=\"Maximum number of training iterations\"),\n    ] = 1500\n    resolution: Annotated[int, Field(640, description=\"Model expected resolution\", ge=128, le=6400)] = 640\n    wandb_project: Annotated[\n        Optional[str],\n        Field(description=\"Wandb project name must be like ORG_ID/PROJECT_NAME\"),\n    ] = None\n    wandb_apikey: Annotated[Optional[str], Field(description=\"Wandb API key\")] = None\n    learning_rate: Annotated[\n        float,\n        Field(gt=0.00001, lt=0.1, description=\"Learning rate\"),\n    ] = 5e-4\n    decoder_multiplier: Annotated[float, Field(description=\"Backbone multiplier\")] = 1\n    backbone_multiplier: float = 0.1\n    amp_enabled: Annotated[bool, Field(description=\"Enable automatic mixed precision\")] = True\n    weight_decay: Annotated[float, Field(description=\"Weight decay\")] = 0.02\n    ema_enabled: Annotated[bool, Field(description=\"Enable EMA (exponential moving average)\")] = False\n    ema_decay: Annotated[float, Field(description=\"EMA decay rate\")] = 0.999\n    ema_warmup: Annotated[int, Field(description=\"EMA warmup\")] = 100\n    freeze_bn: Annotated[bool, Field(description=\"Freeze batch normalization layers\")] = False\n    freeze_bn_bkb: Annotated[bool, Field(description=\"Freeze backbone batch normalization layers\")] = True\n    optimizer: Literal[\"ADAMW\", \"SGD\", \"RMSPROP\"] = \"ADAMW\"\n    scheduler: Literal[\"POLY\", \"FIXED\", \"COSINE\", \"MULTISTEP\"] = \"MULTISTEP\"\n\n    early_stop: Annotated[bool, Field(description=\"Enable early stopping\")] = True\n    patience: Annotated[\n        int,\n        Field(\n            description=\"(Only with early_stop=True) Validation cycles after which the train is stopped if there's no improvement in accuracy.\"\n        ),\n    ] = 5\n\n    @field_validator(\"wandb_project\")\n    def validate_wandb_project(cls, value):\n        if value is not None:\n            # Define a regex pattern to match valid characters\n            if not re.match(r\"^[\\w.-/]+$\", value):\n                raise ValueError(\"Wandb project name must only contain characters, dashes, underscores, and dots.\")\n        return value\n</code></pre>"},{"location":"api/ports/#focoos.ports.LatencyMetrics","title":"<code>LatencyMetrics</code>  <code>dataclass</code>","text":"<p>Performance metrics for model inference.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n# Load model and run benchmark\nmodel = focoos.get_local_model(\"my-model\")\nmetrics = model.benchmark(iterations=20, size=640)\n\n# Access latency metrics\nprint(f\"FPS: {metrics.fps}\")\nprint(f\"Mean latency: {metrics.mean} ms\")\nprint(f\"Engine: {metrics.engine}\")\nprint(f\"Device: {metrics.device}\")\nprint(f\"Input size: {metrics.im_size}x{metrics.im_size}\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass LatencyMetrics:\n    \"\"\"Performance metrics for model inference.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n        # Load model and run benchmark\n        model = focoos.get_local_model(\"my-model\")\n        metrics = model.benchmark(iterations=20, size=640)\n\n        # Access latency metrics\n        print(f\"FPS: {metrics.fps}\")\n        print(f\"Mean latency: {metrics.mean} ms\")\n        print(f\"Engine: {metrics.engine}\")\n        print(f\"Device: {metrics.device}\")\n        print(f\"Input size: {metrics.im_size}x{metrics.im_size}\")\n        ```\n    \"\"\"\n\n    fps: int\n    engine: str\n    min: float\n    max: float\n    mean: float\n    std: float\n    im_size: int\n    device: str\n</code></pre>"},{"location":"api/ports/#focoos.ports.Metrics","title":"<code>Metrics</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Collection of training and inference metrics.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\nmodel = focoos.get_remote_model(\"my-model\")\n\n# Get model metrics\nmetrics = model.metrics()\n\n# Access metrics fields\nprint(f\"Inference metrics: {metrics.infer_metrics}\")\nprint(f\"Validation metrics: {metrics.valid_metrics}\")\nprint(f\"Training metrics: {metrics.train_metrics}\")\nprint(f\"Total iterations: {metrics.iterations}\")\nprint(f\"Best validation: {metrics.best_valid_metric}\")\nprint(f\"Last updated: {metrics.updated_at}\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class Metrics(FocoosBaseModel):\n    \"\"\"Collection of training and inference metrics.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n        model = focoos.get_remote_model(\"my-model\")\n\n        # Get model metrics\n        metrics = model.metrics()\n\n        # Access metrics fields\n        print(f\"Inference metrics: {metrics.infer_metrics}\")\n        print(f\"Validation metrics: {metrics.valid_metrics}\")\n        print(f\"Training metrics: {metrics.train_metrics}\")\n        print(f\"Total iterations: {metrics.iterations}\")\n        print(f\"Best validation: {metrics.best_valid_metric}\")\n        print(f\"Last updated: {metrics.updated_at}\")\n        ```\n    \"\"\"\n\n    infer_metrics: list[dict] = []\n    valid_metrics: list[dict] = []\n    train_metrics: list[dict] = []\n    iterations: Optional[int] = None\n    best_valid_metric: Optional[dict] = None\n    updated_at: Optional[datetime] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelFormat","title":"<code>ModelFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported model formats.</p> Values <ul> <li>ONNX: ONNX format</li> <li>TORCHSCRIPT: TorchScript format</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class ModelFormat(str, Enum):\n    \"\"\"Supported model formats.\n\n    Values:\n        - ONNX: ONNX format\n        - TORCHSCRIPT: TorchScript format\n\n    \"\"\"\n\n    ONNX = \"onnx\"\n    TORCHSCRIPT = \"pt\"\n\n    @classmethod\n    def from_runtime_type(cls, runtime_type: RuntimeTypes):\n        if runtime_type in [\n            RuntimeTypes.ONNX_CUDA32,\n            RuntimeTypes.ONNX_TRT32,\n            RuntimeTypes.ONNX_TRT16,\n            RuntimeTypes.ONNX_CPU,\n            RuntimeTypes.ONNX_COREML,\n        ]:\n            return cls.ONNX\n        elif runtime_type == RuntimeTypes.TORCHSCRIPT_32:\n            return cls.TORCHSCRIPT\n        else:\n            raise ValueError(f\"Invalid runtime type: {runtime_type}\")\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelMetadata","title":"<code>ModelMetadata</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Complete metadata for a Focoos model.</p> Example <pre><code>from focoos import Focoos, RemoteModel\n\n# Initialize Focoos client\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n# Get a remote model instance\nmodel = focoos.get_remote_model(\"my-model\")\n\n# Get model metadata\nmetadata = model.get_info()\n\nprint(f\"Model: {metadata.name}\")\nprint(f\"Reference: {metadata.ref}\")\nprint(f\"Task: {metadata.task}\")\nprint(f\"Status: {metadata.status}\")\nprint(f\"Description: {metadata.description}\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class ModelMetadata(FocoosBaseModel):\n    \"\"\"Complete metadata for a Focoos model.\n\n    Example:\n        ```python\n        from focoos import Focoos, RemoteModel\n\n        # Initialize Focoos client\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n        # Get a remote model instance\n        model = focoos.get_remote_model(\"my-model\")\n\n        # Get model metadata\n        metadata = model.get_info()\n\n        print(f\"Model: {metadata.name}\")\n        print(f\"Reference: {metadata.ref}\")\n        print(f\"Task: {metadata.task}\")\n        print(f\"Status: {metadata.status}\")\n        print(f\"Description: {metadata.description}\")\n        ```\n    \"\"\"\n\n    ref: str\n    name: str\n    description: Optional[str] = None\n    owner_ref: str\n    focoos_model: str\n    task: FocoosTask\n    created_at: datetime\n    updated_at: datetime\n    status: ModelStatus\n    metrics: Optional[dict] = None\n    latencies: Optional[list[dict]] = None\n    classes: Optional[list[str]] = None\n    im_size: Optional[int] = None\n    hyperparameters: Optional[Hyperparameters] = None\n    training_info: Optional[TrainingInfo] = None\n    location: Optional[str] = None\n    dataset: Optional[DatasetMetadata] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelNotFound","title":"<code>ModelNotFound</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when a requested model is not found.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ModelNotFound(Exception):\n    \"\"\"Exception raised when a requested model is not found.\"\"\"\n\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelPreview","title":"<code>ModelPreview</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Preview information for a Focoos model.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n# List all available models\nmodels = focoos.list_models()\n\n# Print info about each model\nfor model in models:\n    print(f\"Model: {model.name}\")\n    print(f\"Reference: {model.ref}\")\n    print(f\"Task: {model.task}\")\n    print(f\"Status: {model.status}\")\n    print(f\"Description: {model.description}\")\n    print(\"---\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class ModelPreview(FocoosBaseModel):\n    \"\"\"Preview information for a Focoos model.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n        # List all available models\n        models = focoos.list_models()\n\n        # Print info about each model\n        for model in models:\n            print(f\"Model: {model.name}\")\n            print(f\"Reference: {model.ref}\")\n            print(f\"Task: {model.task}\")\n            print(f\"Status: {model.status}\")\n            print(f\"Description: {model.description}\")\n            print(\"---\")\n        ```\n    \"\"\"\n\n    ref: str\n    name: str\n    task: FocoosTask\n    description: Optional[str] = None\n    status: ModelStatus\n    focoos_model: str\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelStatus","title":"<code>ModelStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of a Focoos model during its lifecycle.</p> Values <ul> <li>CREATED: Model has been created</li> <li>TRAINING_STARTING: Training is about to start</li> <li>TRAINING_RUNNING: Training is in progress</li> <li>TRAINING_ERROR: Training encountered an error</li> <li>TRAINING_COMPLETED: Training finished successfully</li> <li>TRAINING_STOPPED: Training was stopped</li> <li>DEPLOYED: Model is deployed</li> <li>DEPLOY_ERROR: Deployment encountered an error</li> </ul> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\nmodel = focoos.get_remote_model(\"my-model\")\n\nif model.status == ModelStatus.DEPLOYED:\n    print(\"Model is deployed and ready for inference\")\nelif model.status == ModelStatus.TRAINING_RUNNING:\n    print(\"Model is currently training\")\nelif model.status == ModelStatus.TRAINING_ERROR:\n    print(\"Model training encountered an error\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class ModelStatus(str, Enum):\n    \"\"\"Status of a Focoos model during its lifecycle.\n\n    Values:\n        - CREATED: Model has been created\n        - TRAINING_STARTING: Training is about to start\n        - TRAINING_RUNNING: Training is in progress\n        - TRAINING_ERROR: Training encountered an error\n        - TRAINING_COMPLETED: Training finished successfully\n        - TRAINING_STOPPED: Training was stopped\n        - DEPLOYED: Model is deployed\n        - DEPLOY_ERROR: Deployment encountered an error\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n        model = focoos.get_remote_model(\"my-model\")\n\n        if model.status == ModelStatus.DEPLOYED:\n            print(\"Model is deployed and ready for inference\")\n        elif model.status == ModelStatus.TRAINING_RUNNING:\n            print(\"Model is currently training\")\n        elif model.status == ModelStatus.TRAINING_ERROR:\n            print(\"Model training encountered an error\")\n        ```\n    \"\"\"\n\n    CREATED = \"CREATED\"\n    TRAINING_STARTING = \"TRAINING_STARTING\"\n    TRAINING_RUNNING = \"TRAINING_RUNNING\"\n    TRAINING_ERROR = \"TRAINING_ERROR\"\n    TRAINING_COMPLETED = \"TRAINING_COMPLETED\"\n    TRAINING_STOPPED = \"TRAINING_STOPPED\"\n    DEPLOYED = \"DEPLOYED\"\n    DEPLOY_ERROR = \"DEPLOY_ERROR\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.OnnxRuntimeOpts","title":"<code>OnnxRuntimeOpts</code>  <code>dataclass</code>","text":"<p>ONNX runtime configuration options.</p> Example <pre><code>from focoos import ONNXRuntime, FocoosTask, ModelMetadata\n\n# Configure ONNX Runtime options\nopts = OnnxRuntimeOpts(\n    fp16=True,  # Enable FP16 precision\n    cuda=True,  # Use CUDA execution provider\n    trt=False,  # Disable TensorRT\n    vino=False,  # Disable OpenVINO\n    coreml=False,  # Disable CoreML\n    warmup_iter=10,  # Number of warmup iterations\n    verbose=False,  # Disable verbose logging\n)\n\n# Create ONNX Runtime instance\nmodel_path = \"model.onnx\"\nmodel_metadata = ModelMetadata(task=FocoosTask.DETECTION)\nruntime = ONNXRuntime(model_path, opts, model_metadata)\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass OnnxRuntimeOpts:\n    \"\"\"ONNX runtime configuration options.\n\n    Example:\n        ```python\n        from focoos import ONNXRuntime, FocoosTask, ModelMetadata\n\n        # Configure ONNX Runtime options\n        opts = OnnxRuntimeOpts(\n            fp16=True,  # Enable FP16 precision\n            cuda=True,  # Use CUDA execution provider\n            trt=False,  # Disable TensorRT\n            vino=False,  # Disable OpenVINO\n            coreml=False,  # Disable CoreML\n            warmup_iter=10,  # Number of warmup iterations\n            verbose=False,  # Disable verbose logging\n        )\n\n        # Create ONNX Runtime instance\n        model_path = \"model.onnx\"\n        model_metadata = ModelMetadata(task=FocoosTask.DETECTION)\n        runtime = ONNXRuntime(model_path, opts, model_metadata)\n        ```\n    \"\"\"\n\n    fp16: Optional[bool] = False\n    cuda: Optional[bool] = False\n    vino: Optional[bool] = False\n    verbose: Optional[bool] = False\n    trt: Optional[bool] = False\n    coreml: Optional[bool] = False\n    warmup_iter: int = 0\n</code></pre>"},{"location":"api/ports/#focoos.ports.Quotas","title":"<code>Quotas</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Usage quotas and limits for a user account.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\nuser_info = focoos.get_user_info()\n\n# Access quotas from user info\nquotas = user_info.quotas\nprint(f\"Total inferences: {quotas.total_inferences}\")\nprint(f\"Max inferences: {quotas.max_inferences}\")\nprint(f\"Used storage (GB): {quotas.used_storage_gb}\")\nprint(f\"Max storage (GB): {quotas.max_storage_gb}\")\nprint(f\"Active training jobs: {quotas.active_training_jobs}\")\nprint(f\"Max active training jobs: {quotas.max_active_training_jobs}\")\nprint(f\"Used MLG4DNXLarge training jobs hours: {quotas.used_mlg4dnxlarge_training_jobs_hours}\")\nprint(f\"Max MLG4DNXLarge training jobs hours: {quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class Quotas(FocoosBaseModel):\n    \"\"\"Usage quotas and limits for a user account.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n        user_info = focoos.get_user_info()\n\n        # Access quotas from user info\n        quotas = user_info.quotas\n        print(f\"Total inferences: {quotas.total_inferences}\")\n        print(f\"Max inferences: {quotas.max_inferences}\")\n        print(f\"Used storage (GB): {quotas.used_storage_gb}\")\n        print(f\"Max storage (GB): {quotas.max_storage_gb}\")\n        print(f\"Active training jobs: {quotas.active_training_jobs}\")\n        print(f\"Max active training jobs: {quotas.max_active_training_jobs}\")\n        print(f\"Used MLG4DNXLarge training jobs hours: {quotas.used_mlg4dnxlarge_training_jobs_hours}\")\n        print(f\"Max MLG4DNXLarge training jobs hours: {quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n        ```\n    \"\"\"\n\n    # INFERENCE\n    total_inferences: int\n    max_inferences: int\n    # STORAGE\n    used_storage_gb: float\n    max_storage_gb: float\n    # TRAINING\n    active_training_jobs: list[str]\n    max_active_training_jobs: int\n\n    # ML_G4DN_XLARGE TRAINING HOURS\n    used_mlg4dnxlarge_training_jobs_hours: float\n    max_mlg4dnxlarge_training_jobs_hours: float\n</code></pre>"},{"location":"api/ports/#focoos.ports.RuntimeTypes","title":"<code>RuntimeTypes</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Available runtime configurations for model inference.</p> Values <ul> <li>ONNX_CUDA32: ONNX with CUDA FP32</li> <li>ONNX_TRT32: ONNX with TensorRT FP32</li> <li>ONNX_TRT16: ONNX with TensorRT FP16</li> <li>ONNX_CPU: ONNX on CPU</li> <li>ONNX_COREML: ONNX with CoreML</li> <li>TORCHSCRIPT_32: TorchScript FP32</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class RuntimeTypes(str, Enum):\n    \"\"\"Available runtime configurations for model inference.\n\n    Values:\n        - ONNX_CUDA32: ONNX with CUDA FP32\n        - ONNX_TRT32: ONNX with TensorRT FP32\n        - ONNX_TRT16: ONNX with TensorRT FP16\n        - ONNX_CPU: ONNX on CPU\n        - ONNX_COREML: ONNX with CoreML\n        - TORCHSCRIPT_32: TorchScript FP32\n\n    \"\"\"\n\n    ONNX_CUDA32 = \"onnx_cuda32\"\n    ONNX_TRT32 = \"onnx_trt32\"\n    ONNX_TRT16 = \"onnx_trt16\"\n    ONNX_CPU = \"onnx_cpu\"\n    ONNX_COREML = \"onnx_coreml\"\n    TORCHSCRIPT_32 = \"torchscript_32\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.SystemInfo","title":"<code>SystemInfo</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>System information including hardware and software details.</p> Source code in <code>focoos/ports.py</code> <pre><code>class SystemInfo(FocoosBaseModel):\n    \"\"\"System information including hardware and software details.\"\"\"\n\n    focoos_host: Optional[str] = None\n    system: Optional[str] = None\n    system_name: Optional[str] = None\n    cpu_type: Optional[str] = None\n    cpu_cores: Optional[int] = None\n    memory_gb: Optional[float] = None\n    memory_used_percentage: Optional[float] = None\n    available_providers: Optional[list[str]] = None\n    disk_space_total_gb: Optional[float] = None\n    disk_space_used_percentage: Optional[float] = None\n    gpu_count: Optional[int] = None\n    gpu_driver: Optional[str] = None\n    gpu_cuda_version: Optional[str] = None\n    gpus_info: Optional[list[GPUInfo]] = None\n    packages_versions: Optional[dict[str, str]] = None\n    environment: Optional[dict[str, str]] = None\n\n    def pretty_print(self):\n        print(\"================ SYSTEM INFO ====================\")\n        for key, value in self.model_dump().items():\n            if isinstance(value, list):\n                print(f\"{key}:\")\n                if key == \"gpus_info\":  # Special formatting for gpus_info.\n                    for item in value:\n                        print(f\"- id: {item['gpu_id']}\")\n                        for sub_key, sub_value in item.items():\n                            if sub_key != \"gpu_id\" and sub_value is not None:\n                                formatted_key = sub_key.replace(\"_\", \"-\")\n                                print(f\"    - {formatted_key}: {sub_value}\")\n                else:\n                    for item in value:\n                        print(f\"  - {item}\")\n            elif isinstance(value, dict) and key == \"packages_versions\":  # Special formatting for packages_versions\n                print(f\"{key}:\")\n                for pkg_name, pkg_version in value.items():\n                    print(f\"  - {pkg_name}: {pkg_version}\")\n            elif isinstance(value, dict) and key == \"environment\":  # Special formatting for environment\n                print(f\"{key}:\")\n                for env_key, env_value in value.items():\n                    print(f\"  - {env_key}: {env_value}\")\n            else:\n                print(f\"{key}: {value}\")\n        print(\"================================================\")\n</code></pre>"},{"location":"api/ports/#focoos.ports.TorchscriptRuntimeOpts","title":"<code>TorchscriptRuntimeOpts</code>  <code>dataclass</code>","text":"<p>TorchScript runtime configuration options.</p> Example <pre><code>from focoos import TorchscriptRuntime, FocoosTask, ModelMetadata\n\n# Configure TorchScript Runtime options\nopts = TorchscriptRuntimeOpts(\n    warmup_iter=10,  # Number of warmup iterations\n    optimize_for_inference=True,  # Enable inference optimizations\n    set_fusion_strategy=True,  # Enable operator fusion\n)\n\n# Create TorchScript Runtime instance\nmodel_path = \"model.pt\"\nmodel_metadata = ModelMetadata(task=FocoosTask.DETECTION)\nruntime = TorchscriptRuntime(model_path, opts, model_metadata)\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass TorchscriptRuntimeOpts:\n    \"\"\"TorchScript runtime configuration options.\n\n    Example:\n        ```python\n        from focoos import TorchscriptRuntime, FocoosTask, ModelMetadata\n\n        # Configure TorchScript Runtime options\n        opts = TorchscriptRuntimeOpts(\n            warmup_iter=10,  # Number of warmup iterations\n            optimize_for_inference=True,  # Enable inference optimizations\n            set_fusion_strategy=True,  # Enable operator fusion\n        )\n\n        # Create TorchScript Runtime instance\n        model_path = \"model.pt\"\n        model_metadata = ModelMetadata(task=FocoosTask.DETECTION)\n        runtime = TorchscriptRuntime(model_path, opts, model_metadata)\n        ```\n    \"\"\"\n\n    warmup_iter: int = 0\n    optimize_for_inference: bool = True\n    set_fusion_strategy: bool = True\n</code></pre>"},{"location":"api/ports/#focoos.ports.TrainInstance","title":"<code>TrainInstance</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Available training instance types.</p> Values <ul> <li>ML_G4DN_XLARGE: ml.g4dn.xlarge instance</li> <li>ML_G5_XLARGE: ml.g5.xlarge instance</li> <li>ML_G5_12XLARGE: ml.g5.12xlarge instance</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class TrainInstance(str, Enum):\n    \"\"\"Available training instance types.\n\n    Values:\n        - ML_G4DN_XLARGE: ml.g4dn.xlarge instance\n        - ML_G5_XLARGE: ml.g5.xlarge instance\n        - ML_G5_12XLARGE: ml.g5.12xlarge instance\n    \"\"\"\n\n    ML_G4DN_XLARGE = \"ml.g4dn.xlarge\"\n    ML_G5_XLARGE = \"ml.g5.xlarge\"\n    ML_G5_12XLARGE = \"ml.g5.12xlarge\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.TrainingInfo","title":"<code>TrainingInfo</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>Information about a model's training process.</p> <p>Example: <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodel = focoos.get_remote_model(\"my-model\")\n\ninfo = model.train_info()\nprint(f\"Training status: {info.main_status}\")\nprint(f\"Started at: {info.start_time}\")\nprint(f\"Instance type: {info.instance_type}\")\nprint(f\"Elapsed time: {info.elapsed_time} seconds\")\n</code></pre></p> Source code in <code>focoos/ports.py</code> <pre><code>class TrainingInfo(FocoosBaseModel):\n    \"\"\"Information about a model's training process.\n\n    Example:\n    ```python\n    from focoos import Focoos\n\n    focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\n    model = focoos.get_remote_model(\"my-model\")\n\n    info = model.train_info()\n    print(f\"Training status: {info.main_status}\")\n    print(f\"Started at: {info.start_time}\")\n    print(f\"Instance type: {info.instance_type}\")\n    print(f\"Elapsed time: {info.elapsed_time} seconds\")\n    ```\n\n    \"\"\"\n\n    algorithm_name: str\n    instance_type: Optional[str] = None\n    volume_size: Optional[int] = 100\n    max_runtime_in_seconds: Optional[int] = 36000\n    main_status: Optional[str] = None\n    secondary_status: Optional[str] = None\n    failure_reason: Optional[str] = None\n    elapsed_time: Optional[int] = None\n    status_transitions: list[dict] = []\n    start_time: Optional[datetime] = None\n    end_time: Optional[datetime] = None\n    artifact_location: Optional[str] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.User","title":"<code>User</code>","text":"<p>               Bases: <code>FocoosBaseModel</code></p> <p>User account information.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\nuser_info = focoos.get_user_info()\n\n# Access user info fields\nprint(f\"Email: {user_info.email}\")\nprint(f\"Created at: {user_info.created_at}\")\nprint(f\"Updated at: {user_info.updated_at}\")\nprint(f\"Company: {user_info.company}\")\nprint(f\"API key: {user_info.api_key.key}\")\n\n# Access quotas\nquotas = user_info.quotas\nprint(f\"Total inferences: {quotas.total_inferences}\")\nprint(f\"Max inferences: {quotas.max_inferences}\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class User(FocoosBaseModel):\n    \"\"\"User account information.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n        user_info = focoos.get_user_info()\n\n        # Access user info fields\n        print(f\"Email: {user_info.email}\")\n        print(f\"Created at: {user_info.created_at}\")\n        print(f\"Updated at: {user_info.updated_at}\")\n        print(f\"Company: {user_info.company}\")\n        print(f\"API key: {user_info.api_key.key}\")\n\n        # Access quotas\n        quotas = user_info.quotas\n        print(f\"Total inferences: {quotas.total_inferences}\")\n        print(f\"Max inferences: {quotas.max_inferences}\")\n        ```\n    \"\"\"\n\n    email: str\n    created_at: datetime\n    updated_at: datetime\n    company: Optional[str] = None\n    api_key: ApiKey\n    quotas: Quotas\n</code></pre>"},{"location":"api/remote_model/","title":"remote model","text":"<p>RemoteModel Module</p> <p>This module provides a class to manage remote models in the Focoos ecosystem. It supports various functionalities including model training, deployment, inference, and monitoring.</p> <p>Classes:</p> Name Description <code>RemoteModel</code> <p>A class for interacting with remote models, managing their lifecycle,          and performing inference.</p> <p>Modules:</p> Name Description <code>HttpClient</code> <p>Handles HTTP requests.</p> <code>logger</code> <p>Logging utility.</p> <code>BoxAnnotator, LabelAnnotator, MaskAnnotator</code> <p>Annotation tools for visualizing          detections and segmentation tasks.</p> <code>FocoosDet, FocoosDetections</code> <p>Classes for representing and managing detections.</p> <code>FocoosTask</code> <p>Enum for defining supported tasks (e.g., DETECTION, SEMSEG).</p> <code>Hyperparameters</code> <p>Structure for training configuration parameters.</p> <code>ModelMetadata</code> <p>Contains metadata for the model.</p> <code>ModelStatus</code> <p>Enum for representing the current status of the model.</p> <code>TrainInstance</code> <p>Enum for defining available training instances.</p> <code>image_loader</code> <p>Utility function for loading images.</p> <code>focoos_detections_to_supervision</code> <p>Converter for Focoos detections to supervision format.</p>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel","title":"<code>RemoteModel</code>","text":"<p>Represents a remote model in the Focoos platform.</p> <p>Attributes:</p> Name Type Description <code>model_ref</code> <code>str</code> <p>Reference ID for the model.</p> <code>http_client</code> <code>HttpClient</code> <p>Client for making HTTP requests.</p> <code>max_deploy_wait</code> <code>int</code> <p>Maximum wait time for model deployment.</p> <code>metadata</code> <code>ModelMetadata</code> <p>Metadata of the model.</p> <code>label_annotator</code> <code>LabelAnnotator</code> <p>Annotator for adding labels to images.</p> <code>box_annotator</code> <code>BoxAnnotator</code> <p>Annotator for drawing bounding boxes.</p> <code>mask_annotator</code> <code>MaskAnnotator</code> <p>Annotator for drawing masks on images.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>class RemoteModel:\n    \"\"\"\n    Represents a remote model in the Focoos platform.\n\n    Attributes:\n        model_ref (str): Reference ID for the model.\n        http_client (HttpClient): Client for making HTTP requests.\n        max_deploy_wait (int): Maximum wait time for model deployment.\n        metadata (ModelMetadata): Metadata of the model.\n        label_annotator (LabelAnnotator): Annotator for adding labels to images.\n        box_annotator (sv.BoxAnnotator): Annotator for drawing bounding boxes.\n        mask_annotator (sv.MaskAnnotator): Annotator for drawing masks on images.\n    \"\"\"\n\n    def __init__(self, model_ref: str, http_client: HttpClient):\n        \"\"\"\n        Initialize the RemoteModel instance.\n\n        Args:\n            model_ref (str): Reference ID for the model.\n            http_client (HttpClient): HTTP client instance for communication.\n\n        Raises:\n            ValueError: If model metadata retrieval fails.\n        \"\"\"\n        self.model_ref = model_ref\n        self.http_client = http_client\n        self.max_deploy_wait = 10\n        self.metadata: ModelMetadata = self.get_info()\n\n        self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n        self.box_annotator = sv.BoxAnnotator()\n        self.mask_annotator = sv.MaskAnnotator()\n        logger.info(\n            f\"[RemoteModel]: ref: {self.model_ref} name: {self.metadata.name} description: {self.metadata.description} status: {self.metadata.status}\"\n        )\n\n    def get_info(self) -&gt; ModelMetadata:\n        \"\"\"\n        Retrieve model metadata.\n\n        Returns:\n            ModelMetadata: Metadata of the model.\n\n        Raises:\n            ValueError: If the request fails.\n        \"\"\"\n        res = self.http_client.get(f\"models/{self.model_ref}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n        self.metadata = ModelMetadata(**res.json())\n        return self.metadata\n\n    def train(\n        self,\n        dataset_ref: str,\n        hyperparameters: Hyperparameters,\n        instance_type: TrainInstance = TrainInstance.ML_G4DN_XLARGE,\n        volume_size: int = 50,\n        max_runtime_in_seconds: int = 36000,\n    ) -&gt; dict | None:\n        \"\"\"\n        Initiate the training of a remote model on the Focoos platform.\n\n        This method sends a request to the Focoos platform to start the training process for the model\n        referenced by `self.model_ref`. It requires a dataset reference and hyperparameters for training,\n        as well as optional configuration options for the instance type, volume size, and runtime.\n\n        Args:\n            dataset_ref (str): The reference ID of the dataset to be used for training.\n            hyperparameters (Hyperparameters): A structure containing the hyperparameters for the training process.\n            anyma_version (str, optional): The version of Anyma to use for training. Defaults to \"anyma-sagemaker-cu12-torch22-0111\".\n            instance_type (TrainInstance, optional): The type of training instance to use. Defaults to TrainInstance.ML_G4DN_XLARGE.\n            volume_size (int, optional): The size of the disk volume (in GB) for the training instance. Defaults to 50.\n            max_runtime_in_seconds (int, optional): The maximum runtime for training in seconds. Defaults to 36000.\n\n        Returns:\n            dict: A dictionary containing the response from the training initiation request. The content depends on the Focoos platform's response.\n\n        Raises:\n            ValueError: If the request to start training fails (e.g., due to incorrect parameters or server issues).\n        \"\"\"\n        res = self.http_client.post(\n            f\"models/{self.model_ref}/train\",\n            data={\n                \"dataset_ref\": dataset_ref,\n                \"instance_type\": instance_type,\n                \"volume_size\": volume_size,\n                \"max_runtime_in_seconds\": max_runtime_in_seconds,\n                \"hyperparameters\": hyperparameters.model_dump(),\n            },\n        )\n        if res.status_code != 200:\n            logger.warning(f\"Failed to train model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to train model: {res.status_code} {res.text}\")\n        return res.json()\n\n    def train_info(self) -&gt; Optional[TrainingInfo]:\n        \"\"\"\n        Retrieve the current status of the model training.\n\n        Sends a request to check the training status of the model referenced by `self.model_ref`.\n\n        Returns:\n            dict: A dictionary containing the training status information.\n\n        Raises:\n            ValueError: If the request to get training status fails.\n        \"\"\"\n        res = self.http_client.get(f\"models/{self.model_ref}/train/status\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get train status: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get train status: {res.status_code} {res.text}\")\n        return TrainingInfo(**res.json())\n\n    def train_logs(self) -&gt; list[str]:\n        \"\"\"\n        Retrieve the training logs for the model.\n\n        This method sends a request to fetch the logs of the model's training process. If the request\n        is successful (status code 200), it returns the logs as a list of strings. If the request fails,\n        it logs a warning and returns an empty list.\n\n        Returns:\n            list[str]: A list of training logs as strings.\n\n        Raises:\n            None: Returns an empty list if the request fails.\n        \"\"\"\n        res = self.http_client.get(f\"models/{self.model_ref}/train/logs\")\n        if res.status_code != 200:\n            logger.warning(f\"Failed to get train logs: {res.status_code} {res.text}\")\n            return []\n        return res.json()\n\n    def metrics(self) -&gt; Metrics:  # noqa: F821\n        \"\"\"\n        Retrieve the metrics of the model.\n\n        This method sends a request to fetch the metrics of the model identified by `model_ref`.\n        If the request is successful (status code 200), it returns the metrics as a `Metrics` object.\n        If the request fails, it logs a warning and returns an empty `Metrics` object.\n\n        Returns:\n            Metrics: An object containing the metrics of the model.\n\n        Raises:\n            None: Returns an empty `Metrics` object if the request fails.\n        \"\"\"\n        res = self.http_client.get(f\"models/{self.model_ref}/metrics\")\n        if res.status_code != 200:\n            logger.warning(f\"Failed to get metrics: {res.status_code} {res.text}\")\n            return Metrics()  # noqa: F821\n        return Metrics(**res.json())\n\n    def _annotate(self, im: np.ndarray, detections: sv.Detections) -&gt; np.ndarray:\n        \"\"\"\n        Annotate an image with detection results.\n\n        This method adds visual annotations to the provided image based on the model's detection results.\n        It handles different tasks (e.g., object detection, semantic segmentation, instance segmentation)\n        and uses the corresponding annotator (bounding box, label, or mask) to draw on the image.\n\n        Args:\n            im (np.ndarray): The image to be annotated, represented as a NumPy array.\n            detections (sv.Detections): The detection results to be annotated, including class IDs and confidence scores.\n\n        Returns:\n            np.ndarray: The annotated image as a NumPy array.\n        \"\"\"\n        classes = self.metadata.classes\n        if classes is not None:\n            labels = [\n                f\"{classes[int(class_id)]}: {confid * 100:.0f}%\"\n                for class_id, confid in zip(detections.class_id, detections.confidence)\n            ]\n        else:\n            labels = [\n                f\"{str(class_id)}: {confid * 100:.0f}%\"\n                for class_id, confid in zip(detections.class_id, detections.confidence)\n            ]\n        if self.metadata.task == FocoosTask.DETECTION:\n            annotated_im = self.box_annotator.annotate(scene=im.copy(), detections=detections)\n\n            annotated_im = self.label_annotator.annotate(scene=annotated_im, detections=detections, labels=labels)\n        elif self.metadata.task in [\n            FocoosTask.SEMSEG,\n            FocoosTask.INSTANCE_SEGMENTATION,\n        ]:\n            annotated_im = self.mask_annotator.annotate(scene=im.copy(), detections=detections)\n        return annotated_im\n\n    def infer(\n        self,\n        image: Union[str, Path, np.ndarray, bytes],\n        threshold: float = 0.5,\n        annotate: bool = False,\n    ) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n        \"\"\"\n        Perform inference on the provided image using the remote model.\n\n        This method sends an image to the remote model for inference and retrieves the detection results.\n        Optionally, it can annotate the image with the detection results.\n\n        Args:\n            image (Union[str, Path, bytes]): The image to infer on, which can be a file path, a string representing the path, or raw bytes.\n            threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n            annotate (bool, optional): Whether to annotate the image with the detection results. Defaults to False.\n\n        Returns:\n            Tuple[FocoosDetections, Optional[np.ndarray]]:\n                - FocoosDetections: The detection results including class IDs, confidence scores, etc.\n                - Optional[np.ndarray]: The annotated image if `annotate` is True, else None.\n\n        Raises:\n            FileNotFoundError: If the provided image file path is invalid.\n            ValueError: If the inference request fails.\n        \"\"\"\n        image_bytes = None\n        if isinstance(image, str) or isinstance(image, Path):\n            if not os.path.exists(image):\n                logger.error(f\"Image file not found: {image}\")\n                raise FileNotFoundError(f\"Image file not found: {image}\")\n            image_bytes = open(image, \"rb\").read()\n        elif isinstance(image, np.ndarray):\n            _, buffer = cv2.imencode(\".jpg\", image)\n            image_bytes = buffer.tobytes()\n        else:\n            image_bytes = image\n        files = {\"file\": image_bytes}\n        t0 = time.time()\n        res = self.http_client.post(\n            f\"models/{self.model_ref}/inference?confidence_threshold={threshold}\",\n            files=files,\n        )\n        t1 = time.time()\n        if res.status_code == 200:\n            logger.debug(f\"Inference time: {t1 - t0:.3f} seconds\")\n            detections = FocoosDetections(\n                detections=[FocoosDet.from_json(d) for d in res.json().get(\"detections\", [])],\n                latency=res.json().get(\"latency\", None),\n            )\n            preview = None\n            if annotate:\n                im0 = image_loader(image)\n                sv_detections = focoos_detections_to_supervision(detections)\n                preview = self._annotate(im0, sv_detections)\n            return detections, preview\n        else:\n            logger.error(f\"Failed to infer: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to infer: {res.status_code} {res.text}\")\n\n    def notebook_monitor_train(self, interval: int = 30, plot_metrics: bool = False, max_runtime: int = 36000) -&gt; None:\n        \"\"\"\n        Monitor the training process in a Jupyter notebook and display metrics.\n\n        Periodically checks the training status and displays metrics in a notebook cell.\n        Clears previous output to maintain a clean view.\n\n        Args:\n            interval (int): Time between status checks in seconds. Must be 30-240. Default: 30\n            plot_metrics (bool): Whether to plot metrics graphs. Default: False\n            max_runtime (int): Maximum monitoring time in seconds. Default: 36000 (10 hours)\n\n        Returns:\n            None\n        \"\"\"\n        from IPython.display import clear_output\n\n        if not 30 &lt;= interval &lt;= 240:\n            raise ValueError(\"Interval must be between 30 and 240 seconds\")\n\n        last_update = self.get_info().updated_at\n        start_time = time.time()\n        status_history = []\n\n        while True:\n            # Get current status\n            model_info = self.get_info()\n            status = model_info.status\n\n            # Clear and display status\n            clear_output(wait=True)\n            status_msg = f\"[Live Monitor {self.metadata.name}] {status.value}\"\n            status_history.append(status_msg)\n            for msg in status_history:\n                logger.info(msg)\n\n            # Show metrics if training completed\n            if status == ModelStatus.TRAINING_COMPLETED:\n                metrics = self.metrics()\n                if metrics.best_valid_metric:\n                    logger.info(f\"Best Checkpoint (iter: {metrics.best_valid_metric.get('iteration', 'N/A')}):\")\n                    for k, v in metrics.best_valid_metric.items():\n                        logger.info(f\"  {k}: {v}\")\n                    visualizer = MetricsVisualizer(metrics)\n                    visualizer.log_metrics()\n                    if plot_metrics:\n                        visualizer.notebook_plot_training_metrics()\n\n            # Update metrics during training\n            if status == ModelStatus.TRAINING_RUNNING and model_info.updated_at &gt; last_update:\n                last_update = model_info.updated_at\n                metrics = self.metrics()\n                visualizer = MetricsVisualizer(metrics)\n                visualizer.log_metrics()\n                if plot_metrics:\n                    visualizer.notebook_plot_training_metrics()\n\n            # Check exit conditions\n            if status not in [ModelStatus.CREATED, ModelStatus.TRAINING_RUNNING, ModelStatus.TRAINING_STARTING]:\n                return\n\n            if time.time() - start_time &gt; max_runtime:\n                logger.warning(f\"Monitoring exceeded {max_runtime} seconds limit\")\n                return\n\n            sleep(interval)\n\n    def stop_training(self) -&gt; None:\n        \"\"\"\n        Stop the training process of the model.\n\n        This method sends a request to stop the training of the model identified by `model_ref`.\n        If the request fails, an error is logged and a `ValueError` is raised.\n\n        Raises:\n            ValueError: If the stop training request fails.\n\n        Logs:\n            - Error message if the request to stop training fails, including the status code and response text.\n\n        Returns:\n            None: This method does not return any value.\n        \"\"\"\n        res = self.http_client.delete(f\"models/{self.model_ref}/train\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get stop training: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get stop training: {res.status_code} {res.text}\")\n\n    def delete_model(self) -&gt; None:\n        \"\"\"\n        Delete the model from the system.\n\n        This method sends a request to delete the model identified by `model_ref`.\n        If the request fails or the status code is not 204 (No Content), an error is logged\n        and a `ValueError` is raised.\n\n        Raises:\n            ValueError: If the delete model request fails or does not return a 204 status code.\n\n        Logs:\n            - Error message if the request to delete the model fails, including the status code and response text.\n\n        Returns:\n            None: This method does not return any value.\n        \"\"\"\n        res = self.http_client.delete(f\"models/{self.model_ref}\")\n        if res.status_code != 204:\n            logger.error(f\"Failed to delete model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to delete model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.__init__","title":"<code>__init__(model_ref, http_client)</code>","text":"<p>Initialize the RemoteModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference ID for the model.</p> required <code>http_client</code> <code>HttpClient</code> <p>HTTP client instance for communication.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If model metadata retrieval fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def __init__(self, model_ref: str, http_client: HttpClient):\n    \"\"\"\n    Initialize the RemoteModel instance.\n\n    Args:\n        model_ref (str): Reference ID for the model.\n        http_client (HttpClient): HTTP client instance for communication.\n\n    Raises:\n        ValueError: If model metadata retrieval fails.\n    \"\"\"\n    self.model_ref = model_ref\n    self.http_client = http_client\n    self.max_deploy_wait = 10\n    self.metadata: ModelMetadata = self.get_info()\n\n    self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n    self.box_annotator = sv.BoxAnnotator()\n    self.mask_annotator = sv.MaskAnnotator()\n    logger.info(\n        f\"[RemoteModel]: ref: {self.model_ref} name: {self.metadata.name} description: {self.metadata.description} status: {self.metadata.status}\"\n    )\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.delete_model","title":"<code>delete_model()</code>","text":"<p>Delete the model from the system.</p> <p>This method sends a request to delete the model identified by <code>model_ref</code>. If the request fails or the status code is not 204 (No Content), an error is logged and a <code>ValueError</code> is raised.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the delete model request fails or does not return a 204 status code.</p> Logs <ul> <li>Error message if the request to delete the model fails, including the status code and response text.</li> </ul> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>This method does not return any value.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def delete_model(self) -&gt; None:\n    \"\"\"\n    Delete the model from the system.\n\n    This method sends a request to delete the model identified by `model_ref`.\n    If the request fails or the status code is not 204 (No Content), an error is logged\n    and a `ValueError` is raised.\n\n    Raises:\n        ValueError: If the delete model request fails or does not return a 204 status code.\n\n    Logs:\n        - Error message if the request to delete the model fails, including the status code and response text.\n\n    Returns:\n        None: This method does not return any value.\n    \"\"\"\n    res = self.http_client.delete(f\"models/{self.model_ref}\")\n    if res.status_code != 204:\n        logger.error(f\"Failed to delete model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to delete model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.get_info","title":"<code>get_info()</code>","text":"<p>Retrieve model metadata.</p> <p>Returns:</p> Name Type Description <code>ModelMetadata</code> <code>ModelMetadata</code> <p>Metadata of the model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def get_info(self) -&gt; ModelMetadata:\n    \"\"\"\n    Retrieve model metadata.\n\n    Returns:\n        ModelMetadata: Metadata of the model.\n\n    Raises:\n        ValueError: If the request fails.\n    \"\"\"\n    res = self.http_client.get(f\"models/{self.model_ref}\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n    self.metadata = ModelMetadata(**res.json())\n    return self.metadata\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.infer","title":"<code>infer(image, threshold=0.5, annotate=False)</code>","text":"<p>Perform inference on the provided image using the remote model.</p> <p>This method sends an image to the remote model for inference and retrieves the detection results. Optionally, it can annotate the image with the detection results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Path, bytes]</code> <p>The image to infer on, which can be a file path, a string representing the path, or raw bytes.</p> required <code>threshold</code> <code>float</code> <p>The confidence threshold for detections. Defaults to 0.5.</p> <code>0.5</code> <code>annotate</code> <code>bool</code> <p>Whether to annotate the image with the detection results. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[FocoosDetections, Optional[ndarray]]</code> <p>Tuple[FocoosDetections, Optional[np.ndarray]]: - FocoosDetections: The detection results including class IDs, confidence scores, etc. - Optional[np.ndarray]: The annotated image if <code>annotate</code> is True, else None.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the provided image file path is invalid.</p> <code>ValueError</code> <p>If the inference request fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def infer(\n    self,\n    image: Union[str, Path, np.ndarray, bytes],\n    threshold: float = 0.5,\n    annotate: bool = False,\n) -&gt; Tuple[FocoosDetections, Optional[np.ndarray]]:\n    \"\"\"\n    Perform inference on the provided image using the remote model.\n\n    This method sends an image to the remote model for inference and retrieves the detection results.\n    Optionally, it can annotate the image with the detection results.\n\n    Args:\n        image (Union[str, Path, bytes]): The image to infer on, which can be a file path, a string representing the path, or raw bytes.\n        threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n        annotate (bool, optional): Whether to annotate the image with the detection results. Defaults to False.\n\n    Returns:\n        Tuple[FocoosDetections, Optional[np.ndarray]]:\n            - FocoosDetections: The detection results including class IDs, confidence scores, etc.\n            - Optional[np.ndarray]: The annotated image if `annotate` is True, else None.\n\n    Raises:\n        FileNotFoundError: If the provided image file path is invalid.\n        ValueError: If the inference request fails.\n    \"\"\"\n    image_bytes = None\n    if isinstance(image, str) or isinstance(image, Path):\n        if not os.path.exists(image):\n            logger.error(f\"Image file not found: {image}\")\n            raise FileNotFoundError(f\"Image file not found: {image}\")\n        image_bytes = open(image, \"rb\").read()\n    elif isinstance(image, np.ndarray):\n        _, buffer = cv2.imencode(\".jpg\", image)\n        image_bytes = buffer.tobytes()\n    else:\n        image_bytes = image\n    files = {\"file\": image_bytes}\n    t0 = time.time()\n    res = self.http_client.post(\n        f\"models/{self.model_ref}/inference?confidence_threshold={threshold}\",\n        files=files,\n    )\n    t1 = time.time()\n    if res.status_code == 200:\n        logger.debug(f\"Inference time: {t1 - t0:.3f} seconds\")\n        detections = FocoosDetections(\n            detections=[FocoosDet.from_json(d) for d in res.json().get(\"detections\", [])],\n            latency=res.json().get(\"latency\", None),\n        )\n        preview = None\n        if annotate:\n            im0 = image_loader(image)\n            sv_detections = focoos_detections_to_supervision(detections)\n            preview = self._annotate(im0, sv_detections)\n        return detections, preview\n    else:\n        logger.error(f\"Failed to infer: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to infer: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.metrics","title":"<code>metrics()</code>","text":"<p>Retrieve the metrics of the model.</p> <p>This method sends a request to fetch the metrics of the model identified by <code>model_ref</code>. If the request is successful (status code 200), it returns the metrics as a <code>Metrics</code> object. If the request fails, it logs a warning and returns an empty <code>Metrics</code> object.</p> <p>Returns:</p> Name Type Description <code>Metrics</code> <code>Metrics</code> <p>An object containing the metrics of the model.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns an empty <code>Metrics</code> object if the request fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def metrics(self) -&gt; Metrics:  # noqa: F821\n    \"\"\"\n    Retrieve the metrics of the model.\n\n    This method sends a request to fetch the metrics of the model identified by `model_ref`.\n    If the request is successful (status code 200), it returns the metrics as a `Metrics` object.\n    If the request fails, it logs a warning and returns an empty `Metrics` object.\n\n    Returns:\n        Metrics: An object containing the metrics of the model.\n\n    Raises:\n        None: Returns an empty `Metrics` object if the request fails.\n    \"\"\"\n    res = self.http_client.get(f\"models/{self.model_ref}/metrics\")\n    if res.status_code != 200:\n        logger.warning(f\"Failed to get metrics: {res.status_code} {res.text}\")\n        return Metrics()  # noqa: F821\n    return Metrics(**res.json())\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.notebook_monitor_train","title":"<code>notebook_monitor_train(interval=30, plot_metrics=False, max_runtime=36000)</code>","text":"<p>Monitor the training process in a Jupyter notebook and display metrics.</p> <p>Periodically checks the training status and displays metrics in a notebook cell. Clears previous output to maintain a clean view.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>int</code> <p>Time between status checks in seconds. Must be 30-240. Default: 30</p> <code>30</code> <code>plot_metrics</code> <code>bool</code> <p>Whether to plot metrics graphs. Default: False</p> <code>False</code> <code>max_runtime</code> <code>int</code> <p>Maximum monitoring time in seconds. Default: 36000 (10 hours)</p> <code>36000</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def notebook_monitor_train(self, interval: int = 30, plot_metrics: bool = False, max_runtime: int = 36000) -&gt; None:\n    \"\"\"\n    Monitor the training process in a Jupyter notebook and display metrics.\n\n    Periodically checks the training status and displays metrics in a notebook cell.\n    Clears previous output to maintain a clean view.\n\n    Args:\n        interval (int): Time between status checks in seconds. Must be 30-240. Default: 30\n        plot_metrics (bool): Whether to plot metrics graphs. Default: False\n        max_runtime (int): Maximum monitoring time in seconds. Default: 36000 (10 hours)\n\n    Returns:\n        None\n    \"\"\"\n    from IPython.display import clear_output\n\n    if not 30 &lt;= interval &lt;= 240:\n        raise ValueError(\"Interval must be between 30 and 240 seconds\")\n\n    last_update = self.get_info().updated_at\n    start_time = time.time()\n    status_history = []\n\n    while True:\n        # Get current status\n        model_info = self.get_info()\n        status = model_info.status\n\n        # Clear and display status\n        clear_output(wait=True)\n        status_msg = f\"[Live Monitor {self.metadata.name}] {status.value}\"\n        status_history.append(status_msg)\n        for msg in status_history:\n            logger.info(msg)\n\n        # Show metrics if training completed\n        if status == ModelStatus.TRAINING_COMPLETED:\n            metrics = self.metrics()\n            if metrics.best_valid_metric:\n                logger.info(f\"Best Checkpoint (iter: {metrics.best_valid_metric.get('iteration', 'N/A')}):\")\n                for k, v in metrics.best_valid_metric.items():\n                    logger.info(f\"  {k}: {v}\")\n                visualizer = MetricsVisualizer(metrics)\n                visualizer.log_metrics()\n                if plot_metrics:\n                    visualizer.notebook_plot_training_metrics()\n\n        # Update metrics during training\n        if status == ModelStatus.TRAINING_RUNNING and model_info.updated_at &gt; last_update:\n            last_update = model_info.updated_at\n            metrics = self.metrics()\n            visualizer = MetricsVisualizer(metrics)\n            visualizer.log_metrics()\n            if plot_metrics:\n                visualizer.notebook_plot_training_metrics()\n\n        # Check exit conditions\n        if status not in [ModelStatus.CREATED, ModelStatus.TRAINING_RUNNING, ModelStatus.TRAINING_STARTING]:\n            return\n\n        if time.time() - start_time &gt; max_runtime:\n            logger.warning(f\"Monitoring exceeded {max_runtime} seconds limit\")\n            return\n\n        sleep(interval)\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.stop_training","title":"<code>stop_training()</code>","text":"<p>Stop the training process of the model.</p> <p>This method sends a request to stop the training of the model identified by <code>model_ref</code>. If the request fails, an error is logged and a <code>ValueError</code> is raised.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the stop training request fails.</p> Logs <ul> <li>Error message if the request to stop training fails, including the status code and response text.</li> </ul> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>This method does not return any value.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def stop_training(self) -&gt; None:\n    \"\"\"\n    Stop the training process of the model.\n\n    This method sends a request to stop the training of the model identified by `model_ref`.\n    If the request fails, an error is logged and a `ValueError` is raised.\n\n    Raises:\n        ValueError: If the stop training request fails.\n\n    Logs:\n        - Error message if the request to stop training fails, including the status code and response text.\n\n    Returns:\n        None: This method does not return any value.\n    \"\"\"\n    res = self.http_client.delete(f\"models/{self.model_ref}/train\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get stop training: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get stop training: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.train","title":"<code>train(dataset_ref, hyperparameters, instance_type=TrainInstance.ML_G4DN_XLARGE, volume_size=50, max_runtime_in_seconds=36000)</code>","text":"<p>Initiate the training of a remote model on the Focoos platform.</p> <p>This method sends a request to the Focoos platform to start the training process for the model referenced by <code>self.model_ref</code>. It requires a dataset reference and hyperparameters for training, as well as optional configuration options for the instance type, volume size, and runtime.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_ref</code> <code>str</code> <p>The reference ID of the dataset to be used for training.</p> required <code>hyperparameters</code> <code>Hyperparameters</code> <p>A structure containing the hyperparameters for the training process.</p> required <code>anyma_version</code> <code>str</code> <p>The version of Anyma to use for training. Defaults to \"anyma-sagemaker-cu12-torch22-0111\".</p> required <code>instance_type</code> <code>TrainInstance</code> <p>The type of training instance to use. Defaults to TrainInstance.ML_G4DN_XLARGE.</p> <code>ML_G4DN_XLARGE</code> <code>volume_size</code> <code>int</code> <p>The size of the disk volume (in GB) for the training instance. Defaults to 50.</p> <code>50</code> <code>max_runtime_in_seconds</code> <code>int</code> <p>The maximum runtime for training in seconds. Defaults to 36000.</p> <code>36000</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict | None</code> <p>A dictionary containing the response from the training initiation request. The content depends on the Focoos platform's response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request to start training fails (e.g., due to incorrect parameters or server issues).</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def train(\n    self,\n    dataset_ref: str,\n    hyperparameters: Hyperparameters,\n    instance_type: TrainInstance = TrainInstance.ML_G4DN_XLARGE,\n    volume_size: int = 50,\n    max_runtime_in_seconds: int = 36000,\n) -&gt; dict | None:\n    \"\"\"\n    Initiate the training of a remote model on the Focoos platform.\n\n    This method sends a request to the Focoos platform to start the training process for the model\n    referenced by `self.model_ref`. It requires a dataset reference and hyperparameters for training,\n    as well as optional configuration options for the instance type, volume size, and runtime.\n\n    Args:\n        dataset_ref (str): The reference ID of the dataset to be used for training.\n        hyperparameters (Hyperparameters): A structure containing the hyperparameters for the training process.\n        anyma_version (str, optional): The version of Anyma to use for training. Defaults to \"anyma-sagemaker-cu12-torch22-0111\".\n        instance_type (TrainInstance, optional): The type of training instance to use. Defaults to TrainInstance.ML_G4DN_XLARGE.\n        volume_size (int, optional): The size of the disk volume (in GB) for the training instance. Defaults to 50.\n        max_runtime_in_seconds (int, optional): The maximum runtime for training in seconds. Defaults to 36000.\n\n    Returns:\n        dict: A dictionary containing the response from the training initiation request. The content depends on the Focoos platform's response.\n\n    Raises:\n        ValueError: If the request to start training fails (e.g., due to incorrect parameters or server issues).\n    \"\"\"\n    res = self.http_client.post(\n        f\"models/{self.model_ref}/train\",\n        data={\n            \"dataset_ref\": dataset_ref,\n            \"instance_type\": instance_type,\n            \"volume_size\": volume_size,\n            \"max_runtime_in_seconds\": max_runtime_in_seconds,\n            \"hyperparameters\": hyperparameters.model_dump(),\n        },\n    )\n    if res.status_code != 200:\n        logger.warning(f\"Failed to train model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to train model: {res.status_code} {res.text}\")\n    return res.json()\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.train_info","title":"<code>train_info()</code>","text":"<p>Retrieve the current status of the model training.</p> <p>Sends a request to check the training status of the model referenced by <code>self.model_ref</code>.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[TrainingInfo]</code> <p>A dictionary containing the training status information.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request to get training status fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def train_info(self) -&gt; Optional[TrainingInfo]:\n    \"\"\"\n    Retrieve the current status of the model training.\n\n    Sends a request to check the training status of the model referenced by `self.model_ref`.\n\n    Returns:\n        dict: A dictionary containing the training status information.\n\n    Raises:\n        ValueError: If the request to get training status fails.\n    \"\"\"\n    res = self.http_client.get(f\"models/{self.model_ref}/train/status\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get train status: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get train status: {res.status_code} {res.text}\")\n    return TrainingInfo(**res.json())\n</code></pre>"},{"location":"api/remote_model/#focoos.remote_model.RemoteModel.train_logs","title":"<code>train_logs()</code>","text":"<p>Retrieve the training logs for the model.</p> <p>This method sends a request to fetch the logs of the model's training process. If the request is successful (status code 200), it returns the logs as a list of strings. If the request fails, it logs a warning and returns an empty list.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of training logs as strings.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns an empty list if the request fails.</p> Source code in <code>focoos/remote_model.py</code> <pre><code>def train_logs(self) -&gt; list[str]:\n    \"\"\"\n    Retrieve the training logs for the model.\n\n    This method sends a request to fetch the logs of the model's training process. If the request\n    is successful (status code 200), it returns the logs as a list of strings. If the request fails,\n    it logs a warning and returns an empty list.\n\n    Returns:\n        list[str]: A list of training logs as strings.\n\n    Raises:\n        None: Returns an empty list if the request fails.\n    \"\"\"\n    res = self.http_client.get(f\"models/{self.model_ref}/train/logs\")\n    if res.status_code != 200:\n        logger.warning(f\"Failed to get train logs: {res.status_code} {res.text}\")\n        return []\n    return res.json()\n</code></pre>"},{"location":"api/runtime/","title":"Runtime","text":"<p>Runtime Module for the models</p> <p>This module provides the necessary functionality for loading, preprocessing, running inference, and benchmarking ONNX and TorchScript models using different execution providers such as CUDA, TensorRT, and CPU. It includes utility functions for image preprocessing, postprocessing, and interfacing with the ONNXRuntime and TorchScript libraries.</p> <p>Functions:</p> Name Description <code>det_postprocess</code> <p>Postprocesses detection model outputs into sv.Detections.</p> <code>semseg_postprocess</code> <p>Postprocesses semantic segmentation model outputs into sv.Detections.</p> <code>load_runtime</code> <p>Returns an ONNXRuntime or TorchscriptRuntime instance configured for the given runtime type.</p> <p>Classes:</p> Name Description <code>RuntimeTypes</code> <p>Enum for the different runtime types.</p> <code>ONNXRuntime</code> <p>A class that interfaces with ONNX Runtime for model inference.</p> <code>TorchscriptRuntime</code> <p>A class that interfaces with TorchScript for model inference.</p>"},{"location":"api/runtime/#focoos.runtime.ONNXRuntime","title":"<code>ONNXRuntime</code>","text":"<p>               Bases: <code>BaseRuntime</code></p> <p>ONNX Runtime wrapper for model inference with different execution providers. Handles preprocessing, inference, postprocessing and benchmarking.</p> Source code in <code>focoos/runtime.py</code> <pre><code>class ONNXRuntime(BaseRuntime):\n    \"\"\"\n    ONNX Runtime wrapper for model inference with different execution providers.\n    Handles preprocessing, inference, postprocessing and benchmarking.\n    \"\"\"\n\n    def __init__(self, model_path: str, opts: OnnxRuntimeOpts, model_metadata: ModelMetadata):\n        self.logger = get_logger()\n\n        self.logger.debug(f\"\ud83d\udd27 [onnxruntime device] {ort.get_device()}\")\n        self.logger.debug(f\"\ud83d\udd27 [onnxruntime available providers] {ort.get_available_providers()}\")\n\n        self.name = Path(model_path).stem\n        self.opts = opts\n        self.model_metadata = model_metadata\n        self.postprocess_fn = det_postprocess if model_metadata.task == FocoosTask.DETECTION else semseg_postprocess\n\n        # Setup session options\n        options = ort.SessionOptions()\n        options.log_severity_level = 0 if opts.verbose else 2\n        options.enable_profiling = opts.verbose\n\n        # Setup providers\n        providers = self._setup_providers()\n\n        # Create session\n        self.ort_sess = ort.InferenceSession(model_path, options, providers=providers)\n        self.active_providers = self.ort_sess.get_providers()\n        self.logger.info(f\"[onnxruntime] Active providers:{self.active_providers}\")\n\n        # Set input type\n        self.dtype = np.uint8 if self.ort_sess.get_inputs()[0].type == \"tensor(uint8)\" else np.float32\n\n        # Warmup\n        if self.opts.warmup_iter &gt; 0:\n            self._warmup()\n\n    def _setup_providers(self):\n        providers = []\n        available = ort.get_available_providers()\n\n        # Check and add providers in order of preference\n        provider_configs = [\n            (\n                \"TensorrtExecutionProvider\",\n                self.opts.trt,\n                {\"device_id\": 0, \"trt_fp16_enable\": self.opts.fp16, \"trt_force_sequential_engine_build\": False},\n            ),\n            (\n                \"OpenVINOExecutionProvider\",\n                self.opts.vino,\n                {\"device_type\": \"MYRIAD_FP16\", \"enable_vpu_fast_compile\": True, \"num_of_threads\": 1},\n            ),\n            (\n                \"CUDAExecutionProvider\",\n                self.opts.cuda,\n                {\n                    \"device_id\": GPU_ID,\n                    \"arena_extend_strategy\": \"kSameAsRequested\",\n                    \"gpu_mem_limit\": 16 * 1024 * 1024 * 1024,\n                    \"cudnn_conv_algo_search\": \"EXHAUSTIVE\",\n                    \"do_copy_in_default_stream\": True,\n                },\n            ),\n            (\"CoreMLExecutionProvider\", self.opts.coreml, {}),\n        ]\n\n        for provider, enabled, config in provider_configs:\n            if enabled and provider in available:\n                providers.append((provider, config))\n            elif enabled:\n                self.logger.warning(f\"{provider} not found.\")\n\n        providers.append(\"CPUExecutionProvider\")\n        return providers\n\n    def _warmup(self):\n        self.logger.info(\"\u23f1\ufe0f [onnxruntime] Warming up model ..\")\n        np_image = np.random.rand(1, 3, 640, 640).astype(self.dtype)\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n        for _ in range(self.opts.warmup_iter):\n            self.ort_sess.run(out_name, {input_name: np_image})\n\n        self.logger.info(\"\u23f1\ufe0f [onnxruntime] Warmup done\")\n\n    def __call__(self, im: np.ndarray, conf_threshold: float) -&gt; sv.Detections:\n        \"\"\"Run inference and return detections.\"\"\"\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n        out = self.ort_sess.run(out_name, {input_name: im})\n        return self.postprocess_fn(out=out, im0_shape=(im.shape[2], im.shape[3]), conf_threshold=conf_threshold)\n\n    def benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n        \"\"\"Benchmark model latency.\"\"\"\n        self.logger.info(\"\u23f1\ufe0f [onnxruntime] Benchmarking latency..\")\n        size = size if isinstance(size, (tuple, list)) else (size, size)\n\n        np_input = (255 * np.random.random((1, 3, size[0], size[1]))).astype(self.dtype)\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n        durations = []\n        for step in range(iterations + 5):\n            start = perf_counter()\n            self.ort_sess.run(out_name, {input_name: np_input})\n            end = perf_counter()\n\n            if step &gt;= 5:  # Skip first 5 iterations\n                durations.append((end - start) * 1000)\n\n        durations = np.array(durations)\n        provider = self.active_providers[0]\n        device = (\n            get_gpu_name() if provider in [\"CUDAExecutionProvider\", \"TensorrtExecutionProvider\"] else get_cpu_name()\n        )\n\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean()),\n            engine=f\"onnx.{provider}\",\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size[0],\n            device=str(device),\n        )\n        self.logger.info(f\"\ud83d\udd25 FPS: {metrics.fps}\")\n        return metrics\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.ONNXRuntime.__call__","title":"<code>__call__(im, conf_threshold)</code>","text":"<p>Run inference and return detections.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def __call__(self, im: np.ndarray, conf_threshold: float) -&gt; sv.Detections:\n    \"\"\"Run inference and return detections.\"\"\"\n    input_name = self.ort_sess.get_inputs()[0].name\n    out_name = [output.name for output in self.ort_sess.get_outputs()]\n    out = self.ort_sess.run(out_name, {input_name: im})\n    return self.postprocess_fn(out=out, im0_shape=(im.shape[2], im.shape[3]), conf_threshold=conf_threshold)\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.ONNXRuntime.benchmark","title":"<code>benchmark(iterations=20, size=640)</code>","text":"<p>Benchmark model latency.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n    \"\"\"Benchmark model latency.\"\"\"\n    self.logger.info(\"\u23f1\ufe0f [onnxruntime] Benchmarking latency..\")\n    size = size if isinstance(size, (tuple, list)) else (size, size)\n\n    np_input = (255 * np.random.random((1, 3, size[0], size[1]))).astype(self.dtype)\n    input_name = self.ort_sess.get_inputs()[0].name\n    out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n    durations = []\n    for step in range(iterations + 5):\n        start = perf_counter()\n        self.ort_sess.run(out_name, {input_name: np_input})\n        end = perf_counter()\n\n        if step &gt;= 5:  # Skip first 5 iterations\n            durations.append((end - start) * 1000)\n\n    durations = np.array(durations)\n    provider = self.active_providers[0]\n    device = (\n        get_gpu_name() if provider in [\"CUDAExecutionProvider\", \"TensorrtExecutionProvider\"] else get_cpu_name()\n    )\n\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean()),\n        engine=f\"onnx.{provider}\",\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size[0],\n        device=str(device),\n    )\n    self.logger.info(f\"\ud83d\udd25 FPS: {metrics.fps}\")\n    return metrics\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.TorchscriptRuntime","title":"<code>TorchscriptRuntime</code>","text":"<p>               Bases: <code>BaseRuntime</code></p> Source code in <code>focoos/runtime.py</code> <pre><code>class TorchscriptRuntime(BaseRuntime):\n    def __init__(\n        self,\n        model_path: str,\n        opts: TorchscriptRuntimeOpts,\n        model_metadata: ModelMetadata,\n    ):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.logger = get_logger(name=\"TorchscriptEngine\")\n        self.logger.info(f\"\ud83d\udd27 [torchscript] Device: {self.device}\")\n        self.opts = opts\n        self.postprocess_fn = det_postprocess if model_metadata.task == FocoosTask.DETECTION else semseg_postprocess\n\n        map_location = None if torch.cuda.is_available() else \"cpu\"\n\n        self.model = torch.jit.load(model_path, map_location=map_location)\n        self.model = self.model.to(self.device)\n\n        if self.opts.warmup_iter &gt; 0:\n            self.logger.info(\"\u23f1\ufe0f [torchscript] Warming up model..\")\n            with torch.no_grad():\n                np_image = torch.rand(1, 3, 640, 640, device=self.device)\n                for _ in range(self.opts.warmup_iter):\n                    self.model(np_image)\n            self.logger.info(\"\u23f1\ufe0f [torchscript] WARMUP DONE\")\n\n    def __call__(self, im: np.ndarray, conf_threshold: float) -&gt; sv.Detections:\n        \"\"\"Run inference and return detections.\"\"\"\n        with torch.no_grad():\n            torch_image = torch.from_numpy(im).to(self.device, dtype=torch.float32)\n            res = self.model(torch_image)\n            return self.postprocess_fn([r.cpu().numpy() for r in res], (im.shape[2], im.shape[3]), conf_threshold)\n\n    def benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n        \"\"\"Benchmark model latency.\"\"\"\n        self.logger.info(\"\u23f1\ufe0f [torchscript] Benchmarking latency..\")\n        size = size if isinstance(size, (tuple, list)) else (size, size)\n\n        torch_input = torch.rand(1, 3, size[0], size[1], device=self.device)\n        durations = []\n\n        with torch.no_grad():\n            for step in range(iterations + 5):\n                start = perf_counter()\n                self.model(torch_input)\n                end = perf_counter()\n\n                if step &gt;= 5:  # Skip first 5 iterations\n                    durations.append((end - start) * 1000)\n\n        durations = np.array(durations)\n        device = get_gpu_name() if torch.cuda.is_available() else get_cpu_name()\n\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean().astype(float)),\n            engine=\"torchscript\",\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size[0],\n            device=str(device),\n        )\n        self.logger.info(f\"\ud83d\udd25 FPS: {metrics.fps}\")\n        return metrics\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.TorchscriptRuntime.__call__","title":"<code>__call__(im, conf_threshold)</code>","text":"<p>Run inference and return detections.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def __call__(self, im: np.ndarray, conf_threshold: float) -&gt; sv.Detections:\n    \"\"\"Run inference and return detections.\"\"\"\n    with torch.no_grad():\n        torch_image = torch.from_numpy(im).to(self.device, dtype=torch.float32)\n        res = self.model(torch_image)\n        return self.postprocess_fn([r.cpu().numpy() for r in res], (im.shape[2], im.shape[3]), conf_threshold)\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.TorchscriptRuntime.benchmark","title":"<code>benchmark(iterations=20, size=640)</code>","text":"<p>Benchmark model latency.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def benchmark(self, iterations=20, size=640) -&gt; LatencyMetrics:\n    \"\"\"Benchmark model latency.\"\"\"\n    self.logger.info(\"\u23f1\ufe0f [torchscript] Benchmarking latency..\")\n    size = size if isinstance(size, (tuple, list)) else (size, size)\n\n    torch_input = torch.rand(1, 3, size[0], size[1], device=self.device)\n    durations = []\n\n    with torch.no_grad():\n        for step in range(iterations + 5):\n            start = perf_counter()\n            self.model(torch_input)\n            end = perf_counter()\n\n            if step &gt;= 5:  # Skip first 5 iterations\n                durations.append((end - start) * 1000)\n\n    durations = np.array(durations)\n    device = get_gpu_name() if torch.cuda.is_available() else get_cpu_name()\n\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean().astype(float)),\n        engine=\"torchscript\",\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size[0],\n        device=str(device),\n    )\n    self.logger.info(f\"\ud83d\udd25 FPS: {metrics.fps}\")\n    return metrics\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.det_postprocess","title":"<code>det_postprocess(out, im0_shape, conf_threshold)</code>","text":"<p>Postprocesses the output of an object detection model and filters detections based on a confidence threshold.</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>List[ndarray]</code> <p>The output of the detection model.</p> required <code>im0_shape</code> <code>Tuple[int, int]</code> <p>The original shape of the input image (height, width).</p> required <code>conf_threshold</code> <code>float</code> <p>The confidence threshold for filtering detections.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>sv.Detections: A sv.Detections object containing the filtered bounding boxes, class ids, and confidences.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def det_postprocess(out: List[np.ndarray], im0_shape: Tuple[int, int], conf_threshold: float) -&gt; sv.Detections:\n    \"\"\"\n    Postprocesses the output of an object detection model and filters detections\n    based on a confidence threshold.\n\n    Args:\n        out (List[np.ndarray]): The output of the detection model.\n        im0_shape (Tuple[int, int]): The original shape of the input image (height, width).\n        conf_threshold (float): The confidence threshold for filtering detections.\n\n    Returns:\n        sv.Detections: A sv.Detections object containing the filtered bounding boxes, class ids, and confidences.\n    \"\"\"\n    cls_ids, boxes, confs = out\n    boxes[:, 0::2] *= im0_shape[1]\n    boxes[:, 1::2] *= im0_shape[0]\n    high_conf_indices = (confs &gt; conf_threshold).nonzero()\n\n    return sv.Detections(\n        xyxy=boxes[high_conf_indices].astype(int),\n        class_id=cls_ids[high_conf_indices].astype(int),\n        confidence=confs[high_conf_indices].astype(float),\n    )\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.load_runtime","title":"<code>load_runtime(runtime_type, model_path, model_metadata, warmup_iter=0)</code>","text":"<p>Creates and returns a runtime instance based on the specified runtime type. Supports both ONNX and TorchScript runtimes with various execution providers.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_type</code> <code>RuntimeTypes</code> <p>The type of runtime to use. Can be one of: - ONNX_CUDA32: ONNX runtime with CUDA FP32 - ONNX_TRT32: ONNX runtime with TensorRT FP32 - ONNX_TRT16: ONNX runtime with TensorRT FP16 - ONNX_CPU: ONNX runtime with CPU - ONNX_COREML: ONNX runtime with CoreML - TORCHSCRIPT_32: TorchScript runtime with FP32</p> required <code>model_path</code> <code>str</code> <p>Path to the model file (.onnx or .pt)</p> required <code>model_metadata</code> <code>ModelMetadata</code> <p>Model metadata containing task type, classes etc.</p> required <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations before inference. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>BaseRuntime</code> <code>BaseRuntime</code> <p>A configured runtime instance (ONNXRuntime or TorchscriptRuntime)</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required dependencies (torch/onnxruntime) are not installed</p> Source code in <code>focoos/runtime.py</code> <pre><code>def load_runtime(\n    runtime_type: RuntimeTypes,\n    model_path: str,\n    model_metadata: ModelMetadata,\n    warmup_iter: int = 0,\n) -&gt; BaseRuntime:\n    \"\"\"\n    Creates and returns a runtime instance based on the specified runtime type.\n    Supports both ONNX and TorchScript runtimes with various execution providers.\n\n    Args:\n        runtime_type (RuntimeTypes): The type of runtime to use. Can be one of:\n            - ONNX_CUDA32: ONNX runtime with CUDA FP32\n            - ONNX_TRT32: ONNX runtime with TensorRT FP32\n            - ONNX_TRT16: ONNX runtime with TensorRT FP16\n            - ONNX_CPU: ONNX runtime with CPU\n            - ONNX_COREML: ONNX runtime with CoreML\n            - TORCHSCRIPT_32: TorchScript runtime with FP32\n        model_path (str): Path to the model file (.onnx or .pt)\n        model_metadata (ModelMetadata): Model metadata containing task type, classes etc.\n        warmup_iter (int, optional): Number of warmup iterations before inference. Defaults to 0.\n\n    Returns:\n        BaseRuntime: A configured runtime instance (ONNXRuntime or TorchscriptRuntime)\n\n    Raises:\n        ImportError: If required dependencies (torch/onnxruntime) are not installed\n    \"\"\"\n    if runtime_type == RuntimeTypes.TORCHSCRIPT_32:\n        if not TORCH_AVAILABLE:\n            logger.error(\n                \"\u26a0\ufe0f Pytorch not found =(  please install focoos with ['torch'] extra. See https://focoosai.github.io/focoos/setup/ for more details\"\n            )\n            raise ImportError(\"Pytorch not found\")\n        opts = TorchscriptRuntimeOpts(warmup_iter=warmup_iter)\n        return TorchscriptRuntime(model_path, opts, model_metadata)\n    else:\n        if not ORT_AVAILABLE:\n            logger.error(\n                \"\u26a0\ufe0f onnxruntime not found =(  please install focoos with one of 'cpu', 'cuda', 'tensorrt' extra. See https://focoosai.github.io/focoos/setup/ for more details\"\n            )\n            raise ImportError(\"onnxruntime not found\")\n        opts = OnnxRuntimeOpts(\n            cuda=runtime_type == RuntimeTypes.ONNX_CUDA32,\n            trt=runtime_type in [RuntimeTypes.ONNX_TRT32, RuntimeTypes.ONNX_TRT16],\n            fp16=runtime_type == RuntimeTypes.ONNX_TRT16,\n            warmup_iter=warmup_iter,\n            coreml=runtime_type == RuntimeTypes.ONNX_COREML,\n            verbose=False,\n        )\n    return ONNXRuntime(model_path, opts, model_metadata)\n</code></pre>"},{"location":"api/runtime/#focoos.runtime.semseg_postprocess","title":"<code>semseg_postprocess(out, im0_shape, conf_threshold)</code>","text":"<p>Postprocesses the output of a semantic segmentation model and filters based on a confidence threshold.</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>List[ndarray]</code> <p>The output of the semantic segmentation model.</p> required <code>conf_threshold</code> <code>float</code> <p>The confidence threshold for filtering detections.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>sv.Detections: A sv.Detections object containing the masks, class ids, and confidences.</p> Source code in <code>focoos/runtime.py</code> <pre><code>def semseg_postprocess(out: List[np.ndarray], im0_shape: Tuple[int, int], conf_threshold: float) -&gt; sv.Detections:\n    \"\"\"\n    Postprocesses the output of a semantic segmentation model and filters based\n    on a confidence threshold.\n\n    Args:\n        out (List[np.ndarray]): The output of the semantic segmentation model.\n        conf_threshold (float): The confidence threshold for filtering detections.\n\n    Returns:\n        sv.Detections: A sv.Detections object containing the masks, class ids, and confidences.\n    \"\"\"\n    cls_ids, mask, confs = out[0][0], out[1][0], out[2][0]\n    masks = np.equal(mask, np.arange(len(cls_ids))[:, None, None])\n    high_conf_indices = np.where(confs &gt; conf_threshold)[0]\n    masks = masks[high_conf_indices].astype(bool)\n    cls_ids = cls_ids[high_conf_indices].astype(int)\n    confs = confs[high_conf_indices].astype(float)\n    return sv.Detections(\n        mask=masks,\n        # xyxy is required from supervision\n        xyxy=np.zeros(shape=(len(high_conf_indices), 4), dtype=np.uint8),\n        class_id=cls_ids,\n        confidence=confs,\n    )\n</code></pre>"},{"location":"development/changelog/","title":"Changelog","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"development/code_of_conduct/","title":"Code of Conduct","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"development/contributing/","title":"Contributing","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"helpers/wip/","title":"Wip","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"how_to/cloud_training/","title":"Personalize your model","text":"<p>This section covers the steps to create a model and train it in the cloud using the <code>focoos</code> library. The following example demonstrates how to interact with the Focoos API to manage models, datasets, and training jobs.</p> <p></p> <p>In this guide, we will perform the following steps:</p> <ol> <li>\ud83d\udce6 Load or select a dataset</li> <li>\ud83c\udfaf Create a model</li> <li>\ud83c\udfc3\u200d\u2642\ufe0f Train the model</li> <li>\ud83d\udcca Visualize training metrics</li> <li>\ud83e\uddea Test your model</li> </ol>"},{"location":"how_to/cloud_training/#1-load-or-select-a-dataset","title":"1. Load or select a dataset","text":"<p>Note</p> <p>Currently, we are not supporting dataset creation from the SDK (it's coming really soon) and you can only use a dataset already available on the platform. To upload your own dataset, you can write us a mail to info@focoos.ai and we will load your dataset on the platform on your private workspace (your data will not be shared with anyone and not used for any other purpose than training your model).</p> <p>You can list all available datasets using the following code: <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\ndatasets = focoos.list_shared_datasets()\nprint(datasets)\n</code></pre></p> <p>Find the dataset reference you want to use and store it in a variable: <pre><code>dataset_ref = \"&lt;YOUR-DATASET-REFERENCE&gt;\"\n</code></pre></p>"},{"location":"how_to/cloud_training/#2-create-a-model","title":"2. Create a model","text":"<p>The first step to personalize your model is to create a model. You can create a model by calling the <code>new_model</code> method on the <code>Focoos</code> object. You can choose the model you want to personalize from the list of Focoos Models available on the platform. Make sure to select the correct model for your task.</p> <p><pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodel = focoos.new_model(\n    name=\"&lt;YOUR-MODEL-NAME&gt;\",\n    description=\"&lt;YOUR-MODEL-DESCRIPTION&gt;\",\n    focoos_model=\"&lt;FOCOOS-MODEL-NAME&gt;\",\n)\n</code></pre> An example of how to create a model is the following: <pre><code>model = focoos.new_model(\n    name=\"my-model\",\n    description=\"my-model-description\",\n    focoos_model=\"fai-rtdetr-m-obj365\",\n)\n</code></pre> This function will return a new <code>RemoteModel</code> object that you can use to train the model and to perform remote inference.</p>"},{"location":"how_to/cloud_training/#3-train-the-model","title":"3. Train the model","text":"<p>Once the model is created, you can start the training process by calling the <code>train</code> method on the model object.</p> <p><pre><code>from focoos.ports import Hyperparameters\n\nres = model.train(\n    dataset_ref=dataset_ref,\n    hyperparameters=Hyperparameters(\n        learning_rate=0.0001, # custom learning rate\n        batch_size=16, # custom batch size\n        max_iters=1500, # custom max iterations\n    ),\n)\n</code></pre> For selecting the <code>dataset_ref</code> see the step 1. You can further customize the training process by passing additional parameters to the <code>train</code> method (such as the instance type, the volume size, the maximum runtime, etc.) or use additional hyperparameters (see the list available hyperparameters).</p> <p>Futhermore, you can monitor the training progress by polling the training status. Use the <code>notebook_monitor_train</code> method on a jupyter notebook: <pre><code>model.notebook_monitor_train(interval=30, plot_metrics=True)\n</code></pre></p> <p>You can also get the training logs by calling the <code>train_logs</code> method: <pre><code>logs = model.train_logs()\npprint(logs)\n</code></pre></p> <p>Finally, if for some reason you need to cancel the training, you can do so by calling the <code>stop_training</code> method: <pre><code>model.stop_training()\n</code></pre></p>"},{"location":"how_to/cloud_training/#4-visualize-training-metrics","title":"4. Visualize training metrics","text":"<p>You can visualize the training metrics by calling the <code>metrics</code> method: <pre><code>metrics = model.metrics()\nvisualizer = MetricsVisualizer(metrics)\nvisualizer.log_metrics()\n</code></pre> The function will return an object of type <code>Metrics</code> that you can use to visualize the training metrics using a <code>MetricsVisualizer</code> object.</p> <p>On notebooks, you can also plot the metrics by calling the <code>notebook_plot_training_metrics</code> method: <pre><code>visualizer.notebook_plot_training_metrics()\n</code></pre></p>"},{"location":"how_to/cloud_training/#5-test-your-model","title":"5. Test your model","text":""},{"location":"how_to/cloud_training/#remote-inference","title":"Remote Inference","text":"<p>Once the training is over, you can test your model using remote inference by calling the <code>infer</code> method on the model object.</p> <p><pre><code>image_path = \"&lt;PATH-TO-YOUR-IMAGE&gt;\"\nresult, _ = model.infer(image_path, threshold=0.5, annotate=False)\n\nfor det in result.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n</code></pre> <code>result</code> is a FocoosDetections object, containing a list of FocoosDet objects and optionally a dict of information about the latency of the inference.</p> <p>The <code>threshold</code> parameter is optional and defines the minimum confidence score for a detection to be considered valid (predictions with a confidence score lower than the threshold are discarded).</p> <p>Optionally, you can preview the results by passing the <code>annotate</code> parameter to the <code>infer</code> method. <pre><code>from PIL import Image\n\noutput, preview = model.infer(image_path, threshold=0.5, annotate=True)\npreview = Image.fromarray(preview[:,:,[2,1,0]]) # invert to make it RGB\n</code></pre></p>"},{"location":"how_to/cloud_training/#local-inference","title":"Local Inference","text":"<p>Note</p> <p>To perform local inference, you need to install the package with one of the extra modules (<code>[cpu]</code>, <code>[torch]</code>, <code>[cuda]</code>, <code>[tensorrt]</code>). See the installation page for more details.</p> <p>You can perform inference locally by getting the <code>LocalModel</code> you already trained and calling the <code>infer</code> method on your image. If it's the first time you run the model locally, the model will be downloaded from the cloud and saved on your machine. Additionally, if you use CUDA or TensorRT, the model will be optimized for your GPU before running the inference (it can take few seconds, especially for TensorRT).</p> <p><pre><code>model = focoos.get_local_model(model.model_ref) # get the local model\n\nimage_path = \"&lt;PATH-TO-YOUR-IMAGE&gt;\"\nresult, _ = model.infer(image_path, threshold=0.5, annotate=False)\n\nfor det in result.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n</code></pre> As for remote inference, you can pass the <code>annotate</code> parameter to return a preview of the prediction and play with the <code>threshold</code> parameter to change the minimum confidence score for a detection to be considered valid.</p>"},{"location":"how_to/dashboard/","title":"Model Management","text":"<p>This section covers the steps to monitor the status of your models on the FocoosAI platform.</p> <p></p>"},{"location":"how_to/dashboard/#how-to-list-the-focoos-models","title":"How to list the Focoos Models","text":"<p>To list all the models available on the FocoosAI platform, you can use the following code: <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodels = focoos.list_focoos_models()\nfor model in models:\n    print(f\"Name: {model.name}\")\n    print(f\"Reference: {model.ref}\")\n    print(f\"Status: {model.status}\")\n    print(f\"Task: {model.task}\")\n    print(f\"Description: {model.description}\")\n    print(\"-\" * 50)\n</code></pre> <code>models</code> is a list of <code>ModelPreview</code> objects that contains the following information:</p> <ul> <li><code>name</code>: The name of the model.</li> <li><code>ref</code>: The reference of the model.</li> <li><code>status</code>: The status of the model.</li> <li><code>task</code>: The task of the model.</li> <li><code>description</code>: The description of the model.</li> <li><code>status</code>: The status of the model, which indicates its current state (e.g. CREATED, TRAINING_RUNNING, TRAINING_COMPLETED - see <code>ModelStatus</code>).</li> </ul>"},{"location":"how_to/dashboard/#how-to-list-all-your-models","title":"How to list all your models","text":"<p>To list all your models, the library provides a <code>list_models</code> function. This function will return a list of <code>Model</code> objects.</p> <p><pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodels = focoos.list_models()\nfor model in models:\n    print(f\"Name: {model.name}\")\n    print(f\"Reference: {model.ref}\")\n    print(f\"Status: {model.status}\")\n    print(f\"Task: {model.task}\")\n    print(f\"Description: {model.description}\")\n    print(f\"Focoos Model: {model.focoos_model}\")\n    print(\"-\" * 50)\n</code></pre> <code>models</code> is a list of <code>ModelPreview</code> objects that contains the following information:</p> <ul> <li><code>name</code>: The name of the model.</li> <li><code>ref</code>: The reference of the model.</li> <li><code>status</code>: The status of the model.</li> <li><code>task</code>: The task of the model.</li> <li><code>description</code>: The description of the model.</li> <li><code>focoos_model</code>: The starting Focoos Model used for training.</li> <li><code>status</code>: The status of the model, which indicates its current state (e.g. CREATED, TRAINING_RUNNING, TRAINING_COMPLETED - see <code>ModelStatus</code>).</li> </ul>"},{"location":"how_to/dashboard/#see-the-metrics-for-a-model","title":"See the metrics for a model","text":"<p>To see the validation metrics of a model, you can use the <code>metrics</code> method on the model object.</p> <p><pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodel = focoos.get_remote_model(\"my-model\")\nmetrics = model.metrics()\n\nif metrics.best_valid_metric:\n    print(f\"Best validation metrics:\")\n    for k, v in metrics.best_valid_metric.items():\n        print(f\"  {k}: {v}\")\n\nif metrics.valid_metrics:\n    print(f\"Last iteration validation metrics:\")\n    for k, v in metrics.valid_metrics[-1].items():\n        print(f\"  {k}: {v}\")\n\nif metrics.train_metrics:\n    print(f\"Last iteration training metrics:\")\n    for k, v in metrics.train_metrics[-1].items():\n        print(f\"  {k}: {v}\")\n</code></pre> <code>metrics</code> is a <code>Metrics</code> object that contains the validation metrics of the model.</p>"},{"location":"how_to/dashboard/#delete-a-model","title":"Delete a model","text":"<p>To delete a model, you can use the <code>delete_model</code> method on the model object.</p> <p>Warning</p> <p>This action is irreversible and the model will be deleted forever from the platform.</p> <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodel = focoos.get_remote_model(\"my-model\")\nmodel.delete_model()\n</code></pre>"},{"location":"how_to/inference/","title":"Select and Inference with Focoos Models","text":"<p>This section covers how to perform inference using the Focoos Models on the cloud or locally using the <code>focoos</code> library.</p> <p>As a reference, the following example demonstrates how to perform inference using the <code>fai-rtdetr-m-obj365</code> model, but you can use any of the models listed in the models section.</p> <p></p>"},{"location":"how_to/inference/#see-focoos-models-metrics","title":"\ud83d\udcc8 See Focoos Models metrics","text":"<p>You can see the metrics of the Focoos Models by calling the <code>metrics</code> method on the model.</p> <p><pre><code>model = focoos.get_remote_model(\"fai-rtdetr-m-obj365\")\nmetrics = model.metrics()\nprint(f\"Best validation metrics:\")\nfor k, v in metrics.best_valid_metric.items():\n    print(f\"  {k}: {v}\")\n</code></pre> This code snippet will print the best validation metrics of the model, both considering average and per-class metrics.</p>"},{"location":"how_to/inference/#cloud-inference","title":"\u2601\ufe0f Cloud Inference","text":"<p>Making inference on the cloud is straightforward, you just need to select the model you want to use and call the <code>infer</code> method on your image. The image will be uploaded on the FocoosAI cloud, where the model will perform the inference and return the results.</p> <p><pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nimage_path = \"&lt;PATH-TO-YOUR-IMAGE&gt;\"\nmodel = focoos.get_remote_model(\"fai-rtdetr-m-obj365\")\nresult, _ = model.infer(image_path, threshold=0.5, annotate=False)\n\nfor det in result.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n</code></pre> <code>result</code> is a FocoosDetections object, containing a list of FocoosDet objects and optionally a dict of information about the latency of the inference.</p> <p>The <code>threshold</code> parameter is optional and defines the minimum confidence score for a detection to be considered valid (predictions with a confidence score lower than the threshold are discarded).</p> <p>Optionally, you can preview the results by passing the <code>annotate</code> parameter to the <code>infer</code> method. <pre><code>from PIL import Image\n\noutput, preview = model.infer(image_path, threshold=0.5, annotate=True)\npreview = Image.fromarray(preview[:,:,[2,1,0]]) # invert to make it RGB\n</code></pre></p>"},{"location":"how_to/inference/#local-inference","title":"\ud83e\udd16 Local Inference","text":"<p>Note</p> <p>To perform local inference, you need to install the package with one of the extra modules (<code>[cpu]</code>, <code>[torch]</code>, <code>[cuda]</code>, <code>[tensorrt]</code>). See the installation page for more details.</p> <p>You can perform inference locally by selecting the model you want to use and calling the <code>infer</code> method on your image. If it's the first time you run the model locally, the model will be downloaded from the cloud and saved on your machine. Additionally, if you use CUDA or TensorRT, the model will be optimized for your GPU before running the inference (it can take few seconds, especially for TensorRT).</p> <p><pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nmodel = focoos.get_local_model(\"fai-rtdetr-m-obj365\")\n\nimage_path = \"&lt;PATH-TO-YOUR-IMAGE&gt;\"\nresult, _ = model.infer(image_path, threshold=0.5, annotate=False)\n\nfor det in result.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n</code></pre> <code>result</code> is a FocoosDetections object, containing a list of FocoosDet objects and optionally a dict of information about the latency of the inference.</p> <p>As for remote inference, you can pass the <code>annotate</code> parameter to return a preview of the prediction.</p>"},{"location":"how_to/inference/#cloud-inference-with-gradio","title":"\ud83d\uddbc\ufe0f Cloud Inference with Gradio","text":"<p>You can further use Gradio to create a web interface for your model.</p> <p>First, install the <code>dev</code> extra dependency.</p> <pre><code>pip install '.[dev]'\n</code></pre> <p>To use it, use an environment variable with your Focoos API key and run the app (you will select the model from the UI). <pre><code>export FOCOOS_API_KEY_GRADIO=&lt;YOUR-API-KEY&gt;; python gradio/app.py\n</code></pre></p>"},{"location":"how_to/user/","title":"User Management","text":"<p>This section covers the steps to manage your user information on the FocoosAI platform.</p> <p></p>"},{"location":"how_to/user/#how-to-list-your-user-info","title":"How to list your user info","text":"<p>To list your user info, the library provides a <code>get_user_info</code> function. This function will return a <code>User</code> object.</p> <p><pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n\nuser_info = focoos.get_user_info()\n\nprint(f\"Email: {user_info.email}\")\nprint(f\"Created at: {user_info.created_at}\")\nprint(f\"Updated at: {user_info.updated_at}\")\nif user_info.company:\n    print(f\"Company: {user_info.company}\")\n\nprint(\"\\nQuotas:\")\nprint(f\"Total inferences: {user_info.quotas.total_inferences}\")\nprint(f\"Max inferences: {user_info.quotas.max_inferences}\")\nprint(f\"Used storage (GB): {user_info.quotas.used_storage_gb}\")\nprint(f\"Max storage (GB): {user_info.quotas.max_storage_gb}\")\nprint(f\"Active training jobs: {user_info.quotas.active_training_jobs}\")\nprint(f\"Max active training jobs: {user_info.quotas.max_active_training_jobs}\")\nprint(f\"Used MLG4DNXLarge training jobs hours: {user_info.quotas.used_mlg4dnxlarge_training_jobs_hours}\")\nprint(f\"Max MLG4DNXLarge training jobs hours: {user_info.quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n</code></pre> <code>user_info</code> is a <code>User</code> object that contains information about the authenticated user, including their email address, company affiliation (if any), API key details, and platform usage quotas.</p> <p>The User object contains the following fields:</p> <ul> <li><code>email</code>: The email address associated with the user account</li> <li><code>created_at</code>: Timestamp of when the user account was created</li> <li><code>updated_at</code>: Timestamp of when the user account was last updated</li> <li><code>company</code>: The company affiliation of the user (optional)</li> <li><code>api_key</code>: The API key details used for authentication</li> <li><code>quotas</code>: The platform usage quotas (see <code>Quotas</code>) allocated to the user</li> </ul> <p>Note</p> <p>If you want to increase your quotas, please contact us via support.</p>"},{"location":"models/fai-m2f-l-ade/","title":"fai-m2f-l-ade","text":""},{"location":"models/fai-m2f-l-ade/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the ADE20K dataset. It is a semantic segmentation model able to segment 150 classes, comprising both stuff (sky, road, etc.) and thing (dog, cat, car, etc.).</p>"},{"location":"models/fai-m2f-l-ade/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-m2f-l-ade/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a transformer-based encoder-decoder architecture. Differently from traditional segmentation models (such as DeepLab), Mask2Former uses a mask-classification approach, where the prediction is made by a set of segmentation mask with associated class probabilities.</p>"},{"location":"models/fai-m2f-l-ade/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is a Resnet-101,that guarantees a good performance while having good efficiency.</li> <li>the pixel decoder is a FPN getting the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. Differently from the original paper, for the sake of portability, we removed the deformable attention modules in the pixel decoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 6 decoder layers (instead of 9) and 100 learnable queries.</li> </ul>"},{"location":"models/fai-m2f-l-ade/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the 6 transformer decoder layers. Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-m2f-l-ade/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-m2f-l-ade/#classes","title":"Classes","text":"<p>The model is pretrained on the ADE20K dataset with 150 classes.</p> Class ID Class Name mIoU 1wall77.284 2building81.396 3sky94.337 4floor81.584 5tree74.103 6ceiling83.073 7road, route83.013 8bed88.120 9window61.048 10grass69.099 11cabinet56.303 12sidewalk, pavement62.300 13person82.073 14earth, ground35.094 15door45.140 16table59.436 17mountain, mount60.538 18plant51.829 19curtain71.510 20chair56.219 21car83.766 22water49.028 23painting, picture70.214 24sofa68.081 25shelf35.453 26house45.656 27sea51.205 28mirror61.611 29rug64.144 30field30.577 31armchair45.761 32seat61.850 33fence40.992 34desk41.814 35rock, stone47.600 36wardrobe, closet, press39.846 37lamp64.062 38tub74.760 39rail24.105 40cushion56.811 41base, pedestal, stand27.777 42box24.670 43column, pillar40.094 44signboard, sign33.495 45chest of drawers, chest, bureau, dresser41.847 46counter21.387 47sand29.763 48sink74.092 49skyscraper37.613 50fireplace65.037 51refrigerator, icebox57.648 52grandstand, covered stand46.626 53path24.543 54stairs28.681 55runway73.779 56case, display case, showcase, vitrine38.437 57pool table, billiard table, snooker table91.825 58pillow49.388 59screen door, screen59.058 60stairway, staircase32.832 61river18.597 62bridge, span56.011 63bookcase28.848 64blind, screen43.934 65coffee table59.869 66toilet, can, commode, crapper, pot, potty, stool, throne86.346 67flower38.141 68book42.528 69hill6.905 70bench45.494 71countertop49.007 72stove73.973 73palm, palm tree49.478 74kitchen island42.603 75computer72.142 76swivel chair44.262 77boat73.689 78bar37.749 79arcade machine78.733 80hovel, hut, hutch, shack, shanty30.537 81bus90.808 82towel58.158 83light57.444 84truck31.745 85tower32.058 86chandelier67.524 87awning, sunshade, sunblind28.566 88street lamp30.507 89booth39.696 90tv76.194 91plane50.005 92dirt track18.268 93clothes37.748 94pole23.343 95land, ground, soil0.001 96bannister, banister, balustrade, balusters, handrail16.222 97escalator, moving staircase, moving stairway54.888 98ottoman, pouf, pouffe, puff, hassock32.444 99bottle22.166 100buffet, counter, sideboard48.994 101poster, posting, placard, notice, bill, card31.773 102stage18.731 103van46.747 104ship79.937 105fountain21.205 106conveyer belt, conveyor belt, conveyer, conveyor, transporter62.591 107canopy23.719 108washer, automatic washer, washing machine66.458 109plaything, toy35.377 110pool34.297 111stool41.199 112barrel, cask61.803 113basket, handbasket34.313 114falls57.149 115tent94.077 116bag19.126 117minibike, motorbike71.207 118cradle85.775 119oven50.996 120ball32.601 121food, solid food58.662 122step, stair16.474 123tank, storage tank37.627 124trade name20.788 125microwave37.998 126pot53.411 127animal57.360 128bicycle58.772 129lake41.597 130dishwasher74.543 131screen79.757 132blanket, cover15.202 133sculpture53.537 134hood, exhaust hood52.684 135sconce48.160 136vase45.300 137traffic light35.375 138tray14.093 139trash can30.699 140fan56.574 141pier10.286 142crt screen0.936 143plate53.268 144monitor9.358 145bulletin board29.970 146shower8.978 147radiator59.763 148glass, drinking glass18.246 149clock29.088 150flag37.727"},{"location":"models/fai-m2f-l-ade/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-m2f-l-ade) from Focoos API\nmodel = focoos.get_remote_model(\"fai-m2f-l-ade\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-m2f-l-coco-ins/","title":"fai-m2f-l-coco-ins","text":""},{"location":"models/fai-m2f-l-coco-ins/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the COCO dataset. It is an instance segmentation model able to segment 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-m2f-l-coco-ins/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a mask-classification approach and a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-m2f-l-coco-ins/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is Resnet-50 that show an amazing trade-off between performance and efficiency.</li> <li>the pixel decoder is a transformer-augmented FPN. It gets the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. It first uses a transformer encoder to process the features at the lowest resolution (stage 5) and then uses a feature pyramid network to upsample the features. This part is different from the original implementation using deformable attention modules.</li> <li>the transformer decoder is implemented as in the original paper, having 9 decoder layers and 100 learnable queries.</li> </ul>"},{"location":"models/fai-m2f-l-coco-ins/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the 3 transformer decoder layers. Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-m2f-l-coco-ins/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-m2f-l-coco-ins/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class Segmentation AP 1 person 48.9 2 bicycle 22.2 3 car 41.3 4 motorcycle 40.0 5 airplane 55.6 6 bus 68.2 7 train 69.6 8 truck 40.5 9 boat 26.2 10 traffic light 27.4 11 fire hydrant 69.2 12 stop sign 65.0 13 parking meter 45.4 14 bench 23.4 15 bird 33.8 16 cat 77.7 17 dog 68.9 18 horse 50.1 19 sheep 54.0 20 cow 51.0 21 elephant 63.4 22 bear 81.1 23 zebra 66.0 24 giraffe 60.5 25 backpack 22.7 26 umbrella 52.6 27 handbag 23.3 28 tie 33.2 29 suitcase 45.3 30 frisbee 66.4 31 skis 7.4 32 snowboard 28.2 33 sports ball 42.8 34 kite 30.3 35 baseball bat 32.1 36 baseball glove 42.3 37 skateboard 36.8 38 surfboard 37.3 39 tennis racket 58.7 40 bottle 39.2 41 wine glass 36.9 42 cup 46.0 43 fork 22.2 44 knife 17.8 45 spoon 18.0 46 bowl 44.3 47 banana 26.5 48 apple 23.9 49 sandwich 43.0 50 orange 33.8 51 broccoli 24.4 52 carrot 22.7 53 hot dog 36.3 54 pizza 55.1 55 donut 51.1 56 cake 44.6 57 chair 25.0 58 couch 47.7 59 potted plant 25.0 60 bed 45.0 61 dining table 22.9 62 toilet 67.6 63 tv 64.3 64 laptop 67.2 65 mouse 60.1 66 remote 36.1 67 keyboard 52.6 68 cell phone 42.0 69 microwave 60.7 70 oven 33.8 71 toaster 35.9 72 sink 39.9 73 refrigerator 64.0 74 book 12.0 75 clock 52.5 76 vase 37.7 77 scissors 26.8 78 teddy bear 55.1 79 hair drier 16.8 80 toothbrush 22.4"},{"location":"models/fai-m2f-l-coco-ins/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-m2f-l-coco-ins) from Focoos API\nmodel = focoos.get_remote_model(\"fai-m2f-l-coco-ins\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-m2f-m-ade/","title":"fai-m2f-m-ade","text":""},{"location":"models/fai-m2f-m-ade/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the ADE20K dataset. It is a semantic segmentation model able to segment 150 classes, comprising both stuff (sky, road, etc.) and thing (dog, cat, car, etc.).</p>"},{"location":"models/fai-m2f-m-ade/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-m2f-m-ade/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a transformer-based encoder-decoder architecture. Differently from traditional segmentation models (such as DeepLab), Mask2Former uses a mask-classification approach, where the prediction is made by a set of segmentation mask with associated class probabilities.</p>"},{"location":"models/fai-m2f-m-ade/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-2 that show an amazing trade-off between performance and efficiency.</li> <li>the pixel decoder is a FPN getting the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. Differently from the original paper, for the sake of portability, we removed the deformable attention modules in the pixel decoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 3 decoder layers (instead of 9) and 100 learnable queries.</li> </ul>"},{"location":"models/fai-m2f-m-ade/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the 3 transformer decoder layers. Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-m2f-m-ade/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-m2f-m-ade/#classes","title":"Classes","text":"<p>The model is pretrained on the ADE20K dataset with 150 classes.</p> Class mIoU 1 wall 75.369549 2 building 79.835995 3 sky 94.176995 4 floor 79.620841 5 tree 73.204506 6 ceiling 82.303035 7 road, route 80.822591 8 bed 87.573840 9 window 57.452584 10 grass 70.099493 11 cabinet 56.903790 12 sidewalk, pavement 62.247267 13 person 79.460606 14 earth, ground 38.537802 15 door 43.930878 16 table 56.753292 17 mountain, mount 61.160462 18 plant 48.995487 19 curtain 71.951930 20 chair 52.852125 21 car 80.725703 22 water 51.233498 23 painting, picture 66.989493 24 sofa 58.103663 25 shelf 34.979205 26 house 36.828611 27 sea 51.219096 28 mirror 58.572852 29 rug 54.897799 30 field 29.053876 31 armchair 39.565663 32 seat 53.113668 33 fence 41.113128 34 desk 37.930189 35 rock, stone 44.940982 36 wardrobe, closet, press 39.897858 37 lamp 60.921356 38 tub 78.041637 39 rail 31.893878 40 cushion 53.029316 41 base, pedestal, stand 20.233620 42 box 18.276924 43 column, pillar 42.655306 44 signboard, sign 35.959448 45 chest of drawers, chest, bureau, dresser 36.521600 46 counter 29.353667 47 sand 38.729599 48 sink 72.303141 49 skyscraper 44.122387 50 fireplace 66.614683 51 refrigerator, icebox 72.137179 52 grandstand, covered stand 29.061628 53 path 26.629478 54 stairs 31.833328 55 runway 76.017706 56 case, display case, showcase, vitrine 37.452627 57 pool table, billiard table, snooker table 93.246039 58 pillow 54.689591 59 screen door, screen 58.096890 60 stairway, staircase 29.962829 61 river 15.010211 62 bridge, span 66.617580 63 bookcase 31.383789 64 blind, screen 39.221180 65 coffee table 63.300795 66 toilet, can, commode, crapper, pot, potty, stool, throne 84.038177 67 flower 35.994798 68 book 43.252042 69 hill 6.240850 70 bench 35.007473 71 countertop 56.592858 72 stove 74.866261 73 palm, palm tree 49.092486 74 kitchen island 32.353614 75 computer 57.673329 76 swivel chair 43.202283 77 boat 48.170742 78 bar 24.034261 79 arcade machine 11.467819 80 hovel, hut, hutch, shack, shanty 10.258017 81 bus 81.375072 82 towel 54.954106 83 light 53.256340 84 truck 29.656645 85 tower 36.864496 86 chandelier 63.787459 87 awning, sunshade, sunblind 23.610311 88 street lamp 29.944617 89 booth 29.360433 90 tv 61.512572 91 plane 53.270513 92 dirt track 4.206758 93 clothes 35.342074 94 pole 20.678348 95 land, ground, soil 3.195710 96 bannister, banister, balustrade, balusters, handrail 17.522631 97 escalator, moving staircase, moving stairway 20.889345 98 ottoman, pouf, pouffe, puff, hassock 47.003450 99 bottle 15.504667 100 buffet, counter, sideboard 26.077572 101 poster, posting, placard, notice, bill, card 30.691103 102 stage 11.744151 103 van 40.161822 104 ship 79.300311 105 fountain 0.112958 106 conveyer belt, conveyor belt, conveyer, conveyor, transporter 60.552373 107 canopy 25.086350 108 washer, automatic washer, washing machine 63.550537 109 plaything, toy 18.290597 110 pool 32.873865 111 stool 39.256308 112 barrel, cask 6.358771 113 basket, handbasket 29.850719 114 falls 57.657161 115 tent 93.717152 116 bag 10.629695 117 minibike, motorbike 56.217901 118 cradle 69.441302 119 oven 38.940583 120 ball 45.543376 121 food, solid food 52.779065 122 step, stair 10.843115 123 tank, storage tank 30.871163 124 trade name 27.908376 125 microwave 32.381977 126 pot 41.040635 127 animal 55.882266 128 bicycle 50.185374 129 lake 0.007605 130 dishwasher 58.970317 131 screen 60.016197 132 blanket, cover 26.963189 133 sculpture 27.667732 134 hood, exhaust hood 58.025458 135 sconce 39.341998 136 vase 31.185747 137 traffic light 23.810429 138 tray 7.244281 139 trash can 30.072544 140 fan 52.113861 141 pier 56.678802 142 crt screen 9.133357 143 plate 38.900407 144 monitor 3.323130 145 bulletin board 52.337659 146 shower 4.692180 147 radiator 43.811464 148 glass, drinking glass 14.036491 149 clock 25.044316 150 flag 40.007933"},{"location":"models/fai-m2f-m-ade/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-m2f-m-ade) from Focoos API\nmodel = focoos.get_remote_model(\"fai-m2f-m-ade\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-m2f-s-ade/","title":"fai-m2f-l-ade","text":""},{"location":"models/fai-m2f-s-ade/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the ADE20K dataset. It is a semantic segmentation model able to segment 150 classes, comprising both stuff (sky, road, etc.) and thing (dog, cat, car, etc.).</p>"},{"location":"models/fai-m2f-s-ade/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-m2f-s-ade/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a transformer-based encoder-decoder architecture. Differently from traditional segmentation models (such as DeepLab), Mask2Former uses a mask-classification approach, where the prediction is made by a set of segmentation mask with associated class probabilities.</p>"},{"location":"models/fai-m2f-s-ade/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-1 that shows a trade-off tending to be more efficient.</li> <li>the pixel decoder is a FPN getting the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. Differently from the original paper, for the sake of portability, we removed the deformable attention modules in the pixel decoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a extremely light version of the original, having only 1 decoder layer (instead of 9) and 100 learnable queries.</li> </ul>"},{"location":"models/fai-m2f-s-ade/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-m2f-s-ade/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-m2f-s-ade/#classes","title":"Classes","text":"<p>The model is pretrained on the ADE20K dataset with 150 classes.</p> Class mIoU 1 wall 69.973850 2 building 78.431035 3 sky 91.401107 4 floor 73.162280 5 tree 70.535439 6 ceiling 77.258595 7 road, route 78.314172 8 bed 77.755793 9 window 53.012898 10 grass 64.432303 11 cabinet 51.032268 12 sidewalk, pavement 55.642697 13 person 70.461440 14 eartd, ground 30.454824 15 door 36.431782 16 table 43.096636 17 mountain, mount 54.971609 18 plant 45.115711 19 curtain 64.930372 20 chair 40.465565 21 car 76.888113 22 water 41.666148 23 painting, picture 60.099652 24 sofa 49.840449 25 shelf 31.991519 26 house 45.338182 27 sea 51.614250 28 mirror 55.731406 29 rug 51.858072 30 field 23.065903 31 armchair 30.602317 32 seat 50.277596 33 fence 34.439293 34 desk 35.494495 35 rock, stone 39.573617 36 wardrobe, closet, press 51.343586 37 lamp 47.754304 38 tub 71.511291 39 rail 23.280869 40 cushion 39.251768 41 base, pedestal, stand 28.472143 42 box 16.070477 43 column, pillar 37.924454 44 signboard, sign 29.057276 45 chest of drawers, chest, bureau, dresser 36.343963 46 counter 19.595326 47 sand 31.296151 48 sink 54.413180 49 skyscraper 47.583224 50 fireplace 62.204434 51 refrigerator, icebox 54.270643 52 grandstand, covered stand 31.345801 53 patd 22.330369 54 stairs 20.323718 55 runway 63.892811 56 case, display case, showcase, vitrine 34.649422 57 pool table, billiard table, snooker table 85.365581 58 pillow 46.426184 59 screen door, screen 57.292321 60 stairway, staircase 28.904954 61 river 16.681450 62 bridge, span 52.791513 63 bookcase 26.722881 64 blind, screen 36.787453 65 coffee table 41.603442 66 toilet, can, commode, crapper, pot, potty, stool, tdrone 75.753455 67 flower 30.200230 68 book 37.602484 69 hill 5.509057 70 bench 29.331054 71 countertop 46.661677 72 stove 58.972851 73 palm, palm tree 48.317300 74 kitchen island 25.279206 75 computer 49.335666 76 swivel chair 34.845392 77 boat 48.521646 78 bar 30.174155 79 arcade machine 24.721694 80 hovel, hut, hutch, shack, shanty 32.843717 81 bus 82.174778 82 towel 46.050430 83 light 30.983118 84 truck 23.456256 85 tower 32.147803 86 chandelier 54.045160 87 awning, sunshade, sunblind 18.526182 88 street lamp 13.641714 89 bootd 60.471570 90 tv 55.530715 91 plane 42.894525 92 dirt track 0.001787 93 clotdes 30.124455 94 pole 11.280532 95 land, ground, soil 4.243296 96 bannister, banister, balustrade, balusters, handrail 9.922319 97 escalator, moving staircase, moving stairway 19.186240 98 ottoman, pouf, pouffe, puff, hassock 30.352586 99 bottle 11.872842 100 buffet, counter, sideboard 34.547476 101 poster, posting, placard, notice, bill, card 15.081001 102 stage 17.466091 103 van 39.027877 104 ship 66.778301 105 fountain 18.879113 106 conveyer belt, conveyor belt, conveyer, conveyor, transporter 67.580228 107 canopy 25.654567 108 washer, automatic washer, washing machine 60.187881 109 playtding, toy 13.836259 110 pool 28.796494 111 stool 26.432746 112 barrel, cask 43.777156 113 basket, handbasket 19.144369 114 falls 47.131198 115 tent 88.431441 116 bag 7.634387 117 minibike, motorbike 40.625528 118 cradle 54.247514 119 oven 33.695444 120 ball 36.066130 121 food, solid food 50.837348 122 step, stair 13.071184 123 tank, storage tank 43.042742 124 trade name 21.579095 125 microwave 32.179626 126 pot 27.438416 127 animal 55.993825 128 bicycle 38.273475 129 lake 35.704904 130 dishwasher 37.616793 131 screen 57.100955 132 blanket, cover 15.560568 133 sculpture 31.317035 134 hood, exhaust hood 49.290385 135 sconce 29.971644 136 vase 24.983318 137 traffic light 17.806663 138 tray 5.720345 139 trash can 28.621136 140 fan 39.083851 141 pier 51.310956 142 crt screen 0.858346 143 plate 35.344330 144 monitor 1.994270 145 bulletin board 35.468027 146 shower 1.090403 147 radiator 42.574652 148 glass, drinking glass 8.510381 149 clock 14.128872 150 flag 24.098100"},{"location":"models/fai-m2f-s-ade/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-m2f-s-ade) from Focoos API\nmodel = focoos.get_remote_model(\"fai-m2f-s-ade\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-rtdetr-l-coco/","title":"fai-rtdetr-l-coco","text":""},{"location":"models/fai-rtdetr-l-coco/#overview","title":"Overview","text":"<p>The models is the reimplementation of the RT-DETR model by FocoosAI for the COCO dataset. It is a object detection model able to detect 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-rtdetr-l-coco/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-rtdetr-l-coco/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-rtdetr-l-coco/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>This implementation is a reimplementation of the RT-DETR model by FocoosAI. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is a Resnet-50,that guarantees a good performance while having good efficiency.</li> <li>the encoder is the Hybrid Encoder, as proposed by the paper, and it is a bi-FPN (bilinear feature pyramid network) that includes a transformer encoder on the smaller feature resolution for improving efficiency.</li> <li>The query selection mechanism select the features of the pixels (aka queries) with the highest probability of containing an object and pass them to a transformer decoder head that will generate the final detection output. In this implementation, we select 300 queries and use 6 transformer decoder layers.</li> </ul>"},{"location":"models/fai-rtdetr-l-coco/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. For more details look at GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-rtdetr-l-coco/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-rtdetr-l-coco/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class AP 1 person 63.2 2 bicycle 40.5 3 car 52.3 4 motorcycle 55.0 5 airplane 76.3 6 bus 74.9 7 train 75.0 8 truck 47.9 9 boat 36.6 10 traffic light 32.6 11 fire hydrant 75.5 12 stop sign 71.2 13 parking meter 54.6 14 bench 34.9 15 bird 46.6 16 cat 79.8 17 dog 75.4 18 horse 69.7 19 sheep 63.0 20 cow 68.8 21 elephant 74.1 22 bear 83.2 23 zebra 78.3 24 giraffe 76.9 25 backpack 25.1 26 umbrella 53.8 27 handbag 24.3 28 tie 44.8 29 suitcase 52.6 30 frisbee 75.3 31 skis 37.2 32 snowboard 50.8 33 sports ball 53.9 34 kite 54.8 35 baseball bat 53.2 36 baseball glove 45.3 37 skateboard 63.7 38 surfboard 50.3 39 tennis racket 61.1 40 bottle 48.8 41 wine glass 44.1 42 cup 53.4 43 fork 51.3 44 knife 34.1 45 spoon 33.5 46 bowl 52.1 47 banana 33.0 48 apple 27.1 49 sandwich 48.1 50 orange 37.9 51 broccoli 28.9 52 carrot 28.2 53 hot dog 50.3 54 pizza 62.5 55 donut 62.3 56 cake 47.5 57 chair 41.2 58 couch 57.3 59 potted plant 36.0 60 bed 58.5 61 dining table 39.4 62 toilet 72.6 63 tv 65.8 64 laptop 73.1 65 mouse 67.1 66 remote 48.2 67 keyboard 63.0 68 cell phone 46.0 69 microwave 64.6 70 oven 44.9 71 toaster 50.4 72 sink 45.7 73 refrigerator 69.4 74 book 22.3 75 clock 59.2 76 vase 45.7 77 scissors 42.4 78 teddy bear 59.5 79 hair drier 35.1 80 tootdbrush 42.0"},{"location":"models/fai-rtdetr-l-coco/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-rtdetr-l-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-rtdetr-l-coco\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-rtdetr-m-coco/","title":"fai-rtdetr-m-coco","text":""},{"location":"models/fai-rtdetr-m-coco/#overview","title":"Overview","text":"<p>The models is a RT-DETR model otimized by FocoosAI for the COCO dataset. It is a object detection model able to detect 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-rtdetr-m-coco/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-rtdetr-m-coco/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-rtdetr-m-coco/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The RT-DETR FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-2 that show an amazing trade-off between performance and efficiency.</li> <li>the encoder is a bi-FPN (bilinear feature pyramid network). With respect to the original paper, we removed the attention modules in the encoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 3 decoder layers, instead of 6, and we select 300 queries.</li> </ul>"},{"location":"models/fai-rtdetr-m-coco/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. for more details look here: GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-rtdetr-m-coco/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-rtdetr-m-coco/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class ID Class Name AP 1 person 56.4 2 bicycle 32.8 3 car 44.2 4 motorcycle 47.9 5 airplane 72.5 6 bus 70.4 7 train 69.1 8 truck 39.5 9 boat 27.9 10 traffic light 26.6 11 fire hydrant 68.8 12 stop sign 64.4 13 parking meter 45.5 14 bench 26.0 15 bird 37.4 16 cat 73.4 17 dog 68.6 18 horse 59.5 19 sheep 56.2 20 cow 59.2 21 elephant 67.9 22 bear 77.9 23 zebra 71.4 24 giraffe 72.2 25 backpack 16.7 26 umbrella 43.4 27 handbag 16.6 28 tie 36.5 29 suitcase 44.4 30 frisbee 68.0 31 skis 27.5 32 snowboard 35.0 33 sports ball 46.5 34 kite 44.8 35 baseball bat 29.2 36 baseball glove 38.4 37 skateboard 56.2 38 surfboard 43.3 39 tennis racket 49.5 40 bottle 37.8 41 wine glass 35.7 42 cup 43.1 43 fork 39.0 44 knife 22.6 45 spoon 20.4 46 bowl 43.5 47 banana 27.1 48 apple 22.2 49 sandwich 38.8 50 orange 33.4 51 broccoli 24.7 52 carrot 23.8 53 hot dog 38.4 54 pizza 57.4 55 donut 50.6 56 cake 38.4 57 chair 30.9 58 couch 50.0 59 potted plant 28.9 60 bed 51.4 61 dining table 32.8 62 toilet 67.2 63 tv 59.4 64 laptop 62.7 65 mouse 64.7 66 remote 34.4 67 keyboard 55.8 68 cell phone 38.2 69 microwave 61.4 70 oven 41.8 71 toaster 48.3 72 sink 39.4 73 refrigerator 59.5 74 book 15.5 75 clock 49.0 76 vase 39.3 77 scissors 30.3 78 teddy bear 50.6 79 hair drier 4.8 80 toothbrush 30.2"},{"location":"models/fai-rtdetr-m-coco/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-rtdetr-m-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-rtdetr-m-coco\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-rtdetr-m-obj365/","title":"fai-rtdetr-m-obj365","text":""},{"location":"models/fai-rtdetr-m-obj365/#overview","title":"Overview","text":"<p>The models is a RT-DETR model otimized by FocoosAI for the Objects365. It is a object detection model able to detect 365 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-rtdetr-m-obj365/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-rtdetr-m-obj365/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>This implementation is a reimplementation of the RT-DETR model by FocoosAI. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is a Resnet-50,that guarantees a good performance while having good efficiency.</li> <li>the encoder is the Hybrid Encoder, as proposed by the paper, and it is a bi-FPN (bilinear feature pyramid network) that includes a transformer encoder on the smaller feature resolution for improving efficiency.</li> <li>The query selection mechanism select the features of the pixels (aka queries) with the highest probability of containing an object and pass them to a transformer decoder head that will generate the final detection output. In this implementation, we select 300 queries and use 6 transformer decoder layers.</li> </ul>"},{"location":"models/fai-rtdetr-m-obj365/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. For more details look at GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-rtdetr-m-obj365/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-rtdetr-m-obj365/#classes","title":"Classes","text":"<p>The model is pretrained on the Objects365 with 365 classes.</p> Class ID Class Name AP 1 Person 68.8 2 Sneakers 47.3 3 Chair 52.7 4 Other Shoes 14.8 5 Hat 56.4 6 Car 37.2 7 Lamp 43.7 8 Glasses 47.8 9 Bottle 40.6 10 Desk 48.2 11 Cup 53.9 12 Street Lights 38.9 13 Cabinet/shelf 46.8 14 Handbag/Satchel 29.5 15 Bracelet 27.8 16 Plate 70.5 17 Picture/Frame 66.0 18 Helmet 47.7 19 Book 23.0 20 Gloves 42.9 21 Storage box 24.1 22 Boat 31.1 23 Leather Shoes 29.3 24 Flower 39.6 25 Bench 26.6 26 Potted Plant 42.4 27 Bowl/Basin 57.7 28 Flag 38.6 29 Pillow 56.0 30 Boots 39.8 31 Vase 37.1 32 Microphone 39.1 33 Necklace 30.3 34 Ring 19.5 35 SUV 33.4 36 Wine Glass 68.6 37 Belt 35.8 38 Moniter/TV 75.1 39 Backpack 29.9 40 Umbrella 37.4 41 Traffic Light 39.0 42 Speaker 58.3 43 Watch 47.1 44 Tie 37.0 45 Trash bin Can 49.1 46 Slippers 43.9 47 Bicycle 46.8 48 Stool 48.9 49 Barrel/bucket 39.9 50 Van 30.7 51 Couch 62.5 52 Sandals 43.7 53 Bakset 40.1 54 Drum 54.8 55 Pen/Pencil 29.9 56 Bus 45.3 57 Wild Bird 12.6 58 High Heels 41.9 59 Motorcycle 31.8 60 Guitar 64.7 61 Carpet 59.8 62 Cell Phone 43.8 63 Bread 23.6 64 Camera 32.8 65 Canned 37.4 66 Truck 23.0 67 Traffic cone 44.5 68 Cymbal 55.7 69 Lifesaver 31.2 70 Towel 54.0 71 Stuffed Toy 40.7 72 Candle 30.5 73 Sailboat 55.5 74 Laptop 73.8 75 Awning 27.4 76 Bed 66.1 77 Faucet 41.8 78 Tent 30.8 79 Horse 46.3 80 Mirror 59.0 81 Power outlet 43.1 82 Sink 53.3 83 Apple 22.0 84 Air Conditioner 30.1 85 Knife 46.1 86 Hockey Stick 59.0 87 Paddle 25.5 88 Pickup Truck 45.2 89 Fork 57.7 90 Traffic Sign 30.0 91 Ballon 47.7 92 Tripod 29.7 93 Dog 58.5 94 Spoon 46.7 95 Clock 61.6 96 Pot 44.3 97 Cow 18.7 98 Cake 15.7 99 Dinning Table 46.2 100 Sheep 30.5 101 Hanger 9.9 102 Blackboard/Whiteboard 47.5 103 Napkin 35.5 104 Other Fish 30.4 105 Orange/Tangerine 10.8 106 Toiletry 30.1 107 Keyboard 71.8 108 Tomato 36.4 109 Lantern 47.7 110 Machinery Vehicle 30.5 111 Fan 49.4 112 Green Vegetables 13.2 113 Banana 30.2 114 Baseball Glove 38.8 115 Airplane 60.9 116 Mouse 61.5 117 Train 50.5 118 Pumpkin 53.4 119 Soccer 29.0 120 Skiboard 24.3 121 Luggage 32.5 122 Nightstand 62.3 123 Tea pot 31.7 124 Telephone 45.8 125 Trolley 36.8 126 Head Phone 40.9 127 Sports Car 67.8 128 Stop Sign 49.3 129 Dessert 28.7 130 Scooter 35.5 131 Stroller 42.7 132 Crane 46.0 133 Remote 47.1 134 Refrigerator 70.2 135 Oven 51.9 136 Lemon 33.4 137 Duck 43.3 138 Baseball Bat 40.4 139 Surveillance Camera 24.2 140 Cat 67.5 141 Jug 24.1 142 Broccoli 29.3 143 Piano 41.5 144 Pizza 50.9 145 Elephant 66.9 146 Skateboard 19.0 147 Surfboard 44.0 148 Gun 23.8 149 Skating and Skiing shoes 64.6 150 Gas stove 39.2 151 Donut 45.0 152 Bow Tie 28.8 153 Carrot 15.6 154 Toilet 73.2 155 Kite 44.1 156 Strawberry 24.2 157 Other Balls 36.1 158 Shovel 18.9 159 Pepper 18.5 160 Computer Box 49.3 161 Toilet Paper 39.0 162 Cleaning Products 21.1 163 Chopsticks 40.3 164 Microwave 68.0 165 Pigeon 48.1 166 Baseball 32.6 167 Cutting/chopping Board 40.9 168 Coffee Table 49.8 169 Side Table 34.6 170 Scissors 28.1 171 Marker 20.6 172 Pie 20.8 173 Ladder 34.4 174 Snowboard 36.8 175 Cookies 13.0 176 Radiator 50.3 177 Fire Hydrant 47.6 178 Basketball 32.9 179 Zebra 58.3 180 Grape 10.4 181 Giraffe 64.0 182 Potato 11.0 183 Sausage 25.1 184 Tricycle 27.4 185 Violin 39.6 186 Egg 34.7 187 Fire Extinguisher 47.3 188 Candy 1.9 189 Fire Truck 55.7 190 Billards 63.0 191 Converter 18.0 192 Bathtub 58.8 193 Wheelchair 62.9 194 Golf Club 33.8 195 Briefcase 35.1 196 Cucumber 26.6 197 Cigar/Cigarette 11.2 198 Paint Brush 15.0 199 Pear 5.5 200 Heavy Truck 35.3 201 Hamburger 40.5 202 Extractor 62.7 203 Extention Cord 19.1 204 Tong 16.4 205 Tennis Racket 51.4 206 Folder 8.8 207 American Football 22.6 208 earphone 7.6 209 Mask 36.9 210 Kettle 43.9 211 Tennis 37.3 212 Ship 49.0 213 Swing 48.4 214 Coffee Machine 50.3 215 Slide 46.7 216 Carriage 59.5 217 Onion 7.4 218 Green beans 3.6 219 Projector 48.6 220 Frisbee 33.0 221 Washing Machine/Drying Machine 53.6 222 Chicken 49.5 223 Printer 54.8 224 Watermelon 28.7 225 Saxophone 52.2 226 Tissue 31.6 227 Toothbrush 23.6 228 Ice cream 27.6 229 Hotair ballon 77.2 230 Cello 45.9 231 French Fries 42.7 232 Scale 28.3 233 Trophy 37.6 234 Cabbage 11.9 235 Hot dog 39.9 236 Blender 44.3 237 Peach 6.2 238 Rice 44.3 239 Wallet/Purse 30.4 240 Volleyball 51.2 241 Deer 45.0 242 Goose 17.5 243 Tape 24.0 244 Tablet 39.9 245 Cosmetics 18.8 246 Trumpet 36.7 247 Pineapple 19.1 248 Golf Ball 39.5 249 Ambulance 78.2 250 Parking meter 33.8 251 Mango 0.8 252 Key 3.0 253 Hurdle 33.9 254 Fishing Rod 29.1 255 Medal 22.8 256 Flute 31.9 257 Brush 8.2 258 Penguin 57.5 259 Megaphone 18.8 260 Corn 22.4 261 Lettuce 2.3 262 Garlic 16.8 263 Swan 46.1 264 Helicopter 42.7 265 Green Onion 0.5 266 Sandwich 29.5 267 Nuts 0.6 268 Speed Limit Sign 43.4 269 Induction Cooker 27.3 270 Broom 19.6 271 Trombone 33.0 272 Plum 3.7 273 Rickshaw 24.7 274 Goldfish 14.3 275 Kiwi fruit 8.3 276 Router/modem 14.5 277 Poker Card 31.2 278 Toaster 48.6 279 Shrimp 5.0 280 Sushi 22.3 281 Cheese 21.2 282 Notepaper 9.3 283 Cherry 4.3 284 Pliers 12.2 285 CD 21.1 286 Pasta 36.3 287 Hammer 20.6 288 Cue 46.2 289 Avocado 9.8 290 Hamimelon 2.9 291 Flask 31.2 292 Mushroon 3.1 293 Screwdriver 13.6 294 Soap 19.0 295 Recorder 32.9 296 Bear 46.2 297 Eggplant 8.8 298 Board Eraser 38.7 299 Coconut 10.0 300 Tape Measur/ Ruler 15.0 301 Pig 50.0 302 Showerhead 11.6 303 Globe 57.1 304 Chips 13.7 305 Steak 28.2 306 Crosswalk Sign 48.2 307 Stapler 27.2 308 Campel 55.3 309 Formula 1 59.1 310 Pomegranate 3.9 311 Dishwasher 53.1 312 Crab 11.6 313 Hoverboard 29.9 314 Meat ball 7.6 315 Rice Cooker 30.5 316 Tuba 24.6 317 Calculator 38.0 318 Papaya 4.3 319 Antelope 24.4 320 Parrot 34.1 321 Seal 41.1 322 Buttefly 36.4 323 Dumbbell 8.0 324 Donkey 42.0 325 Lion 33.8 326 Urinal 53.0 327 Dolphin 39.5 328 Electric Drill 24.1 329 Hair Dryer 10.0 330 Egg tart 5.1 331 Jellyfish 40.2 332 Treadmill 43.9 333 Lighter 12.6 334 Grapefruit 1.2 335 Game board 37.3 336 Mop 5.8 337 Radish 0.6 338 Baozi 40.2 339 Target 14.6 340 French 27.3 341 Spring Rolls 29.1 342 Monkey 37.8 343 Rabbit 36.4 344 Pencil Case 22.8 345 Yak 37.7 346 Red Cabbage 7.1 347 Binoculars 15.7 348 Asparagus 4.1 349 Barbell 17.1 350 Scallop 12.4 351 Noddles 21.1 352 Comb 14.1 353 Dumpling 5.2 354 Oyster 17.6 355 Table Teniis paddle 22.2 356 Cosmetics Brush/Eyeliner Pencil 40.3 357 Chainsaw 13.8 358 Eraser 16.4 359 Lobster 18.0 360 Durian 33.7 361 Okra 0.1 362 Lipstick 36.3 363 Cosmetics Mirror 8.2 364 Curling 44.7 365 Table Tennis 25.1"},{"location":"models/fai-rtdetr-m-obj365/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-rtdetr-s-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-rtdetr-m-obj365\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-rtdetr-n-coco/","title":"fai-rtdetr-n-coco","text":""},{"location":"models/fai-rtdetr-n-coco/#overview","title":"Overview","text":"<p>The models is a RT-DETR model otimized by FocoosAI for the COCO dataset. It is a object detection model able to detect 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-rtdetr-n-coco/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-rtdetr-n-coco/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-rtdetr-n-coco/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The RT-DETR FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-1 that shows an amazing speed while maintaining a satisfactory accuracy.</li> <li>the encoder is a bi-FPN (bilinear feature pyramid network). With respect to the original paper, we removed the attention modules in the encoder and we reduce the internal features dimension, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 3 decoder layers, instead of 6, and we select 300 queries.</li> </ul>"},{"location":"models/fai-rtdetr-n-coco/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. for more details look here: GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-rtdetr-n-coco/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-rtdetr-n-coco/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class ID Class Name AP 1 person 53.3 2 bicycle 28.0 3 car 40.5 4 motorcycle 42.6 5 airplane 67.8 6 bus 65.0 7 train 63.7 8 truck 34.7 9 boat 27.4 10 traffic light 25.0 11 fire hydrant 63.9 12 stop sign 62.1 13 parking meter 46.6 14 bench 23.1 15 bird 35.0 16 cat 70.6 17 dog 65.8 18 horse 54.2 19 sheep 52.7 20 cow 56.5 21 elephant 64.0 22 bear 72.9 23 zebra 69.7 24 giraffe 68.1 25 backpack 12.1 26 umbrella 37.1 27 handbag 11.9 28 tie 31.3 29 suitcase 40.2 30 frisbee 66.2 31 skis 22.4 32 snowboard 27.6 33 sports ball 42.7 34 kite 44.9 35 baseball bat 24.8 36 baseball glove 33.4 37 skateboard 49.1 38 surfboard 34.9 39 tennis racket 43.8 40 bottle 34.3 41 wine glass 30.7 42 cup 38.6 43 fork 32.2 44 knife 15.4 45 spoon 15.1 46 bowl 38.1 47 banana 26.0 48 apple 18.8 49 sandwich 36.6 50 orange 30.6 51 broccoli 23.6 52 carrot 22.2 53 hot dog 31.9 54 pizza 53.9 55 donut 45.7 56 cake 34.7 57 chair 26.0 58 couch 44.1 59 potted plant 24.5 60 bed 46.2 61 dining table 28.7 62 toilet 60.6 63 tv 56.0 64 laptop 58.3 65 mouse 58.4 66 remote 27.6 67 keyboard 51.6 68 cell phone 32.6 69 microwave 56.1 70 oven 34.4 71 toaster 45.6 72 sink 35.6 73 refrigerator 53.8 74 book 12.6 75 clock 48.9 76 vase 33.9 77 scissors 26.9 78 teddy bear 45.1 79 hair drier 10.0 80 toothbrush 26.3"},{"location":"models/fai-rtdetr-n-coco/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-rtdetr-n-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-rtdetr-n-coco\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-rtdetr-s-coco/","title":"fai-rtdetr-s-coco","text":""},{"location":"models/fai-rtdetr-s-coco/#overview","title":"Overview","text":"<p>The models is a RT-DETR model otimized by FocoosAI for the COCO dataset. It is a object detection model able to detect 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-rtdetr-s-coco/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-rtdetr-s-coco/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-rtdetr-s-coco/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The RT-DETR FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-2 that show an amazing trade-off between performance and efficiency.</li> <li>the encoder is a bi-FPN (bilinear feature pyramid network). With respect to the original paper, we removed the attention modules in the encoder and we reduce the internal features dimension, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 3 decoder layers, instead of 6, and we select 300 queries.</li> </ul>"},{"location":"models/fai-rtdetr-s-coco/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. for more details look here: GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-rtdetr-s-coco/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-rtdetr-s-coco/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class ID Class Name AP 1 person 54.7 2 bicycle 29.1 3 car 41.4 4 motorcycle 44.9 5 airplane 71.4 6 bus 67.8 7 train 68.9 8 truck 36.4 9 boat 26.8 10 traffic light 25.0 11 fire hydrant 66.0 12 stop sign 62.2 13 parking meter 46.1 14 bench 25.2 15 bird 36.5 16 cat 72.6 17 dog 68.5 18 horse 57.9 19 sheep 54.1 20 cow 56.6 21 elephant 66.2 22 bear 78.3 23 zebra 70.0 24 giraffe 70.0 25 backpack 14.9 26 umbrella 39.9 27 handbag 13.2 28 tie 32.6 29 suitcase 41.2 30 frisbee 66.3 31 skis 24.9 32 snowboard 31.6 33 sports ball 44.8 34 kite 45.1 35 baseball bat 29.7 36 baseball glove 35.2 37 skateboard 54.5 38 surfboard 39.9 39 tennis racket 46.1 40 bottle 35.8 41 wine glass 32.6 42 cup 41.1 43 fork 35.5 44 knife 18.9 45 spoon 18.0 46 bowl 42.2 47 banana 24.6 48 apple 18.6 49 sandwich 41.6 50 orange 33.1 51 broccoli 22.4 52 carrot 22.2 53 hot dog 37.6 54 pizza 55.2 55 donut 48.0 56 cake 36.7 57 chair 28.4 58 couch 47.8 59 potted plant 26.8 60 bed 49.0 61 dining table 30.5 62 toilet 60.1 63 tv 57.2 64 laptop 59.6 65 mouse 62.3 66 remote 27.7 67 keyboard 53.8 68 cell phone 33.2 69 microwave 60.7 70 oven 38.8 71 toaster 41.9 72 sink 37.0 73 refrigerator 57.6 74 book 13.8 75 clock 50.3 76 vase 35.5 77 scissors 31.8 78 teddy bear 44.7 79 hair drier 10.3 80 toothbrush 26.8"},{"location":"models/fai-rtdetr-s-coco/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-rtdetr-s-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-rtdetr-s-coco\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"}]}