{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Focoos Library","text":"<p>Focoos is a comprehensive library for training and deploying efficient open-source computer vision models. It enables seamless model customization on your datasets, integrates with FocoosHub for experiment tracking and model weight management, and provides easy deployment across multiple devices through simple function calls.</p>"},{"location":"#why-choose-focoos","title":"Why choose Focoos?","text":"<ul> <li>\ud83d\udd39 Performance: Achieve up to 10x faster inference speeds compared to traditional computer vision models, enabling real-time processing even on edge devices.</li> <li>\ud83d\udcb0 Cost Efficiency: Reduce computational requirements by 75%, significantly lowering cloud infrastructure and hardware costs while maintaining high accuracy.</li> <li>\u26a1 Developer Experience: Streamline your workflow with an intuitive API that makes model deployment, fine-tuning, and integration seamless and straightforward.</li> </ul>"},{"location":"#start-now","title":"Start now!","text":"<p>By choosing Focoos AI, you can save time, reduce costs, and achieve superior model performance, all while ensuring the privacy and efficiency of your deployments.</p> <p>Reach out to us for any question or go to Focoos AI to register and start using the hub.</p> <p>Otherwise Book A Meeting with us to see Focoos in action.</p>"},{"location":"concepts/","title":"Main Concepts","text":""},{"location":"concepts/#focoosmodel","title":"FocoosModel","text":"<p>The <code>FocoosModel</code> class is the main interface for working with computer vision models in Focoos. It provides high-level methods for training, testing, inference, and model export while handling preprocessing and postprocessing automatically.</p>"},{"location":"concepts/#key-features","title":"Key Features","text":"<ul> <li>End-to-End Inference: Automatic preprocessing and postprocessing</li> <li>Training Support: Built-in training pipeline with distributed training support</li> <li>Model Export: Export to ONNX and TorchScript formats</li> <li>Performance Benchmarking: Built-in latency and throughput measurement</li> <li>Hub Integration: Seamless integration with Focoos Hub for model sharing</li> <li>Multiple Input Formats: Support for PIL Images, NumPy arrays, and PyTorch tensors</li> </ul>"},{"location":"concepts/#loading-strategies","title":"Loading Strategies","text":"<p>The primary method for loading models is using the <code>ModelManager.get()</code> (see <code>ModelManager</code>). It supports multiple loading strategies based on the input parameters. The return value is a Focoos Model.</p> <p>The ModelManager employs different loading strategies based on the input:</p>"},{"location":"concepts/#1-from-focoos-hub","title":"1. From Focoos Hub","text":"<p>The Focoos Hub is a cloud-based model repository where you can store, share, and collaborate on models. This method enables seamless model downloading and caching from the hub using the <code>hub://</code> protocol.</p> <p>When to use: Load models shared by other users, access your own cloud-stored models, or work with models that require authentication.</p> <p>Requirements: Valid API key for private models, internet connection for initial download.</p> <pre><code># Loading from hub using hub:// protocol\n# The model is automatically downloaded and cached locally\nhub = FocoosHUB(api_key=\"your_api_key\")\nmodel = ModelManager.get(\"hub://model_reference\", hub=hub)\n\n# Loading with custom configuration override\nmodel = ModelManager.get(\n    \"hub://model_reference\",\n    hub=hub,\n    cache=True,  # Cache for faster subsequent loads\n    config_parameter=your_value # Override single config parameter\n)\n</code></pre>"},{"location":"concepts/#2-from-model-registry","title":"2. From Model Registry","text":"<p>The Model Registry contains curated, pretrained models that are immediately available without download. These models are optimized, tested, and ready for production use across various computer vision tasks.</p> <p>When to use: Start with proven, high-quality pretrained models, baseline experiments, or when you need reliable performance without customization.</p> <p>Requirements: No internet connection needed, models are bundled with the library.</p> <pre><code># Loading pretrained models from registry\n# Object detection model trained on COCO dataset\nmodel = ModelManager.get(\"fai-detr-l-coco\")\n\n# Semantic segmentation model for ADE20K dataset\nmodel = ModelManager.get(\"fai-mf-l-ade\")\n\n# Check available models first\nfrom focoos import ModelRegistry\navailable_models = ModelRegistry.list_models()\nprint(\"Available models:\", available_models)\n\n# Get detailed information before loading\nmodel_info = ModelRegistry.get_model_info(\"fai-detr-l-coco\")\nprint(f\"Classes: {len(model_info.classes)}, Task: {model_info.task}\")\n</code></pre> <p>Available Model Categories:</p> <ul> <li>Object Detection: <code>fai-detr-l-coco</code>, <code>fai-detr-m-coco</code>, <code>fai-detr-l-obj365</code></li> <li>Instance Segmentation: <code>fai-mf-l-coco-ins</code>, <code>fai-mf-m-coco-ins</code>, <code>fai-mf-s-coco-ins</code></li> <li>Semantic Segmentation: <code>fai-mf-l-ade</code>, <code>fai-mf-m-ade</code>, <code>bisenetformer-l-ade</code>, <code>bisenetformer-m-ade</code>, <code>bisenetformer-s-ade</code></li> </ul>"},{"location":"concepts/#3-from-local-directory","title":"3. From Local Directory","text":"<p>Load models from your local filesystem, whether they're custom-trained models or models stored in non-standard locations. This method provides maximum flexibility for local development and deployment scenarios.</p> <p>When to use: Load custom-trained models, work with locally stored models, integrate with existing model storage systems, or work in offline environments.</p> <p>Requirements: Valid model directory containing model artifacts (weights, configuration, metadata).</p> <pre><code># Loading with custom models directory\nmodel = ModelManager.get(\"my_model\", models_dir=\"/custom/models/dir\")\n\n# Expected directory structure:\n# /path/to/local/model/\n# \u251c\u2500\u2500 model_info.json     # Model metadata and configuration\n# \u251c\u2500\u2500 model_final.pth     # Model weights (optional)\n\n# Loading with configuration override\nmodel = ModelManager.get(\n    \"local_model\",\n    models_dir=\"/custom/models/dir\",\n    arg1=value1,\n    arg2=value2,\n)\n</code></pre>"},{"location":"concepts/#4-from-modelinfo-object","title":"4. From ModelInfo Object","text":"<p>The <code>ModelInfo</code> class represents comprehensive model metadata including architecture specifications, training configuration, class definitions, and performance metrics. This method provides the most programmatic control over model instantiation.</p> <p>When to use: Programmatically construct models, work with dynamic configurations, integrate with custom model management systems, or when you need fine-grained control over model instantiation.</p> <p>Requirements: Properly constructed ModelInfo object with valid configuration parameters.</p> <pre><code># Loading from JSON file\nmodel_info = ModelInfo.from_json(\"path/to/model_info.json\")\nmodel = ModelManager.get(\"any_name\", model_info=model_info)\n\n# Programmatically creating ModelInfo\nfrom focoos.ports import ModelInfo, ModelFamily, Task\n\nmodel_info = ModelInfo(\n    name=\"custom_detector\",\n    model_family=ModelFamily.DETR,\n    classes=[\"person\", \"car\", \"bicycle\"],\n    im_size=640,\n    task=Task.DETECTION,\n    config={\n        \"num_classes\": 3,\n        \"backbone_config\": {\"depth\": 50, \"model_type\": \"resnet\"},\n        \"threshold\": 0.5\n    },\n    weights_uri=\"path/to/weights.pth\",  # Optional\n    description=\"Custom object detector\"\n)\n\nmodel = ModelManager.get(\"custom_detector\", model_info=model_info)\n</code></pre>"},{"location":"concepts/#predict","title":"Predict","text":"<p>Performs end-to-end inference on input images with automatic preprocessing and postprocessing. The model accepts input images in various formats including:</p> <ul> <li>PIL Image objects (<code>PIL.Image.Image</code>)</li> <li>NumPy arrays (<code>numpy.ndarray</code>)</li> <li>PyTorch tensors (<code>torch.Tensor</code>)</li> </ul> <p>The input images are automatically preprocessed to the correct size and format required by the model. After inference, the raw model outputs are postprocessed into a standardized <code>FocoosDetections</code> format that provides easy access to:</p> <ul> <li>Detected object classes and confidence scores</li> <li>Bounding box coordinates</li> <li>Segmentation masks (for segmentation models)</li> <li>Additional model-specific outputs</li> </ul> <p>This provides a simple, unified interface for running inference regardless of the underlying model architecture or task.</p> <p>Parameters: - <code>inputs</code>: Input images in various supported formats (<code>PIL.Image.Image</code>, <code>numpy.ndarray</code>, <code>torch.Tensor</code>) - <code>**kwargs</code>: Additional arguments passed to postprocessing</p> <p>Returns: <code>FocoosDetections</code> containing detection/segmentation results</p> <p>Example: <pre><code>from PIL import Image\n\n# Load an image\nimage = Image.open(\"example.jpg\")\n\n# Run inference\ndetections = model(image)\n\n# Access results\nfor detection in detections.detections:\n    print(f\"Class: {detection.label}, Confidence: {detection.conf}\")\n    print(f\"Bounding box: {detection.bbox}\")\n</code></pre></p>"},{"location":"concepts/#training","title":"Training","text":"<p>Trains the model on provided datasets. The training function accepts:</p> <ul> <li><code>args</code>: Training configuration (TrainerArgs) specifying the main hyperparameters, among which:</li> <li><code>run_name</code>: Name for the training run</li> <li><code>output_dir</code>: Name for the output folder</li> <li><code>num_gpus</code>: Number of GPUs to use (must be &gt;= 1)</li> <li><code>sync_to_hub</code>: For tracking the experiment on the Focoos Hub.   -<code>batch_size</code>, <code>learning_rate</code>, <code>max_iters</code> and other hyperparameters</li> <li><code>data_train</code>: Training dataset (MapDataset)</li> <li><code>data_val</code>: Validation dataset (MapDataset)</li> <li><code>hub</code>: Optional FocoosHUB instance for experiment tracking</li> </ul> <p>The data can be obtained using the AutoDataset helper.</p> <p>After the training is complete, the model will have updated weights and can be used for inference or export. Furthermore, in the <code>output_dir</code> can be found the model metadata (<code>model_info.json</code>) and the PyTorch weights (<code>model_final.pth</code>).</p> <p>Example: <pre><code>from focoos import TrainerArgs\nfrom focoos.data import MapDataset\n\n# Configure training\ntrain_args = TrainerArgs(\n    run_name=\"my_custom_model\",\n    max_iters=5000,\n    batch_size=16,\n    learning_rate=1e-4,\n    num_gpus=2,\n    sync_to_hub=True,\n)\n\n# Train the model\nmodel.train(train_args, train_dataset, val_dataset, hub=hub)\n</code></pre></p> <p>Here you can find an extensive training tutorial.</p>"},{"location":"concepts/#model-export","title":"Model Export","text":"<p>Exports the model to different runtime formats for optimized inference. The main function arguments are:  - <code>runtime_type</code>: specify the target runtime and must be one of the supported (see RuntimeType)  - <code>out_dir</code>: the destination folder for the exported model  - <code>image_size</code>: the target image size, as an optional integer</p> <p>The function returns an <code>InferModel</code> instance for the exported model.</p> <p>Example: <pre><code># Export to ONNX with TensorRT optimization\ninfer_model = model.export(\n    runtime_type=RuntimeType.ONNX_TRT16,\n    out_dir=\"./exported_models\",\n    overwrite=True\n)\n\n# Use exported model for fast inference\nfast_detections = infer_model(image)\n</code></pre></p>"},{"location":"concepts/#infer-model","title":"Infer Model","text":"<p>The <code>InferModel</code> class represents an optimized model for inference, typically created through the export process of a <code>FocoosModel</code>. It provides a streamlined interface focused on fast and efficient inference while maintaining the same input/output format as the original model.</p>"},{"location":"concepts/#key-features_1","title":"Key Features","text":"<ul> <li>Optimized Performance: Models are optimized for the target runtime (e.g., TensorRT, ONNX)</li> <li>Consistent Interface: Uses the same input/output format as FocoosModel</li> <li>Resource Management: Proper cleanup of runtime resources when no longer needed</li> <li>Multiple Input Formats: Support for PIL Images, NumPy arrays, and PyTorch tensors</li> </ul>"},{"location":"concepts/#initialization","title":"Initialization","text":"<p>InferModel instances are typically created through the <code>export()</code> method of a FocoosModel, which handles the model optimization and conversion process. This method allows you to specify the target runtime (see the availables in <code>Runtimetypes</code>) and the output directory for the exported model. The <code>export()</code> method returns an <code>InferModel</code> instance that is optimized for fast and efficient inference.</p> <p>Example: <pre><code># Export the model to ONNX format\ninfer_model = model.export(\n    runtime_type=RuntimeType.TORCHSCRIPT_32,\n    out_dir=\"./exported_models\"\n)\n\n# Use the exported model for inference\nresults = infer_model(input_image)\n</code></pre></p>"},{"location":"concepts/#predict_1","title":"Predict","text":"<p>Performs end-to-end inference on input images with automatic preprocessing and postprocessing on the selected runtime. The model accepts input images in various formats including:</p> <ul> <li>PIL Image objects (<code>PIL.Image.Image</code>)</li> <li>NumPy arrays (<code>numpy.ndarray</code>)</li> <li>PyTorch tensors (<code>torch.Tensor</code>)</li> </ul> <p>The input images are automatically preprocessed to the correct size and format required by the model. After inference, the raw model outputs are postprocessed into a standardized <code>FocoosDetections</code> format that provides easy access to:</p> <ul> <li>Detected object classes and confidence scores</li> <li>Bounding box coordinates</li> <li>Segmentation masks (for segmentation models)</li> <li>Additional model-specific outputs</li> </ul> <p>This provides a simple, unified interface for running inference regardless of the underlying model architecture or task.</p> <p>Parameters: - <code>inputs</code>: Input images in various supported formats (<code>PIL.Image.Image</code>, <code>numpy.ndarray</code>, <code>torch.Tensor</code>) - <code>**kwargs</code>: Additional arguments passed to postprocessing</p> <p>Returns: <code>FocoosDetections</code> containing detection/segmentation results</p> <p>Example: <pre><code>from PIL import Image\n\n# Load an image\nimage = Image.open(\"example.jpg\")\n\n# Run inference\ninfer_model = model.export(\n    runtime_type=RuntimeType.TORCHSCRIPT_32,\n    out_dir=\"./exported_models\"\n)\ndetections = infer_model(image)\n\n# Access results\nfor detection in detections.detections:\n    print(f\"Class: {detection.label}, Confidence: {detection.conf}\")\n    print(f\"Bounding box: {detection.bbox}\")\n</code></pre></p>"},{"location":"inference/","title":"How to Use a Computer Vision Model with Focoos","text":"<p>Focoos provides a powerful inference framework that makes it easy to deploy and use state-of-the-art computer vision models in production. Whether you're working on object detection, image classification, or other vision tasks, Focoos offers flexible deployment options that adapt to your specific needs.</p> <p></p> <p>Key features of the Focoos inference framework include:</p> <ul> <li>Multiple Deployment Options: Choose between cloud-based inference, local PyTorch deployment, or optimized runtime deployment</li> <li>Easy Model Loading: Seamlessly load models from the Focoos Hub or your local environment</li> <li>Production-Ready Features:<ul> <li>Optimized inference performance</li> <li>Hardware acceleration compatibility</li> <li>Memory-efficient execution</li> </ul> </li> <li>Simple Integration: Easy-to-use APIs that work seamlessly with your existing applications</li> </ul> <p>In the following sections, we'll guide you through the different ways to use Focoos models for inference, from cloud deployment to local optimization.</p>"},{"location":"inference/#there-are-three-ways-to-use-a-model","title":"\ud83c\udfa8 There are three ways to use a model:","text":"<ol> <li>\ud83c\udf0d Remote Inference</li> <li>\ud83d\udd25 Pytorch Inference</li> <li>\ud83d\udd28 Optimized Inference</li> </ol>"},{"location":"inference/#0-optional-connect-to-the-focoos-hub","title":"0. [Optional] Connect to the Focoos Hub","text":"<p>Focoos can be used without having an accont on the Focoos Hub. With it, you will unlock additional functionalities, as we will see below. If you have it, just connect to the HUB. <pre><code>from focoos.hub import FocoosHUB\n\nFOCOOS_API_KEY = os.getenv(\"FOCOOS_API_KEY\")  # write here your API key os set env variable FOCOOS_API_KEY, will be used as default\nhub = FocoosHUB(api_key=FOCOOS_API_KEY)\n</code></pre></p> <p>You can see the models available for you on the platform with an intuitive user interface. However, you can also list them using the Hub functionalities.</p> <pre><code>models = hub.list_remote_models()\n</code></pre>"},{"location":"inference/#1-remote-inference","title":"1. \ud83c\udf0d Remote Inference","text":"<p>In this section, you'll run a model on the Focoos' servers instead of on your machine. The image will be packed and sent on the network to the servers, where it is processed and the results is retured to your machine, all in few milliseconds. If you want an example model, you can try <code>fai-detr-l-obj365</code>.</p> <pre><code>model_ref = \"&lt;YOUR-MODEL-REF&gt;\"\ndataset = hub.get_remote_model(model_ref)\n</code></pre> <p>Using the model is as simple as it could! Just call it with an image.</p> <pre><code>from PIL import Image\nimage = Image.open(\"&lt;PATH-TO-IMAGE&gt;\")\ndetections = model(image)\n</code></pre> <p><code>detections</code> is a FocoosDetections object, containing a list of FocoosDet objects and optionally a dict of information about the latency of the inference. The <code>FocoosDet</code> object contains the following attributes:</p> <ul> <li><code>bbox</code>: Bounding box coordinates in x1y1x2y2 absolute format.</li> <li><code>conf</code>: Confidence score (from 0 to 1).</li> <li><code>cls_id</code>: Class ID (0-indexed).</li> <li><code>label</code>: Label (name of the class).</li> <li><code>mask</code>: Mask (base64 encoded string having origin in the top left corner of bbox and the same width and height of the bbox).</li> </ul> <p>If you want to visualize the result on the image, there's a utily for you.</p> <pre><code>from focoos.utils.vision import annotate_image\n\nannotate_image(image, detections, task=model.model_info.task, classes=model.model_info.classes).save(\"predictions.png\")\n</code></pre>"},{"location":"inference/#2-pytorch-inference","title":"2. \ud83d\udd25 PyTorch Inference","text":"<p>This section demonstrates how to perform local inference using a plain Pytorch model. We will load a model and then run inference on a sample image.</p> <p>First, let's get a model. We need to use the <code>ModelManager</code> that will take care of instaciating the right model starting from a model reference (for example, the <code>fai-detr-l-obj365</code>). If you want to use a model from the Hub, please remember to add <code>hub://</code> as prefix to the model reference.</p> Pretrained ModelHUB Model <pre><code>from focoos.model_manager import ModelManager\nmodel_name = \"fai-detr-l-obj365\"\n\nmodel = ModelManager.get(model_name)\n</code></pre> <pre><code>from focoos.model_manager import ModelManager\n\nmodel_ref = \"&lt;YOUR-MODEL-REF&gt;\"\n\n\nmodel = ModelManager.get(f\"hub://{model_ref}\")\n</code></pre> Local Model  <pre><code>from focoos.model_manager import ModelManager\n\nmodel_path = \"/path/to/model\"\n\n\nmodel = ModelManager.get(model_path)\n</code></pre> <p>Now, again, you can now run the model by simply passing it an image and visualize the results.</p> <pre><code>from focoos.utils.vision import annotate_image\n\ndetections = model(image)\n\nannotate_image(image, detections, task=model.model_info.task, classes=model.model_info.classes).save(\"predictions.png\")\n</code></pre> <p><code>detections</code> is a FocoosDetections object.</p> <p>How fast is this model locally? We can compute it's speed by using the benchmark utility.</p> <pre><code>model.benchmark(iterations=10, size=640)\n</code></pre>"},{"location":"inference/#3-optimized-inference","title":"3. \ud83d\udd28 Optimized Inference","text":"<p>As you can see, using the torch model is great, but we can achieve better performance by exporting and running it with a optimized runtime, such as Torchscript, TensorRT, CoreML or the ones available on ONNXRuntime.</p> <p>In the following cells, we will export the previous model for one of these and run it.</p>"},{"location":"inference/#torchscript","title":"Torchscript","text":"<p>We already provide multiple inference runtime, that you can see on the <code>RuntimeTypes</code> enum. Let's select Torchscript as an example.</p> <pre><code>from focoos.ports import RuntimeType\n\nruntime = RuntimeType.TORCHSCRIPT_32\n</code></pre> <p>It's time to export the model. We can use the export method of the models.</p> <pre><code>optimized_model = model.export(runtime_type=runtime, image_size=512)\n</code></pre> <p>Let's visualize the output. As you will see, there are not differences from the model in pure torch.</p> <p><pre><code>from focoos.utils.vision import annotate_image\n\ndetections = optimized_model(image)\nannotate_image(image, detections, task=model.model_info.task, classes=model.model_info.classes).save(\"prediction.png\")\n</code></pre> <code>detections</code> is a FocoosDetections object.</p> <p>But, let's see its latency, that should be substantially lower than the pure pytorch model. <pre><code>optimized_model.benchmark(iterations=10, size=512)\n</code></pre></p> <p>You can use different runtimes that may fit better your device, such as TensorRT. See the list of available Runtimes at <code>RuntimeTypes</code>. Please note that you need to install the relative packages for onnx and tensorRT for using them.</p>"},{"location":"inference/#onnx-with-tensorrt","title":"ONNX with TensorRT","text":"<pre><code>from focoos.ports import RuntimeType\nfrom focoos.utils.vision import annotate_image\n\nruntime = RuntimeType.ONNX_TRT16\noptimized_model = model.export(runtime_type=runtime)\n\ndetections = optimized_model(image)\ndisplay(annotate_image(image, detections, task=model.model_info.task, classes=model.model_info.classes))\n\noptimized_model.benchmark(iterations=10, size=640)\n</code></pre>"},{"location":"setup/","title":"Python SDK Setup \ud83d\udc0d","text":""},{"location":"setup/#install-the-focoos-sdk","title":"Install the Focoos SDK","text":"<p>The Focoos SDK can be installed with different package managers using python 3.10 and above.</p> <p>We recommend using UV (how to install uv) as a package manager and environment manager for a streamlined dependency management experience. however the installation process is the same if you use pip or conda.</p> <p>You can easily create a new virtual environment with UV using the following command: <pre><code>uv venv --python 3.12\nsource .venv/bin/activate\n</code></pre></p> DefaultONNX Runtime (CUDA) ONNX Runtime (Tensorrt) CPU ONNX Runtime <p>The default installation provides compatibility with both CPU and GPU environments, utilizing PyTorch as the default runtime. you can perfom training and inference with PyTorch</p> <pre><code>uv pip install 'focoos @ git+https://github.com/FocoosAI/focoos'\n</code></pre> <p>To perform inference using ONNX Runtime with GPU (CUDA) acceleration Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.22.0. To install cuDNN 9:</p> <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> <pre><code>uv pip install 'focoos[onnx] @ git+https://github.com/FocoosAI/focoos'\n</code></pre> <p>To perform inference using ONNX Runtime with GPU (Tensorrt) acceleration Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.22.0. To install cuDNN 9:</p> <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> <p>To perform inference using TensorRT, ensure you have TensorRT version 10.5 installed.</p> <pre><code>uv pip install 'focoos[onnx,tensorrt] @ git+https://github.com/FocoosAI/focoos'\n</code></pre> <pre><code>uv pip install 'focoos[onnx-cpu] @ git+https://github.com/FocoosAI/focoos'\n</code></pre> <p>Note</p> <p>\ud83e\udd16 Multiple Runtimes: You can install multiple extras by running <code>pip install 'focoos[onnx,tensorrt] @ git+https://github.com/FocoosAI/focoos.git'</code>. Note that you can't use <code>onnx-cpu</code> and <code>onnx</code> or <code>tensorrt</code> at the same time.</p> <p>Note</p> <p>\ud83d\udee0\ufe0f Installation Tip: If you want to install a specific version, for example <code>v0.14.1</code>, use:</p> <pre><code>uv pip install 'focoos @ git+https://github.com/FocoosAI/focoos@v0.14.1'\n</code></pre> <p>\ud83d\udccb Check Versions: Visit https://github.com/FocoosAI/focoos/tags for available versions.</p>"},{"location":"setup/#inference-runtime-support","title":"Inference Runtime support","text":"<p>Focoos models support multiple inference runtimes. The library can be used without any extras for training and inference using the PyTorch runtime. Additional extras are only needed if you want to use ONNX or TensorRT runtimes for optimized inference.</p> RuntimeType Extra Runtime Compatible Devices Available ExecutionProvider TORCHSCRIPT_32 - torchscript CPU, NVIDIA GPUs - ONNX_CUDA32 <code>[onnx]</code> onnxruntime GPU NVIDIA GPUs CUDAExecutionProvider ONNX_TRT32 <code>[tensorrt]</code> onnxruntime TRT NVIDIA GPUs (Optimized) CUDAExecutionProvider, TensorrtExecutionProvider ONNX_TRT16 <code>[tensorrt]</code> onnxruntime TRT NVIDIA GPUs (Optimized) CUDAExecutionProvider, TensorrtExecutionProvider ONNX_CPU <code>[onnx-cpu]</code> onnxruntime CPU CPU (x86, ARM), M1, M2, M3 (Apple Silicon) CPUExecutionProvider, CoreMLExecutionProvider, AzureExecutionProvider ONNX_COREML <code>[onnx-cpu]</code> onnxruntime CPU M1, M2, M3 (Apple Silicon) CoreMLExecutionProvider, CPUExecutionProvider"},{"location":"setup/#docker-and-devcontainers","title":"Docker and Devcontainers","text":"<p>For container support, Focoos offers different Docker images:</p> <ul> <li><code>focoos-gpu</code>: Includes ONNX Runtime (CUDA) support</li> <li><code>focoos-tensorrt</code>: Includes ONNX and TensorRT support</li> <li><code>focoos-cpu</code>: only CPU</li> </ul> <p>to use the docker images, you can run the following command:</p> <pre><code>docker build -t focoos-gpu . --target=focoos-gpu\ndocker run -it focoos-gpu\n</code></pre> <p>This repository also includes a devcontainer configuration for each of the above images. You can launch these devcontainers in Visual Studio Code for a seamless development experience.</p>"},{"location":"training/","title":"How to Train a Computer Vision Model with Focoos","text":"<p>Focoos provides a comprehensive training framework that makes it easy to train state-of-the-art computer vision models on your own datasets. Whether you're working on object detection, image classification, or other vision tasks, Focoos offers an intuitive training pipeline that handles everything from data preparation to model optimization.</p> <p></p> <p>Key features of the Focoos training framework include:</p> <ul> <li>Easy Dataset Integration: Seamlessly import and prepare your datasets using Focoos' data loading utilities</li> <li>Flexible Model Architecture: Choose from a variety of pre-built model architectures or customize your own</li> <li>Advanced Training Features:<ul> <li>Mixed precision training for faster training and reduced memory usage</li> <li>Automatic learning rate scheduling</li> <li>Early stopping and model checkpointing</li> <li>Distributed training support</li> </ul> </li> <li>Experiment Tracking: Monitor training progress, visualize metrics, and compare experiments through the Focoos Hub</li> </ul> <p>In the following sections, we'll guide you through the process of training a model with Focoos, from setting up your environment to deploying your trained model.</p>"},{"location":"training/#fine-tune-a-model-in-3-steps","title":"\ud83c\udfa8 Fine-tune a model in 3 steps","text":"<p>In this guide, we will perform the following steps:</p> <ol> <li>\ud83d\udce6 Select dataset</li> <li>\ud83c\udfc3\u200d\u2642\ufe0f Train model</li> <li>\ud83e\uddea Test model</li> </ol>"},{"location":"training/#0-optional-connect-to-the-focoos-hub","title":"0. [Optional] Connect to the Focoos Hub","text":"<p>Focoos can be used without having an accont on the Focoos Hub. With it, you will unlock additional functionalities, as we will see below. If you have it, just connect to the HUB. <pre><code>from focoos.hub import FocoosHUB\n\nFOCOOS_API_KEY = None  # write here your API key\nhub = FocoosHUB(api_key=FOCOOS_API_KEY)\n</code></pre></p>"},{"location":"training/#1-select-dataset","title":"1. Select dataset","text":"<p>Before starting the training, we need to get a dataset. You can either use a local dataset or you can download one from the hub.</p>"},{"location":"training/#optional-download-the-data-from-the-hub","title":"[Optional] Download the data from the Hub","text":"<p>If you want to download a dataset from the hub, you can use it to directly store it in your local environment. Check the reference of your dataset on the platform and use it in the following cell. If you want to try an example dataset, just use one of the many available on the Focoos Hub.</p> <pre><code>dataset_ref = \"&lt;YOUR-DATASET-REFERENCE&gt;\"\ndataset = hub.get_remote_dataset(dataset_ref)\nprint(dataset)\n\ndataset_path = dataset.download_data()\n</code></pre>"},{"location":"training/#get-the-training-dataset","title":"Get the training dataset","text":"<p>Now that we downloaded the dataset, we can magically \ud83e\ude84 instanciate the dataset using the <code>AutoDataset</code> as will be used in the training. You can optionally specify aumgentations for the training using the <code>DatasetAugmentation</code> dataclass.</p> <pre><code>from focoos.data.auto_dataset import AutoDataset\nfrom focoos.data.default_aug import DatasetAugmentations\nfrom focoos.ports import DatasetSplitType\n\ntask = dataset.task  # see ports.Task for more information\nlayout = dataset.layout  # see ports.DatasetLayout for more information\nauto_dataset = AutoDataset(dataset_name=dataset_path, task=task, layout=layout)\n\naugs = DatasetAugmentations(resolution=512).get_augmentations()\n\ntrain_dataset = auto_dataset.get_split(augs=augs, split=DatasetSplitType.TRAIN)\nvalid_dataset = auto_dataset.get_split(augs=augs, split=DatasetSplitType.VAL)\n</code></pre>"},{"location":"training/#2-train-the-model","title":"2. Train the Model","text":""},{"location":"training/#instanciate-a-model","title":"Instanciate a model","text":"<p>The first step to personalize your model is to instance a model. You can get a model using the ModelManager by specifying a model name. Optionally, you can also get one of your trained models on the hub. If you want to follow the example, just use <code>fai-detr-m-coco</code> as the model reference.</p> <pre><code>from focoos.model_manager import ModelManager\n\nmodel_ref = \"&lt;YOUR-MODEL-REF&gt;\"\nmodel = ModelManager.get(\"hub://\" + model_ref, hub=hub)\n</code></pre>"},{"location":"training/#select-the-hyper-parameters","title":"Select the hyper-parameters","text":"<p>The next step is to create a <code>TrainerArgs</code> with the hyper-parameters such as the learning rate, the number of iterations and so on. Optionally, if you are using the hub, you can specify <code>sync_to_hub=True</code> to track the experiment on the Focoos Hub.</p> <pre><code>from focoos.ports import TrainerArgs\n\nargs = TrainerArgs(\n    run_name=\"football-tutorial\",  # the name of the experiment\n    output_dir=\"./experiments\",  # the folder where the model is saved\n    batch_size=16,  # how many images in each iteration\n    max_iters=500,  # how many iterations lasts the training\n    eval_period=100,  # period after we eval the model on the validation (in iterations)\n    learning_rate=0.0001,  # learning rate\n    weight_decay=0.0001,  # regularization strenght (set it properly to avoid under/over fitting)\n    sync_to_hub=True,  # Use this to see the model under training on the platform\n)\n</code></pre>"},{"location":"training/#train-the-model","title":"Train the model","text":"<p>Now we are set up. We can directly call the train function of the model.</p> <pre><code>model.train(args, train_dataset, valid_dataset, hub=hub)\n</code></pre>"},{"location":"training/#3-test-the-model","title":"3. Test the Model","text":"<p>Now that the model is ready, let's see how it behaves.</p> <pre><code>import random\nfrom PIL import Image\nfrom focoos.utils.vision import annotate_image\n\nindex = random.randint(0, len(valid_dataset))\n\nground_truth = valid_dataset.preview(index, use_augmentations=False).save(\"ground_truth.jpg\")\n\nimage = Image.open(valid_dataset[index][\"file_name\"])\noutputs = model(image)\n\nprediction = annotate_image(image, outputs, task=task, classes=model.model_info.classes).save(\"prediction.jpg\")\n</code></pre>"},{"location":"api/auto_dataset/","title":"AutoDataset","text":""},{"location":"api/auto_dataset/#focoos.data.auto_dataset.AutoDataset","title":"<code>AutoDataset</code>","text":"Source code in <code>focoos/data/auto_dataset.py</code> <pre><code>class AutoDataset:\n    def __init__(\n        self,\n        dataset_name: str,\n        task: Task,\n        layout: DatasetLayout,\n        datasets_dir: str = DATASETS_DIR,\n    ):\n        self.task = task\n        self.layout = layout\n        self.datasets_dir = datasets_dir\n        self.dataset_name = dataset_name\n\n        if self.layout is not DatasetLayout.CATALOG:\n            dataset_path = os.path.join(self.datasets_dir, dataset_name)\n        else:\n            dataset_path = self.datasets_dir\n\n        if dataset_path.endswith(\".zip\") or dataset_path.endswith(\".gz\"):\n            # compressed path: datasets_root_dir/dataset_compressed/{dataset_name}.zip\n            # _dest_path = os.path.join(self.datasets_root_dir, dataset_name.split(\".\")[0])\n            assert not (self.layout == DatasetLayout.CATALOG and not is_inside_sagemaker()), (\n                \"Catalog layout does not support compressed datasets externally to Sagemaker.\"\n            )\n            if self.layout == DatasetLayout.CATALOG:\n                dataset_path = extract_archive(dataset_path)\n                logger.info(f\"Extracted archive: {dataset_path}, {os.listdir(dataset_path)}\")\n            else:\n                dataset_name = dataset_name.split(\".\")[0]\n                _dest_path = os.path.join(self.datasets_dir, dataset_name)\n                dataset_path = extract_archive(dataset_path, _dest_path)\n                logger.info(f\"Extracted archive: {dataset_path}, {os.listdir(dataset_path)}\")\n\n        self.dataset_path = str(dataset_path)\n        self.dataset_name = dataset_name\n        logger.info(\n            f\"\u2705 Dataset name: {self.dataset_name}, Dataset Path: {self.dataset_path}, Dataset Layout: {self.layout}\"\n        )\n\n    def _load_split(self, dataset_name: str, split: DatasetSplitType) -&gt; DictDataset:\n        if self.layout == DatasetLayout.CATALOG:\n            return DictDataset.from_catalog(ds_name=dataset_name, split=split, root=self.dataset_path)\n        else:\n            ds_root = self.dataset_path\n            if not check_folder_exists(ds_root):\n                raise FileNotFoundError(f\"Dataset {ds_root} not found\")\n            split_path = self._get_split_path(dataset_root=ds_root, split_type=split)\n            if self.layout == DatasetLayout.ROBOFLOW_SEG:\n                return DictDataset.from_roboflow_seg(ds_dir=split_path, task=self.task)\n            elif self.layout == DatasetLayout.CLS_FOLDER:\n                return DictDataset.from_folder(root_dir=split_path)\n            # elif self.layout == DatasetLayout.SUPERVISELY:\n            #     return DictDataset.from_supervisely(ds_dir=split_path, task=self.task)\n            elif self.layout == DatasetLayout.ROBOFLOW_COCO:\n                return DictDataset.from_roboflow_coco(ds_dir=split_path, task=self.task)\n            else:  # Focoos\n                raise NotImplementedError(f\"Dataset layout {self.layout} not implemented\")\n\n    def _load_mapper(\n        self,\n        augs: List[T.Transform],\n        is_validation_split: bool,\n    ) -&gt; DatasetMapper:\n        if self.task == Task.SEMSEG:\n            return SemanticDatasetMapper(\n                image_format=\"RGB\",\n                ignore_label=255,\n                augmentations=augs,\n                is_train=not is_validation_split,\n            )\n        elif self.task == Task.DETECTION:\n            return DetectionDatasetMapper(\n                image_format=\"RGB\",\n                is_train=not is_validation_split,\n                augmentations=augs,\n            )\n        elif self.task == Task.INSTANCE_SEGMENTATION:\n            return DetectionDatasetMapper(\n                image_format=\"RGB\",\n                is_train=not is_validation_split,\n                augmentations=augs,\n                use_instance_mask=True,\n            )\n        elif self.task == Task.CLASSIFICATION:\n            return ClassificationDatasetMapper(\n                image_format=\"RGB\",\n                is_train=not is_validation_split,\n                augmentations=augs,\n            )\n        else:\n            raise NotImplementedError(f\"Task {self.task} not found in autodataset _load_mapper()\")\n\n    def _get_split_path(self, dataset_root: str, split_type: DatasetSplitType) -&gt; str:\n        if split_type == DatasetSplitType.TRAIN:\n            possible_names = [\"train\", \"training\"]\n            for name in possible_names:\n                split_path = os.path.join(dataset_root, name)\n                if check_folder_exists(split_path):\n                    return split_path\n            raise FileNotFoundError(f\"Train split not found in {dataset_root}\")\n        elif split_type == DatasetSplitType.VAL:\n            possible_names = [\"valid\", \"val\", \"validation\"]\n            for name in possible_names:\n                split_path = os.path.join(dataset_root, name)\n                if check_folder_exists(split_path):\n                    return split_path\n            raise FileNotFoundError(f\"Validation split not found in {dataset_root}\")\n        else:\n            raise ValueError(f\"Invalid split type: {split_type}\")\n\n    def get_split(\n        self,\n        augs: List[T.Transform],\n        split: DatasetSplitType = DatasetSplitType.TRAIN,\n    ) -&gt; MapDataset:\n        \"\"\"\n        Generate a dataset for a given dataset name with optional augmentations.\n\n        Parameters:\n            short_edge_length (int): The length of the shorter edge of the images.\n            max_size (int): The maximum size of the images.\n            extra_augs (List[Transform]): Extra augmentations to apply.\n\n        Returns:\n            MapDataset: A DictDataset with DatasetMapper for training.\n        \"\"\"\n\n        return MapDataset(\n            dataset=self._load_split(dataset_name=self.dataset_name, split=split),\n            mapper=self._load_mapper(\n                augs=augs,\n                is_validation_split=(split == DatasetSplitType.VAL),\n            ),\n        )  # type: ignore\n</code></pre>"},{"location":"api/auto_dataset/#focoos.data.auto_dataset.AutoDataset.get_split","title":"<code>get_split(augs, split=DatasetSplitType.TRAIN)</code>","text":"<p>Generate a dataset for a given dataset name with optional augmentations.</p> <p>Parameters:</p> Name Type Description Default <code>short_edge_length</code> <code>int</code> <p>The length of the shorter edge of the images.</p> required <code>max_size</code> <code>int</code> <p>The maximum size of the images.</p> required <code>extra_augs</code> <code>List[Transform]</code> <p>Extra augmentations to apply.</p> required <p>Returns:</p> Name Type Description <code>MapDataset</code> <code>MapDataset</code> <p>A DictDataset with DatasetMapper for training.</p> Source code in <code>focoos/data/auto_dataset.py</code> <pre><code>def get_split(\n    self,\n    augs: List[T.Transform],\n    split: DatasetSplitType = DatasetSplitType.TRAIN,\n) -&gt; MapDataset:\n    \"\"\"\n    Generate a dataset for a given dataset name with optional augmentations.\n\n    Parameters:\n        short_edge_length (int): The length of the shorter edge of the images.\n        max_size (int): The maximum size of the images.\n        extra_augs (List[Transform]): Extra augmentations to apply.\n\n    Returns:\n        MapDataset: A DictDataset with DatasetMapper for training.\n    \"\"\"\n\n    return MapDataset(\n        dataset=self._load_split(dataset_name=self.dataset_name, split=split),\n        mapper=self._load_mapper(\n            augs=augs,\n            is_validation_split=(split == DatasetSplitType.VAL),\n        ),\n    )  # type: ignore\n</code></pre>"},{"location":"api/auto_dataset/#focoos.data.default_aug.DatasetAugmentations","title":"<code>DatasetAugmentations</code>  <code>dataclass</code>","text":"<p>Configuration class for dataset augmentations.</p> <p>This class defines parameters for various image transformations used in training and validation pipelines for computer vision tasks. It provides a comprehensive set of options for both color and geometric augmentations.</p> <p>Attributes:</p> Name Type Description <code>resolution</code> <code>int</code> <p>Target image size for resizing operations. Range [256, 1024]. Default: 640.</p> <code>color_augmentation</code> <code>float</code> <p>Strenght of color augmentations. Range [0,1]. Default: 0.0.</p> <code>horizontal_flip</code> <code>float</code> <p>Probability of applying horizontal flip. Range [0,1]. Default: 0.0.</p> <code>vertical_flip</code> <code>float</code> <p>Probability of applying vertical flip. Range [0,1]. Default: 0.0.</p> <code>zoom_out</code> <code>float</code> <p>Probability of applying RandomZoomOut. Range [0,1]. Default: 0.0.</p> <code>zoom_out_side</code> <code>float</code> <p>Zoom out side range. Range [1,5]. Default: 4.0.</p> <code>rotation</code> <code>float</code> <p>Probability of applying RandomRotation. 1 equals +/-180 degrees. Range [0,1]. Default: 0.0.</p> <code>square</code> <code>bool</code> <p>Whether to Square the image. Default: False.</p> <code>aspect_ratio</code> <code>float</code> <p>Aspect ratio for resizing (actual scale range is (2 ** -aspect_ratio, 2 ** aspect_ratio). Range [0,1]. Default: 0.0.</p> <code>scale_ratio</code> <code>Optional[float]</code> <p>scale factor for resizing (actual scale range is (2 ** -scale_ratio, 2 ** scale_ratio). Range [0,1]. Default: None.</p> <code>max_size</code> <code>Optional[int]</code> <p>Maximum allowed dimension after resizing. Range [256, sys.maxsize]. Default: sys.maxsize.</p> <code>crop</code> <code>bool</code> <p>Whether to apply RandomCrop. Default: False.</p> <code>crop_size_min</code> <code>Optional[int]</code> <p>Minimum crop size for RandomCrop. Range [256, 1024]. Default: None.</p> <code>crop_size_max</code> <code>Optional[int]</code> <p>Maximum crop size for RandomCrop. Range [256, 1024]. Default: None.</p> Source code in <code>focoos/data/default_aug.py</code> <pre><code>@dataclass\nclass DatasetAugmentations:\n    \"\"\"\n    Configuration class for dataset augmentations.\n\n    This class defines parameters for various image transformations used in training and validation\n    pipelines for computer vision tasks. It provides a comprehensive set of options for both\n    color and geometric augmentations.\n\n    Attributes:\n        resolution (int): Target image size for resizing operations.\n            Range [256, 1024]. Default: 640.\n        ==\n        color_augmentation (float): Strenght of color augmentations.\n            Range [0,1]. Default: 0.0.\n        ==\n        horizontal_flip (float): Probability of applying horizontal flip.\n            Range [0,1]. Default: 0.0.\n        vertical_flip (float): Probability of applying vertical flip.\n            Range [0,1]. Default: 0.0.\n        zoom_out (float): Probability of applying RandomZoomOut.\n            Range [0,1]. Default: 0.0.\n        zoom_out_side (float): Zoom out side range.\n            Range [1,5]. Default: 4.0.\n        rotation (float): Probability of applying RandomRotation. 1 equals +/-180 degrees.\n            Range [0,1]. Default: 0.0.\n        ==\n        square (bool): Whether to Square the image.\n            Default: False.\n        aspect_ratio (float): Aspect ratio for resizing (actual scale range is (2 ** -aspect_ratio, 2 ** aspect_ratio).\n            Range [0,1]. Default: 0.0.\n        scale_ratio (Optional[float]): scale factor for resizing (actual scale range is (2 ** -scale_ratio, 2 ** scale_ratio).\n            Range [0,1]. Default: None.\n        max_size (Optional[int]): Maximum allowed dimension after resizing.\n            Range [256, sys.maxsize]. Default: sys.maxsize.\n        ==\n        crop (bool): Whether to apply RandomCrop.\n            Default: False.\n        crop_size_min (Optional[int]): Minimum crop size for RandomCrop.\n            Range [256, 1024]. Default: None.\n        crop_size_max (Optional[int]): Maximum crop size for RandomCrop.\n            Range [256, 1024]. Default: None.\n    \"\"\"\n\n    # Resolution for resizing\n    resolution: int = 640\n\n    # Color augmentation parameters\n    color_augmentation: float = 0.0\n    color_base_brightness: int = 32\n    color_base_saturation: float = 0.5\n    color_base_contrast: float = 0.5\n    color_base_hue: float = 18\n    # blur: float = 0.0\n    # noise: float = 0.0\n\n    # Geometric augmentation\n    horizontal_flip: float = 0.0\n    vertical_flip: float = 0.0\n    zoom_out: float = 0.0\n    zoom_out_side: float = 4.0\n    rotation: float = 0.0\n    aspect_ratio: float = 0.0\n\n    ## Rescaling\n    square: float = 0.0\n    scale_ratio: float = 0.0\n    max_size: int = 4096\n\n    # Cropping\n    crop: bool = False\n    crop_size: Optional[int] = None\n\n    # TODO: Add more augmentations like:\n    # - GaussianBlur\n    # - RandomNoise\n    # - RandomResizedCrop\n\n    def override(self, args):\n        if not isinstance(args, dict):\n            args = vars(args)\n        for key, value in args.items():\n            if hasattr(self, key) and value is not None:\n                setattr(self, key, value)\n        return self\n\n    def get_augmentations(self, img_format=\"RGB\", task: Optional[Task] = None) -&gt; List[T.Transform]:\n        \"\"\"Generate augmentation pipeline based on configuration.\"\"\"\n        augs = []\n        self.max_size = self.max_size if self.max_size else sys.maxsize\n\n        ### Add color augmentation if configured\n        if self.color_augmentation &gt; 0:\n            brightness_delta = int(self.color_base_brightness * self.color_augmentation)\n            contrast_delta = self.color_base_contrast * self.color_augmentation\n            saturation_delta = self.color_base_saturation * self.color_augmentation\n            hue_delta = int(self.color_base_hue * self.color_augmentation)\n            augs.append(\n                T.ColorAugSSDTransform(\n                    img_format=img_format,\n                    brightness_delta=brightness_delta,\n                    contrast_low=(1 - contrast_delta),\n                    contrast_high=(1 + contrast_delta),\n                    saturation_low=(1 - saturation_delta),\n                    saturation_high=(1 + saturation_delta),\n                    hue_delta=hue_delta,\n                ),\n            )\n\n        ### Add geometric augmentations\n        # Add flipping augmentations if configured\n        if self.horizontal_flip &gt; 0:\n            augs.append(A.RandomFlip(prob=self.horizontal_flip, horizontal=True))\n        if self.vertical_flip &gt; 0:\n            augs.append(A.RandomFlip(prob=self.vertical_flip, horizontal=False, vertical=True))\n\n        # Add zoom out augmentations if configured\n        if self.zoom_out &gt; 0.0:\n            seg_pad_value = 255 if task == Task.SEMSEG else 0\n            augs.append(\n                A.RandomApply(\n                    A.RandomZoomOut(side_range=(1.0, self.zoom_out_side), pad_value=0, seg_pad_value=seg_pad_value),\n                    prob=self.zoom_out,\n                )\n            )\n\n        ### Add AspectRatio augmentations based on configuration\n        if self.square &gt; 0.0:\n            augs.append(A.RandomApply(A.Resize(shape=(self.resolution, self.resolution)), prob=self.square))\n        elif self.aspect_ratio &gt; 0.0:\n            augs.append(A.RandomAspectRatio(aspect_ratio=self.aspect_ratio))\n\n        ### Add Resizing augmentations based on configuration\n        min_scale, max_scale = 2 ** (-self.scale_ratio), 2**self.scale_ratio\n        augs.append(\n            A.ResizeShortestEdge(\n                short_edge_length=[int(x * self.resolution) for x in [min_scale, max_scale]],\n                sample_style=\"range\",\n                max_size=self.max_size,\n            )\n        )\n\n        ### Add rotation augmentations if configured\n        if self.rotation &gt; 0:\n            angle = self.rotation * 180\n            augs.append(A.RandomRotation(angle=(-angle, angle), expand=False))\n\n        # Add cropping if configured\n        if self.crop:\n            crop_range = (self.crop_size or self.resolution, self.crop_size or self.resolution)\n            augs.append(A.RandomCrop(crop_type=\"absolute_range\", crop_size=crop_range))\n\n        return augs\n</code></pre>"},{"location":"api/auto_dataset/#focoos.data.default_aug.DatasetAugmentations.get_augmentations","title":"<code>get_augmentations(img_format='RGB', task=None)</code>","text":"<p>Generate augmentation pipeline based on configuration.</p> Source code in <code>focoos/data/default_aug.py</code> <pre><code>def get_augmentations(self, img_format=\"RGB\", task: Optional[Task] = None) -&gt; List[T.Transform]:\n    \"\"\"Generate augmentation pipeline based on configuration.\"\"\"\n    augs = []\n    self.max_size = self.max_size if self.max_size else sys.maxsize\n\n    ### Add color augmentation if configured\n    if self.color_augmentation &gt; 0:\n        brightness_delta = int(self.color_base_brightness * self.color_augmentation)\n        contrast_delta = self.color_base_contrast * self.color_augmentation\n        saturation_delta = self.color_base_saturation * self.color_augmentation\n        hue_delta = int(self.color_base_hue * self.color_augmentation)\n        augs.append(\n            T.ColorAugSSDTransform(\n                img_format=img_format,\n                brightness_delta=brightness_delta,\n                contrast_low=(1 - contrast_delta),\n                contrast_high=(1 + contrast_delta),\n                saturation_low=(1 - saturation_delta),\n                saturation_high=(1 + saturation_delta),\n                hue_delta=hue_delta,\n            ),\n        )\n\n    ### Add geometric augmentations\n    # Add flipping augmentations if configured\n    if self.horizontal_flip &gt; 0:\n        augs.append(A.RandomFlip(prob=self.horizontal_flip, horizontal=True))\n    if self.vertical_flip &gt; 0:\n        augs.append(A.RandomFlip(prob=self.vertical_flip, horizontal=False, vertical=True))\n\n    # Add zoom out augmentations if configured\n    if self.zoom_out &gt; 0.0:\n        seg_pad_value = 255 if task == Task.SEMSEG else 0\n        augs.append(\n            A.RandomApply(\n                A.RandomZoomOut(side_range=(1.0, self.zoom_out_side), pad_value=0, seg_pad_value=seg_pad_value),\n                prob=self.zoom_out,\n            )\n        )\n\n    ### Add AspectRatio augmentations based on configuration\n    if self.square &gt; 0.0:\n        augs.append(A.RandomApply(A.Resize(shape=(self.resolution, self.resolution)), prob=self.square))\n    elif self.aspect_ratio &gt; 0.0:\n        augs.append(A.RandomAspectRatio(aspect_ratio=self.aspect_ratio))\n\n    ### Add Resizing augmentations based on configuration\n    min_scale, max_scale = 2 ** (-self.scale_ratio), 2**self.scale_ratio\n    augs.append(\n        A.ResizeShortestEdge(\n            short_edge_length=[int(x * self.resolution) for x in [min_scale, max_scale]],\n            sample_style=\"range\",\n            max_size=self.max_size,\n        )\n    )\n\n    ### Add rotation augmentations if configured\n    if self.rotation &gt; 0:\n        angle = self.rotation * 180\n        augs.append(A.RandomRotation(angle=(-angle, angle), expand=False))\n\n    # Add cropping if configured\n    if self.crop:\n        crop_range = (self.crop_size or self.resolution, self.crop_size or self.resolution)\n        augs.append(A.RandomCrop(crop_type=\"absolute_range\", crop_size=crop_range))\n\n    return augs\n</code></pre>"},{"location":"api/base_model/","title":"BaseModelNN","text":""},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN","title":"<code>BaseModelNN</code>","text":"<p>               Bases: <code>ABC</code>, <code>Module</code></p> <p>Abstract base class for neural network models in Focoos.</p> <p>This class provides a common interface for all neural network models, defining abstract methods that must be implemented by concrete model classes. It extends both ABC (Abstract Base Class) and nn.Module from PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelConfig</code> <p>Model configuration containing hyperparameters and settings.</p> required Source code in <code>focoos/models/base_model.py</code> <pre><code>class BaseModelNN(ABC, nn.Module):\n    \"\"\"Abstract base class for neural network models in Focoos.\n\n    This class provides a common interface for all neural network models,\n    defining abstract methods that must be implemented by concrete model classes.\n    It extends both ABC (Abstract Base Class) and nn.Module from PyTorch.\n\n    Args:\n        config: Model configuration containing hyperparameters and settings.\n    \"\"\"\n\n    def __init__(self, config: ModelConfig):\n        \"\"\"Initialize the base model.\n\n        Args:\n            config: Model configuration object containing model parameters\n                and settings.\n        \"\"\"\n        super().__init__()\n\n    @property\n    @abstractmethod\n    def device(self) -&gt; torch.device:\n        \"\"\"Get the device where the model is located.\n\n        Returns:\n            The PyTorch device (CPU or CUDA) where the model parameters\n            are stored.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(\"Device is not implemented for this model.\")\n\n    @property\n    @abstractmethod\n    def dtype(self) -&gt; torch.dtype:\n        \"\"\"Get the data type of the model parameters.\n\n        Returns:\n            The PyTorch data type (e.g., float32, float16) of the model\n            parameters.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(\"Dtype is not implemented for this model.\")\n\n    @abstractmethod\n    def forward(\n        self,\n        inputs: Union[\n            torch.Tensor,\n            np.ndarray,\n            Image.Image,\n            list[Image.Image],\n            list[np.ndarray],\n            list[torch.Tensor],\n            list[DatasetEntry],\n        ],\n    ) -&gt; ModelOutput:\n        \"\"\"Perform forward pass through the model.\n\n        Args:\n            inputs: Input data in various supported formats:\n                - torch.Tensor: Single tensor input\n                - np.ndarray: Single numpy array input\n                - Image.Image: Single PIL Image input\n                - list[Image.Image]: List of PIL Images\n                - list[np.ndarray]: List of numpy arrays\n                - list[torch.Tensor]: List of tensors\n                - list[DatasetEntry]: List of dataset entries\n\n        Returns:\n            Model output containing predictions and any additional metadata.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(\"Forward is not implemented for this model.\")\n\n    def load_state_dict(self, checkpoint_state_dict: dict, strict: bool = True) -&gt; IncompatibleKeys:\n        \"\"\"Load model state dictionary from checkpoint with preprocessing.\n\n        This method handles common issues when loading checkpoints:\n        - Removes \"module.\" prefix from DataParallel/DistributedDataParallel models\n        - Handles shape mismatches by removing incompatible parameters\n        - Logs incompatible keys for debugging\n\n        Args:\n            checkpoint_state_dict: Dictionary containing model parameters from\n                a saved checkpoint.\n            strict: Whether to strictly enforce that the keys in checkpoint_state_dict\n                match the keys returned by this module's state_dict() function.\n                Defaults to True.\n\n        Returns:\n            IncompatibleKeys object containing information about missing keys,\n            unexpected keys, and parameters with incorrect shapes.\n        \"\"\"\n        # if the state_dict comes from a model that was wrapped in a\n        # DataParallel or DistributedDataParallel during serialization,\n        # remove the \"module\" prefix before performing the matching.\n        strip_prefix_if_present(checkpoint_state_dict, \"module.\")\n\n        # workaround https://github.com/pytorch/pytorch/issues/24139\n        model_state_dict = self.state_dict()\n        incorrect_shapes = []\n        for k in list(checkpoint_state_dict.keys()):\n            if k in model_state_dict:\n                model_param = model_state_dict[k]\n                shape_model = tuple(model_param.shape)\n                shape_checkpoint = tuple(checkpoint_state_dict[k].shape)\n                if shape_model != shape_checkpoint:\n                    incorrect_shapes.append((k, shape_checkpoint, shape_model))\n                    checkpoint_state_dict.pop(k)\n\n        incompatible = super().load_state_dict(checkpoint_state_dict, strict=strict)\n        incompatible = IncompatibleKeys(\n            missing_keys=incompatible.missing_keys,\n            unexpected_keys=incompatible.unexpected_keys,\n            incorrect_shapes=incorrect_shapes,\n        )\n\n        incompatible.log_incompatible_keys()\n\n        return incompatible\n\n    def benchmark(self, iterations: int = 50, size: Tuple[int, int] = (640, 640)) -&gt; LatencyMetrics:\n        \"\"\"Benchmark model inference latency and throughput.\n\n        Performs multiple inference runs on random data to measure model\n        performance metrics including FPS, mean latency, and latency statistics.\n        Uses CUDA events for precise timing when running on GPU.\n\n        Args:\n            iterations: Number of inference runs to perform for benchmarking.\n                Defaults to 50.\n            size: Input image size as (height, width) tuple. Defaults to (640, 640).\n\n        Returns:\n            LatencyMetrics object containing:\n                - fps: Frames per second (throughput)\n                - engine: Hardware/framework used for inference\n                - mean: Mean inference time in milliseconds\n                - max: Maximum inference time in milliseconds\n                - min: Minimum inference time in milliseconds\n                - std: Standard deviation of inference times\n                - im_size: Input image size\n                - device: Device used for inference\n\n        Note:\n            This method assumes the model is running on CUDA for timing.\n            Input data is randomly generated for benchmarking purposes.\n        \"\"\"\n        logger.info(f\"\u23f1\ufe0f Benchmarking latency on {self.device}, size: {size}x{size}..\")\n        # warmup\n        data = 128 * torch.randn(1, 3, size[0], size[1]).to(self.device)\n        durations = []\n        for _ in range(iterations):\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            start.record(stream=torch.cuda.Stream())\n            _ = self(data)\n            end.record(stream=torch.cuda.Stream())\n            torch.cuda.synchronize()\n            durations.append(start.elapsed_time(end))\n\n        durations = np.array(durations)\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean()),\n            engine=f\"torch.{self.device}\",\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n            device=str(self.device),\n        )\n        logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n</code></pre>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.device","title":"<code>device</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the device where the model is located.</p> <p>Returns:</p> Type Description <code>device</code> <p>The PyTorch device (CPU or CUDA) where the model parameters</p> <code>device</code> <p>are stored.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.dtype","title":"<code>dtype</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the data type of the model parameters.</p> <p>Returns:</p> Type Description <code>dtype</code> <p>The PyTorch data type (e.g., float32, float16) of the model</p> <code>dtype</code> <p>parameters.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the base model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelConfig</code> <p>Model configuration object containing model parameters and settings.</p> required Source code in <code>focoos/models/base_model.py</code> <pre><code>def __init__(self, config: ModelConfig):\n    \"\"\"Initialize the base model.\n\n    Args:\n        config: Model configuration object containing model parameters\n            and settings.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.benchmark","title":"<code>benchmark(iterations=50, size=(640, 640))</code>","text":"<p>Benchmark model inference latency and throughput.</p> <p>Performs multiple inference runs on random data to measure model performance metrics including FPS, mean latency, and latency statistics. Uses CUDA events for precise timing when running on GPU.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of inference runs to perform for benchmarking. Defaults to 50.</p> <code>50</code> <code>size</code> <code>Tuple[int, int]</code> <p>Input image size as (height, width) tuple. Defaults to (640, 640).</p> <code>(640, 640)</code> <p>Returns:</p> Type Description <code>LatencyMetrics</code> <p>LatencyMetrics object containing: - fps: Frames per second (throughput) - engine: Hardware/framework used for inference - mean: Mean inference time in milliseconds - max: Maximum inference time in milliseconds - min: Minimum inference time in milliseconds - std: Standard deviation of inference times - im_size: Input image size - device: Device used for inference</p> Note <p>This method assumes the model is running on CUDA for timing. Input data is randomly generated for benchmarking purposes.</p> Source code in <code>focoos/models/base_model.py</code> <pre><code>def benchmark(self, iterations: int = 50, size: Tuple[int, int] = (640, 640)) -&gt; LatencyMetrics:\n    \"\"\"Benchmark model inference latency and throughput.\n\n    Performs multiple inference runs on random data to measure model\n    performance metrics including FPS, mean latency, and latency statistics.\n    Uses CUDA events for precise timing when running on GPU.\n\n    Args:\n        iterations: Number of inference runs to perform for benchmarking.\n            Defaults to 50.\n        size: Input image size as (height, width) tuple. Defaults to (640, 640).\n\n    Returns:\n        LatencyMetrics object containing:\n            - fps: Frames per second (throughput)\n            - engine: Hardware/framework used for inference\n            - mean: Mean inference time in milliseconds\n            - max: Maximum inference time in milliseconds\n            - min: Minimum inference time in milliseconds\n            - std: Standard deviation of inference times\n            - im_size: Input image size\n            - device: Device used for inference\n\n    Note:\n        This method assumes the model is running on CUDA for timing.\n        Input data is randomly generated for benchmarking purposes.\n    \"\"\"\n    logger.info(f\"\u23f1\ufe0f Benchmarking latency on {self.device}, size: {size}x{size}..\")\n    # warmup\n    data = 128 * torch.randn(1, 3, size[0], size[1]).to(self.device)\n    durations = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record(stream=torch.cuda.Stream())\n        _ = self(data)\n        end.record(stream=torch.cuda.Stream())\n        torch.cuda.synchronize()\n        durations.append(start.elapsed_time(end))\n\n    durations = np.array(durations)\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean()),\n        engine=f\"torch.{self.device}\",\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n        device=str(self.device),\n    )\n    logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.forward","title":"<code>forward(inputs)</code>  <code>abstractmethod</code>","text":"<p>Perform forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor], list[DatasetEntry]]</code> <p>Input data in various supported formats: - torch.Tensor: Single tensor input - np.ndarray: Single numpy array input - Image.Image: Single PIL Image input - list[Image.Image]: List of PIL Images - list[np.ndarray]: List of numpy arrays - list[torch.Tensor]: List of tensors - list[DatasetEntry]: List of dataset entries</p> required <p>Returns:</p> Type Description <code>ModelOutput</code> <p>Model output containing predictions and any additional metadata.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>focoos/models/base_model.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    inputs: Union[\n        torch.Tensor,\n        np.ndarray,\n        Image.Image,\n        list[Image.Image],\n        list[np.ndarray],\n        list[torch.Tensor],\n        list[DatasetEntry],\n    ],\n) -&gt; ModelOutput:\n    \"\"\"Perform forward pass through the model.\n\n    Args:\n        inputs: Input data in various supported formats:\n            - torch.Tensor: Single tensor input\n            - np.ndarray: Single numpy array input\n            - Image.Image: Single PIL Image input\n            - list[Image.Image]: List of PIL Images\n            - list[np.ndarray]: List of numpy arrays\n            - list[torch.Tensor]: List of tensors\n            - list[DatasetEntry]: List of dataset entries\n\n    Returns:\n        Model output containing predictions and any additional metadata.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(\"Forward is not implemented for this model.\")\n</code></pre>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.load_state_dict","title":"<code>load_state_dict(checkpoint_state_dict, strict=True)</code>","text":"<p>Load model state dictionary from checkpoint with preprocessing.</p> <p>This method handles common issues when loading checkpoints: - Removes \"module.\" prefix from DataParallel/DistributedDataParallel models - Handles shape mismatches by removing incompatible parameters - Logs incompatible keys for debugging</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_state_dict</code> <code>dict</code> <p>Dictionary containing model parameters from a saved checkpoint.</p> required <code>strict</code> <code>bool</code> <p>Whether to strictly enforce that the keys in checkpoint_state_dict match the keys returned by this module's state_dict() function. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>IncompatibleKeys</code> <p>IncompatibleKeys object containing information about missing keys,</p> <code>IncompatibleKeys</code> <p>unexpected keys, and parameters with incorrect shapes.</p> Source code in <code>focoos/models/base_model.py</code> <pre><code>def load_state_dict(self, checkpoint_state_dict: dict, strict: bool = True) -&gt; IncompatibleKeys:\n    \"\"\"Load model state dictionary from checkpoint with preprocessing.\n\n    This method handles common issues when loading checkpoints:\n    - Removes \"module.\" prefix from DataParallel/DistributedDataParallel models\n    - Handles shape mismatches by removing incompatible parameters\n    - Logs incompatible keys for debugging\n\n    Args:\n        checkpoint_state_dict: Dictionary containing model parameters from\n            a saved checkpoint.\n        strict: Whether to strictly enforce that the keys in checkpoint_state_dict\n            match the keys returned by this module's state_dict() function.\n            Defaults to True.\n\n    Returns:\n        IncompatibleKeys object containing information about missing keys,\n        unexpected keys, and parameters with incorrect shapes.\n    \"\"\"\n    # if the state_dict comes from a model that was wrapped in a\n    # DataParallel or DistributedDataParallel during serialization,\n    # remove the \"module\" prefix before performing the matching.\n    strip_prefix_if_present(checkpoint_state_dict, \"module.\")\n\n    # workaround https://github.com/pytorch/pytorch/issues/24139\n    model_state_dict = self.state_dict()\n    incorrect_shapes = []\n    for k in list(checkpoint_state_dict.keys()):\n        if k in model_state_dict:\n            model_param = model_state_dict[k]\n            shape_model = tuple(model_param.shape)\n            shape_checkpoint = tuple(checkpoint_state_dict[k].shape)\n            if shape_model != shape_checkpoint:\n                incorrect_shapes.append((k, shape_checkpoint, shape_model))\n                checkpoint_state_dict.pop(k)\n\n    incompatible = super().load_state_dict(checkpoint_state_dict, strict=strict)\n    incompatible = IncompatibleKeys(\n        missing_keys=incompatible.missing_keys,\n        unexpected_keys=incompatible.unexpected_keys,\n        incorrect_shapes=incorrect_shapes,\n    )\n\n    incompatible.log_incompatible_keys()\n\n    return incompatible\n</code></pre>"},{"location":"api/config/","title":"config","text":"<p>Configuration module for Focoos AI SDK.</p> <p>This module defines the configuration settings for the Focoos AI SDK, including API credentials, logging levels, default endpoints, and runtime preferences. It provides a centralized way to manage SDK behavior through environment variables.</p> <p>Classes:</p> Name Description <code>FocoosConfig</code> <p>Pydantic settings class for Focoos SDK configuration.</p> Constants <p>LogLevel: Type definition for supported logging levels. FOCOOS_CONFIG: Global configuration instance used throughout the SDK.</p>"},{"location":"api/config/#focoos.config.FocoosConfig","title":"<code>FocoosConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration settings for the Focoos AI SDK.</p> <p>This class uses pydantic_settings to manage configuration values, supporting environment variable overrides and validation. All attributes can be configured via environment variables with the same name.</p> <p>Attributes:</p> Name Type Description <code>focoos_api_key</code> <code>Optional[str]</code> <p>API key for authenticating with Focoos services. Required for accessing protected API endpoints. Defaults to None.</p> <code>focoos_log_level</code> <code>LogLevel</code> <p>Logging level for the SDK to control verbosity. Defaults to \"DEBUG\".</p> <code>default_host_url</code> <code>str</code> <p>Default API endpoint URL for all service requests. Defaults to the production API URL.</p> <code>runtime_type</code> <code>RuntimeTypes</code> <p>Default runtime type for model inference engines. Defaults to TORCHSCRIPT_32 for optimal performance.</p> <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations for model initialization to stabilize performance metrics. Defaults to 2.</p> Example <p>Setting configuration via environment variables in bash: <pre><code># Set API key and change log level\nexport FOCOOS_API_KEY=\"your-api-key-here\"\nexport FOCOOS_LOG_LEVEL=\"INFO\"\nexport DEFAULT_HOST_URL=\"https://custom-api-endpoint.com\"\nexport RUNTIME_TYPE=\"ONNX_CUDA32\"\nexport WARMUP_ITER=\"3\"\n\n# Then run your Python application\npython your_app.py\n</code></pre></p> Source code in <code>focoos/config.py</code> <pre><code>class FocoosConfig(BaseSettings):\n    \"\"\"\n    Configuration settings for the Focoos AI SDK.\n\n    This class uses pydantic_settings to manage configuration values,\n    supporting environment variable overrides and validation. All attributes\n    can be configured via environment variables with the same name.\n\n    Attributes:\n        focoos_api_key (Optional[str]): API key for authenticating with Focoos services.\n            Required for accessing protected API endpoints. Defaults to None.\n        focoos_log_level (LogLevel): Logging level for the SDK to control verbosity.\n            Defaults to \"DEBUG\".\n        default_host_url (str): Default API endpoint URL for all service requests.\n            Defaults to the production API URL.\n        runtime_type (RuntimeTypes): Default runtime type for model inference engines.\n            Defaults to TORCHSCRIPT_32 for optimal performance.\n        warmup_iter (int): Number of warmup iterations for model initialization to\n            stabilize performance metrics. Defaults to 2.\n\n    Example:\n        Setting configuration via environment variables in bash:\n        ```bash\n        # Set API key and change log level\n        export FOCOOS_API_KEY=\"your-api-key-here\"\n        export FOCOOS_LOG_LEVEL=\"INFO\"\n        export DEFAULT_HOST_URL=\"https://custom-api-endpoint.com\"\n        export RUNTIME_TYPE=\"ONNX_CUDA32\"\n        export WARMUP_ITER=\"3\"\n\n        # Then run your Python application\n        python your_app.py\n        ```\n    \"\"\"\n\n    focoos_api_key: Optional[str] = None\n    focoos_log_level: LogLevel = \"DEBUG\"\n    default_host_url: str = PROD_API_URL\n    runtime_type: RuntimeType = RuntimeType.TORCHSCRIPT_32\n    warmup_iter: int = 2\n</code></pre>"},{"location":"api/focoos_model/","title":"FocoosModel","text":""},{"location":"api/focoos_model/#focoos.models.focoos_model.ExportableModel","title":"<code>ExportableModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>A wrapper class for making models exportable to different formats.</p> <p>This class wraps a BaseModelNN model to make it compatible with export formats like ONNX and TorchScript by handling the output formatting.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModelNN</code> <p>The base model to wrap for export.</p> required <code>device</code> <p>The device to move the model to. Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>class ExportableModel(torch.nn.Module):\n    \"\"\"A wrapper class for making models exportable to different formats.\n\n    This class wraps a BaseModelNN model to make it compatible with export formats\n    like ONNX and TorchScript by handling the output formatting.\n\n    Args:\n        model: The base model to wrap for export.\n        device: The device to move the model to. Defaults to \"cuda\".\n    \"\"\"\n\n    def __init__(self, model: BaseModelNN, device=\"cuda\"):\n        \"\"\"Initialize the ExportableModel.\n\n        Args:\n            model: The base model to wrap for export.\n            device: The device to move the model to. Defaults to \"cuda\".\n        \"\"\"\n        super().__init__()\n        self.model = model.eval().to(device)\n\n    def forward(self, x):\n        \"\"\"Forward pass through the wrapped model.\n\n        Args:\n            x: Input tensor to pass through the model.\n\n        Returns:\n            Model output converted to tuple format for export compatibility.\n        \"\"\"\n        return self.model(x).to_tuple()\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.ExportableModel.__init__","title":"<code>__init__(model, device='cuda')</code>","text":"<p>Initialize the ExportableModel.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModelNN</code> <p>The base model to wrap for export.</p> required <code>device</code> <p>The device to move the model to. Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def __init__(self, model: BaseModelNN, device=\"cuda\"):\n    \"\"\"Initialize the ExportableModel.\n\n    Args:\n        model: The base model to wrap for export.\n        device: The device to move the model to. Defaults to \"cuda\".\n    \"\"\"\n    super().__init__()\n    self.model = model.eval().to(device)\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.ExportableModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the wrapped model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input tensor to pass through the model.</p> required <p>Returns:</p> Type Description <p>Model output converted to tuple format for export compatibility.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass through the wrapped model.\n\n    Args:\n        x: Input tensor to pass through the model.\n\n    Returns:\n        Model output converted to tuple format for export compatibility.\n    \"\"\"\n    return self.model(x).to_tuple()\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel","title":"<code>FocoosModel</code>","text":"<p>Main model class for Focoos computer vision models.</p> <p>This class provides a high-level interface for training, testing, exporting, and running inference with Focoos models. It handles model configuration, weight loading, preprocessing, and postprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModelNN</code> <p>The underlying neural network model.</p> required <code>model_info</code> <code>ModelInfo</code> <p>Metadata and configuration information for the model.</p> required Source code in <code>focoos/models/focoos_model.py</code> <pre><code>class FocoosModel:\n    \"\"\"Main model class for Focoos computer vision models.\n\n    This class provides a high-level interface for training, testing, exporting,\n    and running inference with Focoos models. It handles model configuration,\n    weight loading, preprocessing, and postprocessing.\n\n    Args:\n        model: The underlying neural network model.\n        model_info: Metadata and configuration information for the model.\n    \"\"\"\n\n    def __init__(self, model: BaseModelNN, model_info: ModelInfo):\n        \"\"\"Initialize the FocoosModel.\n\n        Args:\n            model: The underlying neural network model.\n            model_info: Metadata and configuration information for the model.\n        \"\"\"\n        self.model = model\n        self.model_info = model_info\n        self.processor = ProcessorManager.get_processor(self.model_info.model_family, self.model_info.config)\n        if self.model_info.weights_uri:\n            self._load_weights()\n        else:\n            logger.warning(f\"\u26a0\ufe0f Model {self.model_info.name} has no pretrained weights\")\n\n    def __str__(self):\n        \"\"\"Return string representation of the model.\n\n        Returns:\n            String containing model name and family.\n        \"\"\"\n        return f\"{self.model_info.name} ({self.model_info.model_family.value})\"\n\n    def __repr__(self):\n        \"\"\"Return detailed string representation of the model.\n\n        Returns:\n            String containing model name and family.\n        \"\"\"\n        return f\"{self.model_info.name} ({self.model_info.model_family.value})\"\n\n    def _setup_model_for_training(self, train_args: TrainerArgs, data_train: MapDataset, data_val: MapDataset):\n        \"\"\"Set up the model and metadata for training.\n\n        This method configures the model information with training parameters,\n        device information, dataset metadata, and initializes training status.\n\n        Args:\n            train_args: Training configuration arguments.\n            data_train: Training dataset.\n            data_val: Validation dataset.\n        \"\"\"\n        device = get_cpu_name()\n        system_info = get_system_info()\n        if system_info.gpu_info and system_info.gpu_info.devices and len(system_info.gpu_info.devices) &gt; 0:\n            device = system_info.gpu_info.devices[0].gpu_name\n        self.model_info.ref = None\n\n        self.model_info.train_args = train_args  # type: ignore\n        self.model_info.val_dataset = data_val.dataset.metadata.name\n        self.model_info.val_metrics = None\n        self.model_info.classes = data_val.dataset.metadata.classes\n        self.model_info.focoos_version = get_focoos_version()\n        self.model_info.status = ModelStatus.TRAINING_STARTING\n        self.model_info.updated_at = datetime.now().isoformat()\n        self.model_info.latency = []\n        self.model_info.metrics = None\n        self.model_info.training_info = TrainingInfo(\n            instance_device=device,\n            main_status=ModelStatus.TRAINING_STARTING,\n            start_time=datetime.now().isoformat(),\n            status_transitions=[\n                dict(\n                    status=ModelStatus.TRAINING_STARTING,\n                    timestamp=datetime.now().isoformat(),\n                )\n            ],\n        )\n\n        self.model_info.classes = data_train.dataset.metadata.classes\n        self.model_info.config[\"num_classes\"] = len(data_train.dataset.metadata.classes)\n        self._reload_model()\n        self.model_info.name = train_args.run_name.strip()\n        assert self.model_info.task == data_train.dataset.metadata.task, \"Task mismatch between model and dataset.\"\n\n    def train(self, args: TrainerArgs, data_train: MapDataset, data_val: MapDataset, hub: Optional[FocoosHUB] = None):\n        \"\"\"Train the model on the provided datasets.\n\n        This method handles both single-GPU and multi-GPU distributed training.\n        It sets up the model for training, optionally syncs with Focoos Hub,\n        and manages the training process.\n\n        Args:\n            args: Training configuration and hyperparameters.\n            data_train: Training dataset containing images and annotations.\n            data_val: Validation dataset for model evaluation.\n            hub: Optional Focoos Hub instance for model syncing.\n\n        Raises:\n            AssertionError: If task mismatch between model and dataset.\n            AssertionError: If number of classes mismatch between model and dataset.\n            AssertionError: If num_gpus is 0 (GPU training is required).\n            FileNotFoundError: If training artifacts are not found after completion.\n        \"\"\"\n        from focoos.trainer.trainer import run_train\n\n        self._setup_model_for_training(args, data_train, data_val)\n        assert self.model_info.task == data_train.dataset.metadata.task, \"Task mismatch between model and dataset.\"\n        assert self.model_info.config[\"num_classes\"] == data_train.dataset.metadata.num_classes, (\n            \"Number of classes mismatch between model and dataset.\"\n        )\n        remote_model = None\n        if args.sync_to_hub:\n            hub = hub or FocoosHUB()\n            remote_model = hub.new_model(self.model_info)\n\n            self.model_info.ref = remote_model.ref\n            logger.info(f\"Model {self.model_info.name} created in hub with ref {self.model_info.ref}\")\n\n        assert args.num_gpus, \"Training without GPUs is not supported. num_gpus must be greater than 0\"\n        if args.num_gpus &gt; 1:\n            launch(\n                run_train,\n                args.num_gpus,\n                dist_url=\"auto\",\n                args=(args, data_train, data_val, self.model, self.processor, self.model_info, remote_model),\n            )\n\n            logger.info(\"Training done, resuming main process.\")\n            # here i should restore the best model and config since in DDP it is not updated\n            final_folder = os.path.join(args.output_dir, args.run_name)\n            model_path = os.path.join(final_folder, ArtifactName.WEIGHTS)\n            metadata_path = os.path.join(final_folder, ArtifactName.INFO)\n\n            if not os.path.exists(model_path):\n                raise FileNotFoundError(f\"Training did not end correctly, model file not found at {model_path}\")\n            if not os.path.exists(metadata_path):\n                raise FileNotFoundError(f\"Training did not end correctly, metadata file not found at {metadata_path}\")\n            self.model_info = ModelInfo.from_json(metadata_path)\n\n            logger.info(f\"Reloading weights from {self.model_info.weights_uri}\")\n            self._reload_model()\n        else:\n            run_train(args, data_train, data_val, self.model, self.processor, self.model_info, remote_model)\n\n    def test(self, args: TrainerArgs, data_test: MapDataset):\n        \"\"\"Test the model on the provided test dataset.\n\n        This method evaluates the model performance on a test dataset,\n        supporting both single-GPU and multi-GPU testing.\n\n        Args:\n            args: Test configuration arguments.\n            data_test: Test dataset for model evaluation.\n\n        Raises:\n            AssertionError: If task mismatch between model and dataset.\n            AssertionError: If num_gpus is 0 (GPU testing is required).\n        \"\"\"\n        from focoos.trainer.trainer import run_test\n\n        self.model_info.val_dataset = data_test.dataset.metadata.name\n        self.model_info.val_metrics = None\n        self.model_info.classes = data_test.dataset.metadata.classes\n        self.model_info.config[\"num_classes\"] = data_test.dataset.metadata.num_classes\n        assert self.model_info.task == data_test.dataset.metadata.task, \"Task mismatch between model and dataset.\"\n\n        assert args.num_gpus, \"Testing without GPUs is not supported. num_gpus must be greater than 0\"\n        if args.num_gpus &gt; 1:\n            launch(\n                run_test,\n                args.num_gpus,\n                dist_url=\"auto\",\n                args=(args, data_test, self.model, self.processor, self.model_info),\n            )\n            logger.info(\"Testing done, resuming main process.\")\n            # here i should restore the best model and config since in DDP it is not updated\n            final_folder = os.path.join(args.output_dir, args.run_name)\n            metadata_path = os.path.join(final_folder, ArtifactName.INFO)\n            self.model_info = ModelInfo.from_json(metadata_path)\n        else:\n            run_test(args, data_test, self.model, self.processor, self.model_info)\n\n    @property\n    def device(self):\n        \"\"\"Get the device where the model is located.\n\n        Returns:\n            The device (CPU or CUDA) where the model is currently located.\n        \"\"\"\n        return self.model.device\n\n    @property\n    def resolution(self):\n        \"\"\"Get the input resolution of the model.\n\n        Returns:\n            The input image resolution expected by the model.\n        \"\"\"\n        return self.model_info.config[\"resolution\"]\n\n    @property\n    def config(self) -&gt; dict:\n        \"\"\"Get the model configuration.\n\n        Returns:\n            Dictionary containing the model configuration parameters.\n        \"\"\"\n        return self.model_info.config\n\n    @property\n    def classes(self):\n        \"\"\"Get the class names the model can predict.\n\n        Returns:\n            List of class names that the model was trained to recognize.\n        \"\"\"\n        return self.model_info.classes\n\n    @property\n    def task(self):\n        \"\"\"Get the computer vision task type.\n\n        Returns:\n            The type of computer vision task (e.g., detection, classification).\n        \"\"\"\n        return self.model_info.task\n\n    def export(\n        self,\n        runtime_type: RuntimeType = RuntimeType.ONNX_CUDA32,\n        onnx_opset: int = 17,\n        out_dir: Optional[str] = None,\n        device: Literal[\"cuda\", \"cpu\"] = \"cuda\",\n        overwrite: bool = False,\n        image_size: Optional[int] = None,\n    ) -&gt; InferModel:\n        \"\"\"Export the model to different runtime formats.\n\n        This method exports the model to formats like ONNX or TorchScript\n        for deployment and inference optimization.\n\n        Args:\n            runtime_type: Target runtime format for export.\n            onnx_opset: ONNX opset version to use for ONNX export.\n            out_dir: Output directory for exported model. If None, uses default location.\n            device: Device to use for export (\"cuda\" or \"cpu\").\n            overwrite: Whether to overwrite existing exported model files.\n            image_size: Custom image size for export. If None, uses model's default size.\n\n        Returns:\n            InferModel instance for the exported model.\n\n        Raises:\n            ValueError: If unsupported PyTorch version or export format.\n        \"\"\"\n        if device == \"cuda\" and not torch.cuda.is_available():\n            device = \"cpu\"\n            logger.warning(\"CUDA is not available. Using CPU for export.\")\n        if out_dir is None:\n            out_dir = os.path.join(MODELS_DIR, self.model_info.ref or self.model_info.name)\n\n        format = runtime_type.to_export_format()\n        exportable_model = ExportableModel(self.model, device=device)\n        os.makedirs(out_dir, exist_ok=True)\n        if image_size is None:\n            data = 128 * torch.randn(1, 3, self.model_info.im_size, self.model_info.im_size).to(device)\n        else:\n            data = 128 * torch.randn(1, 3, image_size, image_size).to(device)\n            self.model_info.im_size = image_size\n\n        export_model_name = ArtifactName.ONNX if format == ExportFormat.ONNX else ArtifactName.PT\n        _out_file = os.path.join(out_dir, export_model_name)\n\n        dynamic_axes = self.processor.get_dynamic_axes()\n\n        # Hack to warm up the model and record the spacial shapes if needed\n        self.model(data)\n\n        if not overwrite and os.path.exists(_out_file):\n            logger.info(f\"Model file {_out_file} already exists. Set overwrite to True to overwrite.\")\n            return InferModel(model_dir=out_dir, runtime_type=runtime_type)\n\n        if format == \"onnx\":\n            with torch.no_grad():\n                logger.info(\"\ud83d\ude80 Exporting ONNX model..\")\n                if TORCH_VERSION &gt;= (2, 5):\n                    exp_program = torch.onnx.export(\n                        exportable_model,\n                        (data,),\n                        f=_out_file,\n                        opset_version=onnx_opset,\n                        verbose=False,\n                        verify=True,\n                        dynamo=False,\n                        external_data=False,  # model weights external to model\n                        input_names=dynamic_axes.input_names,\n                        output_names=dynamic_axes.output_names,\n                        dynamic_axes=dynamic_axes.dynamic_axes,\n                        do_constant_folding=True,\n                        export_params=True,\n                        # dynamic_shapes={\n                        #    \"x\": {\n                        #        0: torch.export.Dim(\"batch\", min=1, max=64),\n                        #        #2: torch.export.Dim(\"height\", min=18, max=4096),\n                        #        #3: torch.export.Dim(\"width\", min=18, max=4096),\n                        #    }\n                        # },\n                    )\n                elif TORCH_VERSION &gt;= (2, 0):\n                    torch.onnx.export(\n                        exportable_model,\n                        (data,),\n                        f=_out_file,\n                        opset_version=onnx_opset,\n                        verbose=False,\n                        input_names=dynamic_axes.input_names,\n                        output_names=dynamic_axes.output_names,\n                        dynamic_axes=dynamic_axes.dynamic_axes,\n                        do_constant_folding=True,\n                        export_params=True,\n                    )\n                else:\n                    raise ValueError(f\"Unsupported Torch version: {TORCH_VERSION}. Install torch 2.x\")\n                # if exp_program is not None:\n                #    exp_program.optimize()\n                #    exp_program.save(_out_file)\n                logger.info(f\"\u2705 Exported {format} model to {_out_file}\")\n\n        elif format == \"torchscript\":\n            with torch.no_grad():\n                logger.info(\"\ud83d\ude80 Exporting TorchScript model..\")\n                exp_program = torch.jit.trace(exportable_model, data)\n                if exp_program is not None:\n                    _out_file = os.path.join(out_dir, ArtifactName.PT)\n                    torch.jit.save(exp_program, _out_file)\n                    logger.info(f\"\u2705 Exported {format} model to {_out_file} \")\n                else:\n                    raise ValueError(f\"Failed to export {format} model\")\n\n        # Fixme: this may override the model_info with the one from the exportable model\n        self.model_info.dump_json(os.path.join(out_dir, ArtifactName.INFO))\n        return InferModel(model_dir=out_dir, runtime_type=runtime_type)\n\n    def __call__(\n        self,\n        inputs: Union[\n            torch.Tensor,\n            np.ndarray,\n            Image.Image,\n            list[Image.Image],\n            list[np.ndarray],\n            list[torch.Tensor],\n        ],\n        **kwargs,\n    ) -&gt; FocoosDetections:\n        \"\"\"Run inference on input images.\n\n        This method performs end-to-end inference including preprocessing,\n        model forward pass, and postprocessing to return detections.\n\n        Args:\n            inputs: Input images in various formats (PIL, numpy, torch tensor, or lists).\n            **kwargs: Additional arguments passed to postprocessing.\n\n        Returns:\n            FocoosDetections containing the detection results.\n        \"\"\"\n        model = self.model.eval()\n        processor = self.processor.eval()\n        try:\n            model = model.cuda()\n        except Exception:\n            logger.warning(\"Unable to use CUDA\")\n        images, _ = processor.preprocess(\n            inputs,\n            device=model.device,\n            dtype=model.dtype,\n            image_size=self.model_info.im_size,\n        )  # second output is targets that we're not using\n        with torch.no_grad():\n            try:\n                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                    output = model.forward(images)\n            except Exception:\n                output = model.forward(images)\n        class_names = self.model_info.classes\n        output_fdet = processor.postprocess(output, inputs, class_names=class_names, **kwargs)\n\n        # FIXME: we don't support batching yet\n        return output_fdet[0]\n\n    def _reload_model(self):\n        \"\"\"Reload the model with updated configuration.\n\n        This method recreates the model instance with the current configuration\n        and reloads the weights. Used when configuration changes during training.\n        \"\"\"\n        from focoos.model_manager import ConfigManager  # here to avoid circular import\n\n        torch.cuda.empty_cache()\n        model_class = self.model.__class__\n        # without the next line, the inner config may be not a ModelConfig but a dict\n        config = ConfigManager.from_dict(self.model_info.model_family, self.model_info.config)\n        self.model_info.config = config\n        model = model_class(config)\n        self.model = model\n        self._load_weights()\n\n    def _load_weights(self) -&gt; int:\n        \"\"\"Load model weights from the specified URI.\n\n        This method loads the model weights from either a local path or a remote URL,\n        depending on the value of `self.model_info.weights_uri`. If the weights are remote,\n        they are downloaded to a local directory. The method then loads the weights into\n        the model, allowing for missing or unexpected keys (non-strict loading).\n\n        Returns:\n            The total number of missing or unexpected keys encountered during loading.\n            Returns 0 if no weights are loaded or an error occurs.\n\n        Raises:\n            FileNotFoundError: If the weights file cannot be found at the specified path.\n        \"\"\"\n        if not self.model_info.weights_uri:\n            logger.warning(f\"\u26a0\ufe0f Model {self.model_info.name} has no pretrained weights\")\n            return 0\n\n        # Determine if weights are remote or local\n        parsed_uri = urlparse(self.model_info.weights_uri)\n        is_remote = bool(parsed_uri.scheme and parsed_uri.netloc)\n\n        # Get weights path\n        if is_remote:\n            logger.info(f\"Downloading weights from remote URL: {self.model_info.weights_uri}\")\n            model_dir = Path(MODELS_DIR) / self.model_info.name\n            weights_path = ApiClient().download_ext_file(\n                self.model_info.weights_uri, str(model_dir), skip_if_exists=True\n            )\n        else:\n            logger.info(f\"Using weights from local path: {self.model_info.weights_uri}\")\n            weights_path = self.model_info.weights_uri\n\n        try:\n            if not os.path.exists(weights_path):\n                raise FileNotFoundError(f\"Weights file not found: {weights_path}\")\n\n            # Load weights and extract model state if needed\n            state_dict = torch.load(weights_path, map_location=\"cpu\", weights_only=True)\n            weights_dict = state_dict.get(\"model\", state_dict) if isinstance(state_dict, dict) else state_dict\n\n        except Exception as e:\n            logger.error(f\"Error loading weights for {self.model_info.name}: {str(e)}\")\n            return 0\n\n        incompatible = self.model.load_state_dict(weights_dict, strict=False)\n        return len(incompatible.missing_keys) + len(incompatible.unexpected_keys)\n\n    def benchmark(\n        self,\n        iterations: int = 50,\n        size: Optional[Union[int, Tuple[int, int]]] = None,\n        device: Literal[\"cuda\", \"cpu\"] = \"cuda\",\n    ) -&gt; LatencyMetrics:\n        \"\"\"Benchmark the model's inference performance.\n\n        This method measures the raw model inference latency without\n        preprocessing and postprocessing overhead.\n\n        Args:\n            iterations: Number of iterations to run for benchmarking.\n            size: Input image size. If None, uses model's default size.\n            device: Device to run benchmarking on (\"cuda\" or \"cpu\").\n\n        Returns:\n            LatencyMetrics containing performance statistics.\n        \"\"\"\n        self.model.eval()\n\n        if size is None:\n            size = self.model_info.im_size\n        if isinstance(size, int):\n            size = (size, size)\n        model = self.model.to(device)\n        metrics = model.benchmark(size=size, iterations=iterations)\n        return metrics\n\n    def end2end_benchmark(\n        self, iterations: int = 50, size: Optional[int] = None, device: Literal[\"cuda\", \"cpu\"] = \"cuda\"\n    ) -&gt; LatencyMetrics:\n        \"\"\"Benchmark the complete end-to-end inference pipeline.\n\n        This method measures the full inference latency including preprocessing,\n        model forward pass, and postprocessing steps.\n\n        Args:\n            iterations: Number of iterations to run for benchmarking.\n            size: Input image size. If None, uses model's default size.\n            device: Device to run benchmarking on (\"cuda\" or \"cpu\").\n\n        Returns:\n            LatencyMetrics containing end-to-end performance statistics.\n        \"\"\"\n        if size is None:\n            size = self.model_info.im_size\n\n        try:\n            model = self.model.cuda()\n        except Exception:\n            logger.warning(\"Unable to use CUDA\")\n        logger.info(f\"\u23f1\ufe0f Benchmarking latency on {model.device}, size: {size}x{size}..\")\n        # warmup\n        data = 128 * torch.randn(1, 3, size, size).to(model.device)\n\n        durations = []\n        for _ in range(iterations):\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            start.record(stream=torch.cuda.Stream())\n            _ = self(data)\n            end.record(stream=torch.cuda.Stream())\n            torch.cuda.synchronize()\n            durations.append(start.elapsed_time(end))\n\n        durations = np.array(durations)\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean()),\n            engine=f\"torch.{self.model.device}\",\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size,\n            device=str(self.model.device),\n        )\n        logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.classes","title":"<code>classes</code>  <code>property</code>","text":"<p>Get the class names the model can predict.</p> <p>Returns:</p> Type Description <p>List of class names that the model was trained to recognize.</p>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.config","title":"<code>config</code>  <code>property</code>","text":"<p>Get the model configuration.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the model configuration parameters.</p>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.device","title":"<code>device</code>  <code>property</code>","text":"<p>Get the device where the model is located.</p> <p>Returns:</p> Type Description <p>The device (CPU or CUDA) where the model is currently located.</p>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.resolution","title":"<code>resolution</code>  <code>property</code>","text":"<p>Get the input resolution of the model.</p> <p>Returns:</p> Type Description <p>The input image resolution expected by the model.</p>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.task","title":"<code>task</code>  <code>property</code>","text":"<p>Get the computer vision task type.</p> <p>Returns:</p> Type Description <p>The type of computer vision task (e.g., detection, classification).</p>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.__call__","title":"<code>__call__(inputs, **kwargs)</code>","text":"<p>Run inference on input images.</p> <p>This method performs end-to-end inference including preprocessing, model forward pass, and postprocessing to return detections.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Input images in various formats (PIL, numpy, torch tensor, or lists).</p> required <code>**kwargs</code> <p>Additional arguments passed to postprocessing.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FocoosDetections</code> <p>FocoosDetections containing the detection results.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def __call__(\n    self,\n    inputs: Union[\n        torch.Tensor,\n        np.ndarray,\n        Image.Image,\n        list[Image.Image],\n        list[np.ndarray],\n        list[torch.Tensor],\n    ],\n    **kwargs,\n) -&gt; FocoosDetections:\n    \"\"\"Run inference on input images.\n\n    This method performs end-to-end inference including preprocessing,\n    model forward pass, and postprocessing to return detections.\n\n    Args:\n        inputs: Input images in various formats (PIL, numpy, torch tensor, or lists).\n        **kwargs: Additional arguments passed to postprocessing.\n\n    Returns:\n        FocoosDetections containing the detection results.\n    \"\"\"\n    model = self.model.eval()\n    processor = self.processor.eval()\n    try:\n        model = model.cuda()\n    except Exception:\n        logger.warning(\"Unable to use CUDA\")\n    images, _ = processor.preprocess(\n        inputs,\n        device=model.device,\n        dtype=model.dtype,\n        image_size=self.model_info.im_size,\n    )  # second output is targets that we're not using\n    with torch.no_grad():\n        try:\n            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                output = model.forward(images)\n        except Exception:\n            output = model.forward(images)\n    class_names = self.model_info.classes\n    output_fdet = processor.postprocess(output, inputs, class_names=class_names, **kwargs)\n\n    # FIXME: we don't support batching yet\n    return output_fdet[0]\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.__init__","title":"<code>__init__(model, model_info)</code>","text":"<p>Initialize the FocoosModel.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModelNN</code> <p>The underlying neural network model.</p> required <code>model_info</code> <code>ModelInfo</code> <p>Metadata and configuration information for the model.</p> required Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def __init__(self, model: BaseModelNN, model_info: ModelInfo):\n    \"\"\"Initialize the FocoosModel.\n\n    Args:\n        model: The underlying neural network model.\n        model_info: Metadata and configuration information for the model.\n    \"\"\"\n    self.model = model\n    self.model_info = model_info\n    self.processor = ProcessorManager.get_processor(self.model_info.model_family, self.model_info.config)\n    if self.model_info.weights_uri:\n        self._load_weights()\n    else:\n        logger.warning(f\"\u26a0\ufe0f Model {self.model_info.name} has no pretrained weights\")\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.__repr__","title":"<code>__repr__()</code>","text":"<p>Return detailed string representation of the model.</p> <p>Returns:</p> Type Description <p>String containing model name and family.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return detailed string representation of the model.\n\n    Returns:\n        String containing model name and family.\n    \"\"\"\n    return f\"{self.model_info.name} ({self.model_info.model_family.value})\"\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation of the model.</p> <p>Returns:</p> Type Description <p>String containing model name and family.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def __str__(self):\n    \"\"\"Return string representation of the model.\n\n    Returns:\n        String containing model name and family.\n    \"\"\"\n    return f\"{self.model_info.name} ({self.model_info.model_family.value})\"\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.benchmark","title":"<code>benchmark(iterations=50, size=None, device='cuda')</code>","text":"<p>Benchmark the model's inference performance.</p> <p>This method measures the raw model inference latency without preprocessing and postprocessing overhead.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of iterations to run for benchmarking.</p> <code>50</code> <code>size</code> <code>Optional[Union[int, Tuple[int, int]]]</code> <p>Input image size. If None, uses model's default size.</p> <code>None</code> <code>device</code> <code>Literal['cuda', 'cpu']</code> <p>Device to run benchmarking on (\"cuda\" or \"cpu\").</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>LatencyMetrics</code> <p>LatencyMetrics containing performance statistics.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def benchmark(\n    self,\n    iterations: int = 50,\n    size: Optional[Union[int, Tuple[int, int]]] = None,\n    device: Literal[\"cuda\", \"cpu\"] = \"cuda\",\n) -&gt; LatencyMetrics:\n    \"\"\"Benchmark the model's inference performance.\n\n    This method measures the raw model inference latency without\n    preprocessing and postprocessing overhead.\n\n    Args:\n        iterations: Number of iterations to run for benchmarking.\n        size: Input image size. If None, uses model's default size.\n        device: Device to run benchmarking on (\"cuda\" or \"cpu\").\n\n    Returns:\n        LatencyMetrics containing performance statistics.\n    \"\"\"\n    self.model.eval()\n\n    if size is None:\n        size = self.model_info.im_size\n    if isinstance(size, int):\n        size = (size, size)\n    model = self.model.to(device)\n    metrics = model.benchmark(size=size, iterations=iterations)\n    return metrics\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.end2end_benchmark","title":"<code>end2end_benchmark(iterations=50, size=None, device='cuda')</code>","text":"<p>Benchmark the complete end-to-end inference pipeline.</p> <p>This method measures the full inference latency including preprocessing, model forward pass, and postprocessing steps.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of iterations to run for benchmarking.</p> <code>50</code> <code>size</code> <code>Optional[int]</code> <p>Input image size. If None, uses model's default size.</p> <code>None</code> <code>device</code> <code>Literal['cuda', 'cpu']</code> <p>Device to run benchmarking on (\"cuda\" or \"cpu\").</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>LatencyMetrics</code> <p>LatencyMetrics containing end-to-end performance statistics.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def end2end_benchmark(\n    self, iterations: int = 50, size: Optional[int] = None, device: Literal[\"cuda\", \"cpu\"] = \"cuda\"\n) -&gt; LatencyMetrics:\n    \"\"\"Benchmark the complete end-to-end inference pipeline.\n\n    This method measures the full inference latency including preprocessing,\n    model forward pass, and postprocessing steps.\n\n    Args:\n        iterations: Number of iterations to run for benchmarking.\n        size: Input image size. If None, uses model's default size.\n        device: Device to run benchmarking on (\"cuda\" or \"cpu\").\n\n    Returns:\n        LatencyMetrics containing end-to-end performance statistics.\n    \"\"\"\n    if size is None:\n        size = self.model_info.im_size\n\n    try:\n        model = self.model.cuda()\n    except Exception:\n        logger.warning(\"Unable to use CUDA\")\n    logger.info(f\"\u23f1\ufe0f Benchmarking latency on {model.device}, size: {size}x{size}..\")\n    # warmup\n    data = 128 * torch.randn(1, 3, size, size).to(model.device)\n\n    durations = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record(stream=torch.cuda.Stream())\n        _ = self(data)\n        end.record(stream=torch.cuda.Stream())\n        torch.cuda.synchronize()\n        durations.append(start.elapsed_time(end))\n\n    durations = np.array(durations)\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean()),\n        engine=f\"torch.{self.model.device}\",\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size,\n        device=str(self.model.device),\n    )\n    logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.export","title":"<code>export(runtime_type=RuntimeType.ONNX_CUDA32, onnx_opset=17, out_dir=None, device='cuda', overwrite=False, image_size=None)</code>","text":"<p>Export the model to different runtime formats.</p> <p>This method exports the model to formats like ONNX or TorchScript for deployment and inference optimization.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_type</code> <code>RuntimeType</code> <p>Target runtime format for export.</p> <code>ONNX_CUDA32</code> <code>onnx_opset</code> <code>int</code> <p>ONNX opset version to use for ONNX export.</p> <code>17</code> <code>out_dir</code> <code>Optional[str]</code> <p>Output directory for exported model. If None, uses default location.</p> <code>None</code> <code>device</code> <code>Literal['cuda', 'cpu']</code> <p>Device to use for export (\"cuda\" or \"cpu\").</p> <code>'cuda'</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing exported model files.</p> <code>False</code> <code>image_size</code> <code>Optional[int]</code> <p>Custom image size for export. If None, uses model's default size.</p> <code>None</code> <p>Returns:</p> Type Description <code>InferModel</code> <p>InferModel instance for the exported model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unsupported PyTorch version or export format.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def export(\n    self,\n    runtime_type: RuntimeType = RuntimeType.ONNX_CUDA32,\n    onnx_opset: int = 17,\n    out_dir: Optional[str] = None,\n    device: Literal[\"cuda\", \"cpu\"] = \"cuda\",\n    overwrite: bool = False,\n    image_size: Optional[int] = None,\n) -&gt; InferModel:\n    \"\"\"Export the model to different runtime formats.\n\n    This method exports the model to formats like ONNX or TorchScript\n    for deployment and inference optimization.\n\n    Args:\n        runtime_type: Target runtime format for export.\n        onnx_opset: ONNX opset version to use for ONNX export.\n        out_dir: Output directory for exported model. If None, uses default location.\n        device: Device to use for export (\"cuda\" or \"cpu\").\n        overwrite: Whether to overwrite existing exported model files.\n        image_size: Custom image size for export. If None, uses model's default size.\n\n    Returns:\n        InferModel instance for the exported model.\n\n    Raises:\n        ValueError: If unsupported PyTorch version or export format.\n    \"\"\"\n    if device == \"cuda\" and not torch.cuda.is_available():\n        device = \"cpu\"\n        logger.warning(\"CUDA is not available. Using CPU for export.\")\n    if out_dir is None:\n        out_dir = os.path.join(MODELS_DIR, self.model_info.ref or self.model_info.name)\n\n    format = runtime_type.to_export_format()\n    exportable_model = ExportableModel(self.model, device=device)\n    os.makedirs(out_dir, exist_ok=True)\n    if image_size is None:\n        data = 128 * torch.randn(1, 3, self.model_info.im_size, self.model_info.im_size).to(device)\n    else:\n        data = 128 * torch.randn(1, 3, image_size, image_size).to(device)\n        self.model_info.im_size = image_size\n\n    export_model_name = ArtifactName.ONNX if format == ExportFormat.ONNX else ArtifactName.PT\n    _out_file = os.path.join(out_dir, export_model_name)\n\n    dynamic_axes = self.processor.get_dynamic_axes()\n\n    # Hack to warm up the model and record the spacial shapes if needed\n    self.model(data)\n\n    if not overwrite and os.path.exists(_out_file):\n        logger.info(f\"Model file {_out_file} already exists. Set overwrite to True to overwrite.\")\n        return InferModel(model_dir=out_dir, runtime_type=runtime_type)\n\n    if format == \"onnx\":\n        with torch.no_grad():\n            logger.info(\"\ud83d\ude80 Exporting ONNX model..\")\n            if TORCH_VERSION &gt;= (2, 5):\n                exp_program = torch.onnx.export(\n                    exportable_model,\n                    (data,),\n                    f=_out_file,\n                    opset_version=onnx_opset,\n                    verbose=False,\n                    verify=True,\n                    dynamo=False,\n                    external_data=False,  # model weights external to model\n                    input_names=dynamic_axes.input_names,\n                    output_names=dynamic_axes.output_names,\n                    dynamic_axes=dynamic_axes.dynamic_axes,\n                    do_constant_folding=True,\n                    export_params=True,\n                    # dynamic_shapes={\n                    #    \"x\": {\n                    #        0: torch.export.Dim(\"batch\", min=1, max=64),\n                    #        #2: torch.export.Dim(\"height\", min=18, max=4096),\n                    #        #3: torch.export.Dim(\"width\", min=18, max=4096),\n                    #    }\n                    # },\n                )\n            elif TORCH_VERSION &gt;= (2, 0):\n                torch.onnx.export(\n                    exportable_model,\n                    (data,),\n                    f=_out_file,\n                    opset_version=onnx_opset,\n                    verbose=False,\n                    input_names=dynamic_axes.input_names,\n                    output_names=dynamic_axes.output_names,\n                    dynamic_axes=dynamic_axes.dynamic_axes,\n                    do_constant_folding=True,\n                    export_params=True,\n                )\n            else:\n                raise ValueError(f\"Unsupported Torch version: {TORCH_VERSION}. Install torch 2.x\")\n            # if exp_program is not None:\n            #    exp_program.optimize()\n            #    exp_program.save(_out_file)\n            logger.info(f\"\u2705 Exported {format} model to {_out_file}\")\n\n    elif format == \"torchscript\":\n        with torch.no_grad():\n            logger.info(\"\ud83d\ude80 Exporting TorchScript model..\")\n            exp_program = torch.jit.trace(exportable_model, data)\n            if exp_program is not None:\n                _out_file = os.path.join(out_dir, ArtifactName.PT)\n                torch.jit.save(exp_program, _out_file)\n                logger.info(f\"\u2705 Exported {format} model to {_out_file} \")\n            else:\n                raise ValueError(f\"Failed to export {format} model\")\n\n    # Fixme: this may override the model_info with the one from the exportable model\n    self.model_info.dump_json(os.path.join(out_dir, ArtifactName.INFO))\n    return InferModel(model_dir=out_dir, runtime_type=runtime_type)\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.test","title":"<code>test(args, data_test)</code>","text":"<p>Test the model on the provided test dataset.</p> <p>This method evaluates the model performance on a test dataset, supporting both single-GPU and multi-GPU testing.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>TrainerArgs</code> <p>Test configuration arguments.</p> required <code>data_test</code> <code>MapDataset</code> <p>Test dataset for model evaluation.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If task mismatch between model and dataset.</p> <code>AssertionError</code> <p>If num_gpus is 0 (GPU testing is required).</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def test(self, args: TrainerArgs, data_test: MapDataset):\n    \"\"\"Test the model on the provided test dataset.\n\n    This method evaluates the model performance on a test dataset,\n    supporting both single-GPU and multi-GPU testing.\n\n    Args:\n        args: Test configuration arguments.\n        data_test: Test dataset for model evaluation.\n\n    Raises:\n        AssertionError: If task mismatch between model and dataset.\n        AssertionError: If num_gpus is 0 (GPU testing is required).\n    \"\"\"\n    from focoos.trainer.trainer import run_test\n\n    self.model_info.val_dataset = data_test.dataset.metadata.name\n    self.model_info.val_metrics = None\n    self.model_info.classes = data_test.dataset.metadata.classes\n    self.model_info.config[\"num_classes\"] = data_test.dataset.metadata.num_classes\n    assert self.model_info.task == data_test.dataset.metadata.task, \"Task mismatch between model and dataset.\"\n\n    assert args.num_gpus, \"Testing without GPUs is not supported. num_gpus must be greater than 0\"\n    if args.num_gpus &gt; 1:\n        launch(\n            run_test,\n            args.num_gpus,\n            dist_url=\"auto\",\n            args=(args, data_test, self.model, self.processor, self.model_info),\n        )\n        logger.info(\"Testing done, resuming main process.\")\n        # here i should restore the best model and config since in DDP it is not updated\n        final_folder = os.path.join(args.output_dir, args.run_name)\n        metadata_path = os.path.join(final_folder, ArtifactName.INFO)\n        self.model_info = ModelInfo.from_json(metadata_path)\n    else:\n        run_test(args, data_test, self.model, self.processor, self.model_info)\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.train","title":"<code>train(args, data_train, data_val, hub=None)</code>","text":"<p>Train the model on the provided datasets.</p> <p>This method handles both single-GPU and multi-GPU distributed training. It sets up the model for training, optionally syncs with Focoos Hub, and manages the training process.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>TrainerArgs</code> <p>Training configuration and hyperparameters.</p> required <code>data_train</code> <code>MapDataset</code> <p>Training dataset containing images and annotations.</p> required <code>data_val</code> <code>MapDataset</code> <p>Validation dataset for model evaluation.</p> required <code>hub</code> <code>Optional[FocoosHUB]</code> <p>Optional Focoos Hub instance for model syncing.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If task mismatch between model and dataset.</p> <code>AssertionError</code> <p>If number of classes mismatch between model and dataset.</p> <code>AssertionError</code> <p>If num_gpus is 0 (GPU training is required).</p> <code>FileNotFoundError</code> <p>If training artifacts are not found after completion.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def train(self, args: TrainerArgs, data_train: MapDataset, data_val: MapDataset, hub: Optional[FocoosHUB] = None):\n    \"\"\"Train the model on the provided datasets.\n\n    This method handles both single-GPU and multi-GPU distributed training.\n    It sets up the model for training, optionally syncs with Focoos Hub,\n    and manages the training process.\n\n    Args:\n        args: Training configuration and hyperparameters.\n        data_train: Training dataset containing images and annotations.\n        data_val: Validation dataset for model evaluation.\n        hub: Optional Focoos Hub instance for model syncing.\n\n    Raises:\n        AssertionError: If task mismatch between model and dataset.\n        AssertionError: If number of classes mismatch between model and dataset.\n        AssertionError: If num_gpus is 0 (GPU training is required).\n        FileNotFoundError: If training artifacts are not found after completion.\n    \"\"\"\n    from focoos.trainer.trainer import run_train\n\n    self._setup_model_for_training(args, data_train, data_val)\n    assert self.model_info.task == data_train.dataset.metadata.task, \"Task mismatch between model and dataset.\"\n    assert self.model_info.config[\"num_classes\"] == data_train.dataset.metadata.num_classes, (\n        \"Number of classes mismatch between model and dataset.\"\n    )\n    remote_model = None\n    if args.sync_to_hub:\n        hub = hub or FocoosHUB()\n        remote_model = hub.new_model(self.model_info)\n\n        self.model_info.ref = remote_model.ref\n        logger.info(f\"Model {self.model_info.name} created in hub with ref {self.model_info.ref}\")\n\n    assert args.num_gpus, \"Training without GPUs is not supported. num_gpus must be greater than 0\"\n    if args.num_gpus &gt; 1:\n        launch(\n            run_train,\n            args.num_gpus,\n            dist_url=\"auto\",\n            args=(args, data_train, data_val, self.model, self.processor, self.model_info, remote_model),\n        )\n\n        logger.info(\"Training done, resuming main process.\")\n        # here i should restore the best model and config since in DDP it is not updated\n        final_folder = os.path.join(args.output_dir, args.run_name)\n        model_path = os.path.join(final_folder, ArtifactName.WEIGHTS)\n        metadata_path = os.path.join(final_folder, ArtifactName.INFO)\n\n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Training did not end correctly, model file not found at {model_path}\")\n        if not os.path.exists(metadata_path):\n            raise FileNotFoundError(f\"Training did not end correctly, metadata file not found at {metadata_path}\")\n        self.model_info = ModelInfo.from_json(metadata_path)\n\n        logger.info(f\"Reloading weights from {self.model_info.weights_uri}\")\n        self._reload_model()\n    else:\n        run_train(args, data_train, data_val, self.model, self.processor, self.model_info, remote_model)\n</code></pre>"},{"location":"api/hub/","title":"FocoosHUB","text":"<p>Focoos Module</p> <p>This module provides a Python interface for interacting with Focoos APIs, allowing users to manage machine learning models and datasets in the Focoos ecosystem. The module supports operations such as retrieving model metadata, downloading models, and listing shared datasets.</p> <p>Classes:</p> Name Description <code>FocoosHUB</code> <p>Main class to interface with Focoos APIs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised for invalid API responses or missing parameters.</p> <p>RemoteModel Module</p> <p>This module provides a class to manage remote models in the Focoos ecosystem. It supports various functionalities including model training, deployment, inference, and monitoring.</p> <p>Classes:</p> Name Description <code>RemoteModel</code> <p>A class for interacting with remote models, managing their lifecycle,          and performing inference.</p> <p>Functions:</p> Name Description <code>__init__</code> <p>Initializes the RemoteModel instance.</p> <code>get_info</code> <p>Retrieves model metadata.</p> <code>train</code> <p>Initiates model training.</p> <code>train_info</code> <p>Retrieves training status.</p> <code>train_logs</code> <p>Retrieves training logs.</p> <code>metrics</code> <p>Retrieves model metrics.</p>"},{"location":"api/hub/#focoos.hub.api_client.ApiClient","title":"<code>ApiClient</code>","text":"<p>A simple HTTP client for making GET, POST, and DELETE requests.</p> <p>This client is initialized with an API key and a host URL, and it automatically includes the API key in the headers of each request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key for authorization.</p> <code>host_url</code> <code>str</code> <p>The base URL for the API.</p> <code>default_headers</code> <code>dict</code> <p>Default headers including authorization and user agent.</p> Source code in <code>focoos/hub/api_client.py</code> <pre><code>class ApiClient:\n    \"\"\"\n    A simple HTTP client for making GET, POST, and DELETE requests.\n\n    This client is initialized with an API key and a host URL, and it\n    automatically includes the API key in the headers of each request.\n\n    Attributes:\n        api_key (str): The API key for authorization.\n        host_url (str): The base URL for the API.\n        default_headers (dict): Default headers including authorization and user agent.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        host_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize the ApiClient with an API key and host URL.\n\n        Args:\n            api_key (str): The API key for authorization.\n            host_url (str): The base URL for the API.\n        \"\"\"\n        # Use provided api_key if not None, otherwise use config\n        self.api_key = api_key if api_key is not None else FOCOOS_CONFIG.focoos_api_key\n        self.host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n        self.default_headers = {\n            \"X-API-Key\": self.api_key,\n            \"user_agent\": f\"focoos/{get_focoos_version()}\",\n        }\n\n    def _check_api_key(self):\n        if not self.api_key or (isinstance(self.api_key, str) and self.api_key.strip() == \"\"):\n            raise ValueError(\"API key is required\")\n\n    def external_get(self, path: str, params: Optional[dict] = None, stream: bool = False):\n        \"\"\"\n        Perform a GET request to an external URL.\n\n        Args:\n            path (str): The URL path to request.\n            params (Optional[dict], optional): Query parameters for the request. Defaults to None.\n            stream (bool, optional): Whether to stream the response. Defaults to False.\n\n        Returns:\n            Response: The response object from the requests library.\n        \"\"\"\n        if params is None:\n            params = {}\n        return requests.get(path, params=params, stream=stream)\n\n    def get(\n        self,\n        path: str,\n        params: Optional[dict] = None,\n        extra_headers: Optional[dict] = None,\n        stream: bool = False,\n    ):\n        \"\"\"\n        Perform a GET request to the specified path on the host URL.\n\n        Args:\n            path (str): The URL path to request.\n            params (Optional[dict], optional): Query parameters for the request. Defaults to None.\n            extra_headers (Optional[dict], optional): Additional headers to include in the request. Defaults to None.\n            stream (bool, optional): Whether to stream the response. Defaults to False.\n\n        Returns:\n            Response: The response object from the requests library.\n        \"\"\"\n        self._check_api_key()\n        url = f\"{self.host_url}/{path}\"\n        headers = self.default_headers.copy()\n        if extra_headers:\n            headers.update(extra_headers)\n        return requests.get(url, headers=headers, params=params, stream=stream)\n\n    def post(\n        self,\n        path: str,\n        data: Optional[dict] = None,\n        extra_headers: Optional[dict] = None,\n        files=None,\n    ):\n        \"\"\"\n        Perform a POST request to the specified path on the host URL.\n\n        Args:\n            path (str): The URL path to request.\n            data (Optional[dict], optional): The JSON data to send in the request body. Defaults to None.\n            extra_headers (Optional[dict], optional): Additional headers to include in the request. Defaults to None.\n            files (optional): Files to send in the request. Defaults to None.\n\n        Returns:\n            Response: The response object from the requests library.\n        \"\"\"\n        self._check_api_key()\n        url = f\"{self.host_url}/{path}\"\n        headers = self.default_headers.copy()\n        if extra_headers:\n            headers.update(extra_headers)\n        return requests.post(url, headers=headers, json=data, files=files)\n\n    def patch(\n        self,\n        path: str,\n        data: Optional[dict] = None,\n        extra_headers: Optional[dict] = None,\n    ):\n        self._check_api_key()\n        url = f\"{self.host_url}/{path}\"\n        headers = self.default_headers.copy()\n        if extra_headers:\n            headers.update(extra_headers)\n        return requests.patch(url, headers=headers, json=data)\n\n    def external_post(\n        self,\n        path: str,\n        data: Optional[dict] = None,\n        extra_headers: Optional[dict] = None,\n        files=None,\n        stream: bool = False,\n    ):\n        \"\"\"\n        Perform a POST request to an external URL without using the default host URL.\n\n        This method is used for making POST requests to external services, such as file upload\n        endpoints that require direct access without the base host URL prefix.\n\n        Args:\n            path (str): The complete URL to send the request to.\n            data (Optional[dict], optional): The JSON data to send in the request body. Defaults to None.\n            extra_headers (Optional[dict], optional): Headers to include in the request. Defaults to None.\n            files (optional): Files to send in the request. Defaults to None.\n\n        Returns:\n            Response: The response object from the requests library.\n        \"\"\"\n        headers = {}\n        if extra_headers:\n            headers.update(extra_headers)\n        return requests.post(path, headers=headers, json=data, files=files, stream=stream)\n\n    def delete(self, path: str, extra_headers: Optional[dict] = None):\n        \"\"\"\n        Perform a DELETE request to the specified path on the host URL.\n\n        Args:\n            path (str): The URL path to request.\n            extra_headers (Optional[dict], optional): Additional headers to include in the request. Defaults to None.\n\n        Returns:\n            Response: The response object from the requests library.\n        \"\"\"\n        self._check_api_key()\n        url = f\"{self.host_url}/{path}\"\n        headers = self.default_headers.copy()\n        if extra_headers:\n            headers.update(extra_headers)\n        return requests.delete(url, headers=headers)\n\n    def upload_file(self, path: str, file_path: str, file_size: int):\n        \"\"\"\n        Upload a file to the specified path.\n\n        Args:\n            path (str): The API endpoint path to upload to\n            file_path (str): Path to the file to upload\n            file_size (int): Size of the file in bytes\n\n        Returns:\n            Response: The response from the upload request\n        \"\"\"\n        self._check_api_key()\n        return self.post(path, data={\"path\": file_path, \"file_size_bytes\": file_size})\n\n    def download_ext_file(self, uri: str, file_dir: str, file_name: Optional[str] = None, skip_if_exists: bool = False):\n        \"\"\"\n        Download a file from a URI to a local directory.\n\n        Args:\n            uri (str): The URI to download the file from\n            file_dir (str): Local directory to save the downloaded file\n\n        Returns:\n            str: Path to the downloaded file\n\n        Raises:\n            ValueError: If the download fails or filename cannot be determined\n        \"\"\"\n        if os.path.exists(file_dir) and not os.path.isdir(file_dir):\n            raise ValueError(f\"Path is not a directory: {file_dir}\")\n        if not os.path.exists(file_dir):\n            logger.debug(f\"\ud83d\udce5 Creating directory: {file_dir}\")\n            os.makedirs(file_dir)\n        parsed_url = urlparse(uri)\n        file_name = file_name or os.path.basename(parsed_url.path)\n        res = self.external_get(uri, stream=True)\n        if res.status_code != 200:\n            logger.error(f\"Failed to download file {file_name}: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to download file {file_name}: {res.status_code} {res.text}\")\n\n        file_path = os.path.join(file_dir, file_name)\n        if skip_if_exists and os.path.exists(file_path):\n            logger.debug(f\"\ud83d\udce5 File already exists: {file_path}\")\n            return file_path\n        total_size = int(res.headers.get(\"content-length\", 0))\n        with (\n            open(file_path, \"wb\") as f,\n            tqdm(\n                desc=file_name,\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as bar,\n        ):\n            for chunk in res.iter_content(chunk_size=8192):\n                f.write(chunk)\n                bar.update(len(chunk))\n        logger.debug(f\"\ud83d\udce5 File downloaded: {file_path} Size: {total_size / (1024**2):.2f} MB\")\n        return file_path\n</code></pre>"},{"location":"api/hub/#focoos.hub.api_client.ApiClient.__init__","title":"<code>__init__(api_key=None, host_url=None)</code>","text":"<p>Initialize the ApiClient with an API key and host URL.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for authorization.</p> <code>None</code> <code>host_url</code> <code>str</code> <p>The base URL for the API.</p> <code>None</code> Source code in <code>focoos/hub/api_client.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = None,\n    host_url: Optional[str] = None,\n):\n    \"\"\"\n    Initialize the ApiClient with an API key and host URL.\n\n    Args:\n        api_key (str): The API key for authorization.\n        host_url (str): The base URL for the API.\n    \"\"\"\n    # Use provided api_key if not None, otherwise use config\n    self.api_key = api_key if api_key is not None else FOCOOS_CONFIG.focoos_api_key\n    self.host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n    self.default_headers = {\n        \"X-API-Key\": self.api_key,\n        \"user_agent\": f\"focoos/{get_focoos_version()}\",\n    }\n</code></pre>"},{"location":"api/hub/#focoos.hub.api_client.ApiClient.delete","title":"<code>delete(path, extra_headers=None)</code>","text":"<p>Perform a DELETE request to the specified path on the host URL.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The URL path to request.</p> required <code>extra_headers</code> <code>Optional[dict]</code> <p>Additional headers to include in the request. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>The response object from the requests library.</p> Source code in <code>focoos/hub/api_client.py</code> <pre><code>def delete(self, path: str, extra_headers: Optional[dict] = None):\n    \"\"\"\n    Perform a DELETE request to the specified path on the host URL.\n\n    Args:\n        path (str): The URL path to request.\n        extra_headers (Optional[dict], optional): Additional headers to include in the request. Defaults to None.\n\n    Returns:\n        Response: The response object from the requests library.\n    \"\"\"\n    self._check_api_key()\n    url = f\"{self.host_url}/{path}\"\n    headers = self.default_headers.copy()\n    if extra_headers:\n        headers.update(extra_headers)\n    return requests.delete(url, headers=headers)\n</code></pre>"},{"location":"api/hub/#focoos.hub.api_client.ApiClient.download_ext_file","title":"<code>download_ext_file(uri, file_dir, file_name=None, skip_if_exists=False)</code>","text":"<p>Download a file from a URI to a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The URI to download the file from</p> required <code>file_dir</code> <code>str</code> <p>Local directory to save the downloaded file</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Path to the downloaded file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the download fails or filename cannot be determined</p> Source code in <code>focoos/hub/api_client.py</code> <pre><code>def download_ext_file(self, uri: str, file_dir: str, file_name: Optional[str] = None, skip_if_exists: bool = False):\n    \"\"\"\n    Download a file from a URI to a local directory.\n\n    Args:\n        uri (str): The URI to download the file from\n        file_dir (str): Local directory to save the downloaded file\n\n    Returns:\n        str: Path to the downloaded file\n\n    Raises:\n        ValueError: If the download fails or filename cannot be determined\n    \"\"\"\n    if os.path.exists(file_dir) and not os.path.isdir(file_dir):\n        raise ValueError(f\"Path is not a directory: {file_dir}\")\n    if not os.path.exists(file_dir):\n        logger.debug(f\"\ud83d\udce5 Creating directory: {file_dir}\")\n        os.makedirs(file_dir)\n    parsed_url = urlparse(uri)\n    file_name = file_name or os.path.basename(parsed_url.path)\n    res = self.external_get(uri, stream=True)\n    if res.status_code != 200:\n        logger.error(f\"Failed to download file {file_name}: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to download file {file_name}: {res.status_code} {res.text}\")\n\n    file_path = os.path.join(file_dir, file_name)\n    if skip_if_exists and os.path.exists(file_path):\n        logger.debug(f\"\ud83d\udce5 File already exists: {file_path}\")\n        return file_path\n    total_size = int(res.headers.get(\"content-length\", 0))\n    with (\n        open(file_path, \"wb\") as f,\n        tqdm(\n            desc=file_name,\n            total=total_size,\n            unit=\"B\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as bar,\n    ):\n        for chunk in res.iter_content(chunk_size=8192):\n            f.write(chunk)\n            bar.update(len(chunk))\n    logger.debug(f\"\ud83d\udce5 File downloaded: {file_path} Size: {total_size / (1024**2):.2f} MB\")\n    return file_path\n</code></pre>"},{"location":"api/hub/#focoos.hub.api_client.ApiClient.external_get","title":"<code>external_get(path, params=None, stream=False)</code>","text":"<p>Perform a GET request to an external URL.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The URL path to request.</p> required <code>params</code> <code>Optional[dict]</code> <p>Query parameters for the request. Defaults to None.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>The response object from the requests library.</p> Source code in <code>focoos/hub/api_client.py</code> <pre><code>def external_get(self, path: str, params: Optional[dict] = None, stream: bool = False):\n    \"\"\"\n    Perform a GET request to an external URL.\n\n    Args:\n        path (str): The URL path to request.\n        params (Optional[dict], optional): Query parameters for the request. Defaults to None.\n        stream (bool, optional): Whether to stream the response. Defaults to False.\n\n    Returns:\n        Response: The response object from the requests library.\n    \"\"\"\n    if params is None:\n        params = {}\n    return requests.get(path, params=params, stream=stream)\n</code></pre>"},{"location":"api/hub/#focoos.hub.api_client.ApiClient.external_post","title":"<code>external_post(path, data=None, extra_headers=None, files=None, stream=False)</code>","text":"<p>Perform a POST request to an external URL without using the default host URL.</p> <p>This method is used for making POST requests to external services, such as file upload endpoints that require direct access without the base host URL prefix.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The complete URL to send the request to.</p> required <code>data</code> <code>Optional[dict]</code> <p>The JSON data to send in the request body. Defaults to None.</p> <code>None</code> <code>extra_headers</code> <code>Optional[dict]</code> <p>Headers to include in the request. Defaults to None.</p> <code>None</code> <code>files</code> <code>optional</code> <p>Files to send in the request. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>The response object from the requests library.</p> Source code in <code>focoos/hub/api_client.py</code> <pre><code>def external_post(\n    self,\n    path: str,\n    data: Optional[dict] = None,\n    extra_headers: Optional[dict] = None,\n    files=None,\n    stream: bool = False,\n):\n    \"\"\"\n    Perform a POST request to an external URL without using the default host URL.\n\n    This method is used for making POST requests to external services, such as file upload\n    endpoints that require direct access without the base host URL prefix.\n\n    Args:\n        path (str): The complete URL to send the request to.\n        data (Optional[dict], optional): The JSON data to send in the request body. Defaults to None.\n        extra_headers (Optional[dict], optional): Headers to include in the request. Defaults to None.\n        files (optional): Files to send in the request. Defaults to None.\n\n    Returns:\n        Response: The response object from the requests library.\n    \"\"\"\n    headers = {}\n    if extra_headers:\n        headers.update(extra_headers)\n    return requests.post(path, headers=headers, json=data, files=files, stream=stream)\n</code></pre>"},{"location":"api/hub/#focoos.hub.api_client.ApiClient.get","title":"<code>get(path, params=None, extra_headers=None, stream=False)</code>","text":"<p>Perform a GET request to the specified path on the host URL.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The URL path to request.</p> required <code>params</code> <code>Optional[dict]</code> <p>Query parameters for the request. Defaults to None.</p> <code>None</code> <code>extra_headers</code> <code>Optional[dict]</code> <p>Additional headers to include in the request. Defaults to None.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>The response object from the requests library.</p> Source code in <code>focoos/hub/api_client.py</code> <pre><code>def get(\n    self,\n    path: str,\n    params: Optional[dict] = None,\n    extra_headers: Optional[dict] = None,\n    stream: bool = False,\n):\n    \"\"\"\n    Perform a GET request to the specified path on the host URL.\n\n    Args:\n        path (str): The URL path to request.\n        params (Optional[dict], optional): Query parameters for the request. Defaults to None.\n        extra_headers (Optional[dict], optional): Additional headers to include in the request. Defaults to None.\n        stream (bool, optional): Whether to stream the response. Defaults to False.\n\n    Returns:\n        Response: The response object from the requests library.\n    \"\"\"\n    self._check_api_key()\n    url = f\"{self.host_url}/{path}\"\n    headers = self.default_headers.copy()\n    if extra_headers:\n        headers.update(extra_headers)\n    return requests.get(url, headers=headers, params=params, stream=stream)\n</code></pre>"},{"location":"api/hub/#focoos.hub.api_client.ApiClient.post","title":"<code>post(path, data=None, extra_headers=None, files=None)</code>","text":"<p>Perform a POST request to the specified path on the host URL.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The URL path to request.</p> required <code>data</code> <code>Optional[dict]</code> <p>The JSON data to send in the request body. Defaults to None.</p> <code>None</code> <code>extra_headers</code> <code>Optional[dict]</code> <p>Additional headers to include in the request. Defaults to None.</p> <code>None</code> <code>files</code> <code>optional</code> <p>Files to send in the request. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>The response object from the requests library.</p> Source code in <code>focoos/hub/api_client.py</code> <pre><code>def post(\n    self,\n    path: str,\n    data: Optional[dict] = None,\n    extra_headers: Optional[dict] = None,\n    files=None,\n):\n    \"\"\"\n    Perform a POST request to the specified path on the host URL.\n\n    Args:\n        path (str): The URL path to request.\n        data (Optional[dict], optional): The JSON data to send in the request body. Defaults to None.\n        extra_headers (Optional[dict], optional): Additional headers to include in the request. Defaults to None.\n        files (optional): Files to send in the request. Defaults to None.\n\n    Returns:\n        Response: The response object from the requests library.\n    \"\"\"\n    self._check_api_key()\n    url = f\"{self.host_url}/{path}\"\n    headers = self.default_headers.copy()\n    if extra_headers:\n        headers.update(extra_headers)\n    return requests.post(url, headers=headers, json=data, files=files)\n</code></pre>"},{"location":"api/hub/#focoos.hub.api_client.ApiClient.upload_file","title":"<code>upload_file(path, file_path, file_size)</code>","text":"<p>Upload a file to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The API endpoint path to upload to</p> required <code>file_path</code> <code>str</code> <p>Path to the file to upload</p> required <code>file_size</code> <code>int</code> <p>Size of the file in bytes</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>The response from the upload request</p> Source code in <code>focoos/hub/api_client.py</code> <pre><code>def upload_file(self, path: str, file_path: str, file_size: int):\n    \"\"\"\n    Upload a file to the specified path.\n\n    Args:\n        path (str): The API endpoint path to upload to\n        file_path (str): Path to the file to upload\n        file_size (int): Size of the file in bytes\n\n    Returns:\n        Response: The response from the upload request\n    \"\"\"\n    self._check_api_key()\n    return self.post(path, data={\"path\": file_path, \"file_size_bytes\": file_size})\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB","title":"<code>FocoosHUB</code>","text":"<p>Main class to interface with Focoos APIs.</p> <p>This class provides methods to interact with Focoos-hosted models and datasets. It supports functionalities such as listing models, retrieving model metadata, downloading models, and creating new models.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key for authentication.</p> <code>api_client</code> <code>ApiClient</code> <p>HTTP client for making API requests.</p> <code>user_info</code> <code>User</code> <p>Information about the currently authenticated user.</p> <code>host_url</code> <code>str</code> <p>Base URL for the Focoos API.</p> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>class FocoosHUB:\n    \"\"\"\n    Main class to interface with Focoos APIs.\n\n    This class provides methods to interact with Focoos-hosted models and datasets.\n    It supports functionalities such as listing models, retrieving model metadata,\n    downloading models, and creating new models.\n\n    Attributes:\n        api_key (str): The API key for authentication.\n        api_client (ApiClient): HTTP client for making API requests.\n        user_info (User): Information about the currently authenticated user.\n        host_url (str): Base URL for the Focoos API.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        host_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes the FocoosHUB client.\n\n        This client provides authenticated access to the Focoos API, enabling various operations\n        through the configured HTTP client. It retrieves user information upon initialization and\n        logs the environment details.\n\n        Args:\n            api_key (Optional[str]): API key for authentication. Defaults to the `focoos_api_key`\n                specified in the FOCOOS_CONFIG.\n            host_url (Optional[str]): Base URL for the Focoos API. Defaults to the `default_host_url`\n                specified in the FOCOOS_CONFIG.\n\n        Raises:\n            ValueError: If the API key is not provided, or if the host URL is not specified in the\n                arguments or the configuration.\n\n        Attributes:\n            api_key (str): The API key used for authentication.\n            api_client (ApiClient): An HTTP client instance configured with the API key and host URL.\n            user_info (User): Information about the authenticated user retrieved from the API.\n            host_url (str): The base URL used for API requests.\n\n        Logs:\n            - Error if the API key or host URL is missing.\n            - Info about the authenticated user and environment upon successful initialization.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            ```\n        \"\"\"\n        self.api_key = api_key or FOCOOS_CONFIG.focoos_api_key\n        if not self.api_key:\n            logger.error(\"API key is required \ud83e\udd16\")\n            raise ValueError(\"API key is required \ud83e\udd16\")\n\n        self.host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n        self.api_client = ApiClient(api_key=self.api_key, host_url=self.host_url)\n        self.user_info = self.get_user_info()\n        logger.info(f\"Currently logged as: {self.user_info.email} environment: {self.host_url}\")\n\n    def get_user_info(self) -&gt; User:\n        \"\"\"\n        Retrieves information about the authenticated user.\n\n        Returns:\n            User: User object containing account information and usage quotas.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            user_info = focoos.get_user_info()\n\n            # Access user info fields\n            print(f\"Email: {user_info.email}\")\n            print(f\"Created at: {user_info.created_at}\")\n            print(f\"Updated at: {user_info.updated_at}\")\n            print(f\"Company: {user_info.company}\")\n            print(f\"API key: {user_info.api_key.key}\")\n\n            # Access quotas\n            quotas = user_info.quotas\n            print(f\"Total inferences: {quotas.total_inferences}\")\n            print(f\"Max inferences: {quotas.max_inferences}\")\n            print(f\"Used storage (GB): {quotas.used_storage_gb}\")\n            print(f\"Max storage (GB): {quotas.max_storage_gb}\")\n            print(f\"Active training jobs: {quotas.active_training_jobs}\")\n            print(f\"Max active training jobs: {quotas.max_active_training_jobs}\")\n            print(f\"Used MLG4DNXLarge training jobs hours: {quotas.used_mlg4dnxlarge_training_jobs_hours}\")\n            print(f\"Max MLG4DNXLarge training jobs hours: {quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n            ```\n        \"\"\"\n        res = self.api_client.get(\"user/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get user info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get user info: {res.status_code} {res.text}\")\n        return User.from_json(res.json())\n\n    def get_model_info(self, model_ref: str) -&gt; RemoteModelInfo:\n        \"\"\"\n        Retrieves metadata for a specific model.\n\n        Args:\n            model_ref (str): Reference identifier for the model.\n\n        Returns:\n            RemoteModelInfo: Metadata of the specified model.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            model_info = focoos.get_model_info(model_ref=\"user-or-fai-model-ref\")\n            ```\n        \"\"\"\n        res = self.api_client.get(f\"models/{model_ref}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n        return RemoteModelInfo.from_json(res.json())\n\n    def list_remote_models(self) -&gt; list[ModelPreview]:\n        \"\"\"\n        Lists all models owned by the user.\n\n        Returns:\n            list[ModelPreview]: List of model previews.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            models = focoos.list_remote_models()\n            ```\n        \"\"\"\n        res = self.api_client.get(\"models/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list models: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list models: {res.status_code} {res.text}\")\n        return [ModelPreview.from_json(r) for r in res.json()]\n\n    def get_remote_model(self, model_ref: str) -&gt; RemoteModel:\n        \"\"\"\n        Retrieves a remote model instance for cloud-based inference.\n\n        Args:\n            model_ref (str): Reference identifier for the model.\n\n        Returns:\n            RemoteModel: The remote model instance configured for cloud-based inference.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            model = focoos.get_remote_model(model_ref=\"fai-model-ref\")\n            results = model.infer(\"image.jpg\", threshold=0.5)  # inference is remote!\n            ```\n        \"\"\"\n        return RemoteModel(model_ref, self.api_client)\n\n    def download_model_pth(self, model_ref: str, skip_if_exists: bool = True) -&gt; str:\n        \"\"\"\n        Downloads a model from the Focoos API.\n\n        Args:\n            model_ref (str): Reference identifier for the model.\n            skip_if_exists (bool): If True, skips the download if the model file already exists.\n                Defaults to True.\n\n        Returns:\n            str: Path to the downloaded model file.\n\n        Raises:\n            ValueError: If the API request fails or the download fails.\n        \"\"\"\n        model_dir = os.path.join(MODELS_DIR, model_ref)\n        model_pth_path = os.path.join(model_dir, ArtifactName.WEIGHTS)\n        if os.path.exists(model_pth_path) and skip_if_exists:\n            logger.info(\"\ud83d\udce5 Model already downloaded\")\n            return model_pth_path\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir)\n        ## download model metadata\n        res = self.api_client.get(f\"models/{model_ref}/download?format=pth\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n\n        download_data = res.json()\n\n        download_uri = download_data.get(\"download_uri\")\n        if download_uri is None:\n            logger.error(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n        ## download model from Focoos Cloud\n        logger.debug(f\"Model URI: {download_uri}\")\n        logger.info(\"\ud83d\udce5 Downloading model from Focoos Cloud.. \")\n        try:\n            model_pth_path = self.api_client.download_ext_file(download_uri, model_dir, skip_if_exists=skip_if_exists)\n        except Exception as e:\n            logger.error(f\"Failed to download model: {e}\")\n            raise ValueError(f\"Failed to download model: {e}\")\n        if model_pth_path is None:\n            logger.error(f\"Failed to download model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to download model: {res.status_code} {res.text}\")\n\n        return model_pth_path\n\n    def list_remote_datasets(self, include_shared: bool = False) -&gt; list[DatasetPreview]:\n        \"\"\"\n        Lists all datasets available to the user.\n\n        This method retrieves all datasets owned by the user and optionally includes\n        shared datasets as well.\n\n        Args:\n            include_shared (bool): If True, includes datasets shared with the user.\n                Defaults to False.\n\n        Returns:\n            list[DatasetPreview]: A list of DatasetPreview objects representing the available datasets.\n\n        Raises:\n            ValueError: If the API request to list datasets fails.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n\n            # List only user's datasets\n            datasets = focoos.list_remote_datasets()\n\n            # List user's datasets and shared datasets\n            all_datasets = focoos.list_remote_datasets(include_shared=True)\n\n            for dataset in all_datasets:\n                print(f\"Dataset: {dataset.name}, Task: {dataset.task}\")\n            ```\n        \"\"\"\n        res = self.api_client.get(\"datasets/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        datasets = [DatasetPreview.from_json(r) for r in res.json()]\n        if include_shared:\n            res = self.api_client.get(\"datasets/shared\")\n            if res.status_code != 200:\n                logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n                raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            datasets.extend([DatasetPreview.from_json(sh_dataset) for sh_dataset in res.json()])\n        return datasets\n\n    def get_remote_dataset(self, ref: str) -&gt; RemoteDataset:\n        \"\"\"\n        Retrieves a remote dataset by its reference ID.\n\n        Args:\n            ref (str): The reference ID of the dataset to retrieve.\n\n        Returns:\n            RemoteDataset: A RemoteDataset instance for the specified reference.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            dataset = focoos.get_remote_dataset(ref=\"my-dataset-ref\")\n            ```\n        \"\"\"\n        return RemoteDataset(ref, self.api_client)\n\n    def new_model(self, model_info: ModelInfo) -&gt; RemoteModel:\n        \"\"\"\n        Creates a new model in the Focoos platform.\n\n        Args:\n            name (str): Name of the new model.\n            focoos_model (str): Reference to the base Focoos model.\n            description (str): Description of the new model.\n\n        Returns:\n            Optional[RemoteModel]: The created model instance, or None if creation fails.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            model = focoos.new_model(name=\"my-model\", focoos_model=\"fai-model-ref\", description=\"my-model-description\")\n            ```\n        \"\"\"\n\n        res = self.api_client.post(\n            \"models/local-model\",\n            data={\n                \"name\": model_info.name,\n                \"focoos_model\": model_info.focoos_model,\n                \"description\": model_info.description,\n                \"config\": model_info.config if model_info.config else {},\n                \"task\": model_info.task,\n                \"classes\": model_info.classes,\n                \"im_size\": model_info.im_size,\n                \"train_args\": asdict(model_info.train_args) if model_info.train_args else None,\n                \"focoos_version\": model_info.focoos_version,\n            },\n        )\n        if res.status_code in [200, 201]:\n            return RemoteModel(res.json()[\"ref\"], self.api_client)\n        if res.status_code == 409:\n            logger.warning(f\"Model already exists: {model_info.name}\")\n            raise ValueError(f\"Failed to create new model: {res.status_code} {res.text}\")\n        else:\n            logger.warning(f\"Failed to create new model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to create new model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.__init__","title":"<code>__init__(api_key=None, host_url=None)</code>","text":"<p>Initializes the FocoosHUB client.</p> <p>This client provides authenticated access to the Focoos API, enabling various operations through the configured HTTP client. It retrieves user information upon initialization and logs the environment details.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>API key for authentication. Defaults to the <code>focoos_api_key</code> specified in the FOCOOS_CONFIG.</p> <code>None</code> <code>host_url</code> <code>Optional[str]</code> <p>Base URL for the Focoos API. Defaults to the <code>default_host_url</code> specified in the FOCOOS_CONFIG.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API key is not provided, or if the host URL is not specified in the arguments or the configuration.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key used for authentication.</p> <code>api_client</code> <code>ApiClient</code> <p>An HTTP client instance configured with the API key and host URL.</p> <code>user_info</code> <code>User</code> <p>Information about the authenticated user retrieved from the API.</p> <code>host_url</code> <code>str</code> <p>The base URL used for API requests.</p> Logs <ul> <li>Error if the API key or host URL is missing.</li> <li>Info about the authenticated user and environment upon successful initialization.</li> </ul> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = None,\n    host_url: Optional[str] = None,\n):\n    \"\"\"\n    Initializes the FocoosHUB client.\n\n    This client provides authenticated access to the Focoos API, enabling various operations\n    through the configured HTTP client. It retrieves user information upon initialization and\n    logs the environment details.\n\n    Args:\n        api_key (Optional[str]): API key for authentication. Defaults to the `focoos_api_key`\n            specified in the FOCOOS_CONFIG.\n        host_url (Optional[str]): Base URL for the Focoos API. Defaults to the `default_host_url`\n            specified in the FOCOOS_CONFIG.\n\n    Raises:\n        ValueError: If the API key is not provided, or if the host URL is not specified in the\n            arguments or the configuration.\n\n    Attributes:\n        api_key (str): The API key used for authentication.\n        api_client (ApiClient): An HTTP client instance configured with the API key and host URL.\n        user_info (User): Information about the authenticated user retrieved from the API.\n        host_url (str): The base URL used for API requests.\n\n    Logs:\n        - Error if the API key or host URL is missing.\n        - Info about the authenticated user and environment upon successful initialization.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        ```\n    \"\"\"\n    self.api_key = api_key or FOCOOS_CONFIG.focoos_api_key\n    if not self.api_key:\n        logger.error(\"API key is required \ud83e\udd16\")\n        raise ValueError(\"API key is required \ud83e\udd16\")\n\n    self.host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n    self.api_client = ApiClient(api_key=self.api_key, host_url=self.host_url)\n    self.user_info = self.get_user_info()\n    logger.info(f\"Currently logged as: {self.user_info.email} environment: {self.host_url}\")\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.download_model_pth","title":"<code>download_model_pth(model_ref, skip_if_exists=True)</code>","text":"<p>Downloads a model from the Focoos API.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference identifier for the model.</p> required <code>skip_if_exists</code> <code>bool</code> <p>If True, skips the download if the model file already exists. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the downloaded model file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails or the download fails.</p> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def download_model_pth(self, model_ref: str, skip_if_exists: bool = True) -&gt; str:\n    \"\"\"\n    Downloads a model from the Focoos API.\n\n    Args:\n        model_ref (str): Reference identifier for the model.\n        skip_if_exists (bool): If True, skips the download if the model file already exists.\n            Defaults to True.\n\n    Returns:\n        str: Path to the downloaded model file.\n\n    Raises:\n        ValueError: If the API request fails or the download fails.\n    \"\"\"\n    model_dir = os.path.join(MODELS_DIR, model_ref)\n    model_pth_path = os.path.join(model_dir, ArtifactName.WEIGHTS)\n    if os.path.exists(model_pth_path) and skip_if_exists:\n        logger.info(\"\ud83d\udce5 Model already downloaded\")\n        return model_pth_path\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    ## download model metadata\n    res = self.api_client.get(f\"models/{model_ref}/download?format=pth\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n\n    download_data = res.json()\n\n    download_uri = download_data.get(\"download_uri\")\n    if download_uri is None:\n        logger.error(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n    ## download model from Focoos Cloud\n    logger.debug(f\"Model URI: {download_uri}\")\n    logger.info(\"\ud83d\udce5 Downloading model from Focoos Cloud.. \")\n    try:\n        model_pth_path = self.api_client.download_ext_file(download_uri, model_dir, skip_if_exists=skip_if_exists)\n    except Exception as e:\n        logger.error(f\"Failed to download model: {e}\")\n        raise ValueError(f\"Failed to download model: {e}\")\n    if model_pth_path is None:\n        logger.error(f\"Failed to download model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to download model: {res.status_code} {res.text}\")\n\n    return model_pth_path\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.get_model_info","title":"<code>get_model_info(model_ref)</code>","text":"<p>Retrieves metadata for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference identifier for the model.</p> required <p>Returns:</p> Name Type Description <code>RemoteModelInfo</code> <code>RemoteModelInfo</code> <p>Metadata of the specified model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\nmodel_info = focoos.get_model_info(model_ref=\"user-or-fai-model-ref\")\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def get_model_info(self, model_ref: str) -&gt; RemoteModelInfo:\n    \"\"\"\n    Retrieves metadata for a specific model.\n\n    Args:\n        model_ref (str): Reference identifier for the model.\n\n    Returns:\n        RemoteModelInfo: Metadata of the specified model.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        model_info = focoos.get_model_info(model_ref=\"user-or-fai-model-ref\")\n        ```\n    \"\"\"\n    res = self.api_client.get(f\"models/{model_ref}\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n    return RemoteModelInfo.from_json(res.json())\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.get_remote_dataset","title":"<code>get_remote_dataset(ref)</code>","text":"<p>Retrieves a remote dataset by its reference ID.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>The reference ID of the dataset to retrieve.</p> required <p>Returns:</p> Name Type Description <code>RemoteDataset</code> <code>RemoteDataset</code> <p>A RemoteDataset instance for the specified reference.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\ndataset = focoos.get_remote_dataset(ref=\"my-dataset-ref\")\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def get_remote_dataset(self, ref: str) -&gt; RemoteDataset:\n    \"\"\"\n    Retrieves a remote dataset by its reference ID.\n\n    Args:\n        ref (str): The reference ID of the dataset to retrieve.\n\n    Returns:\n        RemoteDataset: A RemoteDataset instance for the specified reference.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        dataset = focoos.get_remote_dataset(ref=\"my-dataset-ref\")\n        ```\n    \"\"\"\n    return RemoteDataset(ref, self.api_client)\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.get_remote_model","title":"<code>get_remote_model(model_ref)</code>","text":"<p>Retrieves a remote model instance for cloud-based inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference identifier for the model.</p> required <p>Returns:</p> Name Type Description <code>RemoteModel</code> <code>RemoteModel</code> <p>The remote model instance configured for cloud-based inference.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\nmodel = focoos.get_remote_model(model_ref=\"fai-model-ref\")\nresults = model.infer(\"image.jpg\", threshold=0.5)  # inference is remote!\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def get_remote_model(self, model_ref: str) -&gt; RemoteModel:\n    \"\"\"\n    Retrieves a remote model instance for cloud-based inference.\n\n    Args:\n        model_ref (str): Reference identifier for the model.\n\n    Returns:\n        RemoteModel: The remote model instance configured for cloud-based inference.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        model = focoos.get_remote_model(model_ref=\"fai-model-ref\")\n        results = model.infer(\"image.jpg\", threshold=0.5)  # inference is remote!\n        ```\n    \"\"\"\n    return RemoteModel(model_ref, self.api_client)\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.get_user_info","title":"<code>get_user_info()</code>","text":"<p>Retrieves information about the authenticated user.</p> <p>Returns:</p> Name Type Description <code>User</code> <code>User</code> <p>User object containing account information and usage quotas.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\nuser_info = focoos.get_user_info()\n\n# Access user info fields\nprint(f\"Email: {user_info.email}\")\nprint(f\"Created at: {user_info.created_at}\")\nprint(f\"Updated at: {user_info.updated_at}\")\nprint(f\"Company: {user_info.company}\")\nprint(f\"API key: {user_info.api_key.key}\")\n\n# Access quotas\nquotas = user_info.quotas\nprint(f\"Total inferences: {quotas.total_inferences}\")\nprint(f\"Max inferences: {quotas.max_inferences}\")\nprint(f\"Used storage (GB): {quotas.used_storage_gb}\")\nprint(f\"Max storage (GB): {quotas.max_storage_gb}\")\nprint(f\"Active training jobs: {quotas.active_training_jobs}\")\nprint(f\"Max active training jobs: {quotas.max_active_training_jobs}\")\nprint(f\"Used MLG4DNXLarge training jobs hours: {quotas.used_mlg4dnxlarge_training_jobs_hours}\")\nprint(f\"Max MLG4DNXLarge training jobs hours: {quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def get_user_info(self) -&gt; User:\n    \"\"\"\n    Retrieves information about the authenticated user.\n\n    Returns:\n        User: User object containing account information and usage quotas.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        user_info = focoos.get_user_info()\n\n        # Access user info fields\n        print(f\"Email: {user_info.email}\")\n        print(f\"Created at: {user_info.created_at}\")\n        print(f\"Updated at: {user_info.updated_at}\")\n        print(f\"Company: {user_info.company}\")\n        print(f\"API key: {user_info.api_key.key}\")\n\n        # Access quotas\n        quotas = user_info.quotas\n        print(f\"Total inferences: {quotas.total_inferences}\")\n        print(f\"Max inferences: {quotas.max_inferences}\")\n        print(f\"Used storage (GB): {quotas.used_storage_gb}\")\n        print(f\"Max storage (GB): {quotas.max_storage_gb}\")\n        print(f\"Active training jobs: {quotas.active_training_jobs}\")\n        print(f\"Max active training jobs: {quotas.max_active_training_jobs}\")\n        print(f\"Used MLG4DNXLarge training jobs hours: {quotas.used_mlg4dnxlarge_training_jobs_hours}\")\n        print(f\"Max MLG4DNXLarge training jobs hours: {quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n        ```\n    \"\"\"\n    res = self.api_client.get(\"user/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get user info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get user info: {res.status_code} {res.text}\")\n    return User.from_json(res.json())\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.list_remote_datasets","title":"<code>list_remote_datasets(include_shared=False)</code>","text":"<p>Lists all datasets available to the user.</p> <p>This method retrieves all datasets owned by the user and optionally includes shared datasets as well.</p> <p>Parameters:</p> Name Type Description Default <code>include_shared</code> <code>bool</code> <p>If True, includes datasets shared with the user. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[DatasetPreview]</code> <p>list[DatasetPreview]: A list of DatasetPreview objects representing the available datasets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request to list datasets fails.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\n\n# List only user's datasets\ndatasets = focoos.list_remote_datasets()\n\n# List user's datasets and shared datasets\nall_datasets = focoos.list_remote_datasets(include_shared=True)\n\nfor dataset in all_datasets:\n    print(f\"Dataset: {dataset.name}, Task: {dataset.task}\")\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def list_remote_datasets(self, include_shared: bool = False) -&gt; list[DatasetPreview]:\n    \"\"\"\n    Lists all datasets available to the user.\n\n    This method retrieves all datasets owned by the user and optionally includes\n    shared datasets as well.\n\n    Args:\n        include_shared (bool): If True, includes datasets shared with the user.\n            Defaults to False.\n\n    Returns:\n        list[DatasetPreview]: A list of DatasetPreview objects representing the available datasets.\n\n    Raises:\n        ValueError: If the API request to list datasets fails.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n\n        # List only user's datasets\n        datasets = focoos.list_remote_datasets()\n\n        # List user's datasets and shared datasets\n        all_datasets = focoos.list_remote_datasets(include_shared=True)\n\n        for dataset in all_datasets:\n            print(f\"Dataset: {dataset.name}, Task: {dataset.task}\")\n        ```\n    \"\"\"\n    res = self.api_client.get(\"datasets/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n    datasets = [DatasetPreview.from_json(r) for r in res.json()]\n    if include_shared:\n        res = self.api_client.get(\"datasets/shared\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        datasets.extend([DatasetPreview.from_json(sh_dataset) for sh_dataset in res.json()])\n    return datasets\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.list_remote_models","title":"<code>list_remote_models()</code>","text":"<p>Lists all models owned by the user.</p> <p>Returns:</p> Type Description <code>list[ModelPreview]</code> <p>list[ModelPreview]: List of model previews.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\nmodels = focoos.list_remote_models()\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def list_remote_models(self) -&gt; list[ModelPreview]:\n    \"\"\"\n    Lists all models owned by the user.\n\n    Returns:\n        list[ModelPreview]: List of model previews.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        models = focoos.list_remote_models()\n        ```\n    \"\"\"\n    res = self.api_client.get(\"models/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list models: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list models: {res.status_code} {res.text}\")\n    return [ModelPreview.from_json(r) for r in res.json()]\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.new_model","title":"<code>new_model(model_info)</code>","text":"<p>Creates a new model in the Focoos platform.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the new model.</p> required <code>focoos_model</code> <code>str</code> <p>Reference to the base Focoos model.</p> required <code>description</code> <code>str</code> <p>Description of the new model.</p> required <p>Returns:</p> Type Description <code>RemoteModel</code> <p>Optional[RemoteModel]: The created model instance, or None if creation fails.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\nmodel = focoos.new_model(name=\"my-model\", focoos_model=\"fai-model-ref\", description=\"my-model-description\")\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def new_model(self, model_info: ModelInfo) -&gt; RemoteModel:\n    \"\"\"\n    Creates a new model in the Focoos platform.\n\n    Args:\n        name (str): Name of the new model.\n        focoos_model (str): Reference to the base Focoos model.\n        description (str): Description of the new model.\n\n    Returns:\n        Optional[RemoteModel]: The created model instance, or None if creation fails.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        model = focoos.new_model(name=\"my-model\", focoos_model=\"fai-model-ref\", description=\"my-model-description\")\n        ```\n    \"\"\"\n\n    res = self.api_client.post(\n        \"models/local-model\",\n        data={\n            \"name\": model_info.name,\n            \"focoos_model\": model_info.focoos_model,\n            \"description\": model_info.description,\n            \"config\": model_info.config if model_info.config else {},\n            \"task\": model_info.task,\n            \"classes\": model_info.classes,\n            \"im_size\": model_info.im_size,\n            \"train_args\": asdict(model_info.train_args) if model_info.train_args else None,\n            \"focoos_version\": model_info.focoos_version,\n        },\n    )\n    if res.status_code in [200, 201]:\n        return RemoteModel(res.json()[\"ref\"], self.api_client)\n    if res.status_code == 409:\n        logger.warning(f\"Model already exists: {model_info.name}\")\n        raise ValueError(f\"Failed to create new model: {res.status_code} {res.text}\")\n    else:\n        logger.warning(f\"Failed to create new model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to create new model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_dataset.RemoteDataset","title":"<code>RemoteDataset</code>","text":"<p>A class to manage remote datasets through the Focoos API.</p> <p>This class provides functionality to interact with datasets stored remotely, including uploading, downloading, and managing dataset data.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>The reference identifier for the dataset.</p> required <code>api_client</code> <code>ApiClient</code> <p>The API client instance for making requests.</p> required <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>The dataset reference identifier.</p> <code>api_client</code> <code>ApiClient</code> <p>The API client instance.</p> <code>metadata</code> <code>DatasetPreview</code> <p>The dataset metadata.</p> Source code in <code>focoos/hub/remote_dataset.py</code> <pre><code>class RemoteDataset:\n    \"\"\"\n    A class to manage remote datasets through the Focoos API.\n\n    This class provides functionality to interact with datasets stored remotely,\n    including uploading, downloading, and managing dataset data.\n\n    Args:\n        ref (str): The reference identifier for the dataset.\n        api_client (ApiClient): The API client instance for making requests.\n\n    Attributes:\n        ref (str): The dataset reference identifier.\n        api_client (ApiClient): The API client instance.\n        metadata (DatasetPreview): The dataset metadata.\n    \"\"\"\n\n    def __init__(self, ref: str, api_client: ApiClient):\n        self.ref = ref\n        self.api_client = api_client\n        self.metadata: DatasetPreview = self.get_info()\n\n    def get_info(self) -&gt; DatasetPreview:\n        \"\"\"\n        Retrieves the dataset information from the API.\n\n        Returns:\n            DatasetPreview: The dataset preview information.\n        \"\"\"\n        res = self.api_client.get(f\"datasets/{self.ref}\")\n        if res.status_code != 200:\n            raise ValueError(f\"Failed to get dataset info: {res.status_code} {res.text}\")\n        return DatasetPreview.from_json(res.json())\n\n    def upload_data(self, path: str) -&gt; Optional[DatasetSpec]:\n        \"\"\"\n        Uploads dataset data from a local zip file to the remote storage.\n\n        Args:\n            path (str): Local path to the zip file containing dataset data.\n\n        Returns:\n            Optional[DatasetSpec]: The dataset specification after successful upload.\n\n        Raises:\n            FileNotFoundError: If the specified file does not exist.\n            ValueError: If the file is not a zip file or upload fails.\n        \"\"\"\n        if not path.endswith(\".zip\"):\n            raise ValueError(\"Dataset must be .zip compressed\")\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"File not found: {path}\")\n\n        file_name = os.path.basename(path)\n        file_size = os.path.getsize(path)\n        file_size_mb = file_size / (1024 * 1024)\n        logger.info(f\"\ud83d\udd17 Requesting upload url for {file_name} of size {file_size_mb:.2f} MB\")\n        presigned_url = self.api_client.post(\n            f\"datasets/{self.ref}/generate-upload-url\",\n            data={\"file_size_bytes\": file_size, \"file_name\": file_name},\n        )\n        if presigned_url.status_code != 200:\n            raise ValueError(f\"Failed to generate upload url: {presigned_url.status_code} {presigned_url.text}\")\n        presigned_url = presigned_url.json()\n        fields = {k: v for k, v in presigned_url[\"fields\"].items()}\n        logger.info(f\"\ud83d\udce4 Uploading file {file_name}..\")\n\n        # Use context manager to properly handle file closure\n        with open(path, \"rb\") as file_obj:\n            fields[\"file\"] = (file_name, file_obj, \"application/zip\")\n\n            res = self.api_client.external_post(\n                presigned_url[\"url\"],\n                files=fields,\n                data=presigned_url[\"fields\"],\n                stream=True,\n            )\n\n        logger.info(\"\u2705 Upload file done.\")\n        if res.status_code not in [200, 201, 204]:\n            raise ValueError(f\"Failed to upload dataset: {res.status_code} {res.text}\")\n\n        logger.info(\"\ud83d\udd17 Validating dataset..\")\n        complete_upload = self.api_client.post(\n            f\"datasets/{self.ref}/complete-upload\",\n        )\n        if complete_upload.status_code not in [200, 201, 204]:\n            raise ValueError(f\"Failed to validate dataset: {complete_upload.status_code} {complete_upload.text}\")\n        self.metadata = self.get_info()\n        logger.info(f\"\u2705 Dataset validated! =&gt; {self.metadata.spec}\")\n        return self.metadata.spec\n\n    @property\n    def name(self):\n        return self.metadata.name\n\n    @property\n    def task(self):\n        return self.metadata.task\n\n    @property\n    def layout(self):\n        return self.metadata.layout\n\n    def download_data(self, path: str = DATASETS_DIR):\n        \"\"\"\n        Downloads the dataset data to a local path.\n\n        Args:\n            path (str): Local path where the dataset should be downloaded.\n\n        Returns:\n            str: The path where the file was downloaded.\n\n        Raises:\n            ValueError: If the download fails.\n        \"\"\"\n        res = self.api_client.get(f\"datasets/{self.ref}/download\")\n        if res.status_code != 200:\n            raise ValueError(f\"Failed to download dataset data: {res.status_code} {res.text}\")\n        url = res.json()[\"download_uri\"]\n\n        path = self.api_client.download_ext_file(url, path, skip_if_exists=True)\n        logger.info(f\"\u2705 Dataset data downloaded to {path}\")\n        return path\n\n    def __str__(self):\n        return f\"RemoteDataset(ref={self.ref}, name={self.name}, task={self.task}, layout={self.layout})\"\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_dataset.RemoteDataset.download_data","title":"<code>download_data(path=DATASETS_DIR)</code>","text":"<p>Downloads the dataset data to a local path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Local path where the dataset should be downloaded.</p> <code>DATASETS_DIR</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The path where the file was downloaded.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the download fails.</p> Source code in <code>focoos/hub/remote_dataset.py</code> <pre><code>def download_data(self, path: str = DATASETS_DIR):\n    \"\"\"\n    Downloads the dataset data to a local path.\n\n    Args:\n        path (str): Local path where the dataset should be downloaded.\n\n    Returns:\n        str: The path where the file was downloaded.\n\n    Raises:\n        ValueError: If the download fails.\n    \"\"\"\n    res = self.api_client.get(f\"datasets/{self.ref}/download\")\n    if res.status_code != 200:\n        raise ValueError(f\"Failed to download dataset data: {res.status_code} {res.text}\")\n    url = res.json()[\"download_uri\"]\n\n    path = self.api_client.download_ext_file(url, path, skip_if_exists=True)\n    logger.info(f\"\u2705 Dataset data downloaded to {path}\")\n    return path\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_dataset.RemoteDataset.get_info","title":"<code>get_info()</code>","text":"<p>Retrieves the dataset information from the API.</p> <p>Returns:</p> Name Type Description <code>DatasetPreview</code> <code>DatasetPreview</code> <p>The dataset preview information.</p> Source code in <code>focoos/hub/remote_dataset.py</code> <pre><code>def get_info(self) -&gt; DatasetPreview:\n    \"\"\"\n    Retrieves the dataset information from the API.\n\n    Returns:\n        DatasetPreview: The dataset preview information.\n    \"\"\"\n    res = self.api_client.get(f\"datasets/{self.ref}\")\n    if res.status_code != 200:\n        raise ValueError(f\"Failed to get dataset info: {res.status_code} {res.text}\")\n    return DatasetPreview.from_json(res.json())\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_dataset.RemoteDataset.upload_data","title":"<code>upload_data(path)</code>","text":"<p>Uploads dataset data from a local zip file to the remote storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Local path to the zip file containing dataset data.</p> required <p>Returns:</p> Type Description <code>Optional[DatasetSpec]</code> <p>Optional[DatasetSpec]: The dataset specification after successful upload.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>ValueError</code> <p>If the file is not a zip file or upload fails.</p> Source code in <code>focoos/hub/remote_dataset.py</code> <pre><code>def upload_data(self, path: str) -&gt; Optional[DatasetSpec]:\n    \"\"\"\n    Uploads dataset data from a local zip file to the remote storage.\n\n    Args:\n        path (str): Local path to the zip file containing dataset data.\n\n    Returns:\n        Optional[DatasetSpec]: The dataset specification after successful upload.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        ValueError: If the file is not a zip file or upload fails.\n    \"\"\"\n    if not path.endswith(\".zip\"):\n        raise ValueError(\"Dataset must be .zip compressed\")\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File not found: {path}\")\n\n    file_name = os.path.basename(path)\n    file_size = os.path.getsize(path)\n    file_size_mb = file_size / (1024 * 1024)\n    logger.info(f\"\ud83d\udd17 Requesting upload url for {file_name} of size {file_size_mb:.2f} MB\")\n    presigned_url = self.api_client.post(\n        f\"datasets/{self.ref}/generate-upload-url\",\n        data={\"file_size_bytes\": file_size, \"file_name\": file_name},\n    )\n    if presigned_url.status_code != 200:\n        raise ValueError(f\"Failed to generate upload url: {presigned_url.status_code} {presigned_url.text}\")\n    presigned_url = presigned_url.json()\n    fields = {k: v for k, v in presigned_url[\"fields\"].items()}\n    logger.info(f\"\ud83d\udce4 Uploading file {file_name}..\")\n\n    # Use context manager to properly handle file closure\n    with open(path, \"rb\") as file_obj:\n        fields[\"file\"] = (file_name, file_obj, \"application/zip\")\n\n        res = self.api_client.external_post(\n            presigned_url[\"url\"],\n            files=fields,\n            data=presigned_url[\"fields\"],\n            stream=True,\n        )\n\n    logger.info(\"\u2705 Upload file done.\")\n    if res.status_code not in [200, 201, 204]:\n        raise ValueError(f\"Failed to upload dataset: {res.status_code} {res.text}\")\n\n    logger.info(\"\ud83d\udd17 Validating dataset..\")\n    complete_upload = self.api_client.post(\n        f\"datasets/{self.ref}/complete-upload\",\n    )\n    if complete_upload.status_code not in [200, 201, 204]:\n        raise ValueError(f\"Failed to validate dataset: {complete_upload.status_code} {complete_upload.text}\")\n    self.metadata = self.get_info()\n    logger.info(f\"\u2705 Dataset validated! =&gt; {self.metadata.spec}\")\n    return self.metadata.spec\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel","title":"<code>RemoteModel</code>","text":"<p>Represents a remote model in the Focoos platform.</p> <p>Attributes:</p> Name Type Description <code>model_ref</code> <code>str</code> <p>Reference ID for the model.</p> <code>api_client</code> <code>ApiClient</code> <p>Client for making HTTP requests.</p> <code>model_info</code> <code>RemoteModelInfo</code> <p>Model information of the model.</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>class RemoteModel:\n    \"\"\"\n    Represents a remote model in the Focoos platform.\n\n    Attributes:\n        model_ref (str): Reference ID for the model.\n        api_client (ApiClient): Client for making HTTP requests.\n        model_info (RemoteModelInfo): Model information of the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_ref: str,\n        api_client: ApiClient,\n    ):\n        \"\"\"\n        Initialize the RemoteModel instance.\n\n        Args:\n            model_ref (str): Reference ID for the model.\n            api_client (ApiClient): HTTP client instance for communication.\n\n        Raises:\n            ValueError: If model metadata retrieval fails.\n        \"\"\"\n        self.model_ref = model_ref\n        self.api_client = api_client\n        self.model_info: RemoteModelInfo = self.get_info()\n\n        logger.info(\n            f\"[RemoteModel]: ref: {self.model_ref} name: {self.model_info.name} description: {self.model_info.description} status: {self.model_info.status}\"\n        )\n\n    @property\n    def ref(self) -&gt; str:\n        return self.model_ref\n\n    def get_info(self) -&gt; RemoteModelInfo:\n        \"\"\"\n        Retrieve model metadata.\n\n        Returns:\n            ModelMetadata: Metadata of the model.\n\n        Raises:\n            ValueError: If the request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos, RemoteModel\n\n            focoos = Focoos()\n            model = focoos.get_remote_model(model_ref=\"&lt;model_ref&gt;\")\n            model_info = model.get_info()\n            ```\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n        self.metadata = RemoteModelInfo(**res.json())\n        return self.metadata\n\n    def sync_local_training_job(\n        self, local_training_info: HubSyncLocalTraining, dir: str, upload_artifacts: Optional[List[ArtifactName]] = None\n    ) -&gt; None:\n        if not os.path.exists(os.path.join(dir, ArtifactName.INFO)):\n            logger.warning(f\"Model info not found in {dir}\")\n            raise ValueError(f\"Model info not found in {dir}\")\n        metrics = parse_metrics(os.path.join(dir, ArtifactName.METRICS))\n        local_training_info.metrics = metrics\n        logger.debug(\n            f\"[Syncing Training] iter: {metrics.iterations} {self.metadata.name} status: {local_training_info.status} ref: {self.model_ref}\"\n        )\n\n        ## Update metrics\n        res = self.api_client.patch(\n            f\"models/{self.model_ref}/sync-local-training\",\n            data=asdict(local_training_info),\n        )\n        if res.status_code != 200:\n            logger.error(f\"Failed to sync local training: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to sync local training: {res.status_code} {res.text}\")\n        if upload_artifacts:\n            for artifact in [ArtifactName.METRICS, ArtifactName.WEIGHTS]:\n                file_path = os.path.join(dir, artifact.value)\n                if os.path.isfile(file_path):\n                    try:\n                        self._upload_model_artifact(file_path)\n                    except Exception:\n                        logger.error(f\"Failed to upload artifact: {artifact.value}\")\n                        pass\n\n    def _upload_model_artifact(self, path: str) -&gt; None:\n        \"\"\"\n        Uploads an model artifact to the Focoos platform.\n        \"\"\"\n        if not os.path.exists(path):\n            raise ValueError(f\"File not found: {path}\")\n        file_ext = os.path.splitext(path)[1]\n        if file_ext not in [\".pt\", \".onnx\", \".pth\", \".json\", \".txt\"]:\n            raise ValueError(f\"Unsupported file extension: {file_ext}\")\n        file_name = os.path.basename(path)\n        file_size = os.path.getsize(path)\n        file_size_mb = file_size / (1024 * 1024)\n        logger.debug(f\"\ud83d\udd17 Requesting upload url for {file_name} of size {file_size_mb:.2f} MB\")\n        presigned_url = self.api_client.post(\n            f\"models/{self.model_ref}/generate-upload-url\",\n            data={\"file_size_bytes\": file_size, \"file_name\": file_name},\n        )\n        if presigned_url.status_code != 200:\n            raise ValueError(f\"Failed to generate upload url: {presigned_url.status_code} {presigned_url.text}\")\n        presigned_url = presigned_url.json()\n        fields = {k: v for k, v in presigned_url[\"fields\"].items()}\n        logger.info(f\"\ud83d\udce4 Uploading file {file_name} to HUB, size: {file_size_mb:.2f} MB\")\n        fields[\"file\"] = (file_name, open(path, \"rb\"))\n        res = self.api_client.external_post(\n            presigned_url[\"url\"],\n            files=fields,\n            data=presigned_url[\"fields\"],\n        )\n        if res.status_code not in [200, 201, 204]:\n            raise ValueError(f\"Failed to upload model artifact: {res.status_code} {res.text}\")\n        logger.info(f\"\u2705 Model artifact {file_name} uploaded to HUB.\")\n\n    def train_info(self) -&gt; Optional[TrainingInfo]:\n        \"\"\"\n        Retrieve the current status of the model training.\n\n        Sends a request to check the training status of the model referenced by `self.model_ref`.\n\n        Returns:\n            dict: A dictionary containing the training status information.\n\n        Raises:\n            ValueError: If the request to get training status fails.\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}/train/status\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get train info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get train info: {res.status_code} {res.text}\")\n        dct = {k: v for k, v in res.json().items() if k in TrainingInfo.__dataclass_fields__}\n        return TrainingInfo(**dct)\n\n    def train_logs(self) -&gt; list[str]:\n        \"\"\"\n        Retrieve the training logs for the model.\n\n        This method sends a request to fetch the logs of the model's training process. If the request\n        is successful (status code 200), it returns the logs as a list of strings. If the request fails,\n        it logs a warning and returns an empty list.\n\n        Returns:\n            list[str]: A list of training logs as strings.\n\n        Raises:\n            None: Returns an empty list if the request fails.\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}/train/logs\")\n        if res.status_code != 200:\n            logger.warning(f\"Failed to get train logs: {res.status_code} {res.text}\")\n            return []\n        return res.json()\n\n    def metrics(self) -&gt; Metrics:  # noqa: F821\n        \"\"\"\n        Retrieve the metrics of the model.\n\n        This method sends a request to fetch the metrics of the model identified by `model_ref`.\n        If the request is successful (status code 200), it returns the metrics as a `Metrics` object.\n        If the request fails, it logs a warning and returns an empty `Metrics` object.\n\n        Returns:\n            Metrics: An object containing the metrics of the model.\n\n        Raises:\n            None: Returns an empty `Metrics` object if the request fails.\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}/metrics\")\n        if res.status_code != 200:\n            logger.warning(f\"Failed to get metrics: {res.status_code} {res.text}\")\n            return Metrics()  # noqa: F821\n        return Metrics(**{k: v for k, v in res.json().items() if k in Metrics.__dataclass_fields__})\n\n    def __call__(\n        self, image: Union[str, Path, np.ndarray, bytes, Image.Image], threshold: float = 0.5\n    ) -&gt; FocoosDetections:\n        return self.infer(image, threshold)\n\n    def infer(\n        self,\n        image: Union[str, Path, np.ndarray, bytes, Image.Image],\n        threshold: float = 0.5,\n    ) -&gt; FocoosDetections:\n        \"\"\"\n        Perform inference on the provided image using the remote model.\n\n        This method sends an image to the remote model for inference and retrieves the detection results.\n\n        Args:\n            image (Union[str, Path, np.ndarray, bytes]): The image to infer on, which can be a file path,\n                a string representing the path, a NumPy array, or raw bytes.\n            threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n                Detections with confidence scores below this threshold will be discarded.\n\n        Returns:\n            FocoosDetections: The detection results including class IDs, confidence scores, bounding boxes,\n                and segmentation masks (if applicable).\n\n        Raises:\n            FileNotFoundError: If the provided image file path is invalid.\n            ValueError: If the inference request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            model = focoos.get_remote_model(\"my-model\")\n            results = model.infer(\"image.jpg\", threshold=0.5)\n\n            # Print detection results\n            for det in results.detections:\n                print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n                print(f\"Bounding box: {det.bbox}\")\n                if det.mask:\n                    print(\"Instance segmentation mask included\")\n            ```\n        \"\"\"\n        if isinstance(image, Image.Image):\n            image = np.array(image)\n\n        image_bytes = None\n        if isinstance(image, str) or isinstance(image, Path):\n            if not os.path.exists(image):\n                logger.error(f\"Image file not found: {image}\")\n                raise FileNotFoundError(f\"Image file not found: {image}\")\n            image_bytes = open(image, \"rb\").read()\n        elif isinstance(image, np.ndarray):\n            _, buffer = cv2.imencode(\".jpg\", image)\n            image_bytes = buffer.tobytes()\n        else:\n            image_bytes = image\n        files = {\"file\": (\"image\", image_bytes, \"image/jpeg\")}\n        t0 = time.time()\n        res = self.api_client.post(\n            f\"models/{self.model_ref}/inference?confidence_threshold={threshold}\",\n            files=files,\n        )\n        t1 = time.time()\n        if res.status_code == 200:\n            detections = FocoosDetections(\n                detections=[FocoosDet.from_json(d) for d in res.json().get(\"detections\", [])],\n                latency=res.json().get(\"latency\", None),\n            )\n            logger.debug(\n                f\"Found {len(detections.detections)} detections. Inference Request time: {(t1 - t0) * 1000:.0f}ms\"\n            )\n\n            return detections\n        else:\n            logger.error(f\"Failed to infer: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to infer: {res.status_code} {res.text}\")\n\n    def notebook_monitor_train(self, interval: int = 30, plot_metrics: bool = False, max_runtime: int = 36000) -&gt; None:\n        \"\"\"\n        Monitor the training process in a Jupyter notebook and display metrics.\n\n        Periodically checks the training status and displays metrics in a notebook cell.\n        Clears previous output to maintain a clean view.\n\n        Args:\n            interval (int): Time between status checks in seconds. Must be 30-240. Default: 30\n            plot_metrics (bool): Whether to plot metrics graphs. Default: False\n            max_runtime (int): Maximum monitoring time in seconds. Default: 36000 (10 hours)\n\n        Returns:\n            None\n        \"\"\"\n        from IPython.display import clear_output\n\n        if not 30 &lt;= interval &lt;= 240:\n            raise ValueError(\"Interval must be between 30 and 240 seconds\")\n\n        last_update = self.get_info().updated_at\n        start_time = time.time()\n        status_history = []\n\n        while True:\n            # Get current status\n            model_info = self.get_info()\n            status = model_info.status\n\n            # Clear and display status\n            clear_output(wait=True)\n            status_msg = f\"[Live Monitor {self.metadata.name}] {status.value}\"\n            status_history.append(status_msg)\n            for msg in status_history:\n                logger.info(msg)\n\n            # Show metrics if training completed\n            if status == ModelStatus.TRAINING_COMPLETED:\n                metrics = self.metrics()\n                if metrics.best_valid_metric:\n                    logger.info(f\"Best Checkpoint (iter: {metrics.best_valid_metric.get('iteration', 'N/A')}):\")\n                    for k, v in metrics.best_valid_metric.items():\n                        logger.info(f\"  {k}: {v}\")\n                    visualizer = MetricsVisualizer(metrics)\n                    visualizer.log_metrics()\n                    if plot_metrics:\n                        visualizer.notebook_plot_training_metrics()\n\n            # Update metrics during training\n            if status == ModelStatus.TRAINING_RUNNING and model_info.updated_at &gt; last_update:\n                last_update = model_info.updated_at\n                metrics = self.metrics()\n                visualizer = MetricsVisualizer(metrics)\n                visualizer.log_metrics()\n                if plot_metrics:\n                    visualizer.notebook_plot_training_metrics()\n\n            # Check exit conditions\n            if status not in [ModelStatus.CREATED, ModelStatus.TRAINING_RUNNING, ModelStatus.TRAINING_STARTING]:\n                return\n\n            if time.time() - start_time &gt; max_runtime:\n                logger.warning(f\"Monitoring exceeded {max_runtime} seconds limit\")\n                return\n\n            sleep(interval)\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.__init__","title":"<code>__init__(model_ref, api_client)</code>","text":"<p>Initialize the RemoteModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference ID for the model.</p> required <code>api_client</code> <code>ApiClient</code> <p>HTTP client instance for communication.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If model metadata retrieval fails.</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def __init__(\n    self,\n    model_ref: str,\n    api_client: ApiClient,\n):\n    \"\"\"\n    Initialize the RemoteModel instance.\n\n    Args:\n        model_ref (str): Reference ID for the model.\n        api_client (ApiClient): HTTP client instance for communication.\n\n    Raises:\n        ValueError: If model metadata retrieval fails.\n    \"\"\"\n    self.model_ref = model_ref\n    self.api_client = api_client\n    self.model_info: RemoteModelInfo = self.get_info()\n\n    logger.info(\n        f\"[RemoteModel]: ref: {self.model_ref} name: {self.model_info.name} description: {self.model_info.description} status: {self.model_info.status}\"\n    )\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.get_info","title":"<code>get_info()</code>","text":"<p>Retrieve model metadata.</p> <p>Returns:</p> Name Type Description <code>ModelMetadata</code> <code>RemoteModelInfo</code> <p>Metadata of the model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request fails.</p> Example <pre><code>from focoos import Focoos, RemoteModel\n\nfocoos = Focoos()\nmodel = focoos.get_remote_model(model_ref=\"&lt;model_ref&gt;\")\nmodel_info = model.get_info()\n</code></pre> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def get_info(self) -&gt; RemoteModelInfo:\n    \"\"\"\n    Retrieve model metadata.\n\n    Returns:\n        ModelMetadata: Metadata of the model.\n\n    Raises:\n        ValueError: If the request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos, RemoteModel\n\n        focoos = Focoos()\n        model = focoos.get_remote_model(model_ref=\"&lt;model_ref&gt;\")\n        model_info = model.get_info()\n        ```\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n    self.metadata = RemoteModelInfo(**res.json())\n    return self.metadata\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.infer","title":"<code>infer(image, threshold=0.5)</code>","text":"<p>Perform inference on the provided image using the remote model.</p> <p>This method sends an image to the remote model for inference and retrieves the detection results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Path, ndarray, bytes]</code> <p>The image to infer on, which can be a file path, a string representing the path, a NumPy array, or raw bytes.</p> required <code>threshold</code> <code>float</code> <p>The confidence threshold for detections. Defaults to 0.5. Detections with confidence scores below this threshold will be discarded.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>FocoosDetections</code> <code>FocoosDetections</code> <p>The detection results including class IDs, confidence scores, bounding boxes, and segmentation masks (if applicable).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the provided image file path is invalid.</p> <code>ValueError</code> <p>If the inference request fails.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\nmodel = focoos.get_remote_model(\"my-model\")\nresults = model.infer(\"image.jpg\", threshold=0.5)\n\n# Print detection results\nfor det in results.detections:\n    print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n    print(f\"Bounding box: {det.bbox}\")\n    if det.mask:\n        print(\"Instance segmentation mask included\")\n</code></pre> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def infer(\n    self,\n    image: Union[str, Path, np.ndarray, bytes, Image.Image],\n    threshold: float = 0.5,\n) -&gt; FocoosDetections:\n    \"\"\"\n    Perform inference on the provided image using the remote model.\n\n    This method sends an image to the remote model for inference and retrieves the detection results.\n\n    Args:\n        image (Union[str, Path, np.ndarray, bytes]): The image to infer on, which can be a file path,\n            a string representing the path, a NumPy array, or raw bytes.\n        threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n            Detections with confidence scores below this threshold will be discarded.\n\n    Returns:\n        FocoosDetections: The detection results including class IDs, confidence scores, bounding boxes,\n            and segmentation masks (if applicable).\n\n    Raises:\n        FileNotFoundError: If the provided image file path is invalid.\n        ValueError: If the inference request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        model = focoos.get_remote_model(\"my-model\")\n        results = model.infer(\"image.jpg\", threshold=0.5)\n\n        # Print detection results\n        for det in results.detections:\n            print(f\"Found {det.label} with confidence {det.conf:.2f}\")\n            print(f\"Bounding box: {det.bbox}\")\n            if det.mask:\n                print(\"Instance segmentation mask included\")\n        ```\n    \"\"\"\n    if isinstance(image, Image.Image):\n        image = np.array(image)\n\n    image_bytes = None\n    if isinstance(image, str) or isinstance(image, Path):\n        if not os.path.exists(image):\n            logger.error(f\"Image file not found: {image}\")\n            raise FileNotFoundError(f\"Image file not found: {image}\")\n        image_bytes = open(image, \"rb\").read()\n    elif isinstance(image, np.ndarray):\n        _, buffer = cv2.imencode(\".jpg\", image)\n        image_bytes = buffer.tobytes()\n    else:\n        image_bytes = image\n    files = {\"file\": (\"image\", image_bytes, \"image/jpeg\")}\n    t0 = time.time()\n    res = self.api_client.post(\n        f\"models/{self.model_ref}/inference?confidence_threshold={threshold}\",\n        files=files,\n    )\n    t1 = time.time()\n    if res.status_code == 200:\n        detections = FocoosDetections(\n            detections=[FocoosDet.from_json(d) for d in res.json().get(\"detections\", [])],\n            latency=res.json().get(\"latency\", None),\n        )\n        logger.debug(\n            f\"Found {len(detections.detections)} detections. Inference Request time: {(t1 - t0) * 1000:.0f}ms\"\n        )\n\n        return detections\n    else:\n        logger.error(f\"Failed to infer: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to infer: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.metrics","title":"<code>metrics()</code>","text":"<p>Retrieve the metrics of the model.</p> <p>This method sends a request to fetch the metrics of the model identified by <code>model_ref</code>. If the request is successful (status code 200), it returns the metrics as a <code>Metrics</code> object. If the request fails, it logs a warning and returns an empty <code>Metrics</code> object.</p> <p>Returns:</p> Name Type Description <code>Metrics</code> <code>Metrics</code> <p>An object containing the metrics of the model.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns an empty <code>Metrics</code> object if the request fails.</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def metrics(self) -&gt; Metrics:  # noqa: F821\n    \"\"\"\n    Retrieve the metrics of the model.\n\n    This method sends a request to fetch the metrics of the model identified by `model_ref`.\n    If the request is successful (status code 200), it returns the metrics as a `Metrics` object.\n    If the request fails, it logs a warning and returns an empty `Metrics` object.\n\n    Returns:\n        Metrics: An object containing the metrics of the model.\n\n    Raises:\n        None: Returns an empty `Metrics` object if the request fails.\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}/metrics\")\n    if res.status_code != 200:\n        logger.warning(f\"Failed to get metrics: {res.status_code} {res.text}\")\n        return Metrics()  # noqa: F821\n    return Metrics(**{k: v for k, v in res.json().items() if k in Metrics.__dataclass_fields__})\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.notebook_monitor_train","title":"<code>notebook_monitor_train(interval=30, plot_metrics=False, max_runtime=36000)</code>","text":"<p>Monitor the training process in a Jupyter notebook and display metrics.</p> <p>Periodically checks the training status and displays metrics in a notebook cell. Clears previous output to maintain a clean view.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>int</code> <p>Time between status checks in seconds. Must be 30-240. Default: 30</p> <code>30</code> <code>plot_metrics</code> <code>bool</code> <p>Whether to plot metrics graphs. Default: False</p> <code>False</code> <code>max_runtime</code> <code>int</code> <p>Maximum monitoring time in seconds. Default: 36000 (10 hours)</p> <code>36000</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def notebook_monitor_train(self, interval: int = 30, plot_metrics: bool = False, max_runtime: int = 36000) -&gt; None:\n    \"\"\"\n    Monitor the training process in a Jupyter notebook and display metrics.\n\n    Periodically checks the training status and displays metrics in a notebook cell.\n    Clears previous output to maintain a clean view.\n\n    Args:\n        interval (int): Time between status checks in seconds. Must be 30-240. Default: 30\n        plot_metrics (bool): Whether to plot metrics graphs. Default: False\n        max_runtime (int): Maximum monitoring time in seconds. Default: 36000 (10 hours)\n\n    Returns:\n        None\n    \"\"\"\n    from IPython.display import clear_output\n\n    if not 30 &lt;= interval &lt;= 240:\n        raise ValueError(\"Interval must be between 30 and 240 seconds\")\n\n    last_update = self.get_info().updated_at\n    start_time = time.time()\n    status_history = []\n\n    while True:\n        # Get current status\n        model_info = self.get_info()\n        status = model_info.status\n\n        # Clear and display status\n        clear_output(wait=True)\n        status_msg = f\"[Live Monitor {self.metadata.name}] {status.value}\"\n        status_history.append(status_msg)\n        for msg in status_history:\n            logger.info(msg)\n\n        # Show metrics if training completed\n        if status == ModelStatus.TRAINING_COMPLETED:\n            metrics = self.metrics()\n            if metrics.best_valid_metric:\n                logger.info(f\"Best Checkpoint (iter: {metrics.best_valid_metric.get('iteration', 'N/A')}):\")\n                for k, v in metrics.best_valid_metric.items():\n                    logger.info(f\"  {k}: {v}\")\n                visualizer = MetricsVisualizer(metrics)\n                visualizer.log_metrics()\n                if plot_metrics:\n                    visualizer.notebook_plot_training_metrics()\n\n        # Update metrics during training\n        if status == ModelStatus.TRAINING_RUNNING and model_info.updated_at &gt; last_update:\n            last_update = model_info.updated_at\n            metrics = self.metrics()\n            visualizer = MetricsVisualizer(metrics)\n            visualizer.log_metrics()\n            if plot_metrics:\n                visualizer.notebook_plot_training_metrics()\n\n        # Check exit conditions\n        if status not in [ModelStatus.CREATED, ModelStatus.TRAINING_RUNNING, ModelStatus.TRAINING_STARTING]:\n            return\n\n        if time.time() - start_time &gt; max_runtime:\n            logger.warning(f\"Monitoring exceeded {max_runtime} seconds limit\")\n            return\n\n        sleep(interval)\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.train_info","title":"<code>train_info()</code>","text":"<p>Retrieve the current status of the model training.</p> <p>Sends a request to check the training status of the model referenced by <code>self.model_ref</code>.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[TrainingInfo]</code> <p>A dictionary containing the training status information.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request to get training status fails.</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def train_info(self) -&gt; Optional[TrainingInfo]:\n    \"\"\"\n    Retrieve the current status of the model training.\n\n    Sends a request to check the training status of the model referenced by `self.model_ref`.\n\n    Returns:\n        dict: A dictionary containing the training status information.\n\n    Raises:\n        ValueError: If the request to get training status fails.\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}/train/status\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get train info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get train info: {res.status_code} {res.text}\")\n    dct = {k: v for k, v in res.json().items() if k in TrainingInfo.__dataclass_fields__}\n    return TrainingInfo(**dct)\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.train_logs","title":"<code>train_logs()</code>","text":"<p>Retrieve the training logs for the model.</p> <p>This method sends a request to fetch the logs of the model's training process. If the request is successful (status code 200), it returns the logs as a list of strings. If the request fails, it logs a warning and returns an empty list.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of training logs as strings.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns an empty list if the request fails.</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def train_logs(self) -&gt; list[str]:\n    \"\"\"\n    Retrieve the training logs for the model.\n\n    This method sends a request to fetch the logs of the model's training process. If the request\n    is successful (status code 200), it returns the logs as a list of strings. If the request fails,\n    it logs a warning and returns an empty list.\n\n    Returns:\n        list[str]: A list of training logs as strings.\n\n    Raises:\n        None: Returns an empty list if the request fails.\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}/train/logs\")\n    if res.status_code != 200:\n        logger.warning(f\"Failed to get train logs: {res.status_code} {res.text}\")\n        return []\n    return res.json()\n</code></pre>"},{"location":"api/infer_model/","title":"InferModel","text":"<p>InferModel Module</p> <p>This module provides the <code>InferModel</code> class that allows loading, inference, and benchmark testing of models in a local environment. It supports detection and segmentation tasks, and utilizes various runtime backends including ONNXRuntime and TorchScript for model execution.</p> <p>Classes:</p> Name Description <code>InferModel</code> <p>A class for managing and interacting with local models.</p> <p>Functions:</p> Name Description <code>__init__</code> <p>Initializes the InferModel instance, loading the model, metadata,       and setting up the runtime.</p> <code>_read_metadata</code> <p>Reads the model metadata from a JSON file.</p> <code>_annotate</code> <p>Annotates the input image with detection or segmentation results.</p> <code>infer</code> <p>Runs inference on an input image, with optional annotation.</p> <code>benchmark</code> <p>Benchmarks the model's inference performance over a specified        number of iterations and input size.</p>"},{"location":"api/infer_model/#focoos.infer.infer_model.InferModel","title":"<code>InferModel</code>","text":"Source code in <code>focoos/infer/infer_model.py</code> <pre><code>class InferModel:\n    def __init__(\n        self,\n        model_dir: Union[str, Path],\n        runtime_type: Optional[RuntimeType] = None,\n    ):\n        \"\"\"\n        Initialize a LocalModel instance.\n\n        This class sets up a local model for inference by initializing the runtime environment,\n        loading metadata, and preparing annotation utilities.\n\n        Args:\n            model_dir (Union[str, Path]): The path to the directory containing the model files.\n            runtime_type (Optional[RuntimeTypes]): Specifies the runtime type to use for inference.\n                Defaults to the value of `FOCOOS_CONFIG.runtime_type` if not provided.\n\n        Raises:\n            ValueError: If no runtime type is provided and `FOCOOS_CONFIG.runtime_type` is not set.\n            FileNotFoundError: If the specified model directory does not exist.\n\n        Attributes:\n            model_dir (Union[str, Path]): Path to the model directory.\n            metadata (ModelMetadata): Metadata information for the model.\n            model_ref: Reference identifier for the model obtained from metadata.\n            label_annotator (sv.LabelAnnotator): Utility for adding labels to the output,\n                initialized with text padding and border radius.\n            box_annotator (sv.BoxAnnotator): Utility for annotating bounding boxes.\n            mask_annotator (sv.MaskAnnotator): Utility for annotating masks.\n            runtime (ONNXRuntime): Inference runtime initialized with the specified runtime type,\n                model path, metadata, and warmup iterations.\n\n        The method verifies the existence of the model directory, reads the model metadata,\n        and initializes the runtime for inference using the provided runtime type. Annotation\n        utilities are also prepared for visualizing model outputs.\n        \"\"\"\n\n        # Determine runtime type and model format\n        runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n        extension = ModelExtension.from_runtime_type(runtime_type)\n\n        # Set model directory and path\n        self.model_dir: Union[str, Path] = model_dir\n        self.model_path = os.path.join(model_dir, f\"model.{extension.value}\")\n        logger.debug(f\"Runtime type: {runtime_type}, Loading model from {self.model_path}..\")\n\n        # Check if model path exists\n        if not os.path.exists(self.model_path):\n            raise FileNotFoundError(f\"Model path not found: {self.model_path}\")\n\n        # Load metadata and set model reference\n        # self.metadata: RemoteModelInfo = self._read_metadata()\n\n        self.model_info: ModelInfo = self._read_model_info()\n\n        try:\n            from focoos.model_manager import ConfigManager\n\n            model_config = ConfigManager.from_dict(self.model_info.model_family, self.model_info.config)\n            self.processor = ProcessorManager.get_processor(self.model_info.model_family, model_config)\n        except Exception as e:\n            logger.error(f\"Error creating model config: {e}\")\n            raise e\n\n        # Initialize annotation utilities\n        self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n        self.box_annotator = sv.BoxAnnotator()\n        self.mask_annotator = sv.MaskAnnotator()\n\n        # Load runtime for inference\n        self.runtime: BaseRuntime = load_runtime(\n            runtime_type,\n            str(self.model_path),\n            self.model_info,\n            FOCOOS_CONFIG.warmup_iter,\n        )\n\n    def _read_model_info(self) -&gt; ModelInfo:\n        \"\"\"\n        Reads the model info from a JSON file.\n        \"\"\"\n        model_info_path = os.path.join(self.model_dir, \"model_info.json\")\n        if not os.path.exists(model_info_path):\n            raise FileNotFoundError(f\"Model info file not found: {model_info_path}\")\n        return ModelInfo.from_json(model_info_path)\n\n    def __call__(\n        self, image: Union[bytes, str, Path, np.ndarray, Image.Image], threshold: Optional[float] = None\n    ) -&gt; FocoosDetections:\n        return self.infer(image, threshold)\n\n    def infer(\n        self,\n        image: Union[bytes, str, Path, np.ndarray, Image.Image],\n        threshold: Optional[float] = None,\n    ) -&gt; FocoosDetections:\n        \"\"\"\n        Run inference on an input image and optionally annotate the results.\n\n        Args:\n            image (Union[bytes, str, Path, np.ndarray, Image.Image]): The input image to infer on.\n                This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.\n            threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n                Detections with confidence scores below this threshold will be discarded.\n\n        Returns:\n            FocoosDetections: The detections from the inference, represented as a custom object (`FocoosDetections`).\n\n        Raises:\n            ValueError: If the model is not deployed locally (i.e., `self.runtime` is `None`).\n\n        Example:\n            ```python\n            from focoos import Focoos, LocalModel\n\n            focoos = Focoos()\n            model = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\n            detections = model.infer(image, threshold=0.5)\n            ```\n        \"\"\"\n        assert self.runtime is not None, \"Model is not deployed (locally)\"\n\n        t0 = perf_counter()\n        im1, im0 = image_preprocess(image)\n        tensors, _ = self.processor.preprocess(inputs=im1, device=\"cuda\", image_size=self.model_info.im_size)\n        logger.debug(f\"Input image size: {im0.shape}\")\n        t1 = perf_counter()\n\n        raw_detections = self.runtime(tensors)\n\n        t2 = perf_counter()\n        detections = self.processor.export_postprocess(\n            raw_detections, im0, threshold=threshold, class_names=self.model_info.classes\n        )\n        t3 = perf_counter()\n        latency = {\n            \"inference\": round(t2 - t1, 3),\n            \"preprocess\": round(t1 - t0, 3),\n            \"postprocess\": round(t3 - t2, 3),\n        }\n        res = detections[0]  #!TODO  check for batching\n        res.latency = latency\n\n        logger.debug(\n            f\"Found {len(res)} detections. Inference time: {(t2 - t1) * 1000:.0f}ms, preprocess: {(t1 - t0) * 1000:.0f}ms, postprocess: {(t3 - t2) * 1000:.0f}ms\"\n        )\n        return res\n\n    def benchmark(self, iterations: int = 50, size: Optional[Union[int, Tuple[int, int]]] = None) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model's inference performance over multiple iterations.\n        \"\"\"\n        if size is None:\n            size = self.model_info.im_size\n        if isinstance(size, int):\n            size = (size, size)\n        return self.runtime.benchmark(iterations, size)\n\n    def end2end_benchmark(\n        self, iterations: int = 50, size: Optional[Union[int, Tuple[int, int]]] = None\n    ) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model's inference performance over multiple iterations.\n\n        Args:\n            iterations (int): Number of iterations to run for benchmarking.\n            size (int): The input size for each benchmark iteration.\n\n        Returns:\n            LatencyMetrics: Latency metrics including time taken for inference.\n\n        Example:\n            ```python\n            from focoos import Focoos, LocalModel\n\n            focoos = Focoos()\n            model = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\n            metrics = model.end2end_benchmark(iterations=10, size=640)\n\n            # Access latency metrics\n            print(f\"FPS: {metrics.fps}\")\n            print(f\"Mean latency: {metrics.mean} ms\")\n            print(f\"Engine: {metrics.engine}\")\n            print(f\"Device: {metrics.device}\")\n            print(f\"Input size: {metrics.im_size}x{metrics.im_size}\")\n            ```\n        \"\"\"\n        if size is None:\n            size = self.model_info.im_size\n        if isinstance(size, int):\n            size = (size, size)\n\n        engine, device = self.runtime.get_info()\n        logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device}, size: {size}x{size}..\")\n\n        np_input = (255 * np.random.random((size[0], size[1], 3))).astype(np.uint8)\n\n        durations = []\n        for step in range(iterations + 5):\n            start = perf_counter()\n            self(np_input)\n            end = perf_counter()\n\n            if step &gt;= 5:  # Skip first 5 iterations\n                durations.append((end - start) * 1000)\n\n        durations = np.array(durations)\n\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean()),\n            engine=engine,\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n            device=device,\n        )\n        logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n</code></pre>"},{"location":"api/infer_model/#focoos.infer.infer_model.InferModel.__init__","title":"<code>__init__(model_dir, runtime_type=None)</code>","text":"<p>Initialize a LocalModel instance.</p> <p>This class sets up a local model for inference by initializing the runtime environment, loading metadata, and preparing annotation utilities.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Union[str, Path]</code> <p>The path to the directory containing the model files.</p> required <code>runtime_type</code> <code>Optional[RuntimeTypes]</code> <p>Specifies the runtime type to use for inference. Defaults to the value of <code>FOCOOS_CONFIG.runtime_type</code> if not provided.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no runtime type is provided and <code>FOCOOS_CONFIG.runtime_type</code> is not set.</p> <code>FileNotFoundError</code> <p>If the specified model directory does not exist.</p> <p>Attributes:</p> Name Type Description <code>model_dir</code> <code>Union[str, Path]</code> <p>Path to the model directory.</p> <code>metadata</code> <code>ModelMetadata</code> <p>Metadata information for the model.</p> <code>model_ref</code> <code>ModelMetadata</code> <p>Reference identifier for the model obtained from metadata.</p> <code>label_annotator</code> <code>LabelAnnotator</code> <p>Utility for adding labels to the output, initialized with text padding and border radius.</p> <code>box_annotator</code> <code>BoxAnnotator</code> <p>Utility for annotating bounding boxes.</p> <code>mask_annotator</code> <code>MaskAnnotator</code> <p>Utility for annotating masks.</p> <code>runtime</code> <code>ONNXRuntime</code> <p>Inference runtime initialized with the specified runtime type, model path, metadata, and warmup iterations.</p> <p>The method verifies the existence of the model directory, reads the model metadata, and initializes the runtime for inference using the provided runtime type. Annotation utilities are also prepared for visualizing model outputs.</p> Source code in <code>focoos/infer/infer_model.py</code> <pre><code>def __init__(\n    self,\n    model_dir: Union[str, Path],\n    runtime_type: Optional[RuntimeType] = None,\n):\n    \"\"\"\n    Initialize a LocalModel instance.\n\n    This class sets up a local model for inference by initializing the runtime environment,\n    loading metadata, and preparing annotation utilities.\n\n    Args:\n        model_dir (Union[str, Path]): The path to the directory containing the model files.\n        runtime_type (Optional[RuntimeTypes]): Specifies the runtime type to use for inference.\n            Defaults to the value of `FOCOOS_CONFIG.runtime_type` if not provided.\n\n    Raises:\n        ValueError: If no runtime type is provided and `FOCOOS_CONFIG.runtime_type` is not set.\n        FileNotFoundError: If the specified model directory does not exist.\n\n    Attributes:\n        model_dir (Union[str, Path]): Path to the model directory.\n        metadata (ModelMetadata): Metadata information for the model.\n        model_ref: Reference identifier for the model obtained from metadata.\n        label_annotator (sv.LabelAnnotator): Utility for adding labels to the output,\n            initialized with text padding and border radius.\n        box_annotator (sv.BoxAnnotator): Utility for annotating bounding boxes.\n        mask_annotator (sv.MaskAnnotator): Utility for annotating masks.\n        runtime (ONNXRuntime): Inference runtime initialized with the specified runtime type,\n            model path, metadata, and warmup iterations.\n\n    The method verifies the existence of the model directory, reads the model metadata,\n    and initializes the runtime for inference using the provided runtime type. Annotation\n    utilities are also prepared for visualizing model outputs.\n    \"\"\"\n\n    # Determine runtime type and model format\n    runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n    extension = ModelExtension.from_runtime_type(runtime_type)\n\n    # Set model directory and path\n    self.model_dir: Union[str, Path] = model_dir\n    self.model_path = os.path.join(model_dir, f\"model.{extension.value}\")\n    logger.debug(f\"Runtime type: {runtime_type}, Loading model from {self.model_path}..\")\n\n    # Check if model path exists\n    if not os.path.exists(self.model_path):\n        raise FileNotFoundError(f\"Model path not found: {self.model_path}\")\n\n    # Load metadata and set model reference\n    # self.metadata: RemoteModelInfo = self._read_metadata()\n\n    self.model_info: ModelInfo = self._read_model_info()\n\n    try:\n        from focoos.model_manager import ConfigManager\n\n        model_config = ConfigManager.from_dict(self.model_info.model_family, self.model_info.config)\n        self.processor = ProcessorManager.get_processor(self.model_info.model_family, model_config)\n    except Exception as e:\n        logger.error(f\"Error creating model config: {e}\")\n        raise e\n\n    # Initialize annotation utilities\n    self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n    self.box_annotator = sv.BoxAnnotator()\n    self.mask_annotator = sv.MaskAnnotator()\n\n    # Load runtime for inference\n    self.runtime: BaseRuntime = load_runtime(\n        runtime_type,\n        str(self.model_path),\n        self.model_info,\n        FOCOOS_CONFIG.warmup_iter,\n    )\n</code></pre>"},{"location":"api/infer_model/#focoos.infer.infer_model.InferModel.benchmark","title":"<code>benchmark(iterations=50, size=None)</code>","text":"<p>Benchmark the model's inference performance over multiple iterations.</p> Source code in <code>focoos/infer/infer_model.py</code> <pre><code>def benchmark(self, iterations: int = 50, size: Optional[Union[int, Tuple[int, int]]] = None) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model's inference performance over multiple iterations.\n    \"\"\"\n    if size is None:\n        size = self.model_info.im_size\n    if isinstance(size, int):\n        size = (size, size)\n    return self.runtime.benchmark(iterations, size)\n</code></pre>"},{"location":"api/infer_model/#focoos.infer.infer_model.InferModel.end2end_benchmark","title":"<code>end2end_benchmark(iterations=50, size=None)</code>","text":"<p>Benchmark the model's inference performance over multiple iterations.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of iterations to run for benchmarking.</p> <code>50</code> <code>size</code> <code>int</code> <p>The input size for each benchmark iteration.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Latency metrics including time taken for inference.</p> Example <pre><code>from focoos import Focoos, LocalModel\n\nfocoos = Focoos()\nmodel = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\nmetrics = model.end2end_benchmark(iterations=10, size=640)\n\n# Access latency metrics\nprint(f\"FPS: {metrics.fps}\")\nprint(f\"Mean latency: {metrics.mean} ms\")\nprint(f\"Engine: {metrics.engine}\")\nprint(f\"Device: {metrics.device}\")\nprint(f\"Input size: {metrics.im_size}x{metrics.im_size}\")\n</code></pre> Source code in <code>focoos/infer/infer_model.py</code> <pre><code>def end2end_benchmark(\n    self, iterations: int = 50, size: Optional[Union[int, Tuple[int, int]]] = None\n) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model's inference performance over multiple iterations.\n\n    Args:\n        iterations (int): Number of iterations to run for benchmarking.\n        size (int): The input size for each benchmark iteration.\n\n    Returns:\n        LatencyMetrics: Latency metrics including time taken for inference.\n\n    Example:\n        ```python\n        from focoos import Focoos, LocalModel\n\n        focoos = Focoos()\n        model = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\n        metrics = model.end2end_benchmark(iterations=10, size=640)\n\n        # Access latency metrics\n        print(f\"FPS: {metrics.fps}\")\n        print(f\"Mean latency: {metrics.mean} ms\")\n        print(f\"Engine: {metrics.engine}\")\n        print(f\"Device: {metrics.device}\")\n        print(f\"Input size: {metrics.im_size}x{metrics.im_size}\")\n        ```\n    \"\"\"\n    if size is None:\n        size = self.model_info.im_size\n    if isinstance(size, int):\n        size = (size, size)\n\n    engine, device = self.runtime.get_info()\n    logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device}, size: {size}x{size}..\")\n\n    np_input = (255 * np.random.random((size[0], size[1], 3))).astype(np.uint8)\n\n    durations = []\n    for step in range(iterations + 5):\n        start = perf_counter()\n        self(np_input)\n        end = perf_counter()\n\n        if step &gt;= 5:  # Skip first 5 iterations\n            durations.append((end - start) * 1000)\n\n    durations = np.array(durations)\n\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean()),\n        engine=engine,\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n        device=device,\n    )\n    logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/infer_model/#focoos.infer.infer_model.InferModel.infer","title":"<code>infer(image, threshold=None)</code>","text":"<p>Run inference on an input image and optionally annotate the results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[bytes, str, Path, ndarray, Image]</code> <p>The input image to infer on. This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.</p> required <code>threshold</code> <code>float</code> <p>The confidence threshold for detections. Defaults to 0.5. Detections with confidence scores below this threshold will be discarded.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FocoosDetections</code> <code>FocoosDetections</code> <p>The detections from the inference, represented as a custom object (<code>FocoosDetections</code>).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not deployed locally (i.e., <code>self.runtime</code> is <code>None</code>).</p> Example <pre><code>from focoos import Focoos, LocalModel\n\nfocoos = Focoos()\nmodel = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\ndetections = model.infer(image, threshold=0.5)\n</code></pre> Source code in <code>focoos/infer/infer_model.py</code> <pre><code>def infer(\n    self,\n    image: Union[bytes, str, Path, np.ndarray, Image.Image],\n    threshold: Optional[float] = None,\n) -&gt; FocoosDetections:\n    \"\"\"\n    Run inference on an input image and optionally annotate the results.\n\n    Args:\n        image (Union[bytes, str, Path, np.ndarray, Image.Image]): The input image to infer on.\n            This can be a byte array, file path, or a PIL Image object, or a NumPy array representing the image.\n        threshold (float, optional): The confidence threshold for detections. Defaults to 0.5.\n            Detections with confidence scores below this threshold will be discarded.\n\n    Returns:\n        FocoosDetections: The detections from the inference, represented as a custom object (`FocoosDetections`).\n\n    Raises:\n        ValueError: If the model is not deployed locally (i.e., `self.runtime` is `None`).\n\n    Example:\n        ```python\n        from focoos import Focoos, LocalModel\n\n        focoos = Focoos()\n        model = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\n        detections = model.infer(image, threshold=0.5)\n        ```\n    \"\"\"\n    assert self.runtime is not None, \"Model is not deployed (locally)\"\n\n    t0 = perf_counter()\n    im1, im0 = image_preprocess(image)\n    tensors, _ = self.processor.preprocess(inputs=im1, device=\"cuda\", image_size=self.model_info.im_size)\n    logger.debug(f\"Input image size: {im0.shape}\")\n    t1 = perf_counter()\n\n    raw_detections = self.runtime(tensors)\n\n    t2 = perf_counter()\n    detections = self.processor.export_postprocess(\n        raw_detections, im0, threshold=threshold, class_names=self.model_info.classes\n    )\n    t3 = perf_counter()\n    latency = {\n        \"inference\": round(t2 - t1, 3),\n        \"preprocess\": round(t1 - t0, 3),\n        \"postprocess\": round(t3 - t2, 3),\n    }\n    res = detections[0]  #!TODO  check for batching\n    res.latency = latency\n\n    logger.debug(\n        f\"Found {len(res)} detections. Inference time: {(t2 - t1) * 1000:.0f}ms, preprocess: {(t1 - t0) * 1000:.0f}ms, postprocess: {(t3 - t2) * 1000:.0f}ms\"\n    )\n    return res\n</code></pre>"},{"location":"api/model_manager/","title":"ModelManager","text":""},{"location":"api/model_manager/#focoos.model_manager.BackboneManager","title":"<code>BackboneManager</code>","text":"<p>Automatic backbone manager with lazy loading.</p> <p>The BackboneManager provides a unified interface for loading neural network backbones (feature extractors) from their configurations. It supports multiple backbone architectures like ResNet, STDC, Swin Transformer, MobileNetV2, and others.</p> <p>The manager maintains a mapping between backbone type names and their implementation paths, and handles the dynamic loading of the appropriate classes.</p> Source code in <code>focoos/model_manager.py</code> <pre><code>class BackboneManager:\n    \"\"\"\n    Automatic backbone manager with lazy loading.\n\n    The BackboneManager provides a unified interface for loading neural network backbones\n    (feature extractors) from their configurations. It supports multiple backbone architectures\n    like ResNet, STDC, Swin Transformer, MobileNetV2, and others.\n\n    The manager maintains a mapping between backbone type names and their implementation paths,\n    and handles the dynamic loading of the appropriate classes.\n    \"\"\"\n\n    _BACKBONE_MAPPING: Dict[str, str] = {\n        \"resnet\": \"resnet.ResNet\",\n        \"stdc\": \"stdc.STDC\",\n        \"swin\": \"swin.Swin\",\n        \"mobilenet_v2\": \"mobilenet_v2.MobileNetV2\",\n        \"mit\": \"mit.MIT\",\n        \"convnextv2\": \"convnextv2.ConvNeXtV2\",\n    }\n\n    @classmethod\n    def from_config(cls, config: BackboneConfig) -&gt; BaseBackbone:\n        \"\"\"\n        Load a backbone from a configuration.\n\n        This method instantiates a backbone model based on the provided configuration,\n        dynamically loading the appropriate backbone class based on the model_type.\n\n        Args:\n            config: The backbone configuration containing model_type and other parameters\n\n        Returns:\n            BaseBackbone: The instantiated backbone model\n\n        Raises:\n            ValueError: If the backbone type is not supported\n        \"\"\"\n        if config.model_type not in cls._BACKBONE_MAPPING:\n            raise ValueError(f\"Backbone {config.model_type} not supported\")\n        backbone_class = cls.get_model_class(config.model_type)\n        return backbone_class(config)\n\n    @classmethod\n    def get_model_class(cls, model_type: str):\n        \"\"\"\n        Get the model class based on the model type.\n\n        This method dynamically imports and returns the backbone class\n        corresponding to the specified model type.\n\n        Args:\n            model_type: The type of backbone model to load (e.g., \"resnet\", \"swin\")\n\n        Returns:\n            Type[BaseBackbone]: The backbone class\n\n        Raises:\n            ImportError: If the module cannot be imported\n            AttributeError: If the class is not found in the module\n        \"\"\"\n        import importlib\n\n        module_path, class_name = cls._BACKBONE_MAPPING[model_type].split(\".\")\n        module = importlib.import_module(f\".{module_path}\", package=\"focoos.nn.backbone\")\n        return getattr(module, class_name)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.BackboneManager.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Load a backbone from a configuration.</p> <p>This method instantiates a backbone model based on the provided configuration, dynamically loading the appropriate backbone class based on the model_type.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BackboneConfig</code> <p>The backbone configuration containing model_type and other parameters</p> required <p>Returns:</p> Name Type Description <code>BaseBackbone</code> <code>BaseBackbone</code> <p>The instantiated backbone model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the backbone type is not supported</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef from_config(cls, config: BackboneConfig) -&gt; BaseBackbone:\n    \"\"\"\n    Load a backbone from a configuration.\n\n    This method instantiates a backbone model based on the provided configuration,\n    dynamically loading the appropriate backbone class based on the model_type.\n\n    Args:\n        config: The backbone configuration containing model_type and other parameters\n\n    Returns:\n        BaseBackbone: The instantiated backbone model\n\n    Raises:\n        ValueError: If the backbone type is not supported\n    \"\"\"\n    if config.model_type not in cls._BACKBONE_MAPPING:\n        raise ValueError(f\"Backbone {config.model_type} not supported\")\n    backbone_class = cls.get_model_class(config.model_type)\n    return backbone_class(config)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.BackboneManager.get_model_class","title":"<code>get_model_class(model_type)</code>  <code>classmethod</code>","text":"<p>Get the model class based on the model type.</p> <p>This method dynamically imports and returns the backbone class corresponding to the specified model type.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The type of backbone model to load (e.g., \"resnet\", \"swin\")</p> required <p>Returns:</p> Type Description <p>Type[BaseBackbone]: The backbone class</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module cannot be imported</p> <code>AttributeError</code> <p>If the class is not found in the module</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef get_model_class(cls, model_type: str):\n    \"\"\"\n    Get the model class based on the model type.\n\n    This method dynamically imports and returns the backbone class\n    corresponding to the specified model type.\n\n    Args:\n        model_type: The type of backbone model to load (e.g., \"resnet\", \"swin\")\n\n    Returns:\n        Type[BaseBackbone]: The backbone class\n\n    Raises:\n        ImportError: If the module cannot be imported\n        AttributeError: If the class is not found in the module\n    \"\"\"\n    import importlib\n\n    module_path, class_name = cls._BACKBONE_MAPPING[model_type].split(\".\")\n    module = importlib.import_module(f\".{module_path}\", package=\"focoos.nn.backbone\")\n    return getattr(module, class_name)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigBackboneManager","title":"<code>ConfigBackboneManager</code>","text":"<p>Automatic backbone configuration manager with lazy loading.</p> <p>The ConfigBackboneManager provides a specialized manager for handling backbone configurations. It maintains a mapping between backbone type names and their configuration classes, and handles the dynamic loading of these classes.</p> <p>This manager is used primarily by the ConfigManager when processing nested backbone configurations within model configurations.</p> Source code in <code>focoos/model_manager.py</code> <pre><code>class ConfigBackboneManager:\n    \"\"\"\n    Automatic backbone configuration manager with lazy loading.\n\n    The ConfigBackboneManager provides a specialized manager for handling backbone\n    configurations. It maintains a mapping between backbone type names and their\n    configuration classes, and handles the dynamic loading of these classes.\n\n    This manager is used primarily by the ConfigManager when processing nested\n    backbone configurations within model configurations.\n    \"\"\"\n\n    _BACKBONE_MAPPING: Dict[str, str] = {\n        \"resnet\": \"resnet.ResnetConfig\",\n        \"stdc\": \"stdc.STDCConfig\",\n        \"swin\": \"swin.SwinConfig\",\n        \"mobilenet_v2\": \"mobilenet_v2.MobileNetV2Config\",\n        \"mit\": \"mit.MITConfig\",\n        \"convnextv2\": \"convnextv2.ConvNeXtV2Config\",\n    }\n\n    @classmethod\n    def get_model_class(cls, model_type: str):\n        \"\"\"\n        Get the configuration class based on the model type.\n\n        This method dynamically imports and returns the backbone configuration class\n        corresponding to the specified model type.\n\n        Args:\n            model_type: The type of backbone model (e.g., \"resnet\", \"swin\")\n\n        Returns:\n            Type[BackboneConfig]: The backbone configuration class\n\n        Raises:\n            ImportError: If the module cannot be imported\n            AttributeError: If the class is not found in the module\n        \"\"\"\n        import importlib\n\n        module_path, class_name = cls._BACKBONE_MAPPING[model_type].split(\".\")\n        module = importlib.import_module(f\".{module_path}\", package=\"focoos.nn.backbone\")\n        return getattr(module, class_name)\n\n    @classmethod\n    def from_dict(cls, config_dict: dict) -&gt; BackboneConfig:\n        \"\"\"\n        Create a backbone configuration from a dictionary.\n\n        This method instantiates a backbone configuration object based on the\n        model_type specified in the configuration dictionary.\n\n        Args:\n            config_dict: Dictionary containing configuration parameters including model_type\n\n        Returns:\n            BackboneConfig: The instantiated backbone configuration object\n\n        Raises:\n            ValueError: If the backbone type is not supported\n        \"\"\"\n        if config_dict[\"model_type\"] not in cls._BACKBONE_MAPPING:\n            raise ValueError(f\"Backbone {config_dict['model_type']} not supported\")\n\n        config_class = cls.get_model_class(config_dict[\"model_type\"])\n        return_config = config_class(**config_dict)\n        return return_config\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigBackboneManager.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create a backbone configuration from a dictionary.</p> <p>This method instantiates a backbone configuration object based on the model_type specified in the configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>Dictionary containing configuration parameters including model_type</p> required <p>Returns:</p> Name Type Description <code>BackboneConfig</code> <code>BackboneConfig</code> <p>The instantiated backbone configuration object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the backbone type is not supported</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict) -&gt; BackboneConfig:\n    \"\"\"\n    Create a backbone configuration from a dictionary.\n\n    This method instantiates a backbone configuration object based on the\n    model_type specified in the configuration dictionary.\n\n    Args:\n        config_dict: Dictionary containing configuration parameters including model_type\n\n    Returns:\n        BackboneConfig: The instantiated backbone configuration object\n\n    Raises:\n        ValueError: If the backbone type is not supported\n    \"\"\"\n    if config_dict[\"model_type\"] not in cls._BACKBONE_MAPPING:\n        raise ValueError(f\"Backbone {config_dict['model_type']} not supported\")\n\n    config_class = cls.get_model_class(config_dict[\"model_type\"])\n    return_config = config_class(**config_dict)\n    return return_config\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigBackboneManager.get_model_class","title":"<code>get_model_class(model_type)</code>  <code>classmethod</code>","text":"<p>Get the configuration class based on the model type.</p> <p>This method dynamically imports and returns the backbone configuration class corresponding to the specified model type.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The type of backbone model (e.g., \"resnet\", \"swin\")</p> required <p>Returns:</p> Type Description <p>Type[BackboneConfig]: The backbone configuration class</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module cannot be imported</p> <code>AttributeError</code> <p>If the class is not found in the module</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef get_model_class(cls, model_type: str):\n    \"\"\"\n    Get the configuration class based on the model type.\n\n    This method dynamically imports and returns the backbone configuration class\n    corresponding to the specified model type.\n\n    Args:\n        model_type: The type of backbone model (e.g., \"resnet\", \"swin\")\n\n    Returns:\n        Type[BackboneConfig]: The backbone configuration class\n\n    Raises:\n        ImportError: If the module cannot be imported\n        AttributeError: If the class is not found in the module\n    \"\"\"\n    import importlib\n\n    module_path, class_name = cls._BACKBONE_MAPPING[model_type].split(\".\")\n    module = importlib.import_module(f\".{module_path}\", package=\"focoos.nn.backbone\")\n    return getattr(module, class_name)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigManager","title":"<code>ConfigManager</code>","text":"<p>Automatic model configuration management.</p> <p>The ConfigManager provides a centralized system for managing model configurations. It maintains a registry of configuration classes for different model families and handles the creation of appropriate configuration objects from dictionaries.</p> <p>The manager supports dynamic registration of configuration classes and automatic importing of model family modules as needed.</p> Source code in <code>focoos/model_manager.py</code> <pre><code>class ConfigManager:\n    \"\"\"\n    Automatic model configuration management.\n\n    The ConfigManager provides a centralized system for managing model configurations.\n    It maintains a registry of configuration classes for different model families and\n    handles the creation of appropriate configuration objects from dictionaries.\n\n    The manager supports dynamic registration of configuration classes and automatic\n    importing of model family modules as needed.\n    \"\"\"\n\n    _MODEL_CFG_MAPPING: Dict[str, Callable[[], Type[ModelConfig]]] = {}\n\n    @classmethod\n    def register_config(cls, model_family: ModelFamily, model_config_loader: Callable[[], Type[ModelConfig]]):\n        \"\"\"\n        Register a loader for a specific model configuration.\n\n        This method associates a model family with a loader function that returns\n        the configuration class when called. This enables lazy loading of configuration\n        classes.\n\n        Args:\n            model_family: The ModelFamily enum value to register\n            model_config_loader: A callable that returns the configuration class when invoked\n        \"\"\"\n        cls._MODEL_CFG_MAPPING[model_family.value] = model_config_loader\n\n    @classmethod\n    def from_dict(cls, model_family: ModelFamily, config_dict: dict, **kwargs) -&gt; ModelConfig:\n        \"\"\"\n        Create a configuration from a dictionary.\n\n        This method instantiates a model configuration object based on the model family\n        and the provided configuration dictionary. It handles nested configurations\n        like backbone_config and validates the parameters.\n\n        Args:\n            model_family: The model family enum value\n            config_dict: Dictionary containing configuration parameters\n            **kwargs: Additional keyword arguments to override configuration values\n\n        Returns:\n            ModelConfig: The instantiated configuration object\n\n        Raises:\n            ValueError: If the model family is not supported or if invalid parameters are provided\n        \"\"\"\n        if model_family.value not in cls._MODEL_CFG_MAPPING:\n            # Import the family module\n            family_module = importlib.import_module(f\"focoos.models.{model_family.value}\")\n\n            # Iteratively register all models in the family\n            for attr_name in dir(family_module):\n                if attr_name.startswith(\"_register\"):\n                    register_func = getattr(family_module, attr_name)\n                    if callable(register_func):\n                        register_func()\n\n        if model_family.value not in cls._MODEL_CFG_MAPPING:\n            raise ValueError(f\"Model {model_family} not supported\")\n\n        config_class = cls._MODEL_CFG_MAPPING[model_family.value]()  # this return the config class\n\n        # Convert the input dict to the actual config type\n        if \"backbone_config\" in config_dict and config_dict[\"backbone_config\"] is not None:\n            config_dict[\"backbone_config\"] = ConfigBackboneManager.from_dict(config_dict[\"backbone_config\"])\n\n            # Validate the parameters kwargs\n        valid_fields = {f.name for f in fields(config_class)}\n        invalid_kwargs = set(kwargs.keys()) - valid_fields\n        if invalid_kwargs:\n            raise ValueError(\n                f\"Invalid parameters for {config_class.__name__}: {invalid_kwargs}\\nValid parameters: {valid_fields}\"\n            )\n\n        config_dict = config_class(**config_dict)\n\n        # Update the config with the kwargs\n        if kwargs:\n            config_dict.update(kwargs)\n\n        return config_dict\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigManager.from_dict","title":"<code>from_dict(model_family, config_dict, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a configuration from a dictionary.</p> <p>This method instantiates a model configuration object based on the model family and the provided configuration dictionary. It handles nested configurations like backbone_config and validates the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model_family</code> <code>ModelFamily</code> <p>The model family enum value</p> required <code>config_dict</code> <code>dict</code> <p>Dictionary containing configuration parameters</p> required <code>**kwargs</code> <p>Additional keyword arguments to override configuration values</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ModelConfig</code> <code>ModelConfig</code> <p>The instantiated configuration object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model family is not supported or if invalid parameters are provided</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef from_dict(cls, model_family: ModelFamily, config_dict: dict, **kwargs) -&gt; ModelConfig:\n    \"\"\"\n    Create a configuration from a dictionary.\n\n    This method instantiates a model configuration object based on the model family\n    and the provided configuration dictionary. It handles nested configurations\n    like backbone_config and validates the parameters.\n\n    Args:\n        model_family: The model family enum value\n        config_dict: Dictionary containing configuration parameters\n        **kwargs: Additional keyword arguments to override configuration values\n\n    Returns:\n        ModelConfig: The instantiated configuration object\n\n    Raises:\n        ValueError: If the model family is not supported or if invalid parameters are provided\n    \"\"\"\n    if model_family.value not in cls._MODEL_CFG_MAPPING:\n        # Import the family module\n        family_module = importlib.import_module(f\"focoos.models.{model_family.value}\")\n\n        # Iteratively register all models in the family\n        for attr_name in dir(family_module):\n            if attr_name.startswith(\"_register\"):\n                register_func = getattr(family_module, attr_name)\n                if callable(register_func):\n                    register_func()\n\n    if model_family.value not in cls._MODEL_CFG_MAPPING:\n        raise ValueError(f\"Model {model_family} not supported\")\n\n    config_class = cls._MODEL_CFG_MAPPING[model_family.value]()  # this return the config class\n\n    # Convert the input dict to the actual config type\n    if \"backbone_config\" in config_dict and config_dict[\"backbone_config\"] is not None:\n        config_dict[\"backbone_config\"] = ConfigBackboneManager.from_dict(config_dict[\"backbone_config\"])\n\n        # Validate the parameters kwargs\n    valid_fields = {f.name for f in fields(config_class)}\n    invalid_kwargs = set(kwargs.keys()) - valid_fields\n    if invalid_kwargs:\n        raise ValueError(\n            f\"Invalid parameters for {config_class.__name__}: {invalid_kwargs}\\nValid parameters: {valid_fields}\"\n        )\n\n    config_dict = config_class(**config_dict)\n\n    # Update the config with the kwargs\n    if kwargs:\n        config_dict.update(kwargs)\n\n    return config_dict\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigManager.register_config","title":"<code>register_config(model_family, model_config_loader)</code>  <code>classmethod</code>","text":"<p>Register a loader for a specific model configuration.</p> <p>This method associates a model family with a loader function that returns the configuration class when called. This enables lazy loading of configuration classes.</p> <p>Parameters:</p> Name Type Description Default <code>model_family</code> <code>ModelFamily</code> <p>The ModelFamily enum value to register</p> required <code>model_config_loader</code> <code>Callable[[], Type[ModelConfig]]</code> <p>A callable that returns the configuration class when invoked</p> required Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef register_config(cls, model_family: ModelFamily, model_config_loader: Callable[[], Type[ModelConfig]]):\n    \"\"\"\n    Register a loader for a specific model configuration.\n\n    This method associates a model family with a loader function that returns\n    the configuration class when called. This enables lazy loading of configuration\n    classes.\n\n    Args:\n        model_family: The ModelFamily enum value to register\n        model_config_loader: A callable that returns the configuration class when invoked\n    \"\"\"\n    cls._MODEL_CFG_MAPPING[model_family.value] = model_config_loader\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ModelManager","title":"<code>ModelManager</code>","text":"<p>Automatic model manager with lazy loading.</p> <p>The ModelManager provides a unified interface for loading models from various sources: - From ModelInfo objects - From the Focoos Hub (hub:// protocol) - From local directories - From the model registry</p> <p>It handles model registration, configuration management, and weights loading automatically. Models are loaded lazily when requested and can be accessed through the <code>get</code> method.</p> <p>Examples:</p> <p>Load a registered model:</p> <pre><code>&gt;&gt;&gt; model = ModelManager.get(\"model_name\")\n</code></pre> <p>Load a model from hub:</p> <pre><code>&gt;&gt;&gt; model = ModelManager.get(\"hub://username/model_ref\")\n</code></pre> <p>Load a model with custom config:</p> <pre><code>&gt;&gt;&gt; model = ModelManager.get(\"model_name\", config=custom_config)\n</code></pre> Source code in <code>focoos/model_manager.py</code> <pre><code>class ModelManager:\n    \"\"\"Automatic model manager with lazy loading.\n\n    The ModelManager provides a unified interface for loading models from various sources:\n    - From ModelInfo objects\n    - From the Focoos Hub (hub:// protocol)\n    - From local directories\n    - From the model registry\n\n    It handles model registration, configuration management, and weights loading automatically.\n    Models are loaded lazily when requested and can be accessed through the `get` method.\n\n    Examples:\n        Load a registered model:\n        &gt;&gt;&gt; model = ModelManager.get(\"model_name\")\n\n        Load a model from hub:\n        &gt;&gt;&gt; model = ModelManager.get(\"hub://username/model_ref\")\n\n        Load a model with custom config:\n        &gt;&gt;&gt; model = ModelManager.get(\"model_name\", config=custom_config)\n    \"\"\"\n\n    _models_family_map: Dict[str, Callable[[], Type[BaseModelNN]]] = {}  # {\"fai-detr\": load_fai_detr()}\n\n    @classmethod\n    def get(\n        cls,\n        name: str,\n        model_info: Optional[ModelInfo] = None,\n        config: Optional[ModelConfig] = None,\n        models_dir: Optional[str] = None,\n        hub: Optional[FocoosHUB] = None,\n        cache: bool = True,\n        **kwargs,\n    ) -&gt; FocoosModel:\n        \"\"\"\n        Unified entrypoint to load a model by name or ModelInfo.\n\n        This method provides a single interface for loading models from various sources:\n        - From a ModelInfo object (when model_info is provided)\n        - From the Focoos Hub (when name starts with \"hub://\")\n        - From the ModelRegistry (for pretrained models)\n        - From a local directory (when name is a local path)\n\n        Args:\n            name: Model name, path, or hub reference (e.g., \"hub://username/model_ref\")\n            model_info: Optional ModelInfo object to load the model from directly\n            config: Optional custom model configuration to override defaults\n            models_dir: Optional directory to look for local models (defaults to MODELS_DIR)\n            hub: Optional FocoosHUB instance to use for hub:// references\n            cache: Optional boolean to cache the model info and weights when loading from hub (defaults to True)\n            **kwargs: Additional keyword arguments passed to the model configuration\n\n        Returns:\n            FocoosModel: The loaded model instance\n\n        Raises:\n            ValueError: If the model cannot be found or loaded\n        \"\"\"\n        if model_info is not None:\n            # Load model directly from provided ModelInfo\n            return cls._from_model_info(model_info=model_info, config=config, **kwargs)\n\n        # If name starts with \"hub://\", load from Focoos Hub\n        if name.startswith(\"hub://\"):\n            model_info, hub_config = cls._from_hub(hub_uri=name, hub=hub, cache=cache, **kwargs)\n            if config is None:\n                config = hub_config  # Use hub config if no config is provided\n        # If model exists in ModelRegistry, load as pretrained model\n        elif ModelRegistry.exists(name):\n            model_info = ModelRegistry.get_model_info(name)\n        # Otherwise, attempt to load from a local directory\n        else:\n            model_info = cls._from_local_dir(name=name, models_dir=models_dir)\n        # Load model from the resolved ModelInfo\n        return cls._from_model_info(model_info=model_info, config=config, **kwargs)\n\n    @classmethod\n    def register_model(cls, model_family: ModelFamily, model_loader: Callable[[], Type[BaseModelNN]]):\n        \"\"\"\n        Register a loader for a specific model family.\n\n        This method associates a model family with a loader function that returns\n        the model class when called. This enables lazy loading of model classes.\n\n        Args:\n            model_family: The ModelFamily enum value to register\n            model_loader: A callable that returns the model class when invoked\n        \"\"\"\n        cls._models_family_map[model_family.value] = model_loader\n\n    @classmethod\n    def _ensure_family_registered(cls, model_family: ModelFamily):\n        \"\"\"\n        Ensure the model family is registered, importing if needed.\n\n        This method checks if a model family is registered and if not, attempts to\n        import and register it automatically by calling any registration functions\n        in the family module.\n\n        Args:\n            model_family: The ModelFamily enum value to ensure is registered\n        \"\"\"\n        if model_family.value in cls._models_family_map:\n            return\n        family_module = importlib.import_module(f\"focoos.models.{model_family.value}\")\n        for attr_name in dir(family_module):\n            if attr_name.startswith(\"_register\"):\n                register_func = getattr(family_module, attr_name)\n                if callable(register_func):\n                    register_func()\n\n    @classmethod\n    def _from_model_info(cls, model_info: ModelInfo, config: Optional[ModelConfig] = None, **kwargs) -&gt; FocoosModel:\n        \"\"\"\n        Load a model from ModelInfo, handling config and weights.\n\n        This method instantiates a model based on the ModelInfo, applying the provided\n        configuration (or using the one from ModelInfo) and loading weights if available.\n\n        Args:\n            model_info: ModelInfo object containing model metadata and references\n            config: Optional model configuration to override the one in ModelInfo\n            **kwargs: Additional keyword arguments passed to the model configuration\n\n        Returns:\n            FocoosModel: The instantiated model with weights loaded if available\n\n        Raises:\n            ValueError: If the model family is not supported\n        \"\"\"\n        cls._ensure_family_registered(model_info.model_family)\n        if model_info.model_family.value not in cls._models_family_map:\n            raise ValueError(f\"Model {model_info.model_family} not supported\")\n        model_class = cls._models_family_map[model_info.model_family.value]()\n        config = config or ConfigManager.from_dict(model_info.model_family, model_info.config, **kwargs)\n        model_info.config = config\n        nn_model = model_class(model_info.config)\n        model = FocoosModel(nn_model, model_info)\n        return model\n\n    @classmethod\n    def _from_local_dir(cls, name: str, models_dir: Optional[str] = None) -&gt; ModelInfo:\n        \"\"\"\n        Load a model from a local experiment directory.\n\n        This method loads a model from a local directory by reading its ModelInfo file\n        and resolving paths to weights and other artifacts.\n\n        Args:\n            name: Name or path of the model directory relative to models_dir\n            models_dir: Base directory containing model directories (defaults to MODELS_DIR)\n\n        Returns:\n            ModelInfo: The model information loaded from the local directory\n\n        Raises:\n            ValueError: If the model directory or ModelInfo file cannot be found\n        \"\"\"\n        models_dir = models_dir or MODELS_DIR\n\n        run_dir = os.path.join(models_dir, name)\n        if not os.path.exists(run_dir):\n            raise ValueError(f\"Run {name} not found in {models_dir}\")\n        model_info_path = os.path.join(run_dir, ArtifactName.INFO)\n        if not os.path.exists(model_info_path):\n            raise ValueError(f\"Model info not found in {run_dir}\")\n        model_info = ModelInfo.from_json(model_info_path)\n\n        if model_info.weights_uri == ArtifactName.WEIGHTS:\n            model_info.weights_uri = os.path.join(run_dir, model_info.weights_uri)\n\n        return model_info\n\n    @classmethod\n    def _from_hub(\n        cls, hub_uri: str, hub: Optional[FocoosHUB] = None, cache: bool = True, **kwargs\n    ) -&gt; Tuple[ModelInfo, ModelConfig]:\n        \"\"\"\n        Load a model from the Focoos Hub.\n\n        This method downloads a model from the Focoos Hub using the provided URI,\n        which should be in the format \"hub://username/model_ref\".\n\n        Args:\n            hub_uri: Hub URI in the format \"hub://username/model_ref\"\n            hub: Optional FocoosHUB instance to use (creates a new one if not provided)\n            **kwargs: Additional keyword arguments passed to the model configuration\n\n        Returns:\n            Tuple[ModelInfo, ModelConfig]: The model information and configuration\n\n        Raises:\n            ValueError: If the model reference is invalid or the model cannot be downloaded\n        \"\"\"\n        hub = hub or FocoosHUB()\n        model_ref = hub_uri.split(\"hub://\")[1]\n\n        if not model_ref:\n            raise ValueError(\"Model ref is required\")\n\n        model_pth_path = hub.download_model_pth(model_ref=model_ref, skip_if_exists=cache)\n\n        model_info_path = os.path.join(MODELS_DIR, model_ref, ArtifactName.INFO)\n        if not os.path.exists(model_info_path) or not cache:\n            logger.info(f\"\ud83d\udce5 Downloading model info from hub for model: {model_ref}\")\n            remote_model_info = hub.get_model_info(model_ref=model_ref)\n            model_info = ModelInfo.from_json(remote_model_info.model_dump(mode=\"json\"))\n            model_info.dump_json(model_info_path)\n        else:\n            logger.info(f\"\ud83d\udce5 Loading model info from cache: {model_info_path}\")\n            model_info = ModelInfo.from_json(model_info_path)\n\n        if not model_info.weights_uri:\n            model_info.weights_uri = model_pth_path\n\n        model_config = ConfigManager.from_dict(model_info.model_family, model_info.config, **kwargs)\n\n        return (model_info, model_config)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ModelManager.get","title":"<code>get(name, model_info=None, config=None, models_dir=None, hub=None, cache=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Unified entrypoint to load a model by name or ModelInfo.</p> <p>This method provides a single interface for loading models from various sources: - From a ModelInfo object (when model_info is provided) - From the Focoos Hub (when name starts with \"hub://\") - From the ModelRegistry (for pretrained models) - From a local directory (when name is a local path)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name, path, or hub reference (e.g., \"hub://username/model_ref\")</p> required <code>model_info</code> <code>Optional[ModelInfo]</code> <p>Optional ModelInfo object to load the model from directly</p> <code>None</code> <code>config</code> <code>Optional[ModelConfig]</code> <p>Optional custom model configuration to override defaults</p> <code>None</code> <code>models_dir</code> <code>Optional[str]</code> <p>Optional directory to look for local models (defaults to MODELS_DIR)</p> <code>None</code> <code>hub</code> <code>Optional[FocoosHUB]</code> <p>Optional FocoosHUB instance to use for hub:// references</p> <code>None</code> <code>cache</code> <code>bool</code> <p>Optional boolean to cache the model info and weights when loading from hub (defaults to True)</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the model configuration</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FocoosModel</code> <code>FocoosModel</code> <p>The loaded model instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model cannot be found or loaded</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef get(\n    cls,\n    name: str,\n    model_info: Optional[ModelInfo] = None,\n    config: Optional[ModelConfig] = None,\n    models_dir: Optional[str] = None,\n    hub: Optional[FocoosHUB] = None,\n    cache: bool = True,\n    **kwargs,\n) -&gt; FocoosModel:\n    \"\"\"\n    Unified entrypoint to load a model by name or ModelInfo.\n\n    This method provides a single interface for loading models from various sources:\n    - From a ModelInfo object (when model_info is provided)\n    - From the Focoos Hub (when name starts with \"hub://\")\n    - From the ModelRegistry (for pretrained models)\n    - From a local directory (when name is a local path)\n\n    Args:\n        name: Model name, path, or hub reference (e.g., \"hub://username/model_ref\")\n        model_info: Optional ModelInfo object to load the model from directly\n        config: Optional custom model configuration to override defaults\n        models_dir: Optional directory to look for local models (defaults to MODELS_DIR)\n        hub: Optional FocoosHUB instance to use for hub:// references\n        cache: Optional boolean to cache the model info and weights when loading from hub (defaults to True)\n        **kwargs: Additional keyword arguments passed to the model configuration\n\n    Returns:\n        FocoosModel: The loaded model instance\n\n    Raises:\n        ValueError: If the model cannot be found or loaded\n    \"\"\"\n    if model_info is not None:\n        # Load model directly from provided ModelInfo\n        return cls._from_model_info(model_info=model_info, config=config, **kwargs)\n\n    # If name starts with \"hub://\", load from Focoos Hub\n    if name.startswith(\"hub://\"):\n        model_info, hub_config = cls._from_hub(hub_uri=name, hub=hub, cache=cache, **kwargs)\n        if config is None:\n            config = hub_config  # Use hub config if no config is provided\n    # If model exists in ModelRegistry, load as pretrained model\n    elif ModelRegistry.exists(name):\n        model_info = ModelRegistry.get_model_info(name)\n    # Otherwise, attempt to load from a local directory\n    else:\n        model_info = cls._from_local_dir(name=name, models_dir=models_dir)\n    # Load model from the resolved ModelInfo\n    return cls._from_model_info(model_info=model_info, config=config, **kwargs)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ModelManager.register_model","title":"<code>register_model(model_family, model_loader)</code>  <code>classmethod</code>","text":"<p>Register a loader for a specific model family.</p> <p>This method associates a model family with a loader function that returns the model class when called. This enables lazy loading of model classes.</p> <p>Parameters:</p> Name Type Description Default <code>model_family</code> <code>ModelFamily</code> <p>The ModelFamily enum value to register</p> required <code>model_loader</code> <code>Callable[[], Type[BaseModelNN]]</code> <p>A callable that returns the model class when invoked</p> required Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef register_model(cls, model_family: ModelFamily, model_loader: Callable[[], Type[BaseModelNN]]):\n    \"\"\"\n    Register a loader for a specific model family.\n\n    This method associates a model family with a loader function that returns\n    the model class when called. This enables lazy loading of model classes.\n\n    Args:\n        model_family: The ModelFamily enum value to register\n        model_loader: A callable that returns the model class when invoked\n    \"\"\"\n    cls._models_family_map[model_family.value] = model_loader\n</code></pre>"},{"location":"api/model_registry/","title":"ModelRegistry","text":""},{"location":"api/model_registry/#focoos.model_registry.model_registry.ModelRegistry","title":"<code>ModelRegistry</code>","text":"<p>Central registry of pretrained models.</p> <p>This class serves as a centralized registry for all pretrained models in the Focoos system. It provides methods to access model information, list available models, and check model existence.</p> <p>Attributes:</p> Name Type Description <code>_pretrained_models</code> <code>Dict[str, str]</code> <p>Dictionary mapping model names to their JSON file paths.</p> Source code in <code>focoos/model_registry/model_registry.py</code> <pre><code>class ModelRegistry:\n    \"\"\"Central registry of pretrained models.\n\n    This class serves as a centralized registry for all pretrained models in the Focoos system.\n    It provides methods to access model information, list available models, and check model existence.\n\n    Attributes:\n        _pretrained_models (Dict[str, str]): Dictionary mapping model names to their JSON file paths.\n    \"\"\"\n\n    _pretrained_models: Dict[str, str] = {\n        \"fai-detr-l-obj365\": os.path.join(REGISTRY_PATH, \"fai-detr-l-obj365.json\"),\n        \"fai-detr-l-coco\": os.path.join(REGISTRY_PATH, \"fai-detr-l-coco.json\"),\n        \"fai-detr-m-coco\": os.path.join(REGISTRY_PATH, \"fai-detr-m-coco.json\"),\n        \"fai-mf-l-ade\": os.path.join(REGISTRY_PATH, \"fai-mf-l-ade.json\"),\n        \"fai-mf-m-ade\": os.path.join(REGISTRY_PATH, \"fai-mf-m-ade.json\"),\n        \"fai-mf-l-coco-ins\": os.path.join(REGISTRY_PATH, \"fai-mf-l-coco-ins.json\"),\n        \"fai-mf-m-coco-ins\": os.path.join(REGISTRY_PATH, \"fai-mf-m-coco-ins.json\"),\n        \"fai-mf-s-coco-ins\": os.path.join(REGISTRY_PATH, \"fai-mf-s-coco-ins.json\"),\n        \"bisenetformer-m-ade\": os.path.join(REGISTRY_PATH, \"bisenetformer-m-ade.json\"),\n        \"bisenetformer-l-ade\": os.path.join(REGISTRY_PATH, \"bisenetformer-l-ade.json\"),\n        \"bisenetformer-s-ade\": os.path.join(REGISTRY_PATH, \"bisenetformer-s-ade.json\"),\n    }\n\n    @classmethod\n    def get_model_info(cls, model_name: str) -&gt; ModelInfo:\n        \"\"\"Get the model information for a given model name.\n\n        Args:\n            model_name (str): The name of the model to retrieve information for.\n                Can be either a pretrained model name or a path to a JSON file.\n\n        Returns:\n            ModelInfo: The model information object containing model details.\n\n        Raises:\n            ValueError: If the model is not found in the registry and the provided\n                path does not exist.\n        \"\"\"\n        if model_name in cls._pretrained_models:\n            return ModelInfo.from_json(cls._pretrained_models[model_name])\n        if not os.path.exists(model_name):\n            logger.warning(f\"\u26a0\ufe0f Model {model_name} not found\")\n            raise ValueError(f\"\u26a0\ufe0f Model {model_name} not found\")\n        return ModelInfo.from_json(model_name)\n\n    @classmethod\n    def list_models(cls) -&gt; list[str]:\n        \"\"\"List all available pretrained models.\n\n        Returns:\n            list[str]: A list of all available pretrained model names.\n        \"\"\"\n        return list(cls._pretrained_models.keys())\n\n    @classmethod\n    def exists(cls, model_name: str) -&gt; bool:\n        \"\"\"Check if a model exists in the registry.\n\n        Args:\n            model_name (str): The name of the model to check.\n\n        Returns:\n            bool: True if the model exists in the pretrained models registry,\n                False otherwise.\n        \"\"\"\n        return model_name in cls._pretrained_models\n</code></pre>"},{"location":"api/model_registry/#focoos.model_registry.model_registry.ModelRegistry.exists","title":"<code>exists(model_name)</code>  <code>classmethod</code>","text":"<p>Check if a model exists in the registry.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the model exists in the pretrained models registry, False otherwise.</p> Source code in <code>focoos/model_registry/model_registry.py</code> <pre><code>@classmethod\ndef exists(cls, model_name: str) -&gt; bool:\n    \"\"\"Check if a model exists in the registry.\n\n    Args:\n        model_name (str): The name of the model to check.\n\n    Returns:\n        bool: True if the model exists in the pretrained models registry,\n            False otherwise.\n    \"\"\"\n    return model_name in cls._pretrained_models\n</code></pre>"},{"location":"api/model_registry/#focoos.model_registry.model_registry.ModelRegistry.get_model_info","title":"<code>get_model_info(model_name)</code>  <code>classmethod</code>","text":"<p>Get the model information for a given model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to retrieve information for. Can be either a pretrained model name or a path to a JSON file.</p> required <p>Returns:</p> Name Type Description <code>ModelInfo</code> <code>ModelInfo</code> <p>The model information object containing model details.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not found in the registry and the provided path does not exist.</p> Source code in <code>focoos/model_registry/model_registry.py</code> <pre><code>@classmethod\ndef get_model_info(cls, model_name: str) -&gt; ModelInfo:\n    \"\"\"Get the model information for a given model name.\n\n    Args:\n        model_name (str): The name of the model to retrieve information for.\n            Can be either a pretrained model name or a path to a JSON file.\n\n    Returns:\n        ModelInfo: The model information object containing model details.\n\n    Raises:\n        ValueError: If the model is not found in the registry and the provided\n            path does not exist.\n    \"\"\"\n    if model_name in cls._pretrained_models:\n        return ModelInfo.from_json(cls._pretrained_models[model_name])\n    if not os.path.exists(model_name):\n        logger.warning(f\"\u26a0\ufe0f Model {model_name} not found\")\n        raise ValueError(f\"\u26a0\ufe0f Model {model_name} not found\")\n    return ModelInfo.from_json(model_name)\n</code></pre>"},{"location":"api/model_registry/#focoos.model_registry.model_registry.ModelRegistry.list_models","title":"<code>list_models()</code>  <code>classmethod</code>","text":"<p>List all available pretrained models.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of all available pretrained model names.</p> Source code in <code>focoos/model_registry/model_registry.py</code> <pre><code>@classmethod\ndef list_models(cls) -&gt; list[str]:\n    \"\"\"List all available pretrained models.\n\n    Returns:\n        list[str]: A list of all available pretrained model names.\n    \"\"\"\n    return list(cls._pretrained_models.keys())\n</code></pre>"},{"location":"api/ports/","title":"ports","text":""},{"location":"api/ports/#focoos.ports.ApiKey","title":"<code>ApiKey</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>API key for authentication.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ApiKey(PydanticBase):\n    \"\"\"API key for authentication.\"\"\"\n\n    key: str  # type: ignore\n</code></pre>"},{"location":"api/ports/#focoos.ports.ArtifactName","title":"<code>ArtifactName</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Model artifact type.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ArtifactName(str, Enum):\n    \"\"\"Model artifact type.\"\"\"\n\n    WEIGHTS = \"model_final.pth\"\n    ONNX = \"model.onnx\"\n    PT = \"model.pt\"\n    INFO = \"model_info.json\"\n    METRICS = \"metrics.json\"\n    LOGS = \"log.txt\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetLayout","title":"<code>DatasetLayout</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported dataset formats in Focoos.</p> Values <ul> <li>ROBOFLOW_COCO: (Detection,Instance Segmentation)</li> <li>ROBOFLOW_SEG: (Semantic Segmentation)</li> <li>SUPERVISELY: (Semantic Segmentation)</li> </ul> <p>Example:     <pre><code>- ROBOFLOW_COCO: (Detection,Instance Segmentation) Roboflow COCO format:\n    root/\n        train/\n            - _annotations.coco.json\n            - img_1.jpg\n            - img_2.jpg\n        valid/\n            - _annotations.coco.json\n            - img_3.jpg\n            - img_4.jpg\n- ROBOFLOW_SEG: (Semantic Segmentation) Roboflow segmentation format:\n    root/\n        train/\n            - _classes.csv (comma separated csv)\n            - img_1.jpg\n            - img_2.jpg\n        valid/\n            - _classes.csv (comma separated csv)\n            - img_3_mask.png\n            - img_4_mask.png\n\n- SUPERVISELY: (Semantic Segmentation) format:\n    root/\n        train/\n            meta.json\n            img/\n            ann/\n            mask/\n        valid/\n            meta.json\n            img/\n            ann/\n            mask/\n</code></pre></p> Source code in <code>focoos/ports.py</code> <pre><code>class DatasetLayout(str, Enum):\n    \"\"\"Supported dataset formats in Focoos.\n\n    Values:\n        - ROBOFLOW_COCO: (Detection,Instance Segmentation)\n        - ROBOFLOW_SEG: (Semantic Segmentation)\n        - SUPERVISELY: (Semantic Segmentation)\n    Example:\n        ```python\n        - ROBOFLOW_COCO: (Detection,Instance Segmentation) Roboflow COCO format:\n            root/\n                train/\n                    - _annotations.coco.json\n                    - img_1.jpg\n                    - img_2.jpg\n                valid/\n                    - _annotations.coco.json\n                    - img_3.jpg\n                    - img_4.jpg\n        - ROBOFLOW_SEG: (Semantic Segmentation) Roboflow segmentation format:\n            root/\n                train/\n                    - _classes.csv (comma separated csv)\n                    - img_1.jpg\n                    - img_2.jpg\n                valid/\n                    - _classes.csv (comma separated csv)\n                    - img_3_mask.png\n                    - img_4_mask.png\n\n        - SUPERVISELY: (Semantic Segmentation) format:\n            root/\n                train/\n                    meta.json\n                    img/\n                    ann/\n                    mask/\n                valid/\n                    meta.json\n                    img/\n                    ann/\n                    mask/\n        ```\n    \"\"\"\n\n    ROBOFLOW_COCO = \"roboflow_coco\"\n    ROBOFLOW_SEG = \"roboflow_seg\"\n    CATALOG = \"catalog\"\n    SUPERVISELY = \"supervisely\"\n    CLS_FOLDER = \"cls_folder\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetMetadata","title":"<code>DatasetMetadata</code>  <code>dataclass</code>","text":"<p>Dataclass for storing dataset metadata.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass DatasetMetadata:\n    \"\"\"Dataclass for storing dataset metadata.\"\"\"\n\n    num_classes: int\n    task: Task\n    count: Optional[int] = None\n    name: Optional[str] = None\n    image_root: Optional[str] = None\n    thing_classes: Optional[List[str]] = None\n    _thing_colors: Optional[List[Tuple]] = None\n    stuff_classes: Optional[List[str]] = None\n    _stuff_colors: Optional[List[Tuple]] = None\n    sem_seg_root: Optional[str] = None\n    panoptic_root: Optional[str] = None\n    ignore_label: Optional[int] = None\n    thing_dataset_id_to_contiguous_id: Optional[dict] = None\n    stuff_dataset_id_to_contiguous_id: Optional[dict] = None\n    json_file: Optional[str] = None\n\n    @property\n    def classes(self) -&gt; List[str]:  #!TODO: check if this is correct\n        if self.task == Task.DETECTION or self.task == Task.INSTANCE_SEGMENTATION:\n            assert self.thing_classes is not None, \"thing_classes is required for detection and instance segmentation\"\n            return self.thing_classes\n        if self.task == Task.SEMSEG:\n            # fixme: not sure for panoptic\n            assert self.stuff_classes is not None, \"stuff_classes is required for semantic segmentation\"\n            return self.stuff_classes\n        if self.task == Task.CLASSIFICATION:\n            assert self.thing_classes is not None, \"thing_classes is required for classification\"\n            return self.thing_classes\n        raise ValueError(f\"Task {self.task} not supported\")\n\n    @property\n    def stuff_colors(self):\n        if self._stuff_colors is not None:\n            return self._stuff_colors\n        if self.stuff_classes is None:\n            return []\n        return [((i * 64) % 255, (i * 128) % 255, (i * 32) % 255) for i in range(len(self.stuff_classes))]\n\n    @stuff_colors.setter\n    def stuff_colors(self, colors):\n        self._stuff_colors = colors\n\n    @property\n    def thing_colors(self):\n        if self._thing_colors is not None:\n            return self._thing_colors\n        if self.thing_classes is None:\n            return []\n        return [((i * 64) % 255, (i * 128) % 255, (i * 32) % 255) for i in range(1, len(self.thing_classes) + 1)]\n\n    @thing_colors.setter\n    def thing_colors(self, colors):\n        self._thing_colors = colors\n\n    @classmethod\n    def from_dict(cls, metadata: dict):\n        \"\"\"Create DatasetMetadata from a dictionary.\n\n        Args:\n            metadata (dict): Dictionary containing metadata.\n\n        Returns:\n            DatasetMetadata: Instance of DatasetMetadata.\n        \"\"\"\n        metadata = {k: v for k, v in metadata.items() if k in inspect.signature(cls).parameters}\n        metadata[\"task\"] = Task(metadata[\"task\"])\n        return cls(**metadata)\n\n    @classmethod\n    def from_json(cls, path: str):\n        \"\"\"Create DatasetMetadata from a json file.\n\n        Args:\n            path (str): Path to json file.\n\n        Returns:\n            DatasetMetadata: Instance of DatasetMetadata.\n        \"\"\"\n        with open(path, encoding=\"utf-8\") as f:\n            metadata = json.load(f)\n        metadata[\"task\"] = Task(metadata[\"task\"])\n        return cls(**metadata)\n\n    def dump_json(self, path: str):\n        \"\"\"Dump DatasetMetadata to a json file.\n\n        Args:\n            path (str): Path to json file.\n        \"\"\"\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(asdict(self), f, ensure_ascii=False, indent=4)\n\n    def get(self, attr, default=None):\n        if hasattr(self, attr):\n            return getattr(self, attr)\n        else:\n            return default\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetMetadata.dump_json","title":"<code>dump_json(path)</code>","text":"<p>Dump DatasetMetadata to a json file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to json file.</p> required Source code in <code>focoos/ports.py</code> <pre><code>def dump_json(self, path: str):\n    \"\"\"Dump DatasetMetadata to a json file.\n\n    Args:\n        path (str): Path to json file.\n    \"\"\"\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(asdict(self), f, ensure_ascii=False, indent=4)\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetMetadata.from_dict","title":"<code>from_dict(metadata)</code>  <code>classmethod</code>","text":"<p>Create DatasetMetadata from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>Dictionary containing metadata.</p> required <p>Returns:</p> Name Type Description <code>DatasetMetadata</code> <p>Instance of DatasetMetadata.</p> Source code in <code>focoos/ports.py</code> <pre><code>@classmethod\ndef from_dict(cls, metadata: dict):\n    \"\"\"Create DatasetMetadata from a dictionary.\n\n    Args:\n        metadata (dict): Dictionary containing metadata.\n\n    Returns:\n        DatasetMetadata: Instance of DatasetMetadata.\n    \"\"\"\n    metadata = {k: v for k, v in metadata.items() if k in inspect.signature(cls).parameters}\n    metadata[\"task\"] = Task(metadata[\"task\"])\n    return cls(**metadata)\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetMetadata.from_json","title":"<code>from_json(path)</code>  <code>classmethod</code>","text":"<p>Create DatasetMetadata from a json file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to json file.</p> required <p>Returns:</p> Name Type Description <code>DatasetMetadata</code> <p>Instance of DatasetMetadata.</p> Source code in <code>focoos/ports.py</code> <pre><code>@classmethod\ndef from_json(cls, path: str):\n    \"\"\"Create DatasetMetadata from a json file.\n\n    Args:\n        path (str): Path to json file.\n\n    Returns:\n        DatasetMetadata: Instance of DatasetMetadata.\n    \"\"\"\n    with open(path, encoding=\"utf-8\") as f:\n        metadata = json.load(f)\n    metadata[\"task\"] = Task(metadata[\"task\"])\n    return cls(**metadata)\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetPreview","title":"<code>DatasetPreview</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Preview information for a Focoos dataset.</p> <p>This class provides metadata about a dataset in the Focoos platform, including its identification, task type, and layout format.</p> <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>Unique reference ID for the dataset.</p> <code>name</code> <code>str</code> <p>Human-readable name of the dataset.</p> <code>task</code> <code>FocoosTask</code> <p>The computer vision task this dataset is designed for.</p> <code>layout</code> <code>DatasetLayout</code> <p>The structural format of the dataset (e.g., ROBOFLOW_COCO, ROBOFLOW_SEG, SUPERVISELY).</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the dataset's purpose or contents.</p> <code>spec</code> <code>Optional[DatasetSpec]</code> <p>Detailed specifications about the dataset's composition and size.</p> Source code in <code>focoos/ports.py</code> <pre><code>class DatasetPreview(PydanticBase):\n    \"\"\"Preview information for a Focoos dataset.\n\n    This class provides metadata about a dataset in the Focoos platform,\n    including its identification, task type, and layout format.\n\n    Attributes:\n        ref (str): Unique reference ID for the dataset.\n        name (str): Human-readable name of the dataset.\n        task (FocoosTask): The computer vision task this dataset is designed for.\n        layout (DatasetLayout): The structural format of the dataset (e.g., ROBOFLOW_COCO, ROBOFLOW_SEG, SUPERVISELY).\n        description (Optional[str]): Optional description of the dataset's purpose or contents.\n        spec (Optional[DatasetSpec]): Detailed specifications about the dataset's composition and size.\n    \"\"\"\n\n    ref: str\n    name: str\n    task: Task\n    layout: DatasetLayout\n    description: Optional[str] = None\n    spec: Optional[DatasetSpec] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetSpec","title":"<code>DatasetSpec</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Specification details for a dataset in the Focoos platform.</p> <p>This class provides information about the dataset's size and composition, including the number of samples in training and validation sets and the total size.</p> <p>Attributes:</p> Name Type Description <code>train_length</code> <code>int</code> <p>Number of samples in the training set.</p> <code>valid_length</code> <code>int</code> <p>Number of samples in the validation set.</p> <code>size_mb</code> <code>float</code> <p>Total size of the dataset in megabytes.</p> Source code in <code>focoos/ports.py</code> <pre><code>class DatasetSpec(PydanticBase):\n    \"\"\"Specification details for a dataset in the Focoos platform.\n\n    This class provides information about the dataset's size and composition,\n    including the number of samples in training and validation sets and the total size.\n\n    Attributes:\n        train_length (int): Number of samples in the training set.\n        valid_length (int): Number of samples in the validation set.\n        size_mb (float): Total size of the dataset in megabytes.\n    \"\"\"\n\n    train_length: int\n    valid_length: int\n    size_mb: float\n</code></pre>"},{"location":"api/ports/#focoos.ports.DictClass","title":"<code>DictClass</code>","text":"<p>               Bases: <code>OrderedDict</code></p> Source code in <code>focoos/ports.py</code> <pre><code>class DictClass(OrderedDict):\n    def to_tuple(self) -&gt; tuple[Any]:\n        \"\"\"\n        Convert self to a tuple containing all the attributes/keys that are not `None`.\n        \"\"\"\n        return tuple(\n            self[k] for k in self.keys() if self[k] is not None\n        )  # without this check we are unable to export models with None values\n\n    def __getitem__(self, k):\n        if isinstance(k, str):\n            inner_dict = dict(self.items())\n            return inner_dict[k]\n        else:\n            return self.to_tuple()[k]\n\n    def __setattr__(self, name, value):\n        if name in self.keys() and value is not None:\n            # Don't call self.__setitem__ to avoid recursion errors\n            super().__setitem__(name, value)\n        super().__setattr__(name, value)\n\n    def __setitem__(self, key, value):\n        # Will raise a KeyException if needed\n        super().__setitem__(key, value)\n        # Don't call self.__setattr__ to avoid recursion errors\n        super().__setattr__(key, value)\n\n    def __post_init__(self):\n        \"\"\"Check the BasicContainer dataclass.\n\n        Only occurs if @dataclass decorator has been used.\n        \"\"\"\n        class_fields = fields(self)\n\n        # Safety and consistency checks\n        if not len(class_fields):\n            raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n\n        for _field in class_fields:\n            v = getattr(self, _field.name)\n            # if v is not None:  # without this check we are unable to export models with None values\n            #    self[_field.name] = v\n            self[_field.name] = v\n\n    def __reduce__(self):\n        state_dict = {field.name: getattr(self, field.name) for field in fields(self)}\n        return (self.__class__.__new__, (self.__class__,), state_dict)\n</code></pre>"},{"location":"api/ports/#focoos.ports.DictClass.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check the BasicContainer dataclass.</p> <p>Only occurs if @dataclass decorator has been used.</p> Source code in <code>focoos/ports.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Check the BasicContainer dataclass.\n\n    Only occurs if @dataclass decorator has been used.\n    \"\"\"\n    class_fields = fields(self)\n\n    # Safety and consistency checks\n    if not len(class_fields):\n        raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n\n    for _field in class_fields:\n        v = getattr(self, _field.name)\n        # if v is not None:  # without this check we are unable to export models with None values\n        #    self[_field.name] = v\n        self[_field.name] = v\n</code></pre>"},{"location":"api/ports/#focoos.ports.DictClass.to_tuple","title":"<code>to_tuple()</code>","text":"<p>Convert self to a tuple containing all the attributes/keys that are not <code>None</code>.</p> Source code in <code>focoos/ports.py</code> <pre><code>def to_tuple(self) -&gt; tuple[Any]:\n    \"\"\"\n    Convert self to a tuple containing all the attributes/keys that are not `None`.\n    \"\"\"\n    return tuple(\n        self[k] for k in self.keys() if self[k] is not None\n    )  # without this check we are unable to export models with None values\n</code></pre>"},{"location":"api/ports/#focoos.ports.DynamicAxes","title":"<code>DynamicAxes</code>  <code>dataclass</code>","text":"<p>Dynamic axes for model export.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass DynamicAxes:\n    \"\"\"Dynamic axes for model export.\"\"\"\n\n    input_names: list[str]\n    output_names: list[str]\n    dynamic_axes: dict\n</code></pre>"},{"location":"api/ports/#focoos.ports.ExportCfg","title":"<code>ExportCfg</code>  <code>dataclass</code>","text":"<p>Configuration for model export.</p> <p>Parameters:</p> Name Type Description Default <code>out_dir</code> <code>str</code> <p>Output directory for exported model</p> required <code>onnx_opset</code> <code>int</code> <p>ONNX opset version to use</p> <code>17</code> <code>onnx_dynamic</code> <code>bool</code> <p>Whether to use dynamic axes in ONNX export</p> <code>True</code> <code>onnx_simplify</code> <code>bool</code> <p>Whether to simplify ONNX model</p> <code>True</code> <code>model_fuse</code> <code>bool</code> <p>Whether to fuse model layers</p> <code>True</code> <code>format</code> <code>Literal['onnx', 'torchscript']</code> <p>Export format (\"onnx\" or \"torchscript\")</p> <code>'onnx'</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for export</p> <code>'cuda'</code> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass ExportCfg:\n    \"\"\"Configuration for model export.\n\n    Args:\n        out_dir: Output directory for exported model\n        onnx_opset: ONNX opset version to use\n        onnx_dynamic: Whether to use dynamic axes in ONNX export\n        onnx_simplify: Whether to simplify ONNX model\n        model_fuse: Whether to fuse model layers\n        format: Export format (\"onnx\" or \"torchscript\")\n        device: Device to use for export\n    \"\"\"\n\n    out_dir: str\n    onnx_opset: int = 17\n    onnx_dynamic: bool = True\n    onnx_simplify: bool = True\n    model_fuse: bool = True\n    format: Literal[\"onnx\", \"torchscript\"] = \"onnx\"\n    device: Optional[str] = \"cuda\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.ExportFormat","title":"<code>ExportFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Available export formats for model inference.</p> Values <ul> <li>ONNX: ONNX format</li> <li>TORCHSCRIPT: TorchScript format</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class ExportFormat(str, Enum):\n    \"\"\"Available export formats for model inference.\n\n    Values:\n        - ONNX: ONNX format\n        - TORCHSCRIPT: TorchScript format\n\n    \"\"\"\n\n    ONNX = \"onnx\"\n    TORCHSCRIPT = \"torchscript\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.FocoosDet","title":"<code>FocoosDet</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Single detection result from a model.</p> <p>This class represents a single detection or segmentation result from a Focoos model. It contains information about the detected object including its position, class, confidence score, and optional segmentation mask.</p> <p>Attributes:</p> Name Type Description <code>bbox</code> <code>Optional[list[int]]</code> <p>Bounding box coordinates in [x1, y1, x2, y2] format, where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.</p> <code>conf</code> <code>Optional[float]</code> <p>Confidence score of the detection, ranging from 0 to 1.</p> <code>cls_id</code> <code>Optional[int]</code> <p>Class ID of the detected object, corresponding to the index in the model's class list.</p> <code>label</code> <code>Optional[str]</code> <p>Human-readable label of the detected object.</p> <code>mask</code> <code>Optional[str]</code> <p>Base64-encoded PNG image representing the segmentation mask. Note that the mask is cropped to the bounding box coordinates and does not have the same shape as the input image.</p> <p>Note</p> <p>The mask is only present if the model is an instance segmentation or semantic segmentation model. The mask is a base64 encoded string having origin in the top left corner of bbox and the same width and height of the bbox.</p> Source code in <code>focoos/ports.py</code> <pre><code>class FocoosDet(PydanticBase):\n    \"\"\"Single detection result from a model.\n\n    This class represents a single detection or segmentation result from a Focoos model.\n    It contains information about the detected object including its position, class,\n    confidence score, and optional segmentation mask.\n\n    Attributes:\n        bbox (Optional[list[int]]): Bounding box coordinates in [x1, y1, x2, y2] format,\n            where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.\n        conf (Optional[float]): Confidence score of the detection, ranging from 0 to 1.\n        cls_id (Optional[int]): Class ID of the detected object, corresponding to the index\n            in the model's class list.\n        label (Optional[str]): Human-readable label of the detected object.\n        mask (Optional[str]): Base64-encoded PNG image representing the segmentation mask.\n            Note that the mask is cropped to the bounding box coordinates and does not\n            have the same shape as the input image.\n\n    !!! Note\n        The mask is only present if the model is an instance segmentation or semantic segmentation model.\n        The mask is a base64 encoded string having origin in the top left corner of bbox and the same width and height of the bbox.\n\n    \"\"\"\n\n    bbox: Optional[list[int]] = None\n    conf: Optional[float] = None\n    cls_id: Optional[int] = None\n    label: Optional[str] = None\n    mask: Optional[str] = None\n\n    @classmethod\n    def from_json(cls, data: Union[str, dict]):\n        if isinstance(data, str):\n            with open(data, encoding=\"utf-8\") as f:\n                data_dict = json.load(f)\n        else:\n            data_dict = data\n\n        bbox = data_dict.get(\"bbox\")\n        if bbox is not None:  # Retrocompatibility fix for remote results with float bbox, !TODO remove asap\n            data_dict[\"bbox\"] = list(map(int, bbox))\n\n        return cls.model_validate(data_dict)\n</code></pre>"},{"location":"api/ports/#focoos.ports.FocoosDetections","title":"<code>FocoosDetections</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Collection of detection results from a model.</p> <p>This class represents a collection of detection or segmentation results from a Focoos model. It contains a list of individual detections and optional latency information.</p> <p>Attributes:</p> Name Type Description <code>detections</code> <code>list[FocoosDet]</code> <p>List of detection results, where each detection contains information about a detected object including its position, class, confidence score, and optional segmentation mask.</p> <code>latency</code> <code>Optional[dict]</code> <p>Dictionary containing latency information for the inference process. Typically includes keys like 'inference', 'preprocess', and 'postprocess' with values representing the time taken in seconds for each step.</p> Source code in <code>focoos/ports.py</code> <pre><code>class FocoosDetections(PydanticBase):\n    \"\"\"Collection of detection results from a model.\n\n    This class represents a collection of detection or segmentation results from a Focoos model.\n    It contains a list of individual detections and optional latency information.\n\n    Attributes:\n        detections (list[FocoosDet]): List of detection results, where each detection contains\n            information about a detected object including its position, class, confidence score,\n            and optional segmentation mask.\n        latency (Optional[dict]): Dictionary containing latency information for the inference process.\n            Typically includes keys like 'inference', 'preprocess', and 'postprocess' with values\n            representing the time taken in seconds for each step.\n    \"\"\"\n\n    detections: list[FocoosDet]\n    latency: Optional[dict] = None\n\n    def __len__(self):\n        return len(self.detections)\n</code></pre>"},{"location":"api/ports/#focoos.ports.GPUDevice","title":"<code>GPUDevice</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Information about a GPU device.</p> Source code in <code>focoos/ports.py</code> <pre><code>class GPUDevice(PydanticBase):\n    \"\"\"Information about a GPU device.\"\"\"\n\n    gpu_id: Optional[int] = None\n    gpu_name: Optional[str] = None\n    gpu_memory_total_gb: Optional[float] = None\n    gpu_memory_used_percentage: Optional[float] = None\n    gpu_temperature: Optional[float] = None\n    gpu_load_percentage: Optional[float] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.GPUInfo","title":"<code>GPUInfo</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Information about a GPU driver.</p> Source code in <code>focoos/ports.py</code> <pre><code>class GPUInfo(PydanticBase):\n    \"\"\"Information about a GPU driver.\"\"\"\n\n    gpu_count: Optional[int] = None\n    gpu_driver: Optional[str] = None\n    gpu_cuda_version: Optional[str] = None\n    total_gpu_memory_gb: Optional[float] = None\n    devices: Optional[list[GPUDevice]] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.LatencyMetrics","title":"<code>LatencyMetrics</code>  <code>dataclass</code>","text":"<p>Performance metrics for model inference.</p> <p>This class provides performance metrics for model inference, including frames per second (FPS), engine used, minimum latency, maximum latency, mean latency, standard deviation of latency, input image size, and device type.</p> <p>Attributes:</p> Name Type Description <code>fps</code> <code>int</code> <p>Frames per second (FPS) of the inference process.</p> <code>engine</code> <code>str</code> <p>The inference engine used (e.g., \"onnx\", \"torchscript\").</p> <code>min</code> <code>float</code> <p>Minimum latency in milliseconds.</p> <code>max</code> <code>float</code> <p>Maximum latency in milliseconds.</p> <code>mean</code> <code>float</code> <p>Mean latency in milliseconds.</p> <code>std</code> <code>float</code> <p>Standard deviation of latency in milliseconds.</p> <code>im_size</code> <code>int</code> <p>Input image size.</p> <code>device</code> <code>str</code> <p>Device type.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass LatencyMetrics:\n    \"\"\"Performance metrics for model inference.\n\n    This class provides performance metrics for model inference, including frames per second (FPS),\n    engine used, minimum latency, maximum latency, mean latency, standard deviation of latency,\n    input image size, and device type.\n\n    Attributes:\n        fps (int): Frames per second (FPS) of the inference process.\n        engine (str): The inference engine used (e.g., \"onnx\", \"torchscript\").\n        min (float): Minimum latency in milliseconds.\n        max (float): Maximum latency in milliseconds.\n        mean (float): Mean latency in milliseconds.\n        std (float): Standard deviation of latency in milliseconds.\n        im_size (int): Input image size.\n        device (str): Device type.\n    \"\"\"\n\n    fps: int\n    engine: str\n    min: float\n    max: float\n    mean: float\n    std: float\n    im_size: int\n    device: str\n</code></pre>"},{"location":"api/ports/#focoos.ports.Metrics","title":"<code>Metrics</code>  <code>dataclass</code>","text":"<p>Collection of training and inference metrics.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass Metrics:\n    \"\"\"\n    Collection of training and inference metrics.\n    \"\"\"\n\n    infer_metrics: list[dict] = field(default_factory=list)\n    valid_metrics: list[dict] = field(default_factory=list)\n    train_metrics: list[dict] = field(default_factory=list)\n    iterations: Optional[int] = None\n    best_valid_metric: Optional[dict] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelExtension","title":"<code>ModelExtension</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported model extension.</p> Values <ul> <li>ONNX: ONNX format</li> <li>TORCHSCRIPT: TorchScript format</li> <li>WEIGHTS: Weights format</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class ModelExtension(str, Enum):\n    \"\"\"Supported model extension.\n\n    Values:\n        - ONNX: ONNX format\n        - TORCHSCRIPT: TorchScript format\n        - WEIGHTS: Weights format\n    \"\"\"\n\n    ONNX = \"onnx\"\n    TORCHSCRIPT = \"pt\"\n    WEIGHTS = \"pth\"\n\n    @classmethod\n    def from_runtime_type(cls, runtime_type: RuntimeType):\n        if runtime_type in [\n            RuntimeType.ONNX_CUDA32,\n            RuntimeType.ONNX_TRT32,\n            RuntimeType.ONNX_TRT16,\n            RuntimeType.ONNX_CPU,\n            RuntimeType.ONNX_COREML,\n        ]:\n            return cls.ONNX\n        elif runtime_type == RuntimeType.TORCHSCRIPT_32:\n            return cls.TORCHSCRIPT\n        else:\n            raise ValueError(f\"Invalid runtime type: {runtime_type}\")\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelFamily","title":"<code>ModelFamily</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumerazione delle famiglie di modelli disponibili</p> Source code in <code>focoos/ports.py</code> <pre><code>class ModelFamily(str, Enum):\n    \"\"\"Enumerazione delle famiglie di modelli disponibili\"\"\"\n\n    DETR = \"fai_detr\"\n    MASKFORMER = \"fai_mf\"\n    BISENETFORMER = \"bisenetformer\"\n    IMAGE_CLASSIFIER = \"fai_cls\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelInfo","title":"<code>ModelInfo</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DictClass</code></p> <p>Comprehensive metadata for a Focoos model.</p> <p>This dataclass encapsulates all relevant information required to identify, configure, and evaluate a model within the Focoos platform. It is used for serialization, deserialization, and programmatic access to model properties.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Human-readable name or unique identifier for the model.</p> <code>model_family</code> <code>ModelFamily</code> <p>The model's architecture family (e.g., RTDETR, M2F).</p> <code>classes</code> <code>list[str]</code> <p>List of class names that the model can detect or segment.</p> <code>im_size</code> <code>int</code> <p>Input image size (usually square, e.g., 640).</p> <code>task</code> <code>Task</code> <p>Computer vision task performed by the model (e.g., detection, segmentation).</p> <code>config</code> <code>dict</code> <p>Model-specific configuration parameters.</p> <code>ref</code> <code>Optional[str]</code> <p>Optional unique reference string for the model.</p> <code>focoos_model</code> <code>Optional[str]</code> <p>Optional Focoos base model identifier.</p> <code>status</code> <code>Optional[ModelStatus]</code> <p>Current status of the model (e.g., training, ready).</p> <code>description</code> <code>Optional[str]</code> <p>Optional human-readable description of the model.</p> <code>train_args</code> <code>Optional[TrainerArgs]</code> <p>Optional training arguments used to train the model.</p> <code>weights_uri</code> <code>Optional[str]</code> <p>Optional URI or path to the model weights.</p> <code>val_dataset</code> <code>Optional[str]</code> <p>Optional name or reference of the validation dataset.</p> <code>val_metrics</code> <code>Optional[dict]</code> <p>Optional dictionary of validation metrics (e.g., mAP, accuracy).</p> <code>focoos_version</code> <code>Optional[str]</code> <p>Optional Focoos version string.</p> <code>latency</code> <code>Optional[list[LatencyMetrics]]</code> <p>Optional list of latency measurements for different runtimes.</p> <code>updated_at</code> <code>Optional[str]</code> <p>Optional ISO timestamp of the last update.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass ModelInfo(DictClass):\n    \"\"\"\n    Comprehensive metadata for a Focoos model.\n\n    This dataclass encapsulates all relevant information required to identify, configure, and evaluate a model\n    within the Focoos platform. It is used for serialization, deserialization, and programmatic access to model\n    properties.\n\n    Attributes:\n        name (str): Human-readable name or unique identifier for the model.\n        model_family (ModelFamily): The model's architecture family (e.g., RTDETR, M2F).\n        classes (list[str]): List of class names that the model can detect or segment.\n        im_size (int): Input image size (usually square, e.g., 640).\n        task (Task): Computer vision task performed by the model (e.g., detection, segmentation).\n        config (dict): Model-specific configuration parameters.\n        ref (Optional[str]): Optional unique reference string for the model.\n        focoos_model (Optional[str]): Optional Focoos base model identifier.\n        status (Optional[ModelStatus]): Current status of the model (e.g., training, ready).\n        description (Optional[str]): Optional human-readable description of the model.\n        train_args (Optional[TrainerArgs]): Optional training arguments used to train the model.\n        weights_uri (Optional[str]): Optional URI or path to the model weights.\n        val_dataset (Optional[str]): Optional name or reference of the validation dataset.\n        val_metrics (Optional[dict]): Optional dictionary of validation metrics (e.g., mAP, accuracy).\n        focoos_version (Optional[str]): Optional Focoos version string.\n        latency (Optional[list[LatencyMetrics]]): Optional list of latency measurements for different runtimes.\n        updated_at (Optional[str]): Optional ISO timestamp of the last update.\n    \"\"\"\n\n    name: str\n    model_family: ModelFamily\n    classes: list[str]\n    im_size: int\n    task: Task\n    config: dict\n    ref: Optional[str] = None\n    focoos_model: Optional[str] = None\n    status: Optional[ModelStatus] = None\n    description: Optional[str] = None\n    train_args: Optional[TrainerArgs] = None\n    weights_uri: Optional[str] = None\n    val_dataset: Optional[str] = None\n    val_metrics: Optional[dict] = None  # TODO: Consider making metrics explicit in the future\n    focoos_version: Optional[str] = None\n    latency: Optional[list[LatencyMetrics]] = None\n    training_info: Optional[TrainingInfo] = None\n    updated_at: Optional[str] = None\n\n    @classmethod\n    def from_json(cls, data: Union[str, dict]):\n        \"\"\"\n        Load ModelInfo from a JSON file.\n\n        Args:\n            path (Optional[str]): Path to the JSON file containing model metadata.\n            data (Optional[dict]): Dictionary containing model metadata.\n\n        Returns:\n            ModelInfo: An instance of ModelInfo populated with data from the file.\n        \"\"\"\n        assert isinstance(data, dict) or isinstance(data, str), \"data must be a dictionary or a path to a JSON file\"\n        if isinstance(data, str):\n            with open(data, encoding=\"utf-8\") as f:\n                model_info_json = json.load(f)\n        else:\n            model_info_json = data\n\n        training_info = None\n        if \"training_info\" in model_info_json and model_info_json[\"training_info\"] is not None:\n            training_info = TrainingInfo(\n                **{k: v for k, v in model_info_json[\"training_info\"].items() if k in TrainingInfo.__dataclass_fields__}\n            )\n\n        model_info = cls(\n            name=model_info_json[\"name\"],\n            ref=model_info_json.get(\"ref\", None),\n            model_family=ModelFamily(model_info_json[\"model_family\"]),\n            classes=model_info_json[\"classes\"],\n            im_size=int(model_info_json[\"im_size\"]),\n            status=ModelStatus(model_info_json.get(\"status\")) if model_info_json.get(\"status\") else None,\n            task=Task(model_info_json[\"task\"]),\n            focoos_model=model_info_json.get(\"focoos_model\", None),\n            config=model_info_json[\"config\"],\n            description=model_info_json.get(\"description\", None),\n            train_args=TrainerArgs(\n                **{k: v for k, v in model_info_json[\"train_args\"].items() if k in TrainerArgs.__dataclass_fields__}\n            )\n            if \"train_args\" in model_info_json and model_info_json[\"train_args\"] is not None\n            else None,\n            weights_uri=model_info_json.get(\"weights_uri\", None),\n            val_dataset=model_info_json.get(\"val_dataset\", None),\n            latency=[LatencyMetrics(**latency) for latency in model_info_json.get(\"latency\", [])]\n            if \"latency\" in model_info_json and model_info_json[\"latency\"] is not None\n            else None,\n            updated_at=model_info_json.get(\"updated_at\", None),\n            focoos_version=model_info_json.get(\"focoos_version\", None),\n            val_metrics=model_info_json.get(\"val_metrics\", None),\n            training_info=training_info,\n        )\n        return model_info\n\n    def dump_json(self, path: str):\n        \"\"\"\n        Serialize ModelInfo to a JSON file.\n\n        Args:\n            path (str): Path where the JSON file will be saved.\n        \"\"\"\n        data = asdict(self)\n        # Note: config_class is not included; if needed, convert to string here.\n\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, ensure_ascii=False, indent=4)\n\n    def pprint(self):\n        \"\"\"\n        Pretty-print the main model information using the Focoos logger.\n        \"\"\"\n        from focoos.utils.logger import get_logger\n\n        logger = get_logger(\"model_info\")\n        logger.info(\n            f\"\"\"\n            \ud83d\udccb Name: {self.name}\n            \ud83d\udcdd Description: {self.description}\n            \ud83d\udc6a Family: {self.model_family}\n            \ud83d\udd17 Focoos Model: {self.focoos_model}\n            \ud83c\udfaf Task: {self.task}\n            \ud83c\udff7\ufe0f Classes: {self.classes}\n            \ud83d\uddbc\ufe0f Im size: {self.im_size}\n            \"\"\"\n        )\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelInfo.dump_json","title":"<code>dump_json(path)</code>","text":"<p>Serialize ModelInfo to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where the JSON file will be saved.</p> required Source code in <code>focoos/ports.py</code> <pre><code>def dump_json(self, path: str):\n    \"\"\"\n    Serialize ModelInfo to a JSON file.\n\n    Args:\n        path (str): Path where the JSON file will be saved.\n    \"\"\"\n    data = asdict(self)\n    # Note: config_class is not included; if needed, convert to string here.\n\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, ensure_ascii=False, indent=4)\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelInfo.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Load ModelInfo from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Path to the JSON file containing model metadata.</p> required <code>data</code> <code>Optional[dict]</code> <p>Dictionary containing model metadata.</p> required <p>Returns:</p> Name Type Description <code>ModelInfo</code> <p>An instance of ModelInfo populated with data from the file.</p> Source code in <code>focoos/ports.py</code> <pre><code>@classmethod\ndef from_json(cls, data: Union[str, dict]):\n    \"\"\"\n    Load ModelInfo from a JSON file.\n\n    Args:\n        path (Optional[str]): Path to the JSON file containing model metadata.\n        data (Optional[dict]): Dictionary containing model metadata.\n\n    Returns:\n        ModelInfo: An instance of ModelInfo populated with data from the file.\n    \"\"\"\n    assert isinstance(data, dict) or isinstance(data, str), \"data must be a dictionary or a path to a JSON file\"\n    if isinstance(data, str):\n        with open(data, encoding=\"utf-8\") as f:\n            model_info_json = json.load(f)\n    else:\n        model_info_json = data\n\n    training_info = None\n    if \"training_info\" in model_info_json and model_info_json[\"training_info\"] is not None:\n        training_info = TrainingInfo(\n            **{k: v for k, v in model_info_json[\"training_info\"].items() if k in TrainingInfo.__dataclass_fields__}\n        )\n\n    model_info = cls(\n        name=model_info_json[\"name\"],\n        ref=model_info_json.get(\"ref\", None),\n        model_family=ModelFamily(model_info_json[\"model_family\"]),\n        classes=model_info_json[\"classes\"],\n        im_size=int(model_info_json[\"im_size\"]),\n        status=ModelStatus(model_info_json.get(\"status\")) if model_info_json.get(\"status\") else None,\n        task=Task(model_info_json[\"task\"]),\n        focoos_model=model_info_json.get(\"focoos_model\", None),\n        config=model_info_json[\"config\"],\n        description=model_info_json.get(\"description\", None),\n        train_args=TrainerArgs(\n            **{k: v for k, v in model_info_json[\"train_args\"].items() if k in TrainerArgs.__dataclass_fields__}\n        )\n        if \"train_args\" in model_info_json and model_info_json[\"train_args\"] is not None\n        else None,\n        weights_uri=model_info_json.get(\"weights_uri\", None),\n        val_dataset=model_info_json.get(\"val_dataset\", None),\n        latency=[LatencyMetrics(**latency) for latency in model_info_json.get(\"latency\", [])]\n        if \"latency\" in model_info_json and model_info_json[\"latency\"] is not None\n        else None,\n        updated_at=model_info_json.get(\"updated_at\", None),\n        focoos_version=model_info_json.get(\"focoos_version\", None),\n        val_metrics=model_info_json.get(\"val_metrics\", None),\n        training_info=training_info,\n    )\n    return model_info\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelInfo.pprint","title":"<code>pprint()</code>","text":"<p>Pretty-print the main model information using the Focoos logger.</p> Source code in <code>focoos/ports.py</code> <pre><code>def pprint(self):\n    \"\"\"\n    Pretty-print the main model information using the Focoos logger.\n    \"\"\"\n    from focoos.utils.logger import get_logger\n\n    logger = get_logger(\"model_info\")\n    logger.info(\n        f\"\"\"\n        \ud83d\udccb Name: {self.name}\n        \ud83d\udcdd Description: {self.description}\n        \ud83d\udc6a Family: {self.model_family}\n        \ud83d\udd17 Focoos Model: {self.focoos_model}\n        \ud83c\udfaf Task: {self.task}\n        \ud83c\udff7\ufe0f Classes: {self.classes}\n        \ud83d\uddbc\ufe0f Im size: {self.im_size}\n        \"\"\"\n    )\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelNotFound","title":"<code>ModelNotFound</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when a requested model is not found.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ModelNotFound(Exception):\n    \"\"\"Exception raised when a requested model is not found.\"\"\"\n\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelOutput","title":"<code>ModelOutput</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DictClass</code></p> <p>Model output base container.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass ModelOutput(DictClass):\n    \"\"\"Model output base container.\"\"\"\n\n    loss: Optional[dict]\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelPreview","title":"<code>ModelPreview</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Preview information for a Focoos model.</p> <p>This class provides a lightweight preview of model information in the Focoos platform, containing essential details like reference ID, name, task type, and status.</p> <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>Unique reference ID for the model.</p> <code>name</code> <code>str</code> <p>Human-readable name of the model.</p> <code>task</code> <code>FocoosTask</code> <p>The computer vision task this model is designed for.</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the model's purpose or capabilities.</p> <code>status</code> <code>ModelStatus</code> <p>Current status of the model (e.g., training, ready, failed).</p> <code>focoos_model</code> <code>str</code> <p>The base model architecture identifier.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ModelPreview(PydanticBase):\n    \"\"\"Preview information for a Focoos model.\n\n    This class provides a lightweight preview of model information in the Focoos platform,\n    containing essential details like reference ID, name, task type, and status.\n\n    Attributes:\n        ref (str): Unique reference ID for the model.\n        name (str): Human-readable name of the model.\n        task (FocoosTask): The computer vision task this model is designed for.\n        description (Optional[str]): Optional description of the model's purpose or capabilities.\n        status (ModelStatus): Current status of the model (e.g., training, ready, failed).\n        focoos_model (str): The base model architecture identifier.\n    \"\"\"\n\n    ref: str\n    name: str\n    task: Task\n    description: Optional[str] = None\n    status: ModelStatus\n    focoos_model: str\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelStatus","title":"<code>ModelStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of a Focoos model during its lifecycle.</p> Values <ul> <li>CREATED: Model has been created</li> <li>TRAINING_STARTING: Training is about to start</li> <li>TRAINING_RUNNING: Training is in progress</li> <li>TRAINING_ERROR: Training encountered an error</li> <li>TRAINING_COMPLETED: Training finished successfully</li> <li>TRAINING_STOPPED: Training was stopped</li> <li>DEPLOYED: Model is deployed</li> <li>DEPLOY_ERROR: Deployment encountered an error</li> </ul> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\nmodel = focoos.get_remote_model(\"my-model\")\n\nif model.status == ModelStatus.DEPLOYED:\n    print(\"Model is deployed and ready for inference\")\nelif model.status == ModelStatus.TRAINING_RUNNING:\n    print(\"Model is currently training\")\nelif model.status == ModelStatus.TRAINING_ERROR:\n    print(\"Model training encountered an error\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class ModelStatus(str, Enum):\n    \"\"\"Status of a Focoos model during its lifecycle.\n\n    Values:\n        - CREATED: Model has been created\n        - TRAINING_STARTING: Training is about to start\n        - TRAINING_RUNNING: Training is in progress\n        - TRAINING_ERROR: Training encountered an error\n        - TRAINING_COMPLETED: Training finished successfully\n        - TRAINING_STOPPED: Training was stopped\n        - DEPLOYED: Model is deployed\n        - DEPLOY_ERROR: Deployment encountered an error\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n        model = focoos.get_remote_model(\"my-model\")\n\n        if model.status == ModelStatus.DEPLOYED:\n            print(\"Model is deployed and ready for inference\")\n        elif model.status == ModelStatus.TRAINING_RUNNING:\n            print(\"Model is currently training\")\n        elif model.status == ModelStatus.TRAINING_ERROR:\n            print(\"Model training encountered an error\")\n        ```\n    \"\"\"\n\n    CREATED = \"CREATED\"\n    TRAINING_STARTING = \"TRAINING_STARTING\"\n    TRAINING_RUNNING = \"TRAINING_RUNNING\"\n    TRAINING_ERROR = \"TRAINING_ERROR\"\n    TRAINING_COMPLETED = \"TRAINING_COMPLETED\"\n    TRAINING_STOPPED = \"TRAINING_STOPPED\"\n    DEPLOYED = \"DEPLOYED\"\n    DEPLOY_ERROR = \"DEPLOY_ERROR\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.OnnxRuntimeOpts","title":"<code>OnnxRuntimeOpts</code>  <code>dataclass</code>","text":"<p>ONNX runtime configuration options.</p> <p>This class provides configuration options for the ONNX runtime used for model inference.</p> <p>Attributes:</p> Name Type Description <code>fp16</code> <code>Optional[bool]</code> <p>Enable FP16 precision. Default is False.</p> <code>cuda</code> <code>Optional[bool]</code> <p>Enable CUDA acceleration for GPU inference. Default is False.</p> <code>vino</code> <code>Optional[bool]</code> <p>Enable OpenVINO acceleration for Intel hardware. Default is False.</p> <code>verbose</code> <code>Optional[bool]</code> <p>Enable verbose logging during inference. Default is False.</p> <code>trt</code> <code>Optional[bool]</code> <p>Enable TensorRT acceleration for NVIDIA GPUs. Default is False.</p> <code>coreml</code> <code>Optional[bool]</code> <p>Enable CoreML acceleration for Apple hardware. Default is False.</p> <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations to run before benchmarking. Default is 0.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass OnnxRuntimeOpts:\n    \"\"\"ONNX runtime configuration options.\n\n    This class provides configuration options for the ONNX runtime used for model inference.\n\n    Attributes:\n        fp16 (Optional[bool]): Enable FP16 precision. Default is False.\n        cuda (Optional[bool]): Enable CUDA acceleration for GPU inference. Default is False.\n        vino (Optional[bool]): Enable OpenVINO acceleration for Intel hardware. Default is False.\n        verbose (Optional[bool]): Enable verbose logging during inference. Default is False.\n        trt (Optional[bool]): Enable TensorRT acceleration for NVIDIA GPUs. Default is False.\n        coreml (Optional[bool]): Enable CoreML acceleration for Apple hardware. Default is False.\n        warmup_iter (int): Number of warmup iterations to run before benchmarking. Default is 0.\n\n    \"\"\"\n\n    fp16: Optional[bool] = False\n    cuda: Optional[bool] = False\n    vino: Optional[bool] = False\n    verbose: Optional[bool] = False\n    trt: Optional[bool] = False\n    coreml: Optional[bool] = False\n    warmup_iter: int = 0\n</code></pre>"},{"location":"api/ports/#focoos.ports.Quotas","title":"<code>Quotas</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Usage quotas and limits for a user account.</p> <p>Attributes:</p> Name Type Description <code>total_inferences</code> <code>int</code> <p>Total number of inferences allowed.</p> <code>max_inferences</code> <code>int</code> <p>Maximum number of inferences allowed.</p> <code>used_storage_gb</code> <code>float</code> <p>Used storage in gigabytes.</p> <code>max_storage_gb</code> <code>float</code> <p>Maximum storage in gigabytes.</p> <code>active_training_jobs</code> <code>list[str]</code> <p>List of active training job IDs.</p> <code>max_active_training_jobs</code> <code>int</code> <p>Maximum number of active training jobs allowed.</p> Source code in <code>focoos/ports.py</code> <pre><code>class Quotas(PydanticBase):\n    \"\"\"Usage quotas and limits for a user account.\n\n    Attributes:\n        total_inferences (int): Total number of inferences allowed.\n        max_inferences (int): Maximum number of inferences allowed.\n        used_storage_gb (float): Used storage in gigabytes.\n        max_storage_gb (float): Maximum storage in gigabytes.\n        active_training_jobs (list[str]): List of active training job IDs.\n        max_active_training_jobs (int): Maximum number of active training jobs allowed.\n    \"\"\"\n\n    # INFERENCE\n    total_inferences: int\n    max_inferences: int\n    # STORAGE\n    used_storage_gb: float\n    max_storage_gb: float\n    # TRAINING\n    active_training_jobs: list[str]\n    max_active_training_jobs: int\n\n    # ML_G4DN_XLARGE TRAINING HOURS\n    used_mlg4dnxlarge_training_jobs_hours: float\n    max_mlg4dnxlarge_training_jobs_hours: float\n</code></pre>"},{"location":"api/ports/#focoos.ports.RemoteModelInfo","title":"<code>RemoteModelInfo</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Complete metadata for a Focoos model.</p> <p>This class contains comprehensive information about a model in the Focoos platform, including its identification, configuration, performance metrics, and training details.</p> <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>Unique reference ID for the model.</p> <code>name</code> <code>str</code> <p>Human-readable name of the model.</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the model's purpose or capabilities.</p> <code>owner_ref</code> <code>str</code> <p>Reference ID of the model owner.</p> <code>focoos_model</code> <code>str</code> <p>The base model architecture used.</p> <code>task</code> <code>FocoosTask</code> <p>The task type the model is designed for (e.g., DETECTION, SEMSEG).</p> <code>created_at</code> <code>datetime</code> <p>Timestamp when the model was created.</p> <code>updated_at</code> <code>datetime</code> <p>Timestamp when the model was last updated.</p> <code>status</code> <code>ModelStatus</code> <p>Current status of the model (e.g., TRAINING, DEPLOYED).</p> <code>metrics</code> <code>Optional[dict]</code> <p>Performance metrics of the model (e.g., mAP, accuracy).</p> <code>latencies</code> <code>Optional[list[dict]]</code> <p>Inference latency measurements across different configurations.</p> <code>classes</code> <code>Optional[list[str]]</code> <p>List of class names the model can detect or segment.</p> <code>im_size</code> <code>Optional[int]</code> <p>Input image size the model expects.</p> <code>training_info</code> <code>Optional[TrainingInfo]</code> <p>Information about the training process.</p> <code>location</code> <code>Optional[str]</code> <p>Storage location of the model.</p> <code>dataset</code> <code>Optional[DatasetPreview]</code> <p>Information about the dataset used for training.</p> Source code in <code>focoos/ports.py</code> <pre><code>class RemoteModelInfo(PydanticBase):\n    \"\"\"Complete metadata for a Focoos model.\n\n    This class contains comprehensive information about a model in the Focoos platform,\n    including its identification, configuration, performance metrics, and training details.\n\n    Attributes:\n        ref (str): Unique reference ID for the model.\n        name (str): Human-readable name of the model.\n        description (Optional[str]): Optional description of the model's purpose or capabilities.\n        owner_ref (str): Reference ID of the model owner.\n        focoos_model (str): The base model architecture used.\n        task (FocoosTask): The task type the model is designed for (e.g., DETECTION, SEMSEG).\n        created_at (datetime): Timestamp when the model was created.\n        updated_at (datetime): Timestamp when the model was last updated.\n        status (ModelStatus): Current status of the model (e.g., TRAINING, DEPLOYED).\n        metrics (Optional[dict]): Performance metrics of the model (e.g., mAP, accuracy).\n        latencies (Optional[list[dict]]): Inference latency measurements across different configurations.\n        classes (Optional[list[str]]): List of class names the model can detect or segment.\n        im_size (Optional[int]): Input image size the model expects.\n        training_info (Optional[TrainingInfo]): Information about the training process.\n        location (Optional[str]): Storage location of the model.\n        dataset (Optional[DatasetPreview]): Information about the dataset used for training.\n    \"\"\"\n\n    ref: str\n    name: str\n    description: Optional[str] = None\n    is_managed: bool\n    owner_ref: str\n    focoos_model: str\n    config: Optional[dict] = None\n    task: Task\n    created_at: datetime\n    updated_at: datetime\n    status: ModelStatus\n    model_family: Optional[str] = None\n    metrics: Optional[dict] = None\n    classes: Optional[list[str]] = None\n    im_size: Optional[int] = None\n    training_info: Optional[TrainingInfo] = None\n    dataset: Optional[DatasetPreview] = None\n    hyperparameters: Optional[dict] = None\n    focoos_version: Optional[str] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.RuntimeType","title":"<code>RuntimeType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Available runtime configurations for model inference.</p> Values <ul> <li>ONNX_CUDA32: ONNX with CUDA FP32</li> <li>ONNX_TRT32: ONNX with TensorRT FP32</li> <li>ONNX_TRT16: ONNX with TensorRT FP16</li> <li>ONNX_CPU: ONNX on CPU</li> <li>ONNX_COREML: ONNX with CoreML</li> <li>TORCHSCRIPT_32: TorchScript FP32</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class RuntimeType(str, Enum):\n    \"\"\"Available runtime configurations for model inference.\n\n    Values:\n        - ONNX_CUDA32: ONNX with CUDA FP32\n        - ONNX_TRT32: ONNX with TensorRT FP32\n        - ONNX_TRT16: ONNX with TensorRT FP16\n        - ONNX_CPU: ONNX on CPU\n        - ONNX_COREML: ONNX with CoreML\n        - TORCHSCRIPT_32: TorchScript FP32\n\n    \"\"\"\n\n    ONNX_CUDA32 = \"onnx_cuda32\"\n    ONNX_TRT32 = \"onnx_trt32\"\n    ONNX_TRT16 = \"onnx_trt16\"\n    ONNX_CPU = \"onnx_cpu\"\n    ONNX_COREML = \"onnx_coreml\"\n    TORCHSCRIPT_32 = \"torchscript_32\"\n\n    def to_export_format(self) -&gt; ExportFormat:\n        if self == RuntimeType.TORCHSCRIPT_32:\n            return ExportFormat.TORCHSCRIPT\n        else:\n            return ExportFormat.ONNX\n</code></pre>"},{"location":"api/ports/#focoos.ports.SystemInfo","title":"<code>SystemInfo</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>System information including hardware and software details.</p> Source code in <code>focoos/ports.py</code> <pre><code>class SystemInfo(PydanticBase):\n    \"\"\"System information including hardware and software details.\"\"\"\n\n    focoos_host: Optional[str] = None\n    focoos_version: Optional[str] = None\n    python_version: Optional[str] = None\n    system: Optional[str] = None\n    system_name: Optional[str] = None\n    cpu_type: Optional[str] = None\n    cpu_cores: Optional[int] = None\n    memory_gb: Optional[float] = None\n    memory_used_percentage: Optional[float] = None\n    available_onnx_providers: Optional[list[str]] = None\n    disk_space_total_gb: Optional[float] = None\n    disk_space_used_percentage: Optional[float] = None\n    pytorch_info: Optional[str] = None\n    gpu_info: Optional[GPUInfo] = None\n    packages_versions: Optional[dict[str, str]] = None\n    environment: Optional[dict[str, str]] = None\n\n    def pprint(self, level: Literal[\"INFO\", \"DEBUG\"] = \"DEBUG\"):\n        \"\"\"Pretty print the system info.\"\"\"\n        from focoos.utils.logger import get_logger\n\n        logger = get_logger(\"SystemInfo\", level=level)\n\n        output_lines = [\"\\n================ \ud83d\udd0d SYSTEM INFO \ud83d\udd0d ====================\"]\n        model_data = self.model_dump()\n\n        if \"focoos_host\" in model_data and \"focoos_version\" in model_data:\n            output_lines.append(f\"focoos: {model_data.get('focoos_host')} (v{model_data.get('focoos_version')})\")\n            model_data.pop(\"focoos_host\", None)\n            model_data.pop(\"focoos_version\", None)\n\n        if \"system\" in model_data and \"system_name\" in model_data:\n            output_lines.append(f\"system: {model_data.get('system')} ({model_data.get('system_name')})\")\n            model_data.pop(\"system\", None)\n            model_data.pop(\"system_name\", None)\n\n        if \"cpu_type\" in model_data and \"cpu_cores\" in model_data:\n            output_lines.append(f\"cpu: {model_data.get('cpu_type')} ({model_data.get('cpu_cores')} cores)\")\n            model_data.pop(\"cpu_type\", None)\n            model_data.pop(\"cpu_cores\", None)\n\n        if \"memory_gb\" in model_data and \"memory_used_percentage\" in model_data:\n            output_lines.append(\n                f\"memory_gb: {model_data.get('memory_gb')} ({model_data.get('memory_used_percentage')}% used)\"\n            )\n            model_data.pop(\"memory_gb\", None)\n            model_data.pop(\"memory_used_percentage\", None)\n\n        if \"disk_space_total_gb\" in model_data and \"disk_space_used_percentage\" in model_data:\n            output_lines.append(\n                f\"disk_space_total_gb: {model_data.get('disk_space_total_gb')} ({model_data.get('disk_space_used_percentage')}% used)\"\n            )\n            model_data.pop(\"disk_space_total_gb\", None)\n            model_data.pop(\"disk_space_used_percentage\", None)\n\n        for key, value in model_data.items():\n            if key == \"gpu_info\" and value is not None:\n                output_lines.append(f\"{key}:\")\n                output_lines.append(f\"  - gpu_count: {value.get('gpu_count')}\")\n                output_lines.append(f\"  - total_memory_gb: {value.get('total_gpu_memory_gb')} GB\")\n                output_lines.append(f\"  - gpu_driver: {value.get('gpu_driver')}\")\n                output_lines.append(f\"  - gpu_cuda_version: {value.get('gpu_cuda_version')}\")\n                if value.get(\"devices\"):\n                    output_lines.append(\"  - devices:\")\n                    for device in value.get(\"devices\", []):\n                        gpu_memory_used = (\n                            f\"{device.get('gpu_memory_used_percentage')}%\"\n                            if device.get(\"gpu_memory_used_percentage\") is not None\n                            else \"N/A\"\n                        )\n                        gpu_load = (\n                            f\"{device.get('gpu_load_percentage')}%\"\n                            if device.get(\"gpu_load_percentage\") is not None\n                            else \"N/A\"\n                        )\n                        gpu_memory_total = (\n                            f\"{device.get('gpu_memory_total_gb')} GB\"\n                            if device.get(\"gpu_memory_total_gb\") is not None\n                            else \"N/A\"\n                        )\n\n                        output_lines.append(\n                            f\"    - GPU {device.get('gpu_id')}: {device.get('gpu_name')}, Memory: {gpu_memory_total} ({gpu_memory_used} used), Load: {gpu_load}\"\n                        )\n            elif isinstance(value, list):\n                output_lines.append(f\"{key}: {value}\")\n            elif isinstance(value, dict) and key == \"packages_versions\":  # Special formatting for packages_versions\n                output_lines.append(f\"{key}:\")\n                for pkg_name, pkg_version in value.items():\n                    output_lines.append(f\"  - {pkg_name}: {pkg_version}\")\n            elif isinstance(value, dict) and key == \"environment\":  # Special formatting for environment\n                output_lines.append(f\"{key}:\")\n                for env_key, env_value in value.items():\n                    output_lines.append(f\"  - {env_key}: {env_value}\")\n            else:\n                output_lines.append(f\"{key}: {value}\")\n        output_lines.append(\"================================================\")\n\n        logger.info(\"\\n\".join(output_lines))\n</code></pre>"},{"location":"api/ports/#focoos.ports.SystemInfo.pprint","title":"<code>pprint(level='DEBUG')</code>","text":"<p>Pretty print the system info.</p> Source code in <code>focoos/ports.py</code> <pre><code>def pprint(self, level: Literal[\"INFO\", \"DEBUG\"] = \"DEBUG\"):\n    \"\"\"Pretty print the system info.\"\"\"\n    from focoos.utils.logger import get_logger\n\n    logger = get_logger(\"SystemInfo\", level=level)\n\n    output_lines = [\"\\n================ \ud83d\udd0d SYSTEM INFO \ud83d\udd0d ====================\"]\n    model_data = self.model_dump()\n\n    if \"focoos_host\" in model_data and \"focoos_version\" in model_data:\n        output_lines.append(f\"focoos: {model_data.get('focoos_host')} (v{model_data.get('focoos_version')})\")\n        model_data.pop(\"focoos_host\", None)\n        model_data.pop(\"focoos_version\", None)\n\n    if \"system\" in model_data and \"system_name\" in model_data:\n        output_lines.append(f\"system: {model_data.get('system')} ({model_data.get('system_name')})\")\n        model_data.pop(\"system\", None)\n        model_data.pop(\"system_name\", None)\n\n    if \"cpu_type\" in model_data and \"cpu_cores\" in model_data:\n        output_lines.append(f\"cpu: {model_data.get('cpu_type')} ({model_data.get('cpu_cores')} cores)\")\n        model_data.pop(\"cpu_type\", None)\n        model_data.pop(\"cpu_cores\", None)\n\n    if \"memory_gb\" in model_data and \"memory_used_percentage\" in model_data:\n        output_lines.append(\n            f\"memory_gb: {model_data.get('memory_gb')} ({model_data.get('memory_used_percentage')}% used)\"\n        )\n        model_data.pop(\"memory_gb\", None)\n        model_data.pop(\"memory_used_percentage\", None)\n\n    if \"disk_space_total_gb\" in model_data and \"disk_space_used_percentage\" in model_data:\n        output_lines.append(\n            f\"disk_space_total_gb: {model_data.get('disk_space_total_gb')} ({model_data.get('disk_space_used_percentage')}% used)\"\n        )\n        model_data.pop(\"disk_space_total_gb\", None)\n        model_data.pop(\"disk_space_used_percentage\", None)\n\n    for key, value in model_data.items():\n        if key == \"gpu_info\" and value is not None:\n            output_lines.append(f\"{key}:\")\n            output_lines.append(f\"  - gpu_count: {value.get('gpu_count')}\")\n            output_lines.append(f\"  - total_memory_gb: {value.get('total_gpu_memory_gb')} GB\")\n            output_lines.append(f\"  - gpu_driver: {value.get('gpu_driver')}\")\n            output_lines.append(f\"  - gpu_cuda_version: {value.get('gpu_cuda_version')}\")\n            if value.get(\"devices\"):\n                output_lines.append(\"  - devices:\")\n                for device in value.get(\"devices\", []):\n                    gpu_memory_used = (\n                        f\"{device.get('gpu_memory_used_percentage')}%\"\n                        if device.get(\"gpu_memory_used_percentage\") is not None\n                        else \"N/A\"\n                    )\n                    gpu_load = (\n                        f\"{device.get('gpu_load_percentage')}%\"\n                        if device.get(\"gpu_load_percentage\") is not None\n                        else \"N/A\"\n                    )\n                    gpu_memory_total = (\n                        f\"{device.get('gpu_memory_total_gb')} GB\"\n                        if device.get(\"gpu_memory_total_gb\") is not None\n                        else \"N/A\"\n                    )\n\n                    output_lines.append(\n                        f\"    - GPU {device.get('gpu_id')}: {device.get('gpu_name')}, Memory: {gpu_memory_total} ({gpu_memory_used} used), Load: {gpu_load}\"\n                    )\n        elif isinstance(value, list):\n            output_lines.append(f\"{key}: {value}\")\n        elif isinstance(value, dict) and key == \"packages_versions\":  # Special formatting for packages_versions\n            output_lines.append(f\"{key}:\")\n            for pkg_name, pkg_version in value.items():\n                output_lines.append(f\"  - {pkg_name}: {pkg_version}\")\n        elif isinstance(value, dict) and key == \"environment\":  # Special formatting for environment\n            output_lines.append(f\"{key}:\")\n            for env_key, env_value in value.items():\n                output_lines.append(f\"  - {env_key}: {env_value}\")\n        else:\n            output_lines.append(f\"{key}: {value}\")\n    output_lines.append(\"================================================\")\n\n    logger.info(\"\\n\".join(output_lines))\n</code></pre>"},{"location":"api/ports/#focoos.ports.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of computer vision tasks supported by Focoos.</p> Values <ul> <li>DETECTION: Object detection</li> <li>SEMSEG: Semantic segmentation</li> <li>INSTANCE_SEGMENTATION: Instance segmentation</li> <li>CLASSIFICATION: Image classification</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class Task(str, Enum):\n    \"\"\"Types of computer vision tasks supported by Focoos.\n\n    Values:\n        - DETECTION: Object detection\n        - SEMSEG: Semantic segmentation\n        - INSTANCE_SEGMENTATION: Instance segmentation\n        - CLASSIFICATION: Image classification\n    \"\"\"\n\n    DETECTION = \"detection\"\n    SEMSEG = \"semseg\"\n    INSTANCE_SEGMENTATION = \"instseg\"\n    CLASSIFICATION = \"classification\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.TorchscriptRuntimeOpts","title":"<code>TorchscriptRuntimeOpts</code>  <code>dataclass</code>","text":"<p>TorchScript runtime configuration options.</p> <p>This class provides configuration options for the TorchScript runtime used for model inference.</p> <p>Attributes:</p> Name Type Description <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations to run before benchmarking. Default is 0.</p> <code>optimize_for_inference</code> <code>bool</code> <p>Enable inference optimizations. Default is True.</p> <code>set_fusion_strategy</code> <code>bool</code> <p>Enable operator fusion. Default is True.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass TorchscriptRuntimeOpts:\n    \"\"\"TorchScript runtime configuration options.\n\n    This class provides configuration options for the TorchScript runtime used for model inference.\n\n    Attributes:\n        warmup_iter (int): Number of warmup iterations to run before benchmarking. Default is 0.\n        optimize_for_inference (bool): Enable inference optimizations. Default is True.\n        set_fusion_strategy (bool): Enable operator fusion. Default is True.\n    \"\"\"\n\n    warmup_iter: int = 0\n    optimize_for_inference: bool = True\n    set_fusion_strategy: bool = True\n</code></pre>"},{"location":"api/ports/#focoos.ports.TrainerArgs","title":"<code>TrainerArgs</code>  <code>dataclass</code>","text":"<p>Configuration class for unified model training.</p> <p>Attributes:</p> Name Type Description <code>run_name</code> <code>str</code> <p>Name of the training run</p> <code>output_dir</code> <code>str</code> <p>Directory to save outputs</p> <code>ckpt_dir</code> <code>Optional[str]</code> <p>Directory for checkpoints</p> <code>init_checkpoint</code> <code>Optional[str]</code> <p>Initial checkpoint to load</p> <code>resume</code> <code>bool</code> <p>Whether to resume from checkpoint</p> <code>num_gpus</code> <code>int</code> <p>Number of GPUs to use</p> <code>device</code> <code>str</code> <p>Device to use (cuda/cpu)</p> <code>workers</code> <code>int</code> <p>Number of data loading workers</p> <code>amp_enabled</code> <code>bool</code> <p>Whether to use automatic mixed precision</p> <code>ddp_broadcast_buffers</code> <code>bool</code> <p>Whether to broadcast buffers in DDP</p> <code>ddp_find_unused</code> <code>bool</code> <p>Whether to find unused parameters in DDP</p> <code>checkpointer_period</code> <code>int</code> <p>How often to save checkpoints</p> <code>checkpointer_max_to_keep</code> <code>int</code> <p>Maximum checkpoints to keep</p> <code>eval_period</code> <code>int</code> <p>How often to evaluate</p> <code>log_period</code> <code>int</code> <p>How often to log</p> <code>vis_period</code> <code>int</code> <p>How often to visualize</p> <code>samples</code> <code>int</code> <p>Number of samples for visualization</p> <code>seed</code> <code>int</code> <p>Random seed</p> <code>early_stop</code> <code>bool</code> <p>Whether to use early stopping</p> <code>patience</code> <code>int</code> <p>Early stopping patience</p> <code>ema_enabled</code> <code>bool</code> <p>Whether to use EMA</p> <code>ema_decay</code> <code>float</code> <p>EMA decay rate</p> <code>ema_warmup</code> <code>int</code> <p>EMA warmup period</p> <code>learning_rate</code> <code>float</code> <p>Base learning rate</p> <code>weight_decay</code> <code>float</code> <p>Weight decay</p> <code>max_iters</code> <code>int</code> <p>Maximum training iterations</p> <code>batch_size</code> <code>int</code> <p>Batch size</p> <code>scheduler</code> <code>str</code> <p>Learning rate scheduler type</p> <code>scheduler_extra</code> <code>Optional[dict]</code> <p>Extra scheduler parameters</p> <code>optimizer</code> <code>str</code> <p>Optimizer type</p> <code>optimizer_extra</code> <code>Optional[dict]</code> <p>Extra optimizer parameters</p> <code>weight_decay_norm</code> <code>float</code> <p>Weight decay for normalization layers</p> <code>weight_decay_embed</code> <code>float</code> <p>Weight decay for embeddings</p> <code>backbone_multiplier</code> <code>float</code> <p>Learning rate multiplier for backbone</p> <code>decoder_multiplier</code> <code>float</code> <p>Learning rate multiplier for decoder</p> <code>head_multiplier</code> <code>float</code> <p>Learning rate multiplier for head</p> <code>freeze_bn</code> <code>bool</code> <p>Whether to freeze batch norm</p> <code>clip_gradients</code> <code>float</code> <p>Gradient clipping value</p> <code>size_divisibility</code> <code>int</code> <p>Input size divisibility requirement</p> <code>gather_metric_period</code> <code>int</code> <p>How often to gather metrics</p> <code>zero_grad_before_forward</code> <code>bool</code> <p>Whether to zero gradients before forward pass</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass TrainerArgs:\n    \"\"\"Configuration class for unified model training.\n\n    Attributes:\n        run_name (str): Name of the training run\n        output_dir (str): Directory to save outputs\n        ckpt_dir (Optional[str]): Directory for checkpoints\n        init_checkpoint (Optional[str]): Initial checkpoint to load\n        resume (bool): Whether to resume from checkpoint\n        num_gpus (int): Number of GPUs to use\n        device (str): Device to use (cuda/cpu)\n        workers (int): Number of data loading workers\n        amp_enabled (bool): Whether to use automatic mixed precision\n        ddp_broadcast_buffers (bool): Whether to broadcast buffers in DDP\n        ddp_find_unused (bool): Whether to find unused parameters in DDP\n        checkpointer_period (int): How often to save checkpoints\n        checkpointer_max_to_keep (int): Maximum checkpoints to keep\n        eval_period (int): How often to evaluate\n        log_period (int): How often to log\n        vis_period (int): How often to visualize\n        samples (int): Number of samples for visualization\n        seed (int): Random seed\n        early_stop (bool): Whether to use early stopping\n        patience (int): Early stopping patience\n        ema_enabled (bool): Whether to use EMA\n        ema_decay (float): EMA decay rate\n        ema_warmup (int): EMA warmup period\n        learning_rate (float): Base learning rate\n        weight_decay (float): Weight decay\n        max_iters (int): Maximum training iterations\n        batch_size (int): Batch size\n        scheduler (str): Learning rate scheduler type\n        scheduler_extra (Optional[dict]): Extra scheduler parameters\n        optimizer (str): Optimizer type\n        optimizer_extra (Optional[dict]): Extra optimizer parameters\n        weight_decay_norm (float): Weight decay for normalization layers\n        weight_decay_embed (float): Weight decay for embeddings\n        backbone_multiplier (float): Learning rate multiplier for backbone\n        decoder_multiplier (float): Learning rate multiplier for decoder\n        head_multiplier (float): Learning rate multiplier for head\n        freeze_bn (bool): Whether to freeze batch norm\n        clip_gradients (float): Gradient clipping value\n        size_divisibility (int): Input size divisibility requirement\n        gather_metric_period (int): How often to gather metrics\n        zero_grad_before_forward (bool): Whether to zero gradients before forward pass\n    \"\"\"\n\n    run_name: str\n    output_dir: str = MODELS_DIR\n    ckpt_dir: Optional[str] = None\n    init_checkpoint: Optional[str] = None\n    resume: bool = False\n    # Logistics params\n    num_gpus: int = get_gpus_count()\n    device: str = \"cuda\"\n    workers: int = 4\n    amp_enabled: bool = True\n    ddp_broadcast_buffers: bool = False\n    ddp_find_unused: bool = True\n    checkpointer_period: int = 1000\n    checkpointer_max_to_keep: int = 1\n    eval_period: int = 50\n    log_period: int = 20\n    samples: int = 9\n    seed: int = 42\n    early_stop: bool = True\n    patience: int = 10\n    # EMA\n    ema_enabled: bool = False\n    ema_decay: float = 0.999\n    ema_warmup: int = 2000\n    # Hyperparameters\n    learning_rate: float = 5e-4\n    weight_decay: float = 0.02\n    max_iters: int = 3000\n    batch_size: int = 16\n    scheduler: Literal[\"POLY\", \"FIXED\", \"COSINE\", \"MULTISTEP\"] = \"MULTISTEP\"\n    scheduler_extra: Optional[dict] = None\n    optimizer: Literal[\"ADAMW\", \"SGD\", \"RMSPROP\"] = \"ADAMW\"\n    optimizer_extra: Optional[dict] = None\n    weight_decay_norm: float = 0.0\n    weight_decay_embed: float = 0.0\n    backbone_multiplier: float = 0.1\n    decoder_multiplier: float = 1.0\n    head_multiplier: float = 1.0\n    freeze_bn: bool = False\n    clip_gradients: float = 0.1\n    size_divisibility: int = 0\n    # Training specific\n    gather_metric_period: int = 1\n    zero_grad_before_forward: bool = False\n\n    # Sync to hub\n    sync_to_hub: bool = False\n</code></pre>"},{"location":"api/ports/#focoos.ports.TrainingInfo","title":"<code>TrainingInfo</code>  <code>dataclass</code>","text":"<p>Information about a model's training process.</p> <p>This class contains details about the training job configuration, status, and timing.</p> <p>Attributes:</p> Name Type Description <code>algorithm_name</code> <code>Optional[str]</code> <p>The name of the training algorithm used.</p> <code>instance_type</code> <code>Optional[str]</code> <p>The compute instance type used for training.</p> <code>volume_size</code> <code>Optional[int]</code> <p>The storage volume size in GB allocated for the training job.</p> <code>max_runtime_in_seconds</code> <code>Optional[int]</code> <p>Maximum allowed runtime for the training job in seconds.</p> <code>main_status</code> <code>Optional[str]</code> <p>The primary status of the training job (e.g., \"InProgress\", \"Completed\").</p> <code>secondary_status</code> <code>Optional[str]</code> <p>Additional status information about the training job.</p> <code>failure_reason</code> <code>Optional[str]</code> <p>Description of why the training job failed, if applicable.</p> <code>elapsed_time</code> <code>Optional[str]</code> <p>Time elapsed since the start of the training job in seconds.</p> <code>status_transitions</code> <code>Optional[list[dict]]</code> <p>List of status change events during the training process.</p> <code>start_time</code> <code>Optional[str]</code> <p>Timestamp when the training job started.</p> <code>end_time</code> <code>Optional[str]</code> <p>Timestamp when the training job completed or failed.</p> <code>artifact_location</code> <code>Optional[str]</code> <p>Storage location of the training artifacts and model outputs.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass TrainingInfo:\n    \"\"\"Information about a model's training process.\n\n    This class contains details about the training job configuration, status, and timing.\n\n    Attributes:\n        algorithm_name: The name of the training algorithm used.\n        instance_type: The compute instance type used for training.\n        volume_size: The storage volume size in GB allocated for the training job.\n        max_runtime_in_seconds: Maximum allowed runtime for the training job in seconds.\n        main_status: The primary status of the training job (e.g., \"InProgress\", \"Completed\").\n        secondary_status: Additional status information about the training job.\n        failure_reason: Description of why the training job failed, if applicable.\n        elapsed_time: Time elapsed since the start of the training job in seconds.\n        status_transitions: List of status change events during the training process.\n        start_time: Timestamp when the training job started.\n        end_time: Timestamp when the training job completed or failed.\n        artifact_location: Storage location of the training artifacts and model outputs.\n    \"\"\"\n\n    algorithm_name: Optional[str] = \"\"  # todo: remove\n    instance_device: Optional[str] = None\n    instance_type: Optional[str] = None\n    volume_size: Optional[int] = None\n    main_status: Optional[str] = None\n    failure_reason: Optional[str] = None\n    status_transitions: Optional[list[dict]] = None\n    start_time: Optional[str] = None\n    end_time: Optional[str] = None\n    artifact_location: Optional[str] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.User","title":"<code>User</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>User account information.</p> <p>This class represents a user account in the Focoos platform, containing personal information, API key, and usage quotas.</p> <p>Attributes:</p> Name Type Description <code>email</code> <code>str</code> <p>The user's email address.</p> <code>created_at</code> <code>datetime</code> <p>When the user account was created.</p> <code>updated_at</code> <code>datetime</code> <p>When the user account was last updated.</p> <code>company</code> <code>Optional[str]</code> <p>The user's company name, if provided.</p> <code>api_key</code> <code>ApiKey</code> <p>The API key associated with the user account.</p> <code>quotas</code> <code>Quotas</code> <p>Usage quotas and limits for the user account.</p> Source code in <code>focoos/ports.py</code> <pre><code>class User(PydanticBase):\n    \"\"\"User account information.\n\n    This class represents a user account in the Focoos platform, containing\n    personal information, API key, and usage quotas.\n\n    Attributes:\n        email (str): The user's email address.\n        created_at (datetime): When the user account was created.\n        updated_at (datetime): When the user account was last updated.\n        company (Optional[str]): The user's company name, if provided.\n        api_key (ApiKey): The API key associated with the user account.\n        quotas (Quotas): Usage quotas and limits for the user account.\n    \"\"\"\n\n    email: str\n    created_at: datetime\n    updated_at: datetime\n    company: Optional[str] = None\n    api_key: ApiKey\n    quotas: Quotas\n</code></pre>"},{"location":"api/processor/","title":"Processor","text":""},{"location":"api/processor/#focoos.processor.base_processor.Processor","title":"<code>Processor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for model processors that handle preprocessing and postprocessing.</p> <p>This class defines the interface for processing inputs and outputs for different model types. Subclasses must implement the abstract methods to provide model-specific processing logic.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>ModelConfig</code> <p>Configuration object containing model-specific settings.</p> <code>training</code> <code>bool</code> <p>Flag indicating whether the processor is in training mode.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>class Processor(ABC):\n    \"\"\"Abstract base class for model processors that handle preprocessing and postprocessing.\n\n    This class defines the interface for processing inputs and outputs for different model types.\n    Subclasses must implement the abstract methods to provide model-specific processing logic.\n\n    Attributes:\n        config (ModelConfig): Configuration object containing model-specific settings.\n        training (bool): Flag indicating whether the processor is in training mode.\n    \"\"\"\n\n    def __init__(self, config: ModelConfig):\n        \"\"\"Initialize the processor with the given configuration.\n\n        Args:\n            config (ModelConfig): Model configuration containing settings and parameters.\n        \"\"\"\n        self.config = config\n        self.training = False\n\n    def eval(self):\n        \"\"\"Set the processor to evaluation mode.\n\n        Returns:\n            Processor: Self reference for method chaining.\n        \"\"\"\n        self.training = False\n        return self\n\n    def train(self, training: bool = True):\n        \"\"\"Set the processor training mode.\n\n        Args:\n            training (bool, optional): Whether to set training mode. Defaults to True.\n\n        Returns:\n            Processor: Self reference for method chaining.\n        \"\"\"\n        self.training = training\n        return self\n\n    @abstractmethod\n    def preprocess(\n        self,\n        inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n        device: Union[Literal[\"cuda\", \"cpu\"], torch.device] = \"cuda\",\n        dtype: torch.dtype = torch.float32,\n        image_size: Optional[int] = None,\n    ) -&gt; tuple[torch.Tensor, Any]:\n        \"\"\"Preprocess input data for model inference.\n\n        This method must be implemented by subclasses to handle model-specific preprocessing\n        such as resizing, normalization, and tensor formatting.\n\n        Args:\n            inputs: Input data which can be single or multiple images in various formats.\n            device: Target device for tensor placement. Defaults to \"cuda\".\n            dtype: Target data type for tensors. Defaults to torch.float32.\n            image_size: Optional target image size for resizing. Defaults to None.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Preprocessed tensor and any additional metadata.\n\n        Raises:\n            NotImplementedError: If not implemented by subclass.\n        \"\"\"\n        raise NotImplementedError(\"Pre-processing is not implemented for this model.\")\n\n    @abstractmethod\n    def postprocess(\n        self,\n        outputs: ModelOutput,\n        inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n        class_names: list[str] = [],\n        threshold: float = 0.5,\n        **kwargs,\n    ) -&gt; list[FocoosDetections]:\n        \"\"\"Postprocess model outputs to generate final detection results.\n\n        This method must be implemented by subclasses to convert raw model outputs\n        into structured detection results.\n\n        Args:\n            outputs (ModelOutput): Raw outputs from the model.\n            inputs: Original input data for reference during postprocessing.\n            class_names (list[str], optional): List of class names for detection labels.\n                Defaults to empty list.\n            threshold (float, optional): Confidence threshold for detections. Defaults to 0.5.\n            **kwargs: Additional keyword arguments for model-specific postprocessing.\n\n        Returns:\n            list[FocoosDetections]: List of detection results for each input.\n\n        Raises:\n            NotImplementedError: If not implemented by subclass.\n        \"\"\"\n        raise NotImplementedError(\"Post-processing is not implemented for this model.\")\n\n    @abstractmethod\n    def export_postprocess(\n        self,\n        output: Union[list[torch.Tensor], list[np.ndarray]],\n        inputs: Union[\n            torch.Tensor,\n            np.ndarray,\n            Image.Image,\n            list[Image.Image],\n            list[np.ndarray],\n            list[torch.Tensor],\n        ],\n        threshold: Optional[float] = None,\n        **kwargs,\n    ) -&gt; list[FocoosDetections]:\n        \"\"\"Postprocess outputs from exported model for inference.\n\n        This method handles postprocessing for models that have been exported\n        (e.g., to ONNX format) and may have different output formats.\n\n        Args:\n            output: Raw outputs from exported model as tensors or numpy arrays.\n            inputs: Original input data for reference during postprocessing.\n            threshold: Optional confidence threshold for detections. Defaults to None.\n            **kwargs: Additional keyword arguments for export-specific postprocessing.\n\n        Returns:\n            list[FocoosDetections]: List of detection results for each input.\n\n        Raises:\n            NotImplementedError: If not implemented by subclass.\n        \"\"\"\n        raise NotImplementedError(\"Export post-processing is not implemented for this model.\")\n\n    @abstractmethod\n    def get_dynamic_axes(self) -&gt; DynamicAxes:\n        \"\"\"Get dynamic axes configuration for model export.\n\n        This method defines which axes can vary in size during model export,\n        typically used for ONNX export with dynamic batch sizes or image dimensions.\n\n        Returns:\n            DynamicAxes: Configuration specifying which axes are dynamic.\n\n        Raises:\n            NotImplementedError: If not implemented by subclass.\n        \"\"\"\n        raise NotImplementedError(\"Export axes are not implemented for this model.\")\n\n    @abstractmethod\n    def eval_postprocess(self, outputs: ModelOutput, inputs: list[DatasetEntry]):\n        \"\"\"Postprocess model outputs for evaluation purposes.\n\n        This method handles postprocessing specifically for model evaluation,\n        which may differ from inference postprocessing.\n\n        Args:\n            outputs (ModelOutput): Raw outputs from the model.\n            inputs (list[DatasetEntry]): List of dataset entries used as inputs.\n\n        Raises:\n            NotImplementedError: If not implemented by subclass.\n        \"\"\"\n        raise NotImplementedError(\"Post-processing is not implemented for this model.\")\n\n    def get_image_sizes(\n        self,\n        inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n    ):\n        \"\"\"Extract image dimensions from various input formats.\n\n        This utility method determines the height and width of images from different\n        input types including tensors, numpy arrays, and PIL images.\n\n        Args:\n            inputs: Input data containing one or more images in various formats.\n\n        Returns:\n            list[tuple[int, int]]: List of (height, width) tuples for each image.\n\n        Raises:\n            ValueError: If input type is not supported.\n        \"\"\"\n        image_sizes = []\n\n        if isinstance(inputs, (torch.Tensor, np.ndarray)):\n            # Single tensor/array input\n            if isinstance(inputs, torch.Tensor):\n                height, width = inputs.shape[-2:]\n            else:  # numpy array\n                height, width = inputs.shape[-3:-1] if inputs.ndim &gt; 3 else inputs.shape[:2]\n            image_sizes.append((height, width))\n        elif isinstance(inputs, Image.Image):\n            # Single PIL image\n            width, height = inputs.size\n            image_sizes.append((height, width))\n        elif isinstance(inputs, list):\n            # List of inputs\n            for img in inputs:\n                if isinstance(img, torch.Tensor):\n                    height, width = img.shape[-2:]\n                elif isinstance(img, np.ndarray):\n                    height, width = img.shape[-3:-1] if img.ndim &gt; 3 else img.shape[:2]\n                elif isinstance(img, Image.Image):\n                    width, height = img.size\n                else:\n                    raise ValueError(f\"Unsupported input type in list: {type(img)}\")\n                image_sizes.append((height, width))\n        else:\n            raise ValueError(f\"Unsupported input type: {type(inputs)}\")\n        return image_sizes\n\n    def get_tensors(\n        self,\n        inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n    ) -&gt; torch.Tensor:\n        \"\"\"Convert various input formats to a batched PyTorch tensor.\n\n        This utility method standardizes different input types (PIL Images, numpy arrays,\n        PyTorch tensors) into a single batched tensor with consistent format (BCHW).\n\n        Args:\n            inputs: Input data containing one or more images in various formats.\n\n        Returns:\n            torch.Tensor: Batched tensor with shape (B, C, H, W) where:\n                - B is batch size\n                - C is number of channels (typically 3 for RGB)\n                - H is height\n                - W is width\n\n        Note:\n            This method may break with different image sizes as it uses torch.cat\n            which requires consistent dimensions across inputs.\n        \"\"\"\n        if isinstance(inputs, (Image.Image, np.ndarray, torch.Tensor)):\n            inputs_list = [inputs]\n        else:\n            inputs_list = inputs\n\n        # Process each input based on its type\n        processed_inputs = []\n        for inp in inputs_list:\n            # todo check for tensor of 4 dimesions.\n            if isinstance(inp, Image.Image):\n                inp = np.array(inp)\n            if isinstance(inp, np.ndarray):\n                inp = torch.from_numpy(inp)\n\n            # Ensure input has correct shape and type\n            if inp.dim() == 3:  # Add batch dimension if missing\n                inp = inp.unsqueeze(0)\n            if inp.shape[1] != 3 and inp.shape[-1] == 3:  # Convert HWC to CHW if needed\n                inp = inp.permute(0, 3, 1, 2)\n\n            processed_inputs.append(inp)\n\n        # Stack all inputs into a single batch tensor\n        # use pixel mean to get dtype -&gt; If fp16, pixel_mean is fp16, so inputs will be fp16\n        # TODO: this will break with different image sizes\n        images_torch = torch.cat(processed_inputs, dim=0)\n\n        return images_torch\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the processor with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelConfig</code> <p>Model configuration containing settings and parameters.</p> required Source code in <code>focoos/processor/base_processor.py</code> <pre><code>def __init__(self, config: ModelConfig):\n    \"\"\"Initialize the processor with the given configuration.\n\n    Args:\n        config (ModelConfig): Model configuration containing settings and parameters.\n    \"\"\"\n    self.config = config\n    self.training = False\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.eval","title":"<code>eval()</code>","text":"<p>Set the processor to evaluation mode.</p> <p>Returns:</p> Name Type Description <code>Processor</code> <p>Self reference for method chaining.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>def eval(self):\n    \"\"\"Set the processor to evaluation mode.\n\n    Returns:\n        Processor: Self reference for method chaining.\n    \"\"\"\n    self.training = False\n    return self\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.eval_postprocess","title":"<code>eval_postprocess(outputs, inputs)</code>  <code>abstractmethod</code>","text":"<p>Postprocess model outputs for evaluation purposes.</p> <p>This method handles postprocessing specifically for model evaluation, which may differ from inference postprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>ModelOutput</code> <p>Raw outputs from the model.</p> required <code>inputs</code> <code>list[DatasetEntry]</code> <p>List of dataset entries used as inputs.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>@abstractmethod\ndef eval_postprocess(self, outputs: ModelOutput, inputs: list[DatasetEntry]):\n    \"\"\"Postprocess model outputs for evaluation purposes.\n\n    This method handles postprocessing specifically for model evaluation,\n    which may differ from inference postprocessing.\n\n    Args:\n        outputs (ModelOutput): Raw outputs from the model.\n        inputs (list[DatasetEntry]): List of dataset entries used as inputs.\n\n    Raises:\n        NotImplementedError: If not implemented by subclass.\n    \"\"\"\n    raise NotImplementedError(\"Post-processing is not implemented for this model.\")\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.export_postprocess","title":"<code>export_postprocess(output, inputs, threshold=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Postprocess outputs from exported model for inference.</p> <p>This method handles postprocessing for models that have been exported (e.g., to ONNX format) and may have different output formats.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[list[Tensor], list[ndarray]]</code> <p>Raw outputs from exported model as tensors or numpy arrays.</p> required <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Original input data for reference during postprocessing.</p> required <code>threshold</code> <code>Optional[float]</code> <p>Optional confidence threshold for detections. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for export-specific postprocessing.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[FocoosDetections]</code> <p>list[FocoosDetections]: List of detection results for each input.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>@abstractmethod\ndef export_postprocess(\n    self,\n    output: Union[list[torch.Tensor], list[np.ndarray]],\n    inputs: Union[\n        torch.Tensor,\n        np.ndarray,\n        Image.Image,\n        list[Image.Image],\n        list[np.ndarray],\n        list[torch.Tensor],\n    ],\n    threshold: Optional[float] = None,\n    **kwargs,\n) -&gt; list[FocoosDetections]:\n    \"\"\"Postprocess outputs from exported model for inference.\n\n    This method handles postprocessing for models that have been exported\n    (e.g., to ONNX format) and may have different output formats.\n\n    Args:\n        output: Raw outputs from exported model as tensors or numpy arrays.\n        inputs: Original input data for reference during postprocessing.\n        threshold: Optional confidence threshold for detections. Defaults to None.\n        **kwargs: Additional keyword arguments for export-specific postprocessing.\n\n    Returns:\n        list[FocoosDetections]: List of detection results for each input.\n\n    Raises:\n        NotImplementedError: If not implemented by subclass.\n    \"\"\"\n    raise NotImplementedError(\"Export post-processing is not implemented for this model.\")\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.get_dynamic_axes","title":"<code>get_dynamic_axes()</code>  <code>abstractmethod</code>","text":"<p>Get dynamic axes configuration for model export.</p> <p>This method defines which axes can vary in size during model export, typically used for ONNX export with dynamic batch sizes or image dimensions.</p> <p>Returns:</p> Name Type Description <code>DynamicAxes</code> <code>DynamicAxes</code> <p>Configuration specifying which axes are dynamic.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>@abstractmethod\ndef get_dynamic_axes(self) -&gt; DynamicAxes:\n    \"\"\"Get dynamic axes configuration for model export.\n\n    This method defines which axes can vary in size during model export,\n    typically used for ONNX export with dynamic batch sizes or image dimensions.\n\n    Returns:\n        DynamicAxes: Configuration specifying which axes are dynamic.\n\n    Raises:\n        NotImplementedError: If not implemented by subclass.\n    \"\"\"\n    raise NotImplementedError(\"Export axes are not implemented for this model.\")\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.get_image_sizes","title":"<code>get_image_sizes(inputs)</code>","text":"<p>Extract image dimensions from various input formats.</p> <p>This utility method determines the height and width of images from different input types including tensors, numpy arrays, and PIL images.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Input data containing one or more images in various formats.</p> required <p>Returns:</p> Type Description <p>list[tuple[int, int]]: List of (height, width) tuples for each image.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input type is not supported.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>def get_image_sizes(\n    self,\n    inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n):\n    \"\"\"Extract image dimensions from various input formats.\n\n    This utility method determines the height and width of images from different\n    input types including tensors, numpy arrays, and PIL images.\n\n    Args:\n        inputs: Input data containing one or more images in various formats.\n\n    Returns:\n        list[tuple[int, int]]: List of (height, width) tuples for each image.\n\n    Raises:\n        ValueError: If input type is not supported.\n    \"\"\"\n    image_sizes = []\n\n    if isinstance(inputs, (torch.Tensor, np.ndarray)):\n        # Single tensor/array input\n        if isinstance(inputs, torch.Tensor):\n            height, width = inputs.shape[-2:]\n        else:  # numpy array\n            height, width = inputs.shape[-3:-1] if inputs.ndim &gt; 3 else inputs.shape[:2]\n        image_sizes.append((height, width))\n    elif isinstance(inputs, Image.Image):\n        # Single PIL image\n        width, height = inputs.size\n        image_sizes.append((height, width))\n    elif isinstance(inputs, list):\n        # List of inputs\n        for img in inputs:\n            if isinstance(img, torch.Tensor):\n                height, width = img.shape[-2:]\n            elif isinstance(img, np.ndarray):\n                height, width = img.shape[-3:-1] if img.ndim &gt; 3 else img.shape[:2]\n            elif isinstance(img, Image.Image):\n                width, height = img.size\n            else:\n                raise ValueError(f\"Unsupported input type in list: {type(img)}\")\n            image_sizes.append((height, width))\n    else:\n        raise ValueError(f\"Unsupported input type: {type(inputs)}\")\n    return image_sizes\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.get_tensors","title":"<code>get_tensors(inputs)</code>","text":"<p>Convert various input formats to a batched PyTorch tensor.</p> <p>This utility method standardizes different input types (PIL Images, numpy arrays, PyTorch tensors) into a single batched tensor with consistent format (BCHW).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Input data containing one or more images in various formats.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Batched tensor with shape (B, C, H, W) where: - B is batch size - C is number of channels (typically 3 for RGB) - H is height - W is width</p> Note <p>This method may break with different image sizes as it uses torch.cat which requires consistent dimensions across inputs.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>def get_tensors(\n    self,\n    inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n) -&gt; torch.Tensor:\n    \"\"\"Convert various input formats to a batched PyTorch tensor.\n\n    This utility method standardizes different input types (PIL Images, numpy arrays,\n    PyTorch tensors) into a single batched tensor with consistent format (BCHW).\n\n    Args:\n        inputs: Input data containing one or more images in various formats.\n\n    Returns:\n        torch.Tensor: Batched tensor with shape (B, C, H, W) where:\n            - B is batch size\n            - C is number of channels (typically 3 for RGB)\n            - H is height\n            - W is width\n\n    Note:\n        This method may break with different image sizes as it uses torch.cat\n        which requires consistent dimensions across inputs.\n    \"\"\"\n    if isinstance(inputs, (Image.Image, np.ndarray, torch.Tensor)):\n        inputs_list = [inputs]\n    else:\n        inputs_list = inputs\n\n    # Process each input based on its type\n    processed_inputs = []\n    for inp in inputs_list:\n        # todo check for tensor of 4 dimesions.\n        if isinstance(inp, Image.Image):\n            inp = np.array(inp)\n        if isinstance(inp, np.ndarray):\n            inp = torch.from_numpy(inp)\n\n        # Ensure input has correct shape and type\n        if inp.dim() == 3:  # Add batch dimension if missing\n            inp = inp.unsqueeze(0)\n        if inp.shape[1] != 3 and inp.shape[-1] == 3:  # Convert HWC to CHW if needed\n            inp = inp.permute(0, 3, 1, 2)\n\n        processed_inputs.append(inp)\n\n    # Stack all inputs into a single batch tensor\n    # use pixel mean to get dtype -&gt; If fp16, pixel_mean is fp16, so inputs will be fp16\n    # TODO: this will break with different image sizes\n    images_torch = torch.cat(processed_inputs, dim=0)\n\n    return images_torch\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.postprocess","title":"<code>postprocess(outputs, inputs, class_names=[], threshold=0.5, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Postprocess model outputs to generate final detection results.</p> <p>This method must be implemented by subclasses to convert raw model outputs into structured detection results.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>ModelOutput</code> <p>Raw outputs from the model.</p> required <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Original input data for reference during postprocessing.</p> required <code>class_names</code> <code>list[str]</code> <p>List of class names for detection labels. Defaults to empty list.</p> <code>[]</code> <code>threshold</code> <code>float</code> <p>Confidence threshold for detections. Defaults to 0.5.</p> <code>0.5</code> <code>**kwargs</code> <p>Additional keyword arguments for model-specific postprocessing.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[FocoosDetections]</code> <p>list[FocoosDetections]: List of detection results for each input.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>@abstractmethod\ndef postprocess(\n    self,\n    outputs: ModelOutput,\n    inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n    class_names: list[str] = [],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; list[FocoosDetections]:\n    \"\"\"Postprocess model outputs to generate final detection results.\n\n    This method must be implemented by subclasses to convert raw model outputs\n    into structured detection results.\n\n    Args:\n        outputs (ModelOutput): Raw outputs from the model.\n        inputs: Original input data for reference during postprocessing.\n        class_names (list[str], optional): List of class names for detection labels.\n            Defaults to empty list.\n        threshold (float, optional): Confidence threshold for detections. Defaults to 0.5.\n        **kwargs: Additional keyword arguments for model-specific postprocessing.\n\n    Returns:\n        list[FocoosDetections]: List of detection results for each input.\n\n    Raises:\n        NotImplementedError: If not implemented by subclass.\n    \"\"\"\n    raise NotImplementedError(\"Post-processing is not implemented for this model.\")\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.preprocess","title":"<code>preprocess(inputs, device='cuda', dtype=torch.float32, image_size=None)</code>  <code>abstractmethod</code>","text":"<p>Preprocess input data for model inference.</p> <p>This method must be implemented by subclasses to handle model-specific preprocessing such as resizing, normalization, and tensor formatting.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Input data which can be single or multiple images in various formats.</p> required <code>device</code> <code>Union[Literal['cuda', 'cpu'], device]</code> <p>Target device for tensor placement. Defaults to \"cuda\".</p> <code>'cuda'</code> <code>dtype</code> <code>dtype</code> <p>Target data type for tensors. Defaults to torch.float32.</p> <code>float32</code> <code>image_size</code> <code>Optional[int]</code> <p>Optional target image size for resizing. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Any]</code> <p>tuple[torch.Tensor, Any]: Preprocessed tensor and any additional metadata.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>@abstractmethod\ndef preprocess(\n    self,\n    inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n    device: Union[Literal[\"cuda\", \"cpu\"], torch.device] = \"cuda\",\n    dtype: torch.dtype = torch.float32,\n    image_size: Optional[int] = None,\n) -&gt; tuple[torch.Tensor, Any]:\n    \"\"\"Preprocess input data for model inference.\n\n    This method must be implemented by subclasses to handle model-specific preprocessing\n    such as resizing, normalization, and tensor formatting.\n\n    Args:\n        inputs: Input data which can be single or multiple images in various formats.\n        device: Target device for tensor placement. Defaults to \"cuda\".\n        dtype: Target data type for tensors. Defaults to torch.float32.\n        image_size: Optional target image size for resizing. Defaults to None.\n\n    Returns:\n        tuple[torch.Tensor, Any]: Preprocessed tensor and any additional metadata.\n\n    Raises:\n        NotImplementedError: If not implemented by subclass.\n    \"\"\"\n    raise NotImplementedError(\"Pre-processing is not implemented for this model.\")\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.train","title":"<code>train(training=True)</code>","text":"<p>Set the processor training mode.</p> <p>Parameters:</p> Name Type Description Default <code>training</code> <code>bool</code> <p>Whether to set training mode. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Processor</code> <p>Self reference for method chaining.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>def train(self, training: bool = True):\n    \"\"\"Set the processor training mode.\n\n    Args:\n        training (bool, optional): Whether to set training mode. Defaults to True.\n\n    Returns:\n        Processor: Self reference for method chaining.\n    \"\"\"\n    self.training = training\n    return self\n</code></pre>"},{"location":"api/processor/#focoos.processor.processor_manager.ProcessorManager","title":"<code>ProcessorManager</code>","text":"<p>Automatic processor manager with lazy loading</p> Source code in <code>focoos/processor/processor_manager.py</code> <pre><code>class ProcessorManager:\n    \"\"\"Automatic processor manager with lazy loading\"\"\"\n\n    _PROCESSOR_MAPPING: Dict[str, Callable[[], Type[Processor]]] = {}\n\n    @classmethod\n    def register_processor(cls, model_family: ModelFamily, processor_loader: Callable[[], Type[Processor]]):\n        \"\"\"\n        Register a loader for a specific processor\n        \"\"\"\n        cls._PROCESSOR_MAPPING[model_family.value] = processor_loader\n\n    @classmethod\n    def _ensure_family_registered(cls, model_family: ModelFamily):\n        \"\"\"Ensure the processor family is registered, importing if needed.\"\"\"\n        if model_family.value not in cls._PROCESSOR_MAPPING:\n            family_module = importlib.import_module(f\"focoos.models.{model_family.value}\")\n            for attr_name in dir(family_module):\n                if attr_name.startswith(\"_register\"):\n                    register_func = getattr(family_module, attr_name)\n                    if callable(register_func):\n                        register_func()\n\n    @classmethod\n    def get_processor(cls, model_family: ModelFamily, model_config: ModelConfig) -&gt; Processor:\n        \"\"\"\n        Get a processor instance for the given model family.\n        \"\"\"\n        cls._ensure_family_registered(model_family)\n        if model_family.value not in cls._PROCESSOR_MAPPING:\n            raise ValueError(f\"Processor for {model_family} not supported\")\n        processor_class = cls._PROCESSOR_MAPPING[model_family.value]()\n        return processor_class(config=model_config)\n</code></pre>"},{"location":"api/processor/#focoos.processor.processor_manager.ProcessorManager.get_processor","title":"<code>get_processor(model_family, model_config)</code>  <code>classmethod</code>","text":"<p>Get a processor instance for the given model family.</p> Source code in <code>focoos/processor/processor_manager.py</code> <pre><code>@classmethod\ndef get_processor(cls, model_family: ModelFamily, model_config: ModelConfig) -&gt; Processor:\n    \"\"\"\n    Get a processor instance for the given model family.\n    \"\"\"\n    cls._ensure_family_registered(model_family)\n    if model_family.value not in cls._PROCESSOR_MAPPING:\n        raise ValueError(f\"Processor for {model_family} not supported\")\n    processor_class = cls._PROCESSOR_MAPPING[model_family.value]()\n    return processor_class(config=model_config)\n</code></pre>"},{"location":"api/processor/#focoos.processor.processor_manager.ProcessorManager.register_processor","title":"<code>register_processor(model_family, processor_loader)</code>  <code>classmethod</code>","text":"<p>Register a loader for a specific processor</p> Source code in <code>focoos/processor/processor_manager.py</code> <pre><code>@classmethod\ndef register_processor(cls, model_family: ModelFamily, processor_loader: Callable[[], Type[Processor]]):\n    \"\"\"\n    Register a loader for a specific processor\n    \"\"\"\n    cls._PROCESSOR_MAPPING[model_family.value] = processor_loader\n</code></pre>"},{"location":"api/runtimes/","title":"runtimes","text":""},{"location":"api/runtimes/#focoos.infer.runtimes.base.BaseRuntime","title":"<code>BaseRuntime</code>","text":"<p>Abstract base class for runtime implementations.</p> <p>This class defines the interface that all runtime implementations must follow. It provides methods for model initialization, inference, and performance benchmarking.</p> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>str</code> <p>Path to the model file.</p> <code>opts</code> <code>Any</code> <p>Runtime-specific options.</p> <code>model_info</code> <code>RemoteModelInfo</code> <p>Metadata about the model.</p> Source code in <code>focoos/infer/runtimes/base.py</code> <pre><code>class BaseRuntime:\n    \"\"\"\n    Abstract base class for runtime implementations.\n\n    This class defines the interface that all runtime implementations must follow.\n    It provides methods for model initialization, inference, and performance benchmarking.\n\n    Attributes:\n        model_path (str): Path to the model file.\n        opts (Any): Runtime-specific options.\n        model_info (RemoteModelInfo): Metadata about the model.\n    \"\"\"\n\n    def __init__(self, model_path: str, opts: Any, model_info: RemoteModelInfo):\n        \"\"\"\n        Initialize the runtime with model path, options and metadata.\n\n        Args:\n            model_path (str): Path to the model file.\n            opts (Any): Runtime-specific configuration options.\n            model_info (RemoteModelInfo): Metadata about the model.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n        \"\"\"\n        Run inference on the input image.\n\n        Args:\n            im (np.ndarray): Input image as a numpy array.\n\n        Returns:\n            np.ndarray: Model output as a numpy array.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_info(self) -&gt; tuple[str, str]:\n        \"\"\"\n        Get the engine and device name.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def benchmark(self, iterations: int, size: Union[int, Tuple[int, int]]) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model performance.\n\n        Args:\n            iterations (int): Number of inference iterations to run.\n            size (float): Input image size for benchmarking.\n\n        Returns:\n            LatencyMetrics: Performance metrics including mean, median, and percentile latencies.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.base.BaseRuntime.__call__","title":"<code>__call__(im)</code>  <code>abstractmethod</code>","text":"<p>Run inference on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>Input image as a numpy array.</p> required <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>np.ndarray: Model output as a numpy array.</p> Source code in <code>focoos/infer/runtimes/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n    \"\"\"\n    Run inference on the input image.\n\n    Args:\n        im (np.ndarray): Input image as a numpy array.\n\n    Returns:\n        np.ndarray: Model output as a numpy array.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.base.BaseRuntime.__init__","title":"<code>__init__(model_path, opts, model_info)</code>","text":"<p>Initialize the runtime with model path, options and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model file.</p> required <code>opts</code> <code>Any</code> <p>Runtime-specific configuration options.</p> required <code>model_info</code> <code>RemoteModelInfo</code> <p>Metadata about the model.</p> required Source code in <code>focoos/infer/runtimes/base.py</code> <pre><code>def __init__(self, model_path: str, opts: Any, model_info: RemoteModelInfo):\n    \"\"\"\n    Initialize the runtime with model path, options and metadata.\n\n    Args:\n        model_path (str): Path to the model file.\n        opts (Any): Runtime-specific configuration options.\n        model_info (RemoteModelInfo): Metadata about the model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.base.BaseRuntime.benchmark","title":"<code>benchmark(iterations, size)</code>  <code>abstractmethod</code>","text":"<p>Benchmark the model performance.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of inference iterations to run.</p> required <code>size</code> <code>float</code> <p>Input image size for benchmarking.</p> required <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Performance metrics including mean, median, and percentile latencies.</p> Source code in <code>focoos/infer/runtimes/base.py</code> <pre><code>@abstractmethod\ndef benchmark(self, iterations: int, size: Union[int, Tuple[int, int]]) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model performance.\n\n    Args:\n        iterations (int): Number of inference iterations to run.\n        size (float): Input image size for benchmarking.\n\n    Returns:\n        LatencyMetrics: Performance metrics including mean, median, and percentile latencies.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.base.BaseRuntime.get_info","title":"<code>get_info()</code>  <code>abstractmethod</code>","text":"<p>Get the engine and device name.</p> Source code in <code>focoos/infer/runtimes/base.py</code> <pre><code>@abstractmethod\ndef get_info(self) -&gt; tuple[str, str]:\n    \"\"\"\n    Get the engine and device name.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.load_runtime.load_runtime","title":"<code>load_runtime(runtime_type, model_path, model_info, warmup_iter=50)</code>","text":"<p>Creates and returns a runtime instance based on the specified runtime type. Supports both ONNX and TorchScript runtimes with various execution providers.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_type</code> <code>RuntimeTypes</code> <p>The type of runtime to use. Can be one of: - ONNX_CUDA32: ONNX runtime with CUDA FP32 - ONNX_TRT32: ONNX runtime with TensorRT FP32 - ONNX_TRT16: ONNX runtime with TensorRT FP16 - ONNX_CPU: ONNX runtime with CPU - ONNX_COREML: ONNX runtime with CoreML - TORCHSCRIPT_32: TorchScript runtime with FP32</p> required <code>model_path</code> <code>str</code> <p>Path to the model file (.onnx or .pt)</p> required <code>model_metadata</code> <code>ModelMetadata</code> <p>Model metadata containing task type, classes etc.</p> required <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations before inference. Defaults to 0.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>BaseRuntime</code> <code>BaseRuntime</code> <p>A configured runtime instance (ONNXRuntime or TorchscriptRuntime)</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required dependencies (torch/onnxruntime) are not installed</p> Source code in <code>focoos/infer/runtimes/load_runtime.py</code> <pre><code>def load_runtime(\n    runtime_type: RuntimeType,\n    model_path: str,\n    model_info: ModelInfo,\n    warmup_iter: int = 50,\n) -&gt; BaseRuntime:\n    \"\"\"\n    Creates and returns a runtime instance based on the specified runtime type.\n    Supports both ONNX and TorchScript runtimes with various execution providers.\n\n    Args:\n        runtime_type (RuntimeTypes): The type of runtime to use. Can be one of:\n            - ONNX_CUDA32: ONNX runtime with CUDA FP32\n            - ONNX_TRT32: ONNX runtime with TensorRT FP32\n            - ONNX_TRT16: ONNX runtime with TensorRT FP16\n            - ONNX_CPU: ONNX runtime with CPU\n            - ONNX_COREML: ONNX runtime with CoreML\n            - TORCHSCRIPT_32: TorchScript runtime with FP32\n        model_path (str): Path to the model file (.onnx or .pt)\n        model_metadata (ModelMetadata): Model metadata containing task type, classes etc.\n        warmup_iter (int, optional): Number of warmup iterations before inference. Defaults to 0.\n\n    Returns:\n        BaseRuntime: A configured runtime instance (ONNXRuntime or TorchscriptRuntime)\n\n    Raises:\n        ImportError: If required dependencies (torch/onnxruntime) are not installed\n    \"\"\"\n    if runtime_type == RuntimeType.TORCHSCRIPT_32:\n        if not TORCH_AVAILABLE:\n            logger.error(\n                \"\u26a0\ufe0f Pytorch not found =(  please install focoos with ['torch'] extra. See https://focoosai.github.io/focoos/setup/ for more details\"\n            )\n            raise ImportError(\"Pytorch not found\")\n        from focoos.infer.runtimes.torchscript import TorchscriptRuntime\n\n        opts = TorchscriptRuntimeOpts(warmup_iter=warmup_iter)\n        return TorchscriptRuntime(model_path=model_path, opts=opts, model_info=model_info)\n    else:\n        if not ORT_AVAILABLE:\n            logger.error(\n                \"\u26a0\ufe0f onnxruntime not found =(  please install focoos with one of 'onnx', 'onnx-cpu', extra. See https://focoosai.github.io/focoos/setup/ for more details\"\n            )\n            raise ImportError(\"onnxruntime not found\")\n        from focoos.infer.runtimes.onnx import ONNXRuntime\n\n        opts = OnnxRuntimeOpts(\n            cuda=runtime_type == RuntimeType.ONNX_CUDA32,\n            trt=runtime_type in [RuntimeType.ONNX_TRT32, RuntimeType.ONNX_TRT16],\n            fp16=runtime_type == RuntimeType.ONNX_TRT16,\n            warmup_iter=warmup_iter,\n            coreml=runtime_type == RuntimeType.ONNX_COREML,\n            verbose=False,\n        )\n    return ONNXRuntime(model_path=model_path, opts=opts, model_info=model_info)\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.onnx.ONNXRuntime","title":"<code>ONNXRuntime</code>","text":"<p>               Bases: <code>BaseRuntime</code></p> <p>ONNX Runtime wrapper for model inference with different execution providers.</p> <p>This class implements the BaseRuntime interface for ONNX models, supporting various execution providers like CUDA, TensorRT, OpenVINO, and CoreML. It handles model initialization, provider configuration, warmup, inference, and performance benchmarking.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the model derived from the model path.</p> <code>opts</code> <code>OnnxRuntimeOpts</code> <p>Configuration options for the ONNX runtime.</p> <code>model_info</code> <code>RemoteModelInfo</code> <p>Metadata about the model.</p> <code>ort_sess</code> <code>InferenceSession</code> <p>ONNX Runtime inference session.</p> <code>active_providers</code> <code>list</code> <p>List of active execution providers.</p> <code>dtype</code> <code>dtype</code> <p>Input data type for the model.</p> Source code in <code>focoos/infer/runtimes/onnx.py</code> <pre><code>class ONNXRuntime(BaseRuntime):\n    \"\"\"\n    ONNX Runtime wrapper for model inference with different execution providers.\n\n    This class implements the BaseRuntime interface for ONNX models, supporting\n    various execution providers like CUDA, TensorRT, OpenVINO, and CoreML.\n    It handles model initialization, provider configuration, warmup, inference,\n    and performance benchmarking.\n\n    Attributes:\n        name (str): Name of the model derived from the model path.\n        opts (OnnxRuntimeOpts): Configuration options for the ONNX runtime.\n        model_info (RemoteModelInfo): Metadata about the model.\n        ort_sess (ort.InferenceSession): ONNX Runtime inference session.\n        active_providers (list): List of active execution providers.\n        dtype (np.dtype): Input data type for the model.\n    \"\"\"\n\n    def __init__(self, model_path: Union[str, Path], opts: OnnxRuntimeOpts, model_info: ModelInfo):\n        logger.debug(f\"\ud83d\udd27 [onnxruntime device] {ort.get_device()}\")\n\n        self.name = Path(model_path).stem\n        self.opts = opts\n        self.model_info = model_info\n\n        # Setup session options\n        options = ort.SessionOptions()\n        options.log_severity_level = 0 if opts.verbose else 2\n        options.enable_profiling = opts.verbose\n\n        # Setup providers\n        self.providers = self._setup_providers(model_dir=Path(model_path).parent)\n        self.active_provider = self.providers[0][0]\n        logger.info(f\" using: {self.active_provider}\")\n        # Create session\n        self.ort_sess = ort.InferenceSession(model_path, options, providers=self.providers)\n\n        if self.opts.trt and self.providers[0][0] == \"TensorrtExecutionProvider\":\n            logger.info(\"\ud83d\udfe2  TensorRT enabled. First execution may take longer as it builds the TRT engine.\")\n        # Set input type\n        self.dtype = np.uint8 if self.ort_sess.get_inputs()[0].type == \"tensor(uint8)\" else np.float32\n\n        # Warmup\n        if self.opts.warmup_iter &gt; 0:\n            self._warmup()\n\n        # inputs = self.ort_sess.get_inputs()\n        # outputs = self.ort_sess.get_outputs()\n        # for input in inputs:\n        #     logger.debug(f\"\ud83d\udd27 Input: {input.name} {input.type} {input.shape}\")\n        # for output in outputs:\n        #     logger.debug(f\"\ud83d\udd27 Output: {output.name} {output.type} {output.shape}\")\n\n    def _setup_providers(self, model_dir: Path):\n        providers = []\n        available = ort.get_available_providers()\n        logger.debug(f\"Available providers:{available}\")\n        _dir = Path(model_dir)\n        models_root = _dir.parent\n        # Check and add providers in order of preference\n        provider_configs = [\n            (\n                \"TensorrtExecutionProvider\",\n                self.opts.trt,\n                {\n                    \"device_id\": GPU_ID,\n                    \"trt_fp16_enable\": self.opts.fp16,\n                    \"trt_force_sequential_engine_build\": False,\n                    \"trt_engine_cache_enable\": True,\n                    \"trt_engine_cache_path\": str(_dir / \".trt_cache\"),\n                    \"trt_ep_context_file_path\": str(_dir),\n                    \"trt_timing_cache_enable\": True,  # Timing cache can be shared across multiple models if layers are the same\n                    \"trt_builder_optimization_level\": 3,\n                    \"trt_timing_cache_path\": str(models_root / \".trt_timing_cache\"),\n                },\n            ),\n            (\n                \"OpenVINOExecutionProvider\",\n                self.opts.vino,\n                {\"device_type\": \"MYRIAD_FP16\", \"enable_vpu_fast_compile\": True, \"num_of_threads\": 1},\n            ),\n            (\n                \"CUDAExecutionProvider\",\n                self.opts.cuda,\n                {\n                    \"device_id\": GPU_ID,\n                    \"arena_extend_strategy\": \"kSameAsRequested\",\n                    \"gpu_mem_limit\": 16 * 1024 * 1024 * 1024,\n                    \"cudnn_conv_algo_search\": \"EXHAUSTIVE\",\n                    \"do_copy_in_default_stream\": True,\n                },\n            ),\n            (\"CoreMLExecutionProvider\", self.opts.coreml, {}),\n        ]\n\n        for provider, enabled, config in provider_configs:\n            if enabled and provider in available:\n                providers.append((provider, config))\n            elif enabled:\n                logger.warning(f\"{provider} not found.\")\n\n        providers.append((\"CPUExecutionProvider\", {}))\n        return providers\n\n    def _warmup(self):\n        size = self.model_info.im_size\n        logger.info(f\"\u23f1\ufe0f Warming up model {self.name} on {self.active_provider}, size: {size}x{size}..\")\n        np_image = np.random.rand(1, 3, size, size).astype(self.dtype)\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n        for _ in range(self.opts.warmup_iter):\n            self.ort_sess.run(out_name, {input_name: np_image})\n\n        logger.info(\"\u23f1\ufe0f Warmup done\")\n\n    def __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n        \"\"\"\n        Run inference on the input image.\n\n        Args:\n            im (np.ndarray): Input image as a numpy array.\n\n        Returns:\n            list[np.ndarray]: Model outputs as a list of numpy arrays.\n        \"\"\"\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n        out = self.ort_sess.run(out_name, {input_name: im.cpu().numpy()})\n        return out\n\n    def get_info(self) -&gt; tuple[str, str]:\n        gpu_info = get_gpu_info()\n        device_name = \"CPU\"\n        if gpu_info.devices is not None and len(gpu_info.devices) &gt; 0:\n            device_name = gpu_info.devices[0].gpu_name\n        else:\n            device_name = get_cpu_name()\n            logger.warning(f\"No GPU found, using CPU {device_name}.\")\n        return f\"onnx.{self.active_provider}\", str(device_name)\n\n    def benchmark(self, iterations: int = 50, size: Union[int, Tuple[int, int]] = 640) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model performance.\n\n        Runs multiple inference iterations and measures execution time to calculate\n        performance metrics like FPS, mean latency, and other statistics.\n\n        Args:\n            iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n            size (int or tuple, optional): Input image size for benchmarking. Defaults to 640.\n\n        Returns:\n            LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n        \"\"\"\n        engine, device_name = self.get_info()\n        if isinstance(size, int):\n            size = (size, size)\n\n        logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device_name}, size: {size}x{size}..\")\n\n        np_input = (255 * np.random.random((1, 3, size[0], size[1]))).astype(self.dtype)\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n        durations = []\n        for step in range(iterations + 5):\n            start = perf_counter()\n            self.ort_sess.run(out_name, {input_name: np_input})\n            end = perf_counter()\n\n            if step &gt;= 5:  # Skip first 5 iterations\n                durations.append((end - start) * 1000)\n\n        durations = np.array(durations)\n\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean()),\n            engine=engine,\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n            device=device_name,\n        )\n        logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.onnx.ONNXRuntime.__call__","title":"<code>__call__(im)</code>","text":"<p>Run inference on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>Input image as a numpy array.</p> required <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>list[np.ndarray]: Model outputs as a list of numpy arrays.</p> Source code in <code>focoos/infer/runtimes/onnx.py</code> <pre><code>def __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n    \"\"\"\n    Run inference on the input image.\n\n    Args:\n        im (np.ndarray): Input image as a numpy array.\n\n    Returns:\n        list[np.ndarray]: Model outputs as a list of numpy arrays.\n    \"\"\"\n    input_name = self.ort_sess.get_inputs()[0].name\n    out_name = [output.name for output in self.ort_sess.get_outputs()]\n    out = self.ort_sess.run(out_name, {input_name: im.cpu().numpy()})\n    return out\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.onnx.ONNXRuntime.benchmark","title":"<code>benchmark(iterations=50, size=640)</code>","text":"<p>Benchmark the model performance.</p> <p>Runs multiple inference iterations and measures execution time to calculate performance metrics like FPS, mean latency, and other statistics.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of inference iterations to run. Defaults to 20.</p> <code>50</code> <code>size</code> <code>int or tuple</code> <p>Input image size for benchmarking. Defaults to 640.</p> <code>640</code> <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Performance metrics including FPS, mean, min, max, and std latencies.</p> Source code in <code>focoos/infer/runtimes/onnx.py</code> <pre><code>def benchmark(self, iterations: int = 50, size: Union[int, Tuple[int, int]] = 640) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model performance.\n\n    Runs multiple inference iterations and measures execution time to calculate\n    performance metrics like FPS, mean latency, and other statistics.\n\n    Args:\n        iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n        size (int or tuple, optional): Input image size for benchmarking. Defaults to 640.\n\n    Returns:\n        LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n    \"\"\"\n    engine, device_name = self.get_info()\n    if isinstance(size, int):\n        size = (size, size)\n\n    logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device_name}, size: {size}x{size}..\")\n\n    np_input = (255 * np.random.random((1, 3, size[0], size[1]))).astype(self.dtype)\n    input_name = self.ort_sess.get_inputs()[0].name\n    out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n    durations = []\n    for step in range(iterations + 5):\n        start = perf_counter()\n        self.ort_sess.run(out_name, {input_name: np_input})\n        end = perf_counter()\n\n        if step &gt;= 5:  # Skip first 5 iterations\n            durations.append((end - start) * 1000)\n\n    durations = np.array(durations)\n\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean()),\n        engine=engine,\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n        device=device_name,\n    )\n    logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.torchscript.TorchscriptRuntime","title":"<code>TorchscriptRuntime</code>","text":"<p>               Bases: <code>BaseRuntime</code></p> <p>TorchScript Runtime wrapper for model inference.</p> <p>This class implements the BaseRuntime interface for TorchScript models, supporting both CPU and CUDA devices. It handles model initialization, device placement, warmup, inference, and performance benchmarking.</p> <p>Attributes:</p> Name Type Description <code>device</code> <code>device</code> <p>Device to run inference on (CPU or CUDA).</p> <code>opts</code> <code>TorchscriptRuntimeOpts</code> <p>Configuration options for the TorchScript runtime.</p> <code>model</code> <code>ScriptModule</code> <p>Loaded TorchScript model.</p> <code>model_info</code> <code>RemoteModelInfo</code> <p>Metadata about the model.</p> Source code in <code>focoos/infer/runtimes/torchscript.py</code> <pre><code>class TorchscriptRuntime(BaseRuntime):\n    \"\"\"\n    TorchScript Runtime wrapper for model inference.\n\n    This class implements the BaseRuntime interface for TorchScript models,\n    supporting both CPU and CUDA devices. It handles model initialization,\n    device placement, warmup, inference, and performance benchmarking.\n\n    Attributes:\n        device (torch.device): Device to run inference on (CPU or CUDA).\n        opts (TorchscriptRuntimeOpts): Configuration options for the TorchScript runtime.\n        model (torch.jit.ScriptModule): Loaded TorchScript model.\n        model_info (RemoteModelInfo): Metadata about the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        opts: TorchscriptRuntimeOpts,\n        model_info: ModelInfo,\n    ):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(f\"\ud83d\udd27 Device: {self.device}\")\n        self.opts = opts\n        self.model_info = model_info\n\n        map_location = None if torch.cuda.is_available() else \"cpu\"\n\n        self.model = torch.jit.load(model_path, map_location=map_location)\n        self.model = self.model.to(self.device)\n\n        if self.opts.warmup_iter &gt; 0:\n            size = (\n                self.model_info.im_size if self.model_info.task == Task.DETECTION and self.model_info.im_size else 640\n            )\n            logger.info(f\"\u23f1\ufe0f Warming up model {self.model_info.name} on {self.device}, size: {size}x{size}..\")\n            with torch.no_grad():\n                np_image = torch.rand(1, 3, size, size, device=self.device)\n                for _ in range(self.opts.warmup_iter):\n                    self.model(np_image)\n            logger.info(\"\u23f1\ufe0f WARMUP DONE\")\n\n    def __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n        \"\"\"\n        Run inference on the input image.\n\n        Args:\n            im (np.ndarray): Input image as a numpy array.\n\n        Returns:\n            list[np.ndarray]: Model outputs as a list of numpy arrays.\n        \"\"\"\n        with torch.no_grad():\n            res = self.model(im)\n            return res\n\n    def get_info(self) -&gt; tuple[str, str]:\n        gpu_info = get_gpu_info()\n        device_name = \"CPU\"\n        if gpu_info.devices is not None and len(gpu_info.devices) &gt; 0:\n            device_name = gpu_info.devices[0].gpu_name\n        else:\n            device_name = get_cpu_name()\n            logger.warning(f\"No GPU found, using CPU {device_name}.\")\n\n        return \"torchscript\", str(device_name)\n\n    def benchmark(self, iterations: int = 20, size: Union[int, Tuple[int, int]] = 640) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model performance.\n\n        Runs multiple inference iterations and measures execution time to calculate\n        performance metrics like FPS, mean latency, and other statistics.\n\n        Args:\n            iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n\n        Returns:\n            LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n        \"\"\"\n        engine, device_name = self.get_info()\n        logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device_name}, size: {size}x{size}..\")\n\n        if isinstance(size, int):\n            size = (size, size)\n\n        torch_input = torch.rand(1, 3, size[0], size[1], device=self.device)\n        durations = []\n\n        with torch.no_grad():\n            for step in range(iterations + 5):\n                start = perf_counter()\n                self.model(torch_input)\n                end = perf_counter()\n\n                if step &gt;= 5:  # Skip first 5 iterations\n                    durations.append((end - start) * 1000)\n\n        durations = np.array(durations)\n\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean().astype(float)),\n            engine=engine,\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size,\n            device=device_name,\n        )\n        logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.torchscript.TorchscriptRuntime.__call__","title":"<code>__call__(im)</code>","text":"<p>Run inference on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>Input image as a numpy array.</p> required <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>list[np.ndarray]: Model outputs as a list of numpy arrays.</p> Source code in <code>focoos/infer/runtimes/torchscript.py</code> <pre><code>def __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n    \"\"\"\n    Run inference on the input image.\n\n    Args:\n        im (np.ndarray): Input image as a numpy array.\n\n    Returns:\n        list[np.ndarray]: Model outputs as a list of numpy arrays.\n    \"\"\"\n    with torch.no_grad():\n        res = self.model(im)\n        return res\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.torchscript.TorchscriptRuntime.benchmark","title":"<code>benchmark(iterations=20, size=640)</code>","text":"<p>Benchmark the model performance.</p> <p>Runs multiple inference iterations and measures execution time to calculate performance metrics like FPS, mean latency, and other statistics.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of inference iterations to run. Defaults to 20.</p> <code>20</code> <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Performance metrics including FPS, mean, min, max, and std latencies.</p> Source code in <code>focoos/infer/runtimes/torchscript.py</code> <pre><code>def benchmark(self, iterations: int = 20, size: Union[int, Tuple[int, int]] = 640) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model performance.\n\n    Runs multiple inference iterations and measures execution time to calculate\n    performance metrics like FPS, mean latency, and other statistics.\n\n    Args:\n        iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n\n    Returns:\n        LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n    \"\"\"\n    engine, device_name = self.get_info()\n    logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device_name}, size: {size}x{size}..\")\n\n    if isinstance(size, int):\n        size = (size, size)\n\n    torch_input = torch.rand(1, 3, size[0], size[1], device=self.device)\n    durations = []\n\n    with torch.no_grad():\n        for step in range(iterations + 5):\n            start = perf_counter()\n            self.model(torch_input)\n            end = perf_counter()\n\n            if step &gt;= 5:  # Skip first 5 iterations\n                durations.append((end - start) * 1000)\n\n    durations = np.array(durations)\n\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean().astype(float)),\n        engine=engine,\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size,\n        device=device_name,\n    )\n    logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/trainer_evaluation/","title":"evaluation","text":""},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator","title":"<code>ClassificationEvaluator</code>","text":"<p>               Bases: <code>DatasetEvaluator</code></p> <p>Evaluator for classification tasks with comprehensive metrics computation.</p> <p>This evaluator computes various classification metrics including accuracy, precision, recall, and F1 score both per-class and as macro/weighted averages. It supports distributed evaluation across multiple processes.</p> <p>Attributes:</p> Name Type Description <code>dataset_dict</code> <code>DictDataset</code> <p>Dataset containing ground truth annotations.</p> <code>metadata</code> <p>Metadata from the dataset containing class information.</p> <code>num_classes</code> <code>int</code> <p>Number of classes in the classification task.</p> <code>class_names</code> <code>List[str]</code> <p>Names of the classes.</p> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>class ClassificationEvaluator(DatasetEvaluator):\n    \"\"\"Evaluator for classification tasks with comprehensive metrics computation.\n\n    This evaluator computes various classification metrics including accuracy, precision,\n    recall, and F1 score both per-class and as macro/weighted averages. It supports\n    distributed evaluation across multiple processes.\n\n    Attributes:\n        dataset_dict (DictDataset): Dataset containing ground truth annotations.\n        metadata: Metadata from the dataset containing class information.\n        num_classes (int): Number of classes in the classification task.\n        class_names (List[str]): Names of the classes.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_dict: DictDataset,\n        distributed=True,\n    ):\n        \"\"\"Initialize the ClassificationEvaluator.\n\n        Args:\n            dataset_dict (DictDataset): Dataset in DictDataset format containing\n                the ground truth annotations.\n            distributed (bool, optional): If True, evaluation will be distributed\n                across multiple processes. Defaults to True.\n        \"\"\"\n        self.dataset_dict = dataset_dict\n        self.metadata = self.dataset_dict.metadata\n        self._distributed = distributed\n        self._cpu_device = torch.device(\"cpu\")\n        self.num_classes = self.metadata.num_classes\n        self.class_names = self.metadata.thing_classes\n\n        self._predictions = []\n        self._targets = []\n\n    @classmethod\n    def from_datasetdict(cls, dataset_dict, **kwargs):\n        \"\"\"Create ClassificationEvaluator instance from a dataset dictionary.\n\n        Args:\n            dataset_dict: Dataset dictionary containing the data and metadata.\n            **kwargs: Additional keyword arguments passed to the constructor.\n\n        Returns:\n            ClassificationEvaluator: New instance of the evaluator.\n        \"\"\"\n        return cls(dataset_dict=dataset_dict, **kwargs)\n\n    def reset(self):\n        \"\"\"Clear stored predictions and targets.\n\n        This method resets the internal state of the evaluator by clearing\n        all accumulated predictions and ground truth targets.\n        \"\"\"\n        self._predictions = []\n        self._targets = []\n\n    def process(self, inputs: List[ClassificationDatasetDict], outputs: List[ClassificationModelOutput]):\n        \"\"\"Process a batch of inputs and outputs for evaluation.\n\n        This method extracts predictions and ground truth labels from the provided\n        inputs and outputs, then stores them for later evaluation.\n\n        Args:\n            inputs (List[ClassificationDatasetDict]): List of input dictionaries,\n                each containing ground truth information. Expected to have 'label'\n                field or 'annotations' with category_id.\n            outputs (List[ClassificationModelOutput]): List of model outputs,\n                each containing 'logits' field with predicted class logits or\n                ClassificationModelOutput instances.\n\n        Note:\n            - Ground truth labels are extracted from input['label'] or\n              input['annotations'][0]['category_id']\n            - Predictions are extracted from output['logits'] or output.logits\n            - Items with missing labels or logits are skipped with warnings\n        \"\"\"\n        for input_item, output_item in zip(inputs, outputs):\n            # Get ground truth label from input\n            label = None\n            if \"label\" in input_item:\n                label = input_item[\"label\"]\n            elif hasattr(input_item, \"label\"):\n                label = input_item.label\n            elif \"annotations\" in input_item and len(input_item[\"annotations\"]) &gt; 0:\n                # Handle label from annotations format\n                label = input_item[\"annotations\"][0].get(\"category_id\", None)\n\n            if label is None:\n                logger.warning(f\"Could not find label in input item: {input_item}\")\n                continue\n\n            # Get model predictions from output\n            logits = None\n            if isinstance(output_item, dict) and \"logits\" in output_item:\n                logits = output_item[\"logits\"]\n            elif hasattr(output_item, \"logits\"):\n                # Handle ClassificationModelOutput objects\n                logits = output_item.logits\n\n            if logits is None:\n                logger.warning(f\"Could not find logits in output item: {output_item}\")\n                continue\n\n            # Move tensors to CPU for evaluation\n            logits = logits.to(self._cpu_device)\n\n            # For image classification, logits will be [num_classes]\n            # For batch processing, it could be [batch_size, num_classes]\n            if logits.dim() &gt; 1:\n                # Assume first dimension is batch size\n                predicted_class = torch.argmax(logits, dim=1).item()\n            else:\n                predicted_class = torch.argmax(logits, dim=0).item()\n\n            # Store prediction and ground truth\n            self._predictions.append(predicted_class)\n            self._targets.append(label)\n\n    def _compute_confusion_matrix(self, y_true, y_pred, num_classes):\n        \"\"\"Compute confusion matrix for classification evaluation.\n\n        Args:\n            y_true (torch.Tensor): Ground truth labels tensor.\n            y_pred (torch.Tensor): Predicted labels tensor.\n            num_classes (int): Number of classes in the classification task.\n\n        Returns:\n            torch.Tensor: Confusion matrix of shape (num_classes, num_classes).\n                Element [i,j] represents the number of samples with true label i\n                that were predicted as label j.\n        \"\"\"\n        confusion_matrix = torch.zeros(num_classes, num_classes, dtype=torch.long)\n        for t, p in zip(y_true, y_pred):\n            confusion_matrix[t, p] += 1\n        return confusion_matrix\n\n    def evaluate(self):\n        \"\"\"Evaluate classification metrics on accumulated predictions.\n\n        Computes comprehensive classification metrics including accuracy,\n        per-class precision/recall/F1, and macro/weighted averages.\n\n        Returns:\n            OrderedDict: Dictionary containing evaluation metrics with the following keys:\n                - 'Accuracy': Overall classification accuracy (%)\n                - 'Macro-Precision': Macro-averaged precision (%)\n                - 'Macro-Recall': Macro-averaged recall (%)\n                - 'Macro-F1': Macro-averaged F1 score (%)\n                - 'Weighted-Precision': Weighted precision (%)\n                - 'Weighted-Recall': Weighted recall (%)\n                - 'Weighted-F1': Weighted F1 score (%)\n                - 'Precision-{class_name}': Per-class precision (%)\n                - 'Recall-{class_name}': Per-class recall (%)\n                - 'F1-{class_name}': Per-class F1 score (%)\n\n        Note:\n            - In distributed mode, only the main process returns results\n            - All metrics are expressed as percentages\n            - Returns empty dict if no predictions are available\n            - Results are wrapped in 'classification' key for trainer compatibility\n        \"\"\"\n        if self._distributed:\n            synchronize()\n            predictions_list = all_gather(self._predictions)\n            targets_list = all_gather(self._targets)\n\n            # Flatten gathered lists\n            predictions = []\n            targets = []\n            for p_list, t_list in zip(predictions_list, targets_list):\n                if p_list is not None:\n                    predictions.extend(p_list)\n                if t_list is not None:\n                    targets.extend(t_list)\n\n            if not is_main_process():\n                return\n        else:\n            predictions = self._predictions\n            targets = self._targets\n\n        # Check if we have predictions to evaluate\n        if len(predictions) == 0:\n            logger.warning(\"No predictions to evaluate\")\n            return OrderedDict({\"classification\": {}})\n\n        # Convert lists to tensors\n        y_true = torch.tensor(targets, dtype=torch.long)\n        y_pred = torch.tensor(predictions, dtype=torch.long)\n\n        # Compute confusion matrix\n        cm = self._compute_confusion_matrix(y_true, y_pred, self.num_classes)\n\n        # Calculate accuracy\n        accuracy = 100.0 * cm.diag().sum().float() / cm.sum().float()\n\n        # Calculate per-class metrics\n        tp = cm.diag()  # True positives for each class\n        pred_sum = cm.sum(dim=0)  # Sum over actual classes (columns)\n        target_sum = cm.sum(dim=1)  # Sum over predicted classes (rows)\n\n        # Precision for each class\n        precision_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n        for i in range(self.num_classes):\n            if pred_sum[i] &gt; 0:\n                precision_per_class[i] = 100.0 * tp[i].float() / pred_sum[i].float()\n\n        # Recall for each class\n        recall_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n        for i in range(self.num_classes):\n            if target_sum[i] &gt; 0:\n                recall_per_class[i] = 100.0 * tp[i].float() / target_sum[i].float()\n\n        # F1 score for each class\n        f1_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n        for i in range(self.num_classes):\n            if precision_per_class[i] + recall_per_class[i] &gt; 0:\n                f1_per_class[i] = (\n                    2 * (precision_per_class[i] * recall_per_class[i]) / (precision_per_class[i] + recall_per_class[i])\n                )\n\n        # Calculate macro averages\n        valid_precision_classes = (pred_sum &gt; 0).sum().item()\n        macro_precision = precision_per_class.sum() / max(valid_precision_classes, 1)\n\n        valid_recall_classes = (target_sum &gt; 0).sum().item()\n        macro_recall = recall_per_class.sum() / max(valid_recall_classes, 1)\n\n        if macro_precision + macro_recall &gt; 0:\n            macro_f1 = 2 * (macro_precision * macro_recall) / (macro_precision + macro_recall)\n        else:\n            macro_f1 = torch.tensor(0.0)\n\n        # Calculate weighted averages\n        weights = target_sum.float() / target_sum.sum().float()\n        weighted_precision = (precision_per_class * weights).sum()\n        weighted_recall = (recall_per_class * weights).sum()\n\n        if weighted_precision + weighted_recall &gt; 0:\n            weighted_f1 = 2 * (weighted_precision * weighted_recall) / (weighted_precision + weighted_recall)\n        else:\n            weighted_f1 = torch.tensor(0.0)\n\n        # Create results dictionary\n        results = OrderedDict()\n        results[\"Accuracy\"] = accuracy.item()\n        results[\"Macro-Precision\"] = macro_precision.item()\n        results[\"Macro-Recall\"] = macro_recall.item()\n        results[\"Macro-F1\"] = macro_f1.item()\n        results[\"Weighted-Precision\"] = weighted_precision.item()\n        results[\"Weighted-Recall\"] = weighted_recall.item()\n        results[\"Weighted-F1\"] = weighted_f1.item()\n\n        # Add per-class metrics\n        if self.class_names is not None:\n            for i, class_name in enumerate(self.class_names):\n                if i &lt; self.num_classes:\n                    results[f\"Precision-{class_name}\"] = precision_per_class[i].item()\n                    results[f\"Recall-{class_name}\"] = recall_per_class[i].item()\n                    results[f\"F1-{class_name}\"] = f1_per_class[i].item()\n\n        # Log results\n        logger.info(\"Classification Evaluation Results:\")\n        for k, v in results.items():\n            logger.info(f\"  {k}: {v:.2f}\")\n\n        # Return results in the expected format for trainer\n        return OrderedDict({\"classification\": results})\n</code></pre>"},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator.__init__","title":"<code>__init__(dataset_dict, distributed=True)</code>","text":"<p>Initialize the ClassificationEvaluator.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dict</code> <code>DictDataset</code> <p>Dataset in DictDataset format containing the ground truth annotations.</p> required <code>distributed</code> <code>bool</code> <p>If True, evaluation will be distributed across multiple processes. Defaults to True.</p> <code>True</code> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>def __init__(\n    self,\n    dataset_dict: DictDataset,\n    distributed=True,\n):\n    \"\"\"Initialize the ClassificationEvaluator.\n\n    Args:\n        dataset_dict (DictDataset): Dataset in DictDataset format containing\n            the ground truth annotations.\n        distributed (bool, optional): If True, evaluation will be distributed\n            across multiple processes. Defaults to True.\n    \"\"\"\n    self.dataset_dict = dataset_dict\n    self.metadata = self.dataset_dict.metadata\n    self._distributed = distributed\n    self._cpu_device = torch.device(\"cpu\")\n    self.num_classes = self.metadata.num_classes\n    self.class_names = self.metadata.thing_classes\n\n    self._predictions = []\n    self._targets = []\n</code></pre>"},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator.evaluate","title":"<code>evaluate()</code>","text":"<p>Evaluate classification metrics on accumulated predictions.</p> <p>Computes comprehensive classification metrics including accuracy, per-class precision/recall/F1, and macro/weighted averages.</p> <p>Returns:</p> Name Type Description <code>OrderedDict</code> <p>Dictionary containing evaluation metrics with the following keys: - 'Accuracy': Overall classification accuracy (%) - 'Macro-Precision': Macro-averaged precision (%) - 'Macro-Recall': Macro-averaged recall (%) - 'Macro-F1': Macro-averaged F1 score (%) - 'Weighted-Precision': Weighted precision (%) - 'Weighted-Recall': Weighted recall (%) - 'Weighted-F1': Weighted F1 score (%) - 'Precision-{class_name}': Per-class precision (%) - 'Recall-{class_name}': Per-class recall (%) - 'F1-{class_name}': Per-class F1 score (%)</p> Note <ul> <li>In distributed mode, only the main process returns results</li> <li>All metrics are expressed as percentages</li> <li>Returns empty dict if no predictions are available</li> <li>Results are wrapped in 'classification' key for trainer compatibility</li> </ul> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>def evaluate(self):\n    \"\"\"Evaluate classification metrics on accumulated predictions.\n\n    Computes comprehensive classification metrics including accuracy,\n    per-class precision/recall/F1, and macro/weighted averages.\n\n    Returns:\n        OrderedDict: Dictionary containing evaluation metrics with the following keys:\n            - 'Accuracy': Overall classification accuracy (%)\n            - 'Macro-Precision': Macro-averaged precision (%)\n            - 'Macro-Recall': Macro-averaged recall (%)\n            - 'Macro-F1': Macro-averaged F1 score (%)\n            - 'Weighted-Precision': Weighted precision (%)\n            - 'Weighted-Recall': Weighted recall (%)\n            - 'Weighted-F1': Weighted F1 score (%)\n            - 'Precision-{class_name}': Per-class precision (%)\n            - 'Recall-{class_name}': Per-class recall (%)\n            - 'F1-{class_name}': Per-class F1 score (%)\n\n    Note:\n        - In distributed mode, only the main process returns results\n        - All metrics are expressed as percentages\n        - Returns empty dict if no predictions are available\n        - Results are wrapped in 'classification' key for trainer compatibility\n    \"\"\"\n    if self._distributed:\n        synchronize()\n        predictions_list = all_gather(self._predictions)\n        targets_list = all_gather(self._targets)\n\n        # Flatten gathered lists\n        predictions = []\n        targets = []\n        for p_list, t_list in zip(predictions_list, targets_list):\n            if p_list is not None:\n                predictions.extend(p_list)\n            if t_list is not None:\n                targets.extend(t_list)\n\n        if not is_main_process():\n            return\n    else:\n        predictions = self._predictions\n        targets = self._targets\n\n    # Check if we have predictions to evaluate\n    if len(predictions) == 0:\n        logger.warning(\"No predictions to evaluate\")\n        return OrderedDict({\"classification\": {}})\n\n    # Convert lists to tensors\n    y_true = torch.tensor(targets, dtype=torch.long)\n    y_pred = torch.tensor(predictions, dtype=torch.long)\n\n    # Compute confusion matrix\n    cm = self._compute_confusion_matrix(y_true, y_pred, self.num_classes)\n\n    # Calculate accuracy\n    accuracy = 100.0 * cm.diag().sum().float() / cm.sum().float()\n\n    # Calculate per-class metrics\n    tp = cm.diag()  # True positives for each class\n    pred_sum = cm.sum(dim=0)  # Sum over actual classes (columns)\n    target_sum = cm.sum(dim=1)  # Sum over predicted classes (rows)\n\n    # Precision for each class\n    precision_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n    for i in range(self.num_classes):\n        if pred_sum[i] &gt; 0:\n            precision_per_class[i] = 100.0 * tp[i].float() / pred_sum[i].float()\n\n    # Recall for each class\n    recall_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n    for i in range(self.num_classes):\n        if target_sum[i] &gt; 0:\n            recall_per_class[i] = 100.0 * tp[i].float() / target_sum[i].float()\n\n    # F1 score for each class\n    f1_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n    for i in range(self.num_classes):\n        if precision_per_class[i] + recall_per_class[i] &gt; 0:\n            f1_per_class[i] = (\n                2 * (precision_per_class[i] * recall_per_class[i]) / (precision_per_class[i] + recall_per_class[i])\n            )\n\n    # Calculate macro averages\n    valid_precision_classes = (pred_sum &gt; 0).sum().item()\n    macro_precision = precision_per_class.sum() / max(valid_precision_classes, 1)\n\n    valid_recall_classes = (target_sum &gt; 0).sum().item()\n    macro_recall = recall_per_class.sum() / max(valid_recall_classes, 1)\n\n    if macro_precision + macro_recall &gt; 0:\n        macro_f1 = 2 * (macro_precision * macro_recall) / (macro_precision + macro_recall)\n    else:\n        macro_f1 = torch.tensor(0.0)\n\n    # Calculate weighted averages\n    weights = target_sum.float() / target_sum.sum().float()\n    weighted_precision = (precision_per_class * weights).sum()\n    weighted_recall = (recall_per_class * weights).sum()\n\n    if weighted_precision + weighted_recall &gt; 0:\n        weighted_f1 = 2 * (weighted_precision * weighted_recall) / (weighted_precision + weighted_recall)\n    else:\n        weighted_f1 = torch.tensor(0.0)\n\n    # Create results dictionary\n    results = OrderedDict()\n    results[\"Accuracy\"] = accuracy.item()\n    results[\"Macro-Precision\"] = macro_precision.item()\n    results[\"Macro-Recall\"] = macro_recall.item()\n    results[\"Macro-F1\"] = macro_f1.item()\n    results[\"Weighted-Precision\"] = weighted_precision.item()\n    results[\"Weighted-Recall\"] = weighted_recall.item()\n    results[\"Weighted-F1\"] = weighted_f1.item()\n\n    # Add per-class metrics\n    if self.class_names is not None:\n        for i, class_name in enumerate(self.class_names):\n            if i &lt; self.num_classes:\n                results[f\"Precision-{class_name}\"] = precision_per_class[i].item()\n                results[f\"Recall-{class_name}\"] = recall_per_class[i].item()\n                results[f\"F1-{class_name}\"] = f1_per_class[i].item()\n\n    # Log results\n    logger.info(\"Classification Evaluation Results:\")\n    for k, v in results.items():\n        logger.info(f\"  {k}: {v:.2f}\")\n\n    # Return results in the expected format for trainer\n    return OrderedDict({\"classification\": results})\n</code></pre>"},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator.from_datasetdict","title":"<code>from_datasetdict(dataset_dict, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create ClassificationEvaluator instance from a dataset dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dict</code> <p>Dataset dictionary containing the data and metadata.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ClassificationEvaluator</code> <p>New instance of the evaluator.</p> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>@classmethod\ndef from_datasetdict(cls, dataset_dict, **kwargs):\n    \"\"\"Create ClassificationEvaluator instance from a dataset dictionary.\n\n    Args:\n        dataset_dict: Dataset dictionary containing the data and metadata.\n        **kwargs: Additional keyword arguments passed to the constructor.\n\n    Returns:\n        ClassificationEvaluator: New instance of the evaluator.\n    \"\"\"\n    return cls(dataset_dict=dataset_dict, **kwargs)\n</code></pre>"},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator.process","title":"<code>process(inputs, outputs)</code>","text":"<p>Process a batch of inputs and outputs for evaluation.</p> <p>This method extracts predictions and ground truth labels from the provided inputs and outputs, then stores them for later evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[ClassificationDatasetDict]</code> <p>List of input dictionaries, each containing ground truth information. Expected to have 'label' field or 'annotations' with category_id.</p> required <code>outputs</code> <code>List[ClassificationModelOutput]</code> <p>List of model outputs, each containing 'logits' field with predicted class logits or ClassificationModelOutput instances.</p> required Note <ul> <li>Ground truth labels are extracted from input['label'] or   input['annotations'][0]['category_id']</li> <li>Predictions are extracted from output['logits'] or output.logits</li> <li>Items with missing labels or logits are skipped with warnings</li> </ul> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>def process(self, inputs: List[ClassificationDatasetDict], outputs: List[ClassificationModelOutput]):\n    \"\"\"Process a batch of inputs and outputs for evaluation.\n\n    This method extracts predictions and ground truth labels from the provided\n    inputs and outputs, then stores them for later evaluation.\n\n    Args:\n        inputs (List[ClassificationDatasetDict]): List of input dictionaries,\n            each containing ground truth information. Expected to have 'label'\n            field or 'annotations' with category_id.\n        outputs (List[ClassificationModelOutput]): List of model outputs,\n            each containing 'logits' field with predicted class logits or\n            ClassificationModelOutput instances.\n\n    Note:\n        - Ground truth labels are extracted from input['label'] or\n          input['annotations'][0]['category_id']\n        - Predictions are extracted from output['logits'] or output.logits\n        - Items with missing labels or logits are skipped with warnings\n    \"\"\"\n    for input_item, output_item in zip(inputs, outputs):\n        # Get ground truth label from input\n        label = None\n        if \"label\" in input_item:\n            label = input_item[\"label\"]\n        elif hasattr(input_item, \"label\"):\n            label = input_item.label\n        elif \"annotations\" in input_item and len(input_item[\"annotations\"]) &gt; 0:\n            # Handle label from annotations format\n            label = input_item[\"annotations\"][0].get(\"category_id\", None)\n\n        if label is None:\n            logger.warning(f\"Could not find label in input item: {input_item}\")\n            continue\n\n        # Get model predictions from output\n        logits = None\n        if isinstance(output_item, dict) and \"logits\" in output_item:\n            logits = output_item[\"logits\"]\n        elif hasattr(output_item, \"logits\"):\n            # Handle ClassificationModelOutput objects\n            logits = output_item.logits\n\n        if logits is None:\n            logger.warning(f\"Could not find logits in output item: {output_item}\")\n            continue\n\n        # Move tensors to CPU for evaluation\n        logits = logits.to(self._cpu_device)\n\n        # For image classification, logits will be [num_classes]\n        # For batch processing, it could be [batch_size, num_classes]\n        if logits.dim() &gt; 1:\n            # Assume first dimension is batch size\n            predicted_class = torch.argmax(logits, dim=1).item()\n        else:\n            predicted_class = torch.argmax(logits, dim=0).item()\n\n        # Store prediction and ground truth\n        self._predictions.append(predicted_class)\n        self._targets.append(label)\n</code></pre>"},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator.reset","title":"<code>reset()</code>","text":"<p>Clear stored predictions and targets.</p> <p>This method resets the internal state of the evaluator by clearing all accumulated predictions and ground truth targets.</p> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>def reset(self):\n    \"\"\"Clear stored predictions and targets.\n\n    This method resets the internal state of the evaluator by clearing\n    all accumulated predictions and ground truth targets.\n    \"\"\"\n    self._predictions = []\n    self._targets = []\n</code></pre>"},{"location":"api/trainer_hooks/","title":"hooks","text":""},{"location":"api/trainer_hooks/#focoos.trainer.hooks.early_stop.EarlyStoppingHook","title":"<code>EarlyStoppingHook</code>","text":"<p>               Bases: <code>HookBase</code></p> Source code in <code>focoos/trainer/hooks/early_stop.py</code> <pre><code>class EarlyStoppingHook(HookBase):\n    def __init__(\n        self,\n        enabled: bool,\n        eval_period: int,\n        patience: int,\n        val_metric: str,\n        mode: str = \"max\",\n    ):\n        \"\"\"\n        Initializes the EarlyStoppingHook.\n\n        This hook is designed to monitor a specific validation metric during the training process\n        and stop training when no improvement is observed in the metric for a specified number of\n        iterations. This is particularly useful for preventing overfitting by halting training\n        once the model's performance on the validation set no longer improves.\n\n        Args:\n            eval_period (int): The frequency (in iterations) at which the validation metric is evaluated.\n                               For example, if `eval_period` is 100, the validation metric will be checked\n                               every 100 iterations.\n            patience (int): Number of consecutive evaluations with no improvement after which training will be stopped.\n                            For example, if `patience` is set to 5, training will stop if the validation metric does not\n                            improve for 5 consecutive evaluations.\n            val_metric (str): The name of the validation metric to monitor. This should correspond to one of the metrics\n                              calculated during the validation phase, such as \"accuracy\", \"loss\", etc.\n            mode (str, optional): One of \"min\" or \"max\". This parameter dictates the direction of improvement\n                                  for the validation metric. In \"min\" mode, the training will stop when the monitored\n                                  quantity (e.g., loss) stops decreasing. In \"max\" mode, training will stop when\n                                  the monitored quantity (e.g., accuracy) stops increasing. Defaults to \"max\".\n\n        \"\"\"\n        self.enabled = enabled\n        self.patience = patience\n        self.val_metric = val_metric\n        self.mode = mode\n        self.best_metric = None\n        self.num_bad_epochs = 0\n        self._period = eval_period\n        self._logger = get_logger(__name__)\n\n    def after_step(self):\n        next_iter = self.trainer.iter + 1\n\n        if self._period &gt; 0 and next_iter % self._period == 0 and next_iter != self.trainer.max_iter and self.enabled:\n            metric_tuple = self.trainer.storage.latest().get(self.val_metric)\n\n            if metric_tuple is None:\n                return\n            else:\n                current_metric, metric_iter = metric_tuple\n\n            if (\n                self.best_metric is None\n                or (self.mode == \"max\" and current_metric &gt; self.best_metric)\n                or (self.mode == \"min\" and current_metric &lt; self.best_metric)\n            ):\n                self.best_metric = current_metric\n                self.num_bad_epochs = 0\n            else:\n                self.num_bad_epochs += 1\n                self._logger.info(f\"{self.num_bad_epochs}/{self.patience} without improvements..\")\n\n            if self.num_bad_epochs &gt;= self.patience:\n                self.trainer.storage.put_scalar(\"early_stopping\", True)\n                raise EarlyStopException(\"Early Stopping Exception to stop the training..\")\n</code></pre>"},{"location":"api/trainer_hooks/#focoos.trainer.hooks.early_stop.EarlyStoppingHook.__init__","title":"<code>__init__(enabled, eval_period, patience, val_metric, mode='max')</code>","text":"<p>Initializes the EarlyStoppingHook.</p> <p>This hook is designed to monitor a specific validation metric during the training process and stop training when no improvement is observed in the metric for a specified number of iterations. This is particularly useful for preventing overfitting by halting training once the model's performance on the validation set no longer improves.</p> <p>Parameters:</p> Name Type Description Default <code>eval_period</code> <code>int</code> <p>The frequency (in iterations) at which the validation metric is evaluated.                For example, if <code>eval_period</code> is 100, the validation metric will be checked                every 100 iterations.</p> required <code>patience</code> <code>int</code> <p>Number of consecutive evaluations with no improvement after which training will be stopped.             For example, if <code>patience</code> is set to 5, training will stop if the validation metric does not             improve for 5 consecutive evaluations.</p> required <code>val_metric</code> <code>str</code> <p>The name of the validation metric to monitor. This should correspond to one of the metrics               calculated during the validation phase, such as \"accuracy\", \"loss\", etc.</p> required <code>mode</code> <code>str</code> <p>One of \"min\" or \"max\". This parameter dictates the direction of improvement                   for the validation metric. In \"min\" mode, the training will stop when the monitored                   quantity (e.g., loss) stops decreasing. In \"max\" mode, training will stop when                   the monitored quantity (e.g., accuracy) stops increasing. Defaults to \"max\".</p> <code>'max'</code> Source code in <code>focoos/trainer/hooks/early_stop.py</code> <pre><code>def __init__(\n    self,\n    enabled: bool,\n    eval_period: int,\n    patience: int,\n    val_metric: str,\n    mode: str = \"max\",\n):\n    \"\"\"\n    Initializes the EarlyStoppingHook.\n\n    This hook is designed to monitor a specific validation metric during the training process\n    and stop training when no improvement is observed in the metric for a specified number of\n    iterations. This is particularly useful for preventing overfitting by halting training\n    once the model's performance on the validation set no longer improves.\n\n    Args:\n        eval_period (int): The frequency (in iterations) at which the validation metric is evaluated.\n                           For example, if `eval_period` is 100, the validation metric will be checked\n                           every 100 iterations.\n        patience (int): Number of consecutive evaluations with no improvement after which training will be stopped.\n                        For example, if `patience` is set to 5, training will stop if the validation metric does not\n                        improve for 5 consecutive evaluations.\n        val_metric (str): The name of the validation metric to monitor. This should correspond to one of the metrics\n                          calculated during the validation phase, such as \"accuracy\", \"loss\", etc.\n        mode (str, optional): One of \"min\" or \"max\". This parameter dictates the direction of improvement\n                              for the validation metric. In \"min\" mode, the training will stop when the monitored\n                              quantity (e.g., loss) stops decreasing. In \"max\" mode, training will stop when\n                              the monitored quantity (e.g., accuracy) stops increasing. Defaults to \"max\".\n\n    \"\"\"\n    self.enabled = enabled\n    self.patience = patience\n    self.val_metric = val_metric\n    self.mode = mode\n    self.best_metric = None\n    self.num_bad_epochs = 0\n    self._period = eval_period\n    self._logger = get_logger(__name__)\n</code></pre>"},{"location":"api/trainer_hooks/#focoos.trainer.hooks.sync_to_hub.SyncToHubHook","title":"<code>SyncToHubHook</code>","text":"<p>               Bases: <code>HookBase</code></p> Source code in <code>focoos/trainer/hooks/sync_to_hub.py</code> <pre><code>class SyncToHubHook(HookBase):\n    def __init__(\n        self,\n        remote_model: RemoteModel,\n        model_info: ModelInfo,\n        output_dir: str,\n        sync_period: int = 100,\n        eval_period: int = 100,\n    ):\n        self.remote_model = remote_model\n        self.model_info = model_info\n        self.output_dir = output_dir\n        self.sync_period = sync_period\n        self.eval_period = eval_period\n\n    @property\n    def iteration(self):\n        try:\n            _iter = self.trainer.iter\n        except Exception:\n            _iter = 1\n        return _iter\n\n    def before_train(self):\n        \"\"\"\n        Called before the first iteration.\n        \"\"\"\n        info = HubSyncLocalTraining(\n            status=ModelStatus.TRAINING_RUNNING,  # type: ignore\n            iterations=0,\n            metrics=None,\n        )\n\n        self._sync_train_job(info)\n\n    def after_step(self):\n        if (self.iteration % self.sync_period == 0) and self.iteration &gt; 0:\n            self._sync_train_job(\n                sync_info=HubSyncLocalTraining(\n                    status=ModelStatus.TRAINING_RUNNING,  # type: ignore\n                    iterations=self.iteration,\n                    training_info=self.model_info.training_info,\n                )\n            )\n\n        elif (self.iteration % (self.eval_period + 3) == 0) and self.iteration &gt; 0:\n            self._sync_train_job(\n                sync_info=HubSyncLocalTraining(\n                    status=ModelStatus.TRAINING_RUNNING,  # type: ignore\n                    iterations=self.iteration,\n                    training_info=self.model_info.training_info,\n                ),\n                upload_artifacts=[ArtifactName.WEIGHTS],\n            )\n\n    def after_train(self):\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        status = ModelStatus.TRAINING_COMPLETED\n        if exc_type is not None:\n            logger.error(\n                f\"Exception during training, status set to TRAINING_ERROR: {str(exc_type.__name__)} {str(exc_value)}\"\n            )\n            status = ModelStatus.TRAINING_ERROR\n            self.model_info.status = status\n            if self.model_info.training_info is not None:\n                self.model_info.training_info.main_status = status\n                self.model_info.training_info.failure_reason = str(exc_value)\n                self.model_info.training_info.end_time = datetime.now().isoformat()\n                if self.model_info.training_info.status_transitions is None:\n                    self.model_info.training_info.status_transitions = []\n                self.model_info.training_info.status_transitions.append(\n                    dict(\n                        status=status,\n                        timestamp=datetime.now().isoformat(),\n                        detail=f\"{str(exc_type.__name__)}:  {str(exc_value)}\",\n                    )\n                )\n\n        self.model_info.dump_json(os.path.join(self.output_dir, ArtifactName.INFO))\n        self._sync_train_job(\n            sync_info=HubSyncLocalTraining(\n                status=status,\n                iterations=self.iteration,\n                training_info=self.model_info.training_info,\n            ),\n            upload_artifacts=[\n                ArtifactName.WEIGHTS,\n                ArtifactName.LOGS,\n                ArtifactName.PT,\n                ArtifactName.ONNX,\n                ArtifactName.INFO,\n                ArtifactName.METRICS,\n            ],\n        )\n\n    def _sync_train_job(self, sync_info: HubSyncLocalTraining, upload_artifacts: Optional[List[ArtifactName]] = None):\n        try:\n            self.remote_model.sync_local_training_job(sync_info, self.output_dir, upload_artifacts)\n            # logger.debug(f\"Sync: {self.iteration} {self.model_info.name} ref: {self.model_info.ref}\")\n        except Exception as e:\n            logger.error(f\"[sync_train_job] failed to sync train job: {str(e)}\")\n</code></pre>"},{"location":"api/trainer_hooks/#focoos.trainer.hooks.sync_to_hub.SyncToHubHook.before_train","title":"<code>before_train()</code>","text":"<p>Called before the first iteration.</p> Source code in <code>focoos/trainer/hooks/sync_to_hub.py</code> <pre><code>def before_train(self):\n    \"\"\"\n    Called before the first iteration.\n    \"\"\"\n    info = HubSyncLocalTraining(\n        status=ModelStatus.TRAINING_RUNNING,  # type: ignore\n        iterations=0,\n        metrics=None,\n    )\n\n    self._sync_train_job(info)\n</code></pre>"},{"location":"development/changelog/","title":"Changelog","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"development/code_of_conduct/","title":"Code of conduct","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"development/contributing/","title":"Contribute","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"helpers/wip/","title":"Wip","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"hub/hub/","title":"\ud83d\ude80 Focoos HUB","text":"<p>The FocoosHUB class is your main interface for interacting with the Focoos cloud platform. It provides comprehensive functionality for managing models, datasets, and performing cloud operations.</p>"},{"location":"hub/hub/#getting-started","title":"Getting Started","text":""},{"location":"hub/hub/#authentication","title":"Authentication","text":"<p>Before using the HUB, you need to authenticate with your API key:</p> <pre><code>from focoos import FocoosHUB\n\n# Option 1: Use API key from configuration\nhub = FocoosHUB()\n\n# Option 2: Explicitly provide API key\nhub = FocoosHUB(api_key=\"your-api-key-here\")\n</code></pre>"},{"location":"hub/hub/#get-user-information","title":"Get User Information","text":"<p>Check your account details and quotas:</p> <pre><code>user_info = hub.get_user_info()\n\nprint(f\"Email: {user_info.email}\")\nprint(f\"Company: {user_info.company}\")\nprint(f\"Storage used: {user_info.quotas.used_storage_gb}GB\")\nprint(f\"Inferences used: {user_info.quotas.total_inferences}\")\n</code></pre>"},{"location":"hub/hub/#working-with-models","title":"Working with Models","text":""},{"location":"hub/hub/#list-your-models","title":"List Your Models","text":"<p>Get an overview of all models in your account:</p> <pre><code>models = hub.list_remote_models()\n\nfor model in models:\n    print(f\"Model: {model.name}\")\n    print(f\"  - Reference: {model.ref}\")\n    print(f\"  - Task: {model.task}\")\n    print(f\"  - Status: {model.status}\")\n    print(f\"  - Created: {model.created_at}\")\n</code></pre>"},{"location":"hub/hub/#get-model-information","title":"Get Model Information","text":"<p>Retrieve detailed information about a specific model:</p> <pre><code>model_ref = \"your-model-reference\"\nmodel_info = hub.get_model_info(model_ref)\n\nprint(f\"Model Name: {model_info.name}\")\nprint(f\"Description: {model_info.description}\")\nprint(f\"Task: {model_info.task}\")\nprint(f\"Classes: {model_info.classes}\")\nprint(f\"Image Size: {model_info.im_size}\")\nprint(f\"Status: {model_info.status}\")\n\n# Access training information if available\nif model_info.training:\n    print(f\"Training Status: {model_info.training.status}\")\n    print(f\"Training Progress: {model_info.training.progress}%\")\n</code></pre>"},{"location":"hub/hub/#get-remote-model-instance","title":"Get Remote Model Instance","text":"<p>Get a remote model instance for cloud-based operations:</p> <pre><code>model_ref = \"your-model-reference\"\nremote_model = hub.get_remote_model(model_ref)\n\n# This model can be used for remote inference\nresults = remote_model.infer(\"path/to/image.jpg\", threshold=0.5)\n\n# Get model training information\ntraining_info = remote_model.train_info()\nprint(f\"Training status: {training_info.status}\")\n\n# Get model metrics\nmetrics = remote_model.metrics()\nprint(f\"Validation mAP: {metrics.map}\")\n</code></pre>"},{"location":"hub/hub/#working-with-datasets","title":"Working with Datasets","text":""},{"location":"hub/hub/#list-available-datasets","title":"List Available Datasets","text":"<p>View datasets you own and optionally shared datasets:</p> <pre><code># List only your datasets\nmy_datasets = hub.list_remote_datasets()\n\n# List your datasets and shared datasets\nall_datasets = hub.list_remote_datasets(include_shared=True)\n\nfor dataset in all_datasets:\n    print(f\"Dataset: {dataset.name}\")\n    print(f\"  - Reference: {dataset.ref}\")\n    print(f\"  - Task: {dataset.task}\")\n    print(f\"  - Layout: {dataset.layout}\")\n    if dataset.spec:\n        print(f\"  - spec.train_length: {dataset.spec.train_length}\")\n        print(f\"  - spec.valid_length: {dataset.spec.valid_length}\")\n        print(f\"  - spec.size_mb: {dataset.spec.size_mb}\")\n</code></pre>"},{"location":"hub/hub/#working-with-remote-datasets","title":"Working with Remote Datasets","text":"<p>Get a remote dataset instance and work with it:</p> <pre><code>dataset_ref = \"your-dataset-reference\"\nremote_dataset = hub.get_remote_dataset(dataset_ref)\n\nprint(f\"Dataset Name: {remote_dataset.name}\")\nprint(f\"Task: {remote_dataset.task}\")\nprint(f\"Layout: {remote_dataset.layout}\")\n\n# Download dataset data\nlocal_path = remote_dataset.download_data(\"./datasets\")\nprint(f\"Dataset downloaded to: {local_path}\")\n</code></pre>"},{"location":"hub/hub/#error-handling","title":"Error Handling","text":"<p>The HUB client raises <code>ValueError</code> exceptions for API errors:</p> <pre><code>try:\n    model_info = hub.get_model_info(\"non-existent-model\")\nexcept ValueError as e:\n    print(f\"Error retrieving model: {e}\")\n\ntry:\n    models = hub.list_remote_models()\nexcept ValueError as e:\n    print(f\"Error listing models: {e}\")\n</code></pre>"},{"location":"hub/hub/#configuration","title":"Configuration","text":"<p>The HUB client uses configuration from the global <code>FOCOOS_CONFIG</code>:</p> <pre><code>from focoos.config import FOCOOS_CONFIG\n\n# Check current configuration\nprint(f\"API Key: {FOCOOS_CONFIG.focoos_api_key}\")\nprint(f\"Log Level: {FOCOOS_CONFIG.focoos_log_level}\")\nprint(f\"Runtime type: {FOCOOS_CONFIG.runtime_type}\")\nprint(f\"Warmup iter: {FOCOOS_CONFIG.warmup_iter}\")\n\n# The HUB client will use these values by default\nhub = FocoosHUB()  # Uses FOCOOS_CONFIG values\n</code></pre>"},{"location":"hub/hub/#advanced-usage","title":"Advanced Usage","text":""},{"location":"hub/hub/#model-training-integration","title":"Model Training Integration","text":"<p>When training models locally, you can sync them to the HUB:</p> <pre><code>from focoos.models.focoos_model import FocoosModel\nfrom focoos.ports import TrainerArgs\nfrom focoos import ModelManager\n\n# Load a model for training\nmodel = ModelManager.get(\"fai-detr-l-obj365\")\n\n# Configure training with HUB sync\ntrain_args = TrainerArgs(\n    max_iters=1000,\n    batch_size=16,\n    sync_to_hub=True  # This will automatically create a remote model\n)\n\n# Train the model (this will sync to HUB)\nmodel.train(train_args, train_dataset, val_dataset, hub=hub)\n</code></pre>"},{"location":"hub/hub/#monitoring-training","title":"Monitoring Training","text":"<p>Monitor training progress of remote models:</p> <pre><code>remote_model = hub.get_remote_model(\"training-model-ref\")\n\n# Get training info\ntraining_info = remote_model.train_info()\n\nif training_info:\n    print(f\"Algorithm Name: {training_info.algorithm_name}\")\n    print(f\"Instance Device: {training_info.instance_device}\")\n    print(f\"Instance Type: {training_info.instance_type}\")\n    print(f\"Volume Size: {training_info.volume_size}\")\n    print(f\"Main Status: {training_info.main_status}\")\n    print(f\"Failure Reason: {training_info.failure_reason}\")\n    print(f\"Status Transitions: {training_info.status_transitions}\")\n    print(f\"Start Time: {training_info.start_time}\")\n    print(f\"End Time: {training_info.end_time}\")\n    print(f\"Artifact Location: {training_info.artifact_location}\")\n\n# Get training logs\nlogs = remote_model.train_logs()\nfor log_entry in logs:\n    print(log_entry)\n</code></pre>"},{"location":"hub/hub/#see-also","title":"See Also","text":"<ul> <li>Remote Inference - Learn about cloud-based inference</li> <li>Overview - Understand the HUB architecture</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"hub/overview/","title":"\ud83d\ude80 Focoos HUB Overview","text":"<p>The Focoos HUB is a cloud-based platform that provides seamless integration between your local development environment and the Focoos AI ecosystem. It enables you to manage models, datasets, perform remote inference operations, and monitor training progress through a unified API.</p>"},{"location":"hub/overview/#what-is-focoos-hub","title":"What is Focoos HUB?","text":"<p>Focoos HUB serves as your gateway to:</p> <ul> <li>Model Management: Store, version, and share your trained computer vision models</li> <li>Remote Inference: Run inference on cloud infrastructure without local GPU requirements</li> <li>Dataset Management: Upload, download, and manage datasets in the cloud</li> <li>Collaboration: Share models and datasets with team members</li> <li>Monitoring: Track model performance and usage metrics</li> </ul>"},{"location":"hub/overview/#key-components","title":"Key Components","text":""},{"location":"hub/overview/#focooshub-client","title":"FocoosHUB Client","text":"<p>The main interface for interacting with the Focoos platform. It provides authentication and access to all HUB services.</p> <pre><code>from focoos import FocoosHUB\n\n# Initialize the HUB client\nhub = FocoosHUB()\n\n# Get user information\nuser_info = hub.get_user_info()\nprint(f\"Welcome {user_info.email}!\")\n</code></pre>"},{"location":"hub/overview/#remote-models","title":"Remote Models","text":"<p>Access and manage models stored in the cloud:</p> <ul> <li>List your available models</li> <li>Get detailed model information</li> <li>Download models for local use</li> <li>Perform remote inference without downloading</li> </ul>"},{"location":"hub/overview/#remote-datasets","title":"Remote Datasets","text":"<p>Manage datasets in the cloud:</p> <ul> <li>Upload local datasets to the cloud</li> <li>Download shared datasets</li> <li>Access dataset metadata and specifications</li> <li>List available datasets (owned and shared)</li> </ul>"},{"location":"hub/overview/#getting-started","title":"Getting Started","text":"<p>To start using Focoos HUB:</p> <ol> <li>Authentication: Set up your API key in the configuration</li> <li>Initialize: Create a FocoosHUB instance</li> <li>Explore: List your models and datasets</li> <li>Use: Run inference or manage your ML artifacts</li> </ol> <p>See the HUB section for detailed usage examples and the Remote Inference guide for cloud-based inference workflows.</p>"},{"location":"hub/overview/#benefits","title":"Benefits","text":"<ul> <li>Scalability: Access to cloud GPU resources for inference</li> <li>Collaboration: Easy sharing of models and datasets</li> <li>Cost Efficiency: Pay-per-use inference without maintaining infrastructure</li> <li>Integration: Seamless workflow between local development and cloud deployment</li> </ul>"},{"location":"hub/remote_inference/","title":"\ud83c\udf0d Remote Inference","text":"<p>Remote inference allows you to run computer vision models in the cloud without needing local GPU resources. This is perfect for production deployments, edge devices, or when you want to avoid the overhead of managing local model inference.</p>"},{"location":"hub/remote_inference/#what-is-remote-inference","title":"What is Remote Inference?","text":"<p>Remote inference uses the Focoos cloud infrastructure to run your models. Instead of loading models locally, you send images to the cloud API and receive inference results. This provides several advantages:</p> <ul> <li>No Local GPU Required: Run inference on any device, including CPU-only machines</li> <li>Scalability: Handle varying inference loads without managing infrastructure</li> <li>Always Updated: Use the latest version of your models automatically</li> <li>Cost Efficient: Pay per inference without maintaining dedicated hardware</li> <li>Low Latency: Optimized cloud infrastructure for fast inference</li> </ul>"},{"location":"hub/remote_inference/#getting-started","title":"Getting Started","text":""},{"location":"hub/remote_inference/#basic-remote-inference","title":"Basic Remote Inference","text":"<p>Here's how to perform remote inference with a model:</p> <pre><code>from focoos import FocoosHUB\n\n# Initialize the HUB client\nhub = FocoosHUB()\n\n# Get a remote model instance\nmodel_ref = \"fai-detr-l-obj365\"  # Use any available model\nremote_model = hub.get_remote_model(model_ref)\n\n# Perform inference\nresults = remote_model.infer(\"path/to/image.jpg\", threshold=0.5)\n\n# Process results\nfor detection in results.detections:\n    print(f\"Class ID: {detection.cls_id}\")\n    print(f\"Confidence: {detection.conf:.3f}\")\n    print(f\"Bounding Box: {detection.bbox}\")\n</code></pre>"},{"location":"hub/remote_inference/#using-the-callable-interface","title":"Using the Callable Interface","text":"<p>Remote models can also be called directly like functions:</p> <pre><code># This is equivalent to calling remote_model.infer()\nresults = remote_model(\"path/to/image.jpg\", threshold=0.5)\n</code></pre>"},{"location":"hub/remote_inference/#supported-input-types","title":"Supported Input Types","text":"<p>Remote inference accepts various input types:</p>"},{"location":"hub/remote_inference/#file-paths","title":"File Paths","text":"<pre><code>results = remote_model.infer(\"./images/photo.jpg\")\n</code></pre>"},{"location":"hub/remote_inference/#numpy-arrays","title":"NumPy Arrays","text":"<pre><code>import cv2\nimport numpy as np\n\n# Load image as numpy array\nimage = cv2.imread(\"photo.jpg\")\nresults = remote_model.infer(image, threshold=0.3)\n</code></pre>"},{"location":"hub/remote_inference/#pil-images","title":"PIL Images","text":"<pre><code>from PIL import Image\n\n# Load with PIL\npil_image = Image.open(\"photo.jpg\")\nresults = remote_model.infer(pil_image)\n</code></pre>"},{"location":"hub/remote_inference/#raw-bytes","title":"Raw Bytes","text":"<pre><code># Image as bytes\nwith open(\"photo.jpg\", \"rb\") as f:\n    image_bytes = f.read()\n\nresults = remote_model.infer(image_bytes)\n</code></pre>"},{"location":"hub/remote_inference/#inference-parameters","title":"Inference Parameters","text":""},{"location":"hub/remote_inference/#threshold-control","title":"Threshold Control","text":"<p>Control detection sensitivity with the threshold parameter:</p> <pre><code># High threshold - only very confident detections\nresults = remote_model.infer(\"image.jpg\", threshold=0.8)\n\n# Low threshold - more detections, potentially less accurate\nresults = remote_model.infer(\"image.jpg\", threshold=0.2)\n\n# Default threshold (usually 0.5)\nresults = remote_model.infer(\"image.jpg\")\n</code></pre>"},{"location":"hub/remote_inference/#working-with-results","title":"Working with Results","text":""},{"location":"hub/remote_inference/#detection-results","title":"Detection Results","text":"<p>For object detection models, results contain bounding boxes and classifications:</p> <pre><code>results = remote_model.infer(\"image.jpg\")\n\nprint(f\"Found {len(results.detections)} objects\")\n\nfor i, detection in enumerate(results.detections):\n    print(f\"Detection {i+1}:\")\n    print(f\"  Class ID: {detection.cls_id}\")\n    print(f\"  Confidence: {detection.conf:.3f}\")\n    print(f\"  Bounding Box: {detection.bbox}\")\n\n    # Box coordinates\n    if detection.bbox:\n        x1, y1, x2, y2 = detection.bbox[0], detection.bbox[1], detection.bbox[2], detection.bbox[3]\n    print(f\"  Coordinates: ({x1}, {y1}) to ({x2}, {y2})\")\n</code></pre>"},{"location":"hub/remote_inference/#visualization","title":"Visualization","text":"<p>Visualize results using the built-in utilities:</p> <pre><code>from focoos.utils.vision import annotate_image\n\nresults = model.infer(image=image, threshold=0.5)\n\nannotated_image = annotate_image(\n    im=image, detections=results, task=model.model_info.task, classes=model.model_info.classes\n)\n</code></pre>"},{"location":"hub/remote_inference/#model-management-for-remote-inference","title":"Model Management for Remote Inference","text":""},{"location":"hub/remote_inference/#checking-model-status","title":"Checking Model Status","text":"<p>Before using a model for inference, check its status:</p> <pre><code>model_info = remote_model.get_info()\n\nif model_info.status == ModelStatus.TRAINING_COMPLETED:\n    print(\"Model is ready for inference\")\n    results = remote_model.infer(\"image.jpg\")\nelif model_info.status == ModelStatus.TRAINING_RUNNING:\n    print(\"Model is still training\")\nelif model_info.status == ModelStatus.TRAINING_ERROR:\n    print(\"Model has an error\")\n</code></pre>"},{"location":"hub/remote_inference/#model-information","title":"Model Information","text":"<p>Get detailed information about the remote model:</p> <pre><code>model_info = remote_model.get_info()\n\nprint(f\"Model: {model_info.name}\")\nprint(f\"Task: {model_info.task}\")\nprint(f\"Classes: {model_info.classes}\")\nprint(f\"Image Size: {model_info.im_size}\")\nprint(f\"Status: {model_info.status}\")\n</code></pre>"},{"location":"hub/remote_inference/#comparison-remote-vs-local-inference","title":"Comparison: Remote vs Local Inference","text":"Aspect Remote Inference Local Inference Hardware No GPU required GPU recommended Setup Instant Model download required Scalability Automatic Manual scaling Cost Pay per use Infrastructure costs Latency Network dependent Very low Privacy Data sent to cloud Data stays local Offline Requires internet Works offline"},{"location":"hub/remote_inference/#best-practices","title":"Best Practices","text":"<ol> <li>Optimize Images: Resize large images to reduce upload time and costs</li> <li>Handle Errors: Implement retry logic for network issues</li> <li>Batch Smartly: Group related inferences to minimize overhead</li> <li>Monitor Usage: Track inference costs and quotas</li> <li>Cache Results: Store results for identical inputs when appropriate</li> <li>Use Appropriate Thresholds: Tune detection thresholds for your use case</li> </ol>"},{"location":"hub/remote_inference/#see-also","title":"See Also","text":"<ul> <li>HUB - Complete HUB documentation</li> <li>Overview - HUB architecture overview</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"models/bisenetformer/","title":"BisenetFormer (Bilateral Segmentation Network with Transformer)","text":""},{"location":"models/bisenetformer/#overview","title":"Overview","text":"<p>BisenetFormer is an advanced semantic segmentation model that combines the efficiency of BiSeNet (Bilateral Segmentation Network) with the power of transformer architectures. Developed by FocoosAI, this model is designed for real-time semantic segmentation tasks requiring both high accuracy and computational efficiency.</p> <p>The model employs a dual-path architecture where spatial details are preserved through one path while semantic information is processed through another, then fused with transformer-based attention mechanisms for superior segmentation performance.</p>"},{"location":"models/bisenetformer/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The BisenetFormer architecture consists of four main components working in concert:</p>"},{"location":"models/bisenetformer/#backbone","title":"Backbone","text":"<ul> <li>Purpose: Feature extraction from input images</li> <li>Design: Configurable backbone network (e.g., ResNet, STDC)</li> <li>Output: Multi-scale features at different resolutions (1/4, 1/8, 1/16, 1/32)</li> </ul>"},{"location":"models/bisenetformer/#context-path","title":"Context Path","text":"<ul> <li>Component: Global context extraction path</li> <li>Features:</li> <li>Attention Refinement Module (ARM) for feature enhancement</li> <li>Global Average Pooling for context aggregation</li> <li>Multi-scale feature fusion with upsampling</li> <li>Purpose: Captures high-level semantic information</li> </ul>"},{"location":"models/bisenetformer/#spatial-path-detail-branch","title":"Spatial Path (Detail Branch)","text":"<ul> <li>Component: Spatial detail preservation path</li> <li>Features:</li> <li>Bilateral structure maintaining spatial resolution</li> <li>ConvBNReLU blocks for efficient processing</li> <li>Feature Fusion Module (FFM) for combining paths</li> <li>Purpose: Preserves fine-grained spatial details</li> </ul>"},{"location":"models/bisenetformer/#transformer-decoder","title":"Transformer Decoder","text":"<ul> <li>Design: Lightweight transformer decoder with attention mechanisms</li> <li>Components:</li> <li>Self-attention layers for feature refinement</li> <li>Cross-attention layers for multi-scale feature integration</li> <li>Feed-forward networks (FFN) for feature transformation</li> <li>100 learnable object queries</li> <li>Layers: Configurable number of decoder layers (default: 6)</li> </ul>"},{"location":"models/bisenetformer/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"models/bisenetformer/#core-model-parameters","title":"Core Model Parameters","text":"<ul> <li><code>num_classes</code> (int): Number of segmentation classes</li> <li><code>num_queries</code> (int, default=100): Number of learnable object queries</li> <li><code>backbone_config</code> (BackboneConfig): Backbone network configuration</li> </ul>"},{"location":"models/bisenetformer/#image-preprocessing","title":"Image Preprocessing","text":"<ul> <li><code>pixel_mean</code> (List[float]): RGB normalization means [123.675, 116.28, 103.53]</li> <li><code>pixel_std</code> (List[float]): RGB normalization standard deviations [58.395, 57.12, 57.375]</li> <li><code>size_divisibility</code> (int, default=0): Input size divisibility constraint</li> </ul>"},{"location":"models/bisenetformer/#architecture-dimensions","title":"Architecture Dimensions","text":"<ul> <li><code>pixel_decoder_out_dim</code> (int, default=256): Pixel decoder output channels</li> <li><code>pixel_decoder_feat_dim</code> (int, default=256): Pixel decoder feature channels</li> <li><code>transformer_predictor_hidden_dim</code> (int, default=256): Transformer hidden dimension</li> <li><code>transformer_predictor_dec_layers</code> (int, default=6): Number of decoder layers</li> <li><code>transformer_predictor_dim_feedforward</code> (int, default=1024): FFN dimension</li> <li><code>head_out_dim</code> (int, default=256): Prediction head output dimension</li> </ul>"},{"location":"models/bisenetformer/#inference-configuration","title":"Inference Configuration","text":"<ul> <li><code>postprocessing_type</code> (str): Either \"semantic\" or \"instance\" segmentation</li> <li><code>mask_threshold</code> (float, default=0.5): Binary mask threshold</li> <li><code>threshold</code> (float, default=0.5): Confidence threshold for detections</li> <li><code>top_k</code> (int, default=300): Maximum number of detections to return</li> <li><code>use_mask_score</code> (bool, default=False): Whether to use mask quality scores</li> <li><code>predict_all_pixels</code> (bool, default=False): Predict class for every pixel</li> <li><code>cls_sigmoid</code> (bool, default=False): Use sigmoid activation for classification</li> </ul>"},{"location":"models/bisenetformer/#loss-configuration","title":"Loss Configuration","text":"<ul> <li><code>criterion_deep_supervision</code> (bool, default=True): Enable deep supervision</li> <li><code>criterion_eos_coef</code> (float, default=0.1): End-of-sequence coefficient</li> <li><code>criterion_num_points</code> (int, default=12544): Number of sampling points</li> <li><code>weight_dict_loss_ce</code> (int, default=2): Cross-entropy loss weight</li> <li><code>weight_dict_loss_mask</code> (int, default=5): Mask loss weight</li> <li><code>weight_dict_loss_dice</code> (int, default=5): Dice loss weight</li> </ul>"},{"location":"models/bisenetformer/#hungarian-matcher-configuration","title":"Hungarian Matcher Configuration","text":"<ul> <li><code>matcher_cost_class</code> (int, default=2): Classification cost for matching</li> <li><code>matcher_cost_mask</code> (int, default=5): Mask cost for matching</li> <li><code>matcher_cost_dice</code> (int, default=5): Dice cost for matching</li> </ul>"},{"location":"models/bisenetformer/#supported-tasks","title":"Supported Tasks","text":""},{"location":"models/bisenetformer/#semantic-segmentation","title":"Semantic Segmentation","text":"<ul> <li>Output: Dense pixel-wise class predictions</li> <li>Use Cases: Scene understanding, autonomous driving, medical imaging</li> <li>Configuration: Set <code>postprocessing_type=\"semantic\"</code></li> </ul>"},{"location":"models/bisenetformer/#instance-segmentation","title":"Instance Segmentation","text":"<ul> <li>Output: Individual object instances with masks and bounding boxes</li> <li>Use Cases: Object detection and counting, robotics applications</li> <li>Configuration: Set <code>postprocessing_type=\"instance\"</code></li> </ul>"},{"location":"models/bisenetformer/#model-outputs","title":"Model Outputs","text":""},{"location":"models/bisenetformer/#training-output-bisenetformeroutput","title":"Training Output (<code>BisenetFormerOutput</code>)","text":"<ul> <li><code>masks</code> (torch.Tensor): Shape [B, num_queries, H, W] - Query mask predictions</li> <li><code>logits</code> (torch.Tensor): Shape [B, num_queries, num_classes] - Class predictions</li> <li><code>loss</code> (Optional[dict]): Training losses including:</li> <li><code>loss_ce</code>: Cross-entropy classification loss</li> <li><code>loss_mask</code>: Binary cross-entropy mask loss</li> <li><code>loss_dice</code>: Dice coefficient loss</li> </ul>"},{"location":"models/bisenetformer/#inference-output-focoosdetections","title":"Inference Output (<code>FocoosDetections</code>)","text":"<p>For each detected object: - <code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2] - <code>conf</code> (float): Confidence score - <code>cls_id</code> (int): Class identifier - <code>mask</code> (str): Base64-encoded binary mask - <code>label</code> (Optional[str]): Human-readable class name</p>"},{"location":"models/bisenetformer/#key-features","title":"Key Features","text":""},{"location":"models/bisenetformer/#efficiency-optimizations","title":"Efficiency Optimizations","text":"<ul> <li>Bilateral Architecture: Separate paths for spatial details and semantic context</li> <li>Attention Refinement: ARM modules enhance feature quality without computational overhead</li> <li>Lightweight Transformer: Reduced decoder complexity for faster inference</li> </ul>"},{"location":"models/bisenetformer/#performance-advantages","title":"Performance Advantages","text":"<ul> <li>Real-time Capable: Optimized for efficient inference</li> <li>Multi-scale Processing: Leverages features at multiple resolutions</li> <li>Context Preservation: Global context path maintains semantic understanding</li> <li>Detail Retention: Spatial path preserves fine-grained details</li> </ul>"},{"location":"models/bisenetformer/#training-features","title":"Training Features","text":"<ul> <li>Deep Supervision: Auxiliary losses at multiple decoder layers</li> <li>Hungarian Matching: Optimal assignment between predictions and ground truth</li> <li>Flexible Loss Functions: Combines classification, mask, and shape-aware losses</li> </ul>"},{"location":"models/bisenetformer/#architecture-innovations","title":"Architecture Innovations","text":"<p>The BisenetFormer introduces several key innovations:</p> <ol> <li>Hybrid Architecture: Combines the efficiency of bilateral networks with transformer attention</li> <li>Feature Fusion Module: Intelligent fusion of spatial and context paths</li> <li>Attention Refinement: ARM modules refine features at multiple scales</li> <li>Query-based Segmentation: Transformer queries enable instance-aware segmentation</li> </ol> <p>This architecture achieves an optimal balance between accuracy and efficiency, making it suitable for both research and production deployments requiring real-time semantic segmentation capabilities.</p>"},{"location":"models/bisenetformer/#available-models","title":"Available Models","text":"<p>Currently, you can find 5 fai-mf models on the Focoos Hub, 2 for semantic segmentation and 3 for instance-segmentation.</p>"},{"location":"models/bisenetformer/#semantic-segmentation-models","title":"Semantic Segmentation Models","text":"Model Name Architecture Dataset Metric FPS Nvidia-T4 fai-mf-l-ade Mask2Former (Resnet-101) ADE20K mIoU: 48.27mAcc: 62.15 73 fai-mf-m-ade Mask2Former (STDC-2) ADE20K mIoU: 45.32mACC: 57.75 127"},{"location":"models/bisenetformer/#instance-segmentation-models","title":"Instance Segmentation Models","text":"Model Name Architecture Dataset Metric FPS Nvidia-T4 fai-m2f-s-coco-ins Mask2Former (Resnet-50) COCO segm/AP: 41.45segm/AP50: 64.12 86 fai-m2f-m-coco-ins Mask2Former (Resnet-101) COCO segm/AP: 43.09segm/AP50: 65.87 70 fai-m2f-l-coco-ins Mask2Former (Resnet-101) COCO segm/AP: 44.23segm/AP50: 67.53 55"},{"location":"models/fai-cls/","title":"FAI-CLS (FocoosAI Classification)","text":""},{"location":"models/fai-cls/#overview","title":"Overview","text":"<p>FAI-CLS is a versatile image classification model developed by FocoosAI that can utilize any backbone architecture for feature extraction. This model is designed for both single-label and multi-label image classification tasks, offering flexibility in architecture choices and training configurations.</p> <p>The model employs a simple yet effective approach: a configurable backbone extracts features from input images, followed by a classification head that produces class predictions. This design enables easy adaptation to different domains and datasets while maintaining high performance and computational efficiency.</p>"},{"location":"models/fai-cls/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The FAI-CLS architecture consists of two main components:</p>"},{"location":"models/fai-cls/#backbone","title":"Backbone","text":"<ul> <li>Purpose: Feature extraction from input images</li> <li>Design: Configurable backbone network (ResNet, EfficientNet, STDC, etc.)</li> <li>Output: High-level feature representations</li> <li>Feature Selection: Uses specified feature level (default: \"res5\" for highest-level features)</li> <li>Flexibility: Supports any backbone that provides the required output shape</li> </ul>"},{"location":"models/fai-cls/#classification-head","title":"Classification Head","text":"<ul> <li>Architecture: Multi-layer perceptron (MLP) with configurable depth</li> <li>Components:</li> <li>Global Average Pooling (AdaptiveAvgPool2d) for spatial dimension reduction</li> <li>Flatten layer to convert 2D features to 1D</li> <li>Linear layers with ReLU activation</li> <li>Dropout for regularization</li> <li>Final linear layer for class predictions</li> <li>Configurations:</li> <li>Single Layer: Direct mapping from features to classes</li> <li>Two Layer: Hidden layer with ReLU and dropout for better feature transformation</li> </ul>"},{"location":"models/fai-cls/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"models/fai-cls/#core-model-parameters","title":"Core Model Parameters","text":"<ul> <li><code>num_classes</code> (int): Number of classification classes</li> <li><code>backbone_config</code> (BackboneConfig): Backbone network configuration</li> <li><code>resolution</code> (int, default=224): Input image resolution</li> </ul>"},{"location":"models/fai-cls/#image-preprocessing","title":"Image Preprocessing","text":"<ul> <li><code>pixel_mean</code> (List[float]): RGB normalization means [123.675, 116.28, 103.53]</li> <li><code>pixel_std</code> (List[float]): RGB normalization standard deviations [58.395, 57.12, 57.375]</li> </ul>"},{"location":"models/fai-cls/#architecture-configuration","title":"Architecture Configuration","text":"<ul> <li><code>hidden_dim</code> (int, default=512): Hidden layer dimension for two-layer classifier</li> <li><code>dropout_rate</code> (float, default=0.2): Dropout probability for regularization</li> <li><code>features</code> (str, default=\"res5\"): Feature level to extract from backbone</li> <li><code>num_layers</code> (int, default=2): Number of classification layers (1 or 2)</li> </ul>"},{"location":"models/fai-cls/#loss-configuration","title":"Loss Configuration","text":"<ul> <li><code>use_focal_loss</code> (bool, default=False): Use focal loss instead of cross-entropy</li> <li><code>focal_alpha</code> (float, default=0.75): Alpha parameter for focal loss</li> <li><code>focal_gamma</code> (float, default=2.0): Gamma parameter for focal loss</li> <li><code>label_smoothing</code> (float, default=0.0): Label smoothing factor</li> <li><code>multi_label</code> (bool, default=False): Enable multi-label classification</li> </ul>"},{"location":"models/fai-cls/#supported-tasks","title":"Supported Tasks","text":""},{"location":"models/fai-cls/#single-label-classification","title":"Single-Label Classification","text":"<ul> <li>Output: Single class prediction per image</li> <li>Use Cases:</li> <li>Image categorization (animals, objects, scenes)</li> <li>Medical image diagnosis</li> <li>Quality control in manufacturing</li> <li>Content moderation</li> <li>Agricultural crop classification</li> <li>Loss: Cross-entropy or focal loss</li> <li>Configuration: Set <code>multi_label=False</code></li> </ul>"},{"location":"models/fai-cls/#multi-label-classification","title":"Multi-Label Classification","text":"<ul> <li>Output: Multiple class predictions per image</li> <li>Use Cases:</li> <li>Multi-object recognition</li> <li>Image tagging and annotation</li> <li>Scene attribute recognition</li> <li>Medical condition classification</li> <li>Content-based image retrieval</li> <li>Loss: Binary cross-entropy with logits</li> <li>Configuration: Set <code>multi_label=True</code></li> </ul>"},{"location":"models/fai-cls/#model-outputs","title":"Model Outputs","text":""},{"location":"models/fai-cls/#training-output-classificationmodeloutput","title":"Training Output (<code>ClassificationModelOutput</code>)","text":"<ul> <li><code>logits</code> (torch.Tensor): Shape [B, num_classes] - Raw class predictions</li> <li><code>loss</code> (Optional[dict]): Training loss including:</li> <li><code>loss_cls</code>: Classification loss (cross-entropy, focal, or BCE)</li> </ul>"},{"location":"models/fai-cls/#inference-output","title":"Inference Output","text":"<ul> <li>Single-Label: Class probabilities after softmax activation</li> <li>Multi-Label: Class probabilities after sigmoid activation</li> <li>Post-Processing: Can be processed into <code>FocoosDetections</code> format with confidence scores</li> </ul>"},{"location":"models/fai-cls/#key-features","title":"Key Features","text":""},{"location":"models/fai-cls/#flexibility","title":"Flexibility","text":"<ul> <li>Backbone Agnostic: Compatible with any feature extraction backbone</li> <li>Configurable Depth: Choose between 1 or 2-layer classification heads</li> <li>Multi-Task Ready: Supports both single-label and multi-label scenarios</li> <li>Resolution Adaptive: Configurable input resolution for different use cases</li> </ul>"},{"location":"models/fai-cls/#training-features","title":"Training Features","text":"<ul> <li>Advanced Loss Functions: Focal loss for handling class imbalance</li> <li>Label Smoothing: Reduces overfitting and improves generalization</li> <li>Dropout Regularization: Prevents overfitting in the classification head</li> <li>Multi-Label Support: Binary cross-entropy for multi-label scenarios</li> </ul>"},{"location":"models/fai-cls/#performance-optimizations","title":"Performance Optimizations","text":"<ul> <li>Efficient Head Design: Lightweight classification layers</li> <li>Global Average Pooling: Reduces spatial dimensions efficiently</li> <li>Proper Initialization: Truncated normal initialization for better training</li> </ul>"},{"location":"models/fai-cls/#loss-functions","title":"Loss Functions","text":"<p>The model supports multiple loss function configurations:</p>"},{"location":"models/fai-cls/#cross-entropy-loss-default","title":"Cross-Entropy Loss (Default)","text":"<ul> <li>Use Case: Standard single-label classification</li> <li>Features: Optional label smoothing for better generalization</li> <li>Activation: Softmax for probability distribution</li> </ul>"},{"location":"models/fai-cls/#focal-loss","title":"Focal Loss","text":"<ul> <li>Use Case: Imbalanced datasets with hard-to-classify examples</li> <li>Parameters:</li> <li>Alpha (\u03b1): Controls importance of rare class</li> <li>Gamma (\u03b3): Focuses learning on hard examples</li> <li>Benefits: Improved performance on imbalanced datasets</li> </ul>"},{"location":"models/fai-cls/#binary-cross-entropy-loss","title":"Binary Cross-Entropy Loss","text":"<ul> <li>Use Case: Multi-label classification tasks</li> <li>Features: Independent probability for each class</li> <li>Activation: Sigmoid for per-class probabilities</li> </ul>"},{"location":"models/fai-cls/#architecture-variants","title":"Architecture Variants","text":""},{"location":"models/fai-cls/#single-layer-classifier","title":"Single-Layer Classifier","text":"<p><pre><code>AdaptiveAvgPool2d(1) \u2192 Flatten \u2192 Dropout \u2192 Linear(features \u2192 num_classes)\n</code></pre> - Benefits: Faster inference, fewer parameters - Use Case: Simple datasets or when computational efficiency is critical</p>"},{"location":"models/fai-cls/#two-layer-classifier","title":"Two-Layer Classifier","text":"<p><pre><code>AdaptiveAvgPool2d(1) \u2192 Flatten \u2192 Linear(features \u2192 hidden_dim) \u2192 ReLU \u2192 Dropout \u2192 Linear(hidden_dim \u2192 num_classes)\n</code></pre> - Benefits: Better feature transformation, improved accuracy - Use Case: Complex datasets requiring more sophisticated feature processing</p>"},{"location":"models/fai-cls/#training-strategies","title":"Training Strategies","text":""},{"location":"models/fai-cls/#standard-training","title":"Standard Training","text":"<ul> <li>Use cross-entropy loss with appropriate learning rate scheduling</li> <li>Apply data augmentation for better generalization</li> <li>Monitor validation accuracy for early stopping</li> </ul>"},{"location":"models/fai-cls/#imbalanced-data","title":"Imbalanced Data","text":"<ul> <li>Enable focal loss with appropriate \u03b1 and \u03b3 parameters</li> <li>Consider class weighting strategies</li> <li>Use stratified sampling for validation</li> </ul>"},{"location":"models/fai-cls/#multi-label-scenarios","title":"Multi-Label Scenarios","text":"<ul> <li>Set <code>multi_label=True</code> in configuration</li> <li>Use appropriate evaluation metrics (F1-score, mAP)</li> <li>Consider threshold optimization for final predictions</li> </ul> <p>This flexible architecture makes FAI-CLS suitable for a wide range of image classification applications, from simple binary classification to complex multi-label scenarios, while maintaining computational efficiency and ease of use.</p>"},{"location":"models/fai-detr-l-coco/","title":"fai-detr-l-coco","text":""},{"location":"models/fai-detr-l-coco/#overview","title":"Overview","text":"<p>The models is the reimplementation of the RT-DETR model by FocoosAI for the COCO dataset. It is a object detection model able to detect 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-detr-l-coco/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-detr-l-coco/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-detr-l-coco/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>This implementation is a reimplementation of the RT-DETR model by FocoosAI. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is a Resnet-50,that guarantees a good performance while having good efficiency.</li> <li>the encoder is the Hybrid Encoder, as proposed by the paper, and it is a bi-FPN (bilinear feature pyramid network) that includes a transformer encoder on the smaller feature resolution for improving efficiency.</li> <li>The query selection mechanism select the features of the pixels (aka queries) with the highest probability of containing an object and pass them to a transformer decoder head that will generate the final detection output. In this implementation, we select 300 queries and use 6 transformer decoder layers.</li> </ul>"},{"location":"models/fai-detr-l-coco/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. For more details look at GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-detr-l-coco/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-detr-l-coco/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class AP 1 person 63.2 2 bicycle 40.5 3 car 52.3 4 motorcycle 55.0 5 airplane 76.3 6 bus 74.9 7 train 75.0 8 truck 47.9 9 boat 36.6 10 traffic light 32.6 11 fire hydrant 75.5 12 stop sign 71.2 13 parking meter 54.6 14 bench 34.9 15 bird 46.6 16 cat 79.8 17 dog 75.4 18 horse 69.7 19 sheep 63.0 20 cow 68.8 21 elephant 74.1 22 bear 83.2 23 zebra 78.3 24 giraffe 76.9 25 backpack 25.1 26 umbrella 53.8 27 handbag 24.3 28 tie 44.8 29 suitcase 52.6 30 frisbee 75.3 31 skis 37.2 32 snowboard 50.8 33 sports ball 53.9 34 kite 54.8 35 baseball bat 53.2 36 baseball glove 45.3 37 skateboard 63.7 38 surfboard 50.3 39 tennis racket 61.1 40 bottle 48.8 41 wine glass 44.1 42 cup 53.4 43 fork 51.3 44 knife 34.1 45 spoon 33.5 46 bowl 52.1 47 banana 33.0 48 apple 27.1 49 sandwich 48.1 50 orange 37.9 51 broccoli 28.9 52 carrot 28.2 53 hot dog 50.3 54 pizza 62.5 55 donut 62.3 56 cake 47.5 57 chair 41.2 58 couch 57.3 59 potted plant 36.0 60 bed 58.5 61 dining table 39.4 62 toilet 72.6 63 tv 65.8 64 laptop 73.1 65 mouse 67.1 66 remote 48.2 67 keyboard 63.0 68 cell phone 46.0 69 microwave 64.6 70 oven 44.9 71 toaster 50.4 72 sink 45.7 73 refrigerator 69.4 74 book 22.3 75 clock 59.2 76 vase 45.7 77 scissors 42.4 78 teddy bear 59.5 79 hair drier 35.1 80 tootdbrush 42.0"},{"location":"models/fai-detr-l-coco/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-detr-l-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-detr-l-coco\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-detr-l-obj365/","title":"fai-detr-l-obj365","text":""},{"location":"models/fai-detr-l-obj365/#overview","title":"Overview","text":"<p>The models is a RT-DETR model otimized by FocoosAI for the Objects365. It is a object detection model able to detect 365 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-detr-l-obj365/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-detr-l-obj365/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>This implementation is a reimplementation of the RT-DETR model by FocoosAI. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is a Resnet-50,that guarantees a good performance while having good efficiency.</li> <li>the encoder is the Hybrid Encoder, as proposed by the paper, and it is a bi-FPN (bilinear feature pyramid network) that includes a transformer encoder on the smaller feature resolution for improving efficiency.</li> <li>The query selection mechanism select the features of the pixels (aka queries) with the highest probability of containing an object and pass them to a transformer decoder head that will generate the final detection output. In this implementation, we select 300 queries and use 6 transformer decoder layers.</li> </ul>"},{"location":"models/fai-detr-l-obj365/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. For more details look at GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-detr-l-obj365/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-detr-l-obj365/#classes","title":"Classes","text":"<p>The model is pretrained on the Objects365 with 365 classes.</p> Class ID Class Name AP 1 Person 68.8 2 Sneakers 47.3 3 Chair 52.7 4 Other Shoes 14.8 5 Hat 56.4 6 Car 37.2 7 Lamp 43.7 8 Glasses 47.8 9 Bottle 40.6 10 Desk 48.2 11 Cup 53.9 12 Street Lights 38.9 13 Cabinet/shelf 46.8 14 Handbag/Satchel 29.5 15 Bracelet 27.8 16 Plate 70.5 17 Picture/Frame 66.0 18 Helmet 47.7 19 Book 23.0 20 Gloves 42.9 21 Storage box 24.1 22 Boat 31.1 23 Leather Shoes 29.3 24 Flower 39.6 25 Bench 26.6 26 Potted Plant 42.4 27 Bowl/Basin 57.7 28 Flag 38.6 29 Pillow 56.0 30 Boots 39.8 31 Vase 37.1 32 Microphone 39.1 33 Necklace 30.3 34 Ring 19.5 35 SUV 33.4 36 Wine Glass 68.6 37 Belt 35.8 38 Moniter/TV 75.1 39 Backpack 29.9 40 Umbrella 37.4 41 Traffic Light 39.0 42 Speaker 58.3 43 Watch 47.1 44 Tie 37.0 45 Trash bin Can 49.1 46 Slippers 43.9 47 Bicycle 46.8 48 Stool 48.9 49 Barrel/bucket 39.9 50 Van 30.7 51 Couch 62.5 52 Sandals 43.7 53 Bakset 40.1 54 Drum 54.8 55 Pen/Pencil 29.9 56 Bus 45.3 57 Wild Bird 12.6 58 High Heels 41.9 59 Motorcycle 31.8 60 Guitar 64.7 61 Carpet 59.8 62 Cell Phone 43.8 63 Bread 23.6 64 Camera 32.8 65 Canned 37.4 66 Truck 23.0 67 Traffic cone 44.5 68 Cymbal 55.7 69 Lifesaver 31.2 70 Towel 54.0 71 Stuffed Toy 40.7 72 Candle 30.5 73 Sailboat 55.5 74 Laptop 73.8 75 Awning 27.4 76 Bed 66.1 77 Faucet 41.8 78 Tent 30.8 79 Horse 46.3 80 Mirror 59.0 81 Power outlet 43.1 82 Sink 53.3 83 Apple 22.0 84 Air Conditioner 30.1 85 Knife 46.1 86 Hockey Stick 59.0 87 Paddle 25.5 88 Pickup Truck 45.2 89 Fork 57.7 90 Traffic Sign 30.0 91 Ballon 47.7 92 Tripod 29.7 93 Dog 58.5 94 Spoon 46.7 95 Clock 61.6 96 Pot 44.3 97 Cow 18.7 98 Cake 15.7 99 Dinning Table 46.2 100 Sheep 30.5 101 Hanger 9.9 102 Blackboard/Whiteboard 47.5 103 Napkin 35.5 104 Other Fish 30.4 105 Orange/Tangerine 10.8 106 Toiletry 30.1 107 Keyboard 71.8 108 Tomato 36.4 109 Lantern 47.7 110 Machinery Vehicle 30.5 111 Fan 49.4 112 Green Vegetables 13.2 113 Banana 30.2 114 Baseball Glove 38.8 115 Airplane 60.9 116 Mouse 61.5 117 Train 50.5 118 Pumpkin 53.4 119 Soccer 29.0 120 Skiboard 24.3 121 Luggage 32.5 122 Nightstand 62.3 123 Tea pot 31.7 124 Telephone 45.8 125 Trolley 36.8 126 Head Phone 40.9 127 Sports Car 67.8 128 Stop Sign 49.3 129 Dessert 28.7 130 Scooter 35.5 131 Stroller 42.7 132 Crane 46.0 133 Remote 47.1 134 Refrigerator 70.2 135 Oven 51.9 136 Lemon 33.4 137 Duck 43.3 138 Baseball Bat 40.4 139 Surveillance Camera 24.2 140 Cat 67.5 141 Jug 24.1 142 Broccoli 29.3 143 Piano 41.5 144 Pizza 50.9 145 Elephant 66.9 146 Skateboard 19.0 147 Surfboard 44.0 148 Gun 23.8 149 Skating and Skiing shoes 64.6 150 Gas stove 39.2 151 Donut 45.0 152 Bow Tie 28.8 153 Carrot 15.6 154 Toilet 73.2 155 Kite 44.1 156 Strawberry 24.2 157 Other Balls 36.1 158 Shovel 18.9 159 Pepper 18.5 160 Computer Box 49.3 161 Toilet Paper 39.0 162 Cleaning Products 21.1 163 Chopsticks 40.3 164 Microwave 68.0 165 Pigeon 48.1 166 Baseball 32.6 167 Cutting/chopping Board 40.9 168 Coffee Table 49.8 169 Side Table 34.6 170 Scissors 28.1 171 Marker 20.6 172 Pie 20.8 173 Ladder 34.4 174 Snowboard 36.8 175 Cookies 13.0 176 Radiator 50.3 177 Fire Hydrant 47.6 178 Basketball 32.9 179 Zebra 58.3 180 Grape 10.4 181 Giraffe 64.0 182 Potato 11.0 183 Sausage 25.1 184 Tricycle 27.4 185 Violin 39.6 186 Egg 34.7 187 Fire Extinguisher 47.3 188 Candy 1.9 189 Fire Truck 55.7 190 Billards 63.0 191 Converter 18.0 192 Bathtub 58.8 193 Wheelchair 62.9 194 Golf Club 33.8 195 Briefcase 35.1 196 Cucumber 26.6 197 Cigar/Cigarette 11.2 198 Paint Brush 15.0 199 Pear 5.5 200 Heavy Truck 35.3 201 Hamburger 40.5 202 Extractor 62.7 203 Extention Cord 19.1 204 Tong 16.4 205 Tennis Racket 51.4 206 Folder 8.8 207 American Football 22.6 208 earphone 7.6 209 Mask 36.9 210 Kettle 43.9 211 Tennis 37.3 212 Ship 49.0 213 Swing 48.4 214 Coffee Machine 50.3 215 Slide 46.7 216 Carriage 59.5 217 Onion 7.4 218 Green beans 3.6 219 Projector 48.6 220 Frisbee 33.0 221 Washing Machine/Drying Machine 53.6 222 Chicken 49.5 223 Printer 54.8 224 Watermelon 28.7 225 Saxophone 52.2 226 Tissue 31.6 227 Toothbrush 23.6 228 Ice cream 27.6 229 Hotair ballon 77.2 230 Cello 45.9 231 French Fries 42.7 232 Scale 28.3 233 Trophy 37.6 234 Cabbage 11.9 235 Hot dog 39.9 236 Blender 44.3 237 Peach 6.2 238 Rice 44.3 239 Wallet/Purse 30.4 240 Volleyball 51.2 241 Deer 45.0 242 Goose 17.5 243 Tape 24.0 244 Tablet 39.9 245 Cosmetics 18.8 246 Trumpet 36.7 247 Pineapple 19.1 248 Golf Ball 39.5 249 Ambulance 78.2 250 Parking meter 33.8 251 Mango 0.8 252 Key 3.0 253 Hurdle 33.9 254 Fishing Rod 29.1 255 Medal 22.8 256 Flute 31.9 257 Brush 8.2 258 Penguin 57.5 259 Megaphone 18.8 260 Corn 22.4 261 Lettuce 2.3 262 Garlic 16.8 263 Swan 46.1 264 Helicopter 42.7 265 Green Onion 0.5 266 Sandwich 29.5 267 Nuts 0.6 268 Speed Limit Sign 43.4 269 Induction Cooker 27.3 270 Broom 19.6 271 Trombone 33.0 272 Plum 3.7 273 Rickshaw 24.7 274 Goldfish 14.3 275 Kiwi fruit 8.3 276 Router/modem 14.5 277 Poker Card 31.2 278 Toaster 48.6 279 Shrimp 5.0 280 Sushi 22.3 281 Cheese 21.2 282 Notepaper 9.3 283 Cherry 4.3 284 Pliers 12.2 285 CD 21.1 286 Pasta 36.3 287 Hammer 20.6 288 Cue 46.2 289 Avocado 9.8 290 Hamimelon 2.9 291 Flask 31.2 292 Mushroon 3.1 293 Screwdriver 13.6 294 Soap 19.0 295 Recorder 32.9 296 Bear 46.2 297 Eggplant 8.8 298 Board Eraser 38.7 299 Coconut 10.0 300 Tape Measur/ Ruler 15.0 301 Pig 50.0 302 Showerhead 11.6 303 Globe 57.1 304 Chips 13.7 305 Steak 28.2 306 Crosswalk Sign 48.2 307 Stapler 27.2 308 Campel 55.3 309 Formula 1 59.1 310 Pomegranate 3.9 311 Dishwasher 53.1 312 Crab 11.6 313 Hoverboard 29.9 314 Meat ball 7.6 315 Rice Cooker 30.5 316 Tuba 24.6 317 Calculator 38.0 318 Papaya 4.3 319 Antelope 24.4 320 Parrot 34.1 321 Seal 41.1 322 Buttefly 36.4 323 Dumbbell 8.0 324 Donkey 42.0 325 Lion 33.8 326 Urinal 53.0 327 Dolphin 39.5 328 Electric Drill 24.1 329 Hair Dryer 10.0 330 Egg tart 5.1 331 Jellyfish 40.2 332 Treadmill 43.9 333 Lighter 12.6 334 Grapefruit 1.2 335 Game board 37.3 336 Mop 5.8 337 Radish 0.6 338 Baozi 40.2 339 Target 14.6 340 French 27.3 341 Spring Rolls 29.1 342 Monkey 37.8 343 Rabbit 36.4 344 Pencil Case 22.8 345 Yak 37.7 346 Red Cabbage 7.1 347 Binoculars 15.7 348 Asparagus 4.1 349 Barbell 17.1 350 Scallop 12.4 351 Noddles 21.1 352 Comb 14.1 353 Dumpling 5.2 354 Oyster 17.6 355 Table Teniis paddle 22.2 356 Cosmetics Brush/Eyeliner Pencil 40.3 357 Chainsaw 13.8 358 Eraser 16.4 359 Lobster 18.0 360 Durian 33.7 361 Okra 0.1 362 Lipstick 36.3 363 Cosmetics Mirror 8.2 364 Curling 44.7 365 Table Tennis 25.1"},{"location":"models/fai-detr-l-obj365/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-detr-s-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-detr-l-obj365\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-detr-m-coco/","title":"fai-detr-m-coco","text":""},{"location":"models/fai-detr-m-coco/#overview","title":"Overview","text":"<p>The models is a RT-DETR model otimized by FocoosAI for the COCO dataset. It is a object detection model able to detect 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-detr-m-coco/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-detr-m-coco/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-detr-m-coco/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The RT-DETR FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-2 that show an amazing trade-off between performance and efficiency.</li> <li>the encoder is a bi-FPN (bilinear feature pyramid network). With respect to the original paper, we removed the attention modules in the encoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 3 decoder layers, instead of 6, and we select 300 queries.</li> </ul>"},{"location":"models/fai-detr-m-coco/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. for more details look here: GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai-detr-m-coco/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-detr-m-coco/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class ID Class Name AP 1 person 56.4 2 bicycle 32.8 3 car 44.2 4 motorcycle 47.9 5 airplane 72.5 6 bus 70.4 7 train 69.1 8 truck 39.5 9 boat 27.9 10 traffic light 26.6 11 fire hydrant 68.8 12 stop sign 64.4 13 parking meter 45.5 14 bench 26.0 15 bird 37.4 16 cat 73.4 17 dog 68.6 18 horse 59.5 19 sheep 56.2 20 cow 59.2 21 elephant 67.9 22 bear 77.9 23 zebra 71.4 24 giraffe 72.2 25 backpack 16.7 26 umbrella 43.4 27 handbag 16.6 28 tie 36.5 29 suitcase 44.4 30 frisbee 68.0 31 skis 27.5 32 snowboard 35.0 33 sports ball 46.5 34 kite 44.8 35 baseball bat 29.2 36 baseball glove 38.4 37 skateboard 56.2 38 surfboard 43.3 39 tennis racket 49.5 40 bottle 37.8 41 wine glass 35.7 42 cup 43.1 43 fork 39.0 44 knife 22.6 45 spoon 20.4 46 bowl 43.5 47 banana 27.1 48 apple 22.2 49 sandwich 38.8 50 orange 33.4 51 broccoli 24.7 52 carrot 23.8 53 hot dog 38.4 54 pizza 57.4 55 donut 50.6 56 cake 38.4 57 chair 30.9 58 couch 50.0 59 potted plant 28.9 60 bed 51.4 61 dining table 32.8 62 toilet 67.2 63 tv 59.4 64 laptop 62.7 65 mouse 64.7 66 remote 34.4 67 keyboard 55.8 68 cell phone 38.2 69 microwave 61.4 70 oven 41.8 71 toaster 48.3 72 sink 39.4 73 refrigerator 59.5 74 book 15.5 75 clock 49.0 76 vase 39.3 77 scissors 30.3 78 teddy bear 50.6 79 hair drier 4.8 80 toothbrush 30.2"},{"location":"models/fai-detr-m-coco/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-detr-m-coco) from Focoos API\nmodel = focoos.get_remote_model(\"fai-detr-m-coco\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-detr/","title":"FAI-DETR (FocoosAI Detection Transformer)","text":""},{"location":"models/fai-detr/#overview","title":"Overview","text":"<p>FAI-DETR is an advanced object detection model based on the DETR (Detection Transformer) architecture, optimized by FocoosAI for efficient and accurate object detection tasks. This model eliminates the need for hand-crafted components like non-maximum suppression (NMS) and anchor generation by using a transformer-based approach with learnable object queries.</p> <p>The model employs a set-based global loss through bipartite matching and a transformer encoder-decoder architecture that directly predicts bounding boxes and class labels. This end-to-end approach simplifies the detection pipeline while achieving competitive performance.</p>"},{"location":"models/fai-detr/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The FAI-DETR architecture consists of four main components:</p>"},{"location":"models/fai-detr/#backbone","title":"Backbone","text":"<ul> <li>Purpose: Feature extraction from input images</li> <li>Design: Configurable backbone network (ResNet, STDC, etc.)</li> <li>Output: Multi-scale features at different resolutions</li> <li>Integration: Features are processed through the encoder for global context</li> </ul>"},{"location":"models/fai-detr/#encoder","title":"Encoder","text":"<ul> <li>Architecture: Transformer encoder with multi-scale deformable attention</li> <li>Components:</li> <li>Multi-scale deformable self-attention layers</li> <li>Position embeddings (sine-based)</li> <li>Feed-forward networks (FFN)</li> <li>CSPRep layers for efficient feature processing</li> <li>Features: Processes multi-scale features to capture global context</li> <li>Layers: Configurable number of encoder layers (default: 1)</li> </ul>"},{"location":"models/fai-detr/#decoder","title":"Decoder","text":"<ul> <li>Architecture: Multi-scale deformable transformer decoder</li> <li>Components:</li> <li>Self-attention layers for query interaction</li> <li>Cross-attention layers with deformable attention</li> <li>Feed-forward networks</li> <li>Reference point refinement</li> <li>Queries: 300 learnable object queries (configurable)</li> <li>Layers: Configurable number of decoder layers (default: 6)</li> </ul>"},{"location":"models/fai-detr/#detection-head","title":"Detection Head","text":"<ul> <li>Classification Head: Predicts class probabilities for each query</li> <li>Regression Head: Predicts bounding box coordinates (center, width, height)</li> <li>Output Format: Direct box predictions without anchors or post-processing</li> </ul>"},{"location":"models/fai-detr/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"models/fai-detr/#core-model-parameters","title":"Core Model Parameters","text":"<ul> <li><code>num_classes</code> (int): Number of object detection classes</li> <li><code>num_queries</code> (int, default=300): Number of learnable object queries</li> <li><code>resolution</code> (int, default=640): Input image resolution</li> <li><code>backbone_config</code> (BackboneConfig): Backbone network configuration</li> </ul>"},{"location":"models/fai-detr/#image-preprocessing","title":"Image Preprocessing","text":"<ul> <li><code>pixel_mean</code> (List[float]): RGB normalization means [123.675, 116.28, 103.53]</li> <li><code>pixel_std</code> (List[float]): RGB normalization standard deviations [58.395, 57.12, 57.375]</li> <li><code>size_divisibility</code> (int, default=0): Input size divisibility constraint</li> </ul>"},{"location":"models/fai-detr/#encoder-configuration","title":"Encoder Configuration","text":"<ul> <li><code>pixel_decoder_out_dim</code> (int, default=256): Encoder output dimension</li> <li><code>pixel_decoder_feat_dim</code> (int, default=256): Encoder feature dimension</li> <li><code>pixel_decoder_num_encoder_layers</code> (int, default=1): Number of encoder layers</li> <li><code>pixel_decoder_expansion</code> (float, default=1.0): Channel expansion ratio</li> <li><code>pixel_decoder_dim_feedforward</code> (int, default=1024): FFN dimension</li> <li><code>pixel_decoder_dropout</code> (float, default=0.0): Dropout rate</li> <li><code>pixel_decoder_nhead</code> (int, default=8): Number of attention heads</li> </ul>"},{"location":"models/fai-detr/#decoder-configuration","title":"Decoder Configuration","text":"<ul> <li><code>transformer_predictor_hidden_dim</code> (int, default=256): Decoder hidden dimension</li> <li><code>transformer_predictor_dec_layers</code> (int, default=6): Number of decoder layers</li> <li><code>transformer_predictor_dim_feedforward</code> (int, default=1024): FFN dimension</li> <li><code>transformer_predictor_nhead</code> (int, default=8): Number of attention heads</li> <li><code>transformer_predictor_out_dim</code> (int, default=256): Decoder output dimension</li> <li><code>head_out_dim</code> (int, default=256): Detection head output dimension</li> </ul>"},{"location":"models/fai-detr/#inference-configuration","title":"Inference Configuration","text":"<ul> <li><code>threshold</code> (float, default=0.5): Confidence threshold for detections</li> <li><code>top_k</code> (int, default=300): Maximum number of detections to return</li> </ul>"},{"location":"models/fai-detr/#loss-configuration","title":"Loss Configuration","text":"<ul> <li><code>criterion_deep_supervision</code> (bool, default=True): Enable deep supervision</li> <li><code>criterion_eos_coef</code> (float, default=0.1): End-of-sequence coefficient</li> <li><code>criterion_losses</code> (List[str]): Loss types [\"vfl\", \"boxes\"]</li> <li><code>criterion_focal_alpha</code> (float, default=0.75): Focal loss alpha parameter</li> <li><code>criterion_focal_gamma</code> (float, default=2.0): Focal loss gamma parameter</li> <li><code>weight_dict_loss_vfl</code> (int, default=1): Varifocal loss weight</li> <li><code>weight_dict_loss_bbox</code> (int, default=5): Bounding box loss weight</li> <li><code>weight_dict_loss_giou</code> (int, default=2): GIoU loss weight</li> </ul>"},{"location":"models/fai-detr/#hungarian-matcher-configuration","title":"Hungarian Matcher Configuration","text":"<ul> <li><code>matcher_cost_class</code> (int, default=2): Classification cost for matching</li> <li><code>matcher_cost_bbox</code> (int, default=5): Bounding box cost for matching</li> <li><code>matcher_cost_giou</code> (int, default=2): GIoU cost for matching</li> <li><code>matcher_use_focal_loss</code> (bool, default=True): Use focal loss in matcher</li> <li><code>matcher_alpha</code> (float, default=0.25): Matcher focal loss alpha</li> <li><code>matcher_gamma</code> (float, default=2.0): Matcher focal loss gamma</li> </ul>"},{"location":"models/fai-detr/#supported-tasks","title":"Supported Tasks","text":""},{"location":"models/fai-detr/#object-detection","title":"Object Detection","text":"<ul> <li>Output: Bounding boxes with class labels and confidence scores</li> <li>Use Cases:</li> <li>General object detection in natural images</li> <li>Autonomous driving (vehicle, pedestrian detection)</li> <li>Surveillance and security applications</li> <li>Industrial quality control</li> <li>Medical image analysis</li> <li>Performance: End-to-end detection without NMS post-processing</li> </ul>"},{"location":"models/fai-detr/#model-outputs","title":"Model Outputs","text":""},{"location":"models/fai-detr/#training-output-detrmodeloutput","title":"Training Output (<code>DETRModelOutput</code>)","text":"<ul> <li><code>boxes</code> (torch.Tensor): Shape [B, num_queries, 4] - Bounding boxes in XYXY format normalized to [0, 1]</li> <li><code>logits</code> (torch.Tensor): Shape [B, num_queries, num_classes] - Class predictions</li> <li><code>loss</code> (Optional[dict]): Training losses including:</li> <li><code>loss_vfl</code>: Varifocal loss for classification</li> <li><code>loss_bbox</code>: L1 loss for bounding box regression</li> <li><code>loss_giou</code>: Generalized IoU loss for box alignment</li> </ul>"},{"location":"models/fai-detr/#inference-output-focoosdetections","title":"Inference Output (<code>FocoosDetections</code>)","text":"<p>For each detected object: - <code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2] - <code>conf</code> (float): Confidence score - <code>cls_id</code> (int): Class identifier - <code>label</code> (Optional[str]): Human-readable class name</p>"},{"location":"models/fai-detr/#key-features","title":"Key Features","text":""},{"location":"models/fai-detr/#architecture-advantages","title":"Architecture Advantages","text":"<ul> <li>End-to-End Training: Direct optimization of detection metrics</li> <li>No Hand-Crafted Components: Eliminates anchors, NMS, and heuristic post-processing</li> <li>Set-Based Global Loss: Bipartite matching enables global optimization</li> <li>Parallel Prediction: All objects predicted simultaneously</li> </ul>"},{"location":"models/fai-detr/#performance-optimizations","title":"Performance Optimizations","text":"<ul> <li>Deformable Attention: Efficient multi-scale feature processing</li> <li>CSP Layers: Channel Split Pooling for computational efficiency</li> <li>RepVGG Blocks: Efficient convolution blocks for feature extraction</li> <li>Focal Loss Variants: Improved handling of class imbalance</li> </ul>"},{"location":"models/fai-detr/#training-features","title":"Training Features","text":"<ul> <li>Hungarian Matching: Optimal assignment between predictions and ground truth</li> <li>Deep Supervision: Auxiliary losses at each decoder layer</li> <li>Multi-Scale Training: Robust to different object sizes</li> <li>Flexible Loss Combination: Balances classification and localization objectives</li> </ul>"},{"location":"models/fai-detr/#loss-functions","title":"Loss Functions","text":"<p>The model employs three main loss components:</p> <ol> <li>Varifocal Loss (<code>loss_vfl</code>):</li> <li>Advanced focal loss variant for classification</li> <li>Handles foreground-background imbalance</li> <li> <p>Joint optimization of classification and localization quality</p> </li> <li> <p>Bounding Box Loss (<code>loss_bbox</code>):</p> </li> <li>L1 loss for direct coordinate regression</li> <li> <p>Normalized coordinates for scale invariance</p> </li> <li> <p>Generalized IoU Loss (<code>loss_giou</code>):</p> </li> <li>Shape-aware bounding box loss</li> <li>Better gradient flow for overlapping boxes</li> <li>Improved localization accuracy</li> </ol>"},{"location":"models/fai-detr/#architecture-innovations","title":"Architecture Innovations","text":"<p>The FAI-DETR introduces several key innovations:</p> <ol> <li>Efficient Encoder Design: Lightweight encoder with deformable attention</li> <li>Multi-Scale Processing: Handles objects at different scales effectively</li> <li>Reference Point Refinement: Iterative improvement of object localization</li> <li>CSP Integration: Efficient feature processing with Channel Split Pooling</li> <li>Optimized Matching: Hungarian algorithm for optimal query-target assignment</li> </ol> <p>This architecture achieves an optimal balance between accuracy and efficiency, making it suitable for both research applications and production deployments requiring fast and accurate object detection capabilities.</p>"},{"location":"models/fai-mf-l-ade/","title":"fai-mf-l-ade","text":""},{"location":"models/fai-mf-l-ade/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the ADE20K dataset. It is a semantic segmentation model able to segment 150 classes, comprising both stuff (sky, road, etc.) and thing (dog, cat, car, etc.).</p>"},{"location":"models/fai-mf-l-ade/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-mf-l-ade/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a transformer-based encoder-decoder architecture. Differently from traditional segmentation models (such as DeepLab), Mask2Former uses a mask-classification approach, where the prediction is made by a set of segmentation mask with associated class probabilities.</p>"},{"location":"models/fai-mf-l-ade/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is a Resnet-101,that guarantees a good performance while having good efficiency.</li> <li>the pixel decoder is a FPN getting the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. Differently from the original paper, for the sake of portability, we removed the deformable attention modules in the pixel decoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 6 decoder layers (instead of 9) and 100 learnable queries.</li> </ul>"},{"location":"models/fai-mf-l-ade/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the 6 transformer decoder layers. Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-mf-l-ade/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-mf-l-ade/#classes","title":"Classes","text":"<p>The model is pretrained on the ADE20K dataset with 150 classes.</p> Class ID Class Name mIoU 1wall77.284 2building81.396 3sky94.337 4floor81.584 5tree74.103 6ceiling83.073 7road, route83.013 8bed88.120 9window61.048 10grass69.099 11cabinet56.303 12sidewalk, pavement62.300 13person82.073 14earth, ground35.094 15door45.140 16table59.436 17mountain, mount60.538 18plant51.829 19curtain71.510 20chair56.219 21car83.766 22water49.028 23painting, picture70.214 24sofa68.081 25shelf35.453 26house45.656 27sea51.205 28mirror61.611 29rug64.144 30field30.577 31armchair45.761 32seat61.850 33fence40.992 34desk41.814 35rock, stone47.600 36wardrobe, closet, press39.846 37lamp64.062 38tub74.760 39rail24.105 40cushion56.811 41base, pedestal, stand27.777 42box24.670 43column, pillar40.094 44signboard, sign33.495 45chest of drawers, chest, bureau, dresser41.847 46counter21.387 47sand29.763 48sink74.092 49skyscraper37.613 50fireplace65.037 51refrigerator, icebox57.648 52grandstand, covered stand46.626 53path24.543 54stairs28.681 55runway73.779 56case, display case, showcase, vitrine38.437 57pool table, billiard table, snooker table91.825 58pillow49.388 59screen door, screen59.058 60stairway, staircase32.832 61river18.597 62bridge, span56.011 63bookcase28.848 64blind, screen43.934 65coffee table59.869 66toilet, can, commode, crapper, pot, potty, stool, throne86.346 67flower38.141 68book42.528 69hill6.905 70bench45.494 71countertop49.007 72stove73.973 73palm, palm tree49.478 74kitchen island42.603 75computer72.142 76swivel chair44.262 77boat73.689 78bar37.749 79arcade machine78.733 80hovel, hut, hutch, shack, shanty30.537 81bus90.808 82towel58.158 83light57.444 84truck31.745 85tower32.058 86chandelier67.524 87awning, sunshade, sunblind28.566 88street lamp30.507 89booth39.696 90tv76.194 91plane50.005 92dirt track18.268 93clothes37.748 94pole23.343 95land, ground, soil0.001 96bannister, banister, balustrade, balusters, handrail16.222 97escalator, moving staircase, moving stairway54.888 98ottoman, pouf, pouffe, puff, hassock32.444 99bottle22.166 100buffet, counter, sideboard48.994 101poster, posting, placard, notice, bill, card31.773 102stage18.731 103van46.747 104ship79.937 105fountain21.205 106conveyer belt, conveyor belt, conveyer, conveyor, transporter62.591 107canopy23.719 108washer, automatic washer, washing machine66.458 109plaything, toy35.377 110pool34.297 111stool41.199 112barrel, cask61.803 113basket, handbasket34.313 114falls57.149 115tent94.077 116bag19.126 117minibike, motorbike71.207 118cradle85.775 119oven50.996 120ball32.601 121food, solid food58.662 122step, stair16.474 123tank, storage tank37.627 124trade name20.788 125microwave37.998 126pot53.411 127animal57.360 128bicycle58.772 129lake41.597 130dishwasher74.543 131screen79.757 132blanket, cover15.202 133sculpture53.537 134hood, exhaust hood52.684 135sconce48.160 136vase45.300 137traffic light35.375 138tray14.093 139trash can30.699 140fan56.574 141pier10.286 142crt screen0.936 143plate53.268 144monitor9.358 145bulletin board29.970 146shower8.978 147radiator59.763 148glass, drinking glass18.246 149clock29.088 150flag37.727"},{"location":"models/fai-mf-l-ade/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-mf-l-ade) from Focoos API\nmodel = focoos.get_remote_model(\"fai-mf-l-ade\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-mf-l-coco-ins/","title":"fai-mf-l-coco-ins","text":""},{"location":"models/fai-mf-l-coco-ins/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the COCO dataset. It is an instance segmentation model able to segment 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai-mf-l-coco-ins/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a mask-classification approach and a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai-mf-l-coco-ins/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is Resnet-50 that show an amazing trade-off between performance and efficiency.</li> <li>the pixel decoder is a transformer-augmented FPN. It gets the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. It first uses a transformer encoder to process the features at the lowest resolution (stage 5) and then uses a feature pyramid network to upsample the features. This part is different from the original implementation using deformable attention modules.</li> <li>the transformer decoder is implemented as in the original paper, having 9 decoder layers and 100 learnable queries.</li> </ul>"},{"location":"models/fai-mf-l-coco-ins/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the 3 transformer decoder layers. Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-mf-l-coco-ins/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-mf-l-coco-ins/#classes","title":"Classes","text":"<p>The model is pretrained on the COCO dataset with 80 classes.</p> Class Segmentation AP 1 person 48.9 2 bicycle 22.2 3 car 41.3 4 motorcycle 40.0 5 airplane 55.6 6 bus 68.2 7 train 69.6 8 truck 40.5 9 boat 26.2 10 traffic light 27.4 11 fire hydrant 69.2 12 stop sign 65.0 13 parking meter 45.4 14 bench 23.4 15 bird 33.8 16 cat 77.7 17 dog 68.9 18 horse 50.1 19 sheep 54.0 20 cow 51.0 21 elephant 63.4 22 bear 81.1 23 zebra 66.0 24 giraffe 60.5 25 backpack 22.7 26 umbrella 52.6 27 handbag 23.3 28 tie 33.2 29 suitcase 45.3 30 frisbee 66.4 31 skis 7.4 32 snowboard 28.2 33 sports ball 42.8 34 kite 30.3 35 baseball bat 32.1 36 baseball glove 42.3 37 skateboard 36.8 38 surfboard 37.3 39 tennis racket 58.7 40 bottle 39.2 41 wine glass 36.9 42 cup 46.0 43 fork 22.2 44 knife 17.8 45 spoon 18.0 46 bowl 44.3 47 banana 26.5 48 apple 23.9 49 sandwich 43.0 50 orange 33.8 51 broccoli 24.4 52 carrot 22.7 53 hot dog 36.3 54 pizza 55.1 55 donut 51.1 56 cake 44.6 57 chair 25.0 58 couch 47.7 59 potted plant 25.0 60 bed 45.0 61 dining table 22.9 62 toilet 67.6 63 tv 64.3 64 laptop 67.2 65 mouse 60.1 66 remote 36.1 67 keyboard 52.6 68 cell phone 42.0 69 microwave 60.7 70 oven 33.8 71 toaster 35.9 72 sink 39.9 73 refrigerator 64.0 74 book 12.0 75 clock 52.5 76 vase 37.7 77 scissors 26.8 78 teddy bear 55.1 79 hair drier 16.8 80 toothbrush 22.4"},{"location":"models/fai-mf-l-coco-ins/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-mf-l-coco-ins) from Focoos API\nmodel = focoos.get_remote_model(\"fai-mf-l-coco-ins\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-mf-m-ade/","title":"fai-mf-m-ade","text":""},{"location":"models/fai-mf-m-ade/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the ADE20K dataset. It is a semantic segmentation model able to segment 150 classes, comprising both stuff (sky, road, etc.) and thing (dog, cat, car, etc.).</p>"},{"location":"models/fai-mf-m-ade/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-mf-m-ade/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a transformer-based encoder-decoder architecture. Differently from traditional segmentation models (such as DeepLab), Mask2Former uses a mask-classification approach, where the prediction is made by a set of segmentation mask with associated class probabilities.</p>"},{"location":"models/fai-mf-m-ade/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-2 that show an amazing trade-off between performance and efficiency.</li> <li>the pixel decoder is a FPN getting the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. Differently from the original paper, for the sake of portability, we removed the deformable attention modules in the pixel decoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a lighter version of the original, having only 3 decoder layers (instead of 9) and 100 learnable queries.</li> </ul>"},{"location":"models/fai-mf-m-ade/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the 3 transformer decoder layers. Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-mf-m-ade/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-mf-m-ade/#classes","title":"Classes","text":"<p>The model is pretrained on the ADE20K dataset with 150 classes.</p> Class mIoU 1 wall 75.369549 2 building 79.835995 3 sky 94.176995 4 floor 79.620841 5 tree 73.204506 6 ceiling 82.303035 7 road, route 80.822591 8 bed 87.573840 9 window 57.452584 10 grass 70.099493 11 cabinet 56.903790 12 sidewalk, pavement 62.247267 13 person 79.460606 14 earth, ground 38.537802 15 door 43.930878 16 table 56.753292 17 mountain, mount 61.160462 18 plant 48.995487 19 curtain 71.951930 20 chair 52.852125 21 car 80.725703 22 water 51.233498 23 painting, picture 66.989493 24 sofa 58.103663 25 shelf 34.979205 26 house 36.828611 27 sea 51.219096 28 mirror 58.572852 29 rug 54.897799 30 field 29.053876 31 armchair 39.565663 32 seat 53.113668 33 fence 41.113128 34 desk 37.930189 35 rock, stone 44.940982 36 wardrobe, closet, press 39.897858 37 lamp 60.921356 38 tub 78.041637 39 rail 31.893878 40 cushion 53.029316 41 base, pedestal, stand 20.233620 42 box 18.276924 43 column, pillar 42.655306 44 signboard, sign 35.959448 45 chest of drawers, chest, bureau, dresser 36.521600 46 counter 29.353667 47 sand 38.729599 48 sink 72.303141 49 skyscraper 44.122387 50 fireplace 66.614683 51 refrigerator, icebox 72.137179 52 grandstand, covered stand 29.061628 53 path 26.629478 54 stairs 31.833328 55 runway 76.017706 56 case, display case, showcase, vitrine 37.452627 57 pool table, billiard table, snooker table 93.246039 58 pillow 54.689591 59 screen door, screen 58.096890 60 stairway, staircase 29.962829 61 river 15.010211 62 bridge, span 66.617580 63 bookcase 31.383789 64 blind, screen 39.221180 65 coffee table 63.300795 66 toilet, can, commode, crapper, pot, potty, stool, throne 84.038177 67 flower 35.994798 68 book 43.252042 69 hill 6.240850 70 bench 35.007473 71 countertop 56.592858 72 stove 74.866261 73 palm, palm tree 49.092486 74 kitchen island 32.353614 75 computer 57.673329 76 swivel chair 43.202283 77 boat 48.170742 78 bar 24.034261 79 arcade machine 11.467819 80 hovel, hut, hutch, shack, shanty 10.258017 81 bus 81.375072 82 towel 54.954106 83 light 53.256340 84 truck 29.656645 85 tower 36.864496 86 chandelier 63.787459 87 awning, sunshade, sunblind 23.610311 88 street lamp 29.944617 89 booth 29.360433 90 tv 61.512572 91 plane 53.270513 92 dirt track 4.206758 93 clothes 35.342074 94 pole 20.678348 95 land, ground, soil 3.195710 96 bannister, banister, balustrade, balusters, handrail 17.522631 97 escalator, moving staircase, moving stairway 20.889345 98 ottoman, pouf, pouffe, puff, hassock 47.003450 99 bottle 15.504667 100 buffet, counter, sideboard 26.077572 101 poster, posting, placard, notice, bill, card 30.691103 102 stage 11.744151 103 van 40.161822 104 ship 79.300311 105 fountain 0.112958 106 conveyer belt, conveyor belt, conveyer, conveyor, transporter 60.552373 107 canopy 25.086350 108 washer, automatic washer, washing machine 63.550537 109 plaything, toy 18.290597 110 pool 32.873865 111 stool 39.256308 112 barrel, cask 6.358771 113 basket, handbasket 29.850719 114 falls 57.657161 115 tent 93.717152 116 bag 10.629695 117 minibike, motorbike 56.217901 118 cradle 69.441302 119 oven 38.940583 120 ball 45.543376 121 food, solid food 52.779065 122 step, stair 10.843115 123 tank, storage tank 30.871163 124 trade name 27.908376 125 microwave 32.381977 126 pot 41.040635 127 animal 55.882266 128 bicycle 50.185374 129 lake 0.007605 130 dishwasher 58.970317 131 screen 60.016197 132 blanket, cover 26.963189 133 sculpture 27.667732 134 hood, exhaust hood 58.025458 135 sconce 39.341998 136 vase 31.185747 137 traffic light 23.810429 138 tray 7.244281 139 trash can 30.072544 140 fan 52.113861 141 pier 56.678802 142 crt screen 9.133357 143 plate 38.900407 144 monitor 3.323130 145 bulletin board 52.337659 146 shower 4.692180 147 radiator 43.811464 148 glass, drinking glass 14.036491 149 clock 25.044316 150 flag 40.007933"},{"location":"models/fai-mf-m-ade/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-mf-m-ade) from Focoos API\nmodel = focoos.get_remote_model(\"fai-mf-m-ade\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai-mf-s-ade/","title":"fai-mf-l-ade","text":""},{"location":"models/fai-mf-s-ade/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the ADE20K dataset. It is a semantic segmentation model able to segment 150 classes, comprising both stuff (sky, road, etc.) and thing (dog, cat, car, etc.).</p>"},{"location":"models/fai-mf-s-ade/#benchmark","title":"Benchmark","text":"<p> Note: FPS are computed on NVIDIA T4 using TensorRT and image size 640x640.</p>"},{"location":"models/fai-mf-s-ade/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a transformer-based encoder-decoder architecture. Differently from traditional segmentation models (such as DeepLab), Mask2Former uses a mask-classification approach, where the prediction is made by a set of segmentation mask with associated class probabilities.</p>"},{"location":"models/fai-mf-s-ade/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-1 that shows a trade-off tending to be more efficient.</li> <li>the pixel decoder is a FPN getting the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. Differently from the original paper, for the sake of portability, we removed the deformable attention modules in the pixel decoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a extremely light version of the original, having only 1 decoder layer (instead of 9) and 100 learnable queries.</li> </ul>"},{"location":"models/fai-mf-s-ade/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai-mf-s-ade/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai-mf-s-ade/#classes","title":"Classes","text":"<p>The model is pretrained on the ADE20K dataset with 150 classes.</p> Class mIoU 1 wall 69.973850 2 building 78.431035 3 sky 91.401107 4 floor 73.162280 5 tree 70.535439 6 ceiling 77.258595 7 road, route 78.314172 8 bed 77.755793 9 window 53.012898 10 grass 64.432303 11 cabinet 51.032268 12 sidewalk, pavement 55.642697 13 person 70.461440 14 eartd, ground 30.454824 15 door 36.431782 16 table 43.096636 17 mountain, mount 54.971609 18 plant 45.115711 19 curtain 64.930372 20 chair 40.465565 21 car 76.888113 22 water 41.666148 23 painting, picture 60.099652 24 sofa 49.840449 25 shelf 31.991519 26 house 45.338182 27 sea 51.614250 28 mirror 55.731406 29 rug 51.858072 30 field 23.065903 31 armchair 30.602317 32 seat 50.277596 33 fence 34.439293 34 desk 35.494495 35 rock, stone 39.573617 36 wardrobe, closet, press 51.343586 37 lamp 47.754304 38 tub 71.511291 39 rail 23.280869 40 cushion 39.251768 41 base, pedestal, stand 28.472143 42 box 16.070477 43 column, pillar 37.924454 44 signboard, sign 29.057276 45 chest of drawers, chest, bureau, dresser 36.343963 46 counter 19.595326 47 sand 31.296151 48 sink 54.413180 49 skyscraper 47.583224 50 fireplace 62.204434 51 refrigerator, icebox 54.270643 52 grandstand, covered stand 31.345801 53 patd 22.330369 54 stairs 20.323718 55 runway 63.892811 56 case, display case, showcase, vitrine 34.649422 57 pool table, billiard table, snooker table 85.365581 58 pillow 46.426184 59 screen door, screen 57.292321 60 stairway, staircase 28.904954 61 river 16.681450 62 bridge, span 52.791513 63 bookcase 26.722881 64 blind, screen 36.787453 65 coffee table 41.603442 66 toilet, can, commode, crapper, pot, potty, stool, tdrone 75.753455 67 flower 30.200230 68 book 37.602484 69 hill 5.509057 70 bench 29.331054 71 countertop 46.661677 72 stove 58.972851 73 palm, palm tree 48.317300 74 kitchen island 25.279206 75 computer 49.335666 76 swivel chair 34.845392 77 boat 48.521646 78 bar 30.174155 79 arcade machine 24.721694 80 hovel, hut, hutch, shack, shanty 32.843717 81 bus 82.174778 82 towel 46.050430 83 light 30.983118 84 truck 23.456256 85 tower 32.147803 86 chandelier 54.045160 87 awning, sunshade, sunblind 18.526182 88 street lamp 13.641714 89 bootd 60.471570 90 tv 55.530715 91 plane 42.894525 92 dirt track 0.001787 93 clotdes 30.124455 94 pole 11.280532 95 land, ground, soil 4.243296 96 bannister, banister, balustrade, balusters, handrail 9.922319 97 escalator, moving staircase, moving stairway 19.186240 98 ottoman, pouf, pouffe, puff, hassock 30.352586 99 bottle 11.872842 100 buffet, counter, sideboard 34.547476 101 poster, posting, placard, notice, bill, card 15.081001 102 stage 17.466091 103 van 39.027877 104 ship 66.778301 105 fountain 18.879113 106 conveyer belt, conveyor belt, conveyer, conveyor, transporter 67.580228 107 canopy 25.654567 108 washer, automatic washer, washing machine 60.187881 109 playtding, toy 13.836259 110 pool 28.796494 111 stool 26.432746 112 barrel, cask 43.777156 113 basket, handbasket 19.144369 114 falls 47.131198 115 tent 88.431441 116 bag 7.634387 117 minibike, motorbike 40.625528 118 cradle 54.247514 119 oven 33.695444 120 ball 36.066130 121 food, solid food 50.837348 122 step, stair 13.071184 123 tank, storage tank 43.042742 124 trade name 21.579095 125 microwave 32.179626 126 pot 27.438416 127 animal 55.993825 128 bicycle 38.273475 129 lake 35.704904 130 dishwasher 37.616793 131 screen 57.100955 132 blanket, cover 15.560568 133 sculpture 31.317035 134 hood, exhaust hood 49.290385 135 sconce 29.971644 136 vase 24.983318 137 traffic light 17.806663 138 tray 5.720345 139 trash can 28.621136 140 fan 39.083851 141 pier 51.310956 142 crt screen 0.858346 143 plate 35.344330 144 monitor 1.994270 145 bulletin board 35.468027 146 shower 1.090403 147 radiator 42.574652 148 glass, drinking glass 8.510381 149 clock 14.128872 150 flag 24.098100"},{"location":"models/fai-mf-s-ade/#what-are-you-waiting-try-it","title":"What are you waiting? Try it!","text":"<pre><code>from focoos import Focoos\nimport os\n\n# Initialize the Focoos client with your API key\nfocoos = Focoos(api_key=os.getenv(\"FOCOOS_API_KEY\"))\n\n# Get the remote model (fai-mf-s-ade) from Focoos API\nmodel = focoos.get_remote_model(\"fai-mf-s-ade\")\n\n# Run inference on an image\npredictions = model.infer(\"./image.jpg\", threshold=0.5)\n\n# Output the predictions\nprint(predictions)\n</code></pre>"},{"location":"models/fai_cls/","title":"fai-cls","text":""},{"location":"models/fai_cls/#overview","title":"Overview","text":"<p>The models is a Mask2Former model otimized by FocoosAI for the ADE20K dataset. It is a semantic segmentation model able to segment 150 classes, comprising both stuff (sky, road, etc.) and thing (dog, cat, car, etc.).</p>"},{"location":"models/fai_cls/#model-details","title":"Model Details","text":"<p>The model is based on the Mask2Former architecture. It is a segmentation model that uses a transformer-based encoder-decoder architecture. Differently from traditional segmentation models (such as DeepLab), Mask2Former uses a mask-classification approach, where the prediction is made by a set of segmentation mask with associated class probabilities.</p>"},{"location":"models/fai_cls/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The Mask2Former FocoosAI implementation optimize the original neural network architecture for improving the model's efficiency and performance. The original model is fully described in this paper.</p> <p>Mask2Former is a hybrid model that uses three main components: a backbone for extracting features, a pixel decoder for upscaling the features, and a transformer-based decoder for generating the segmentation output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is STDC-1 that shows a trade-off tending to be more efficient.</li> <li>the pixel decoder is a FPN getting the features from the stage 2 (1/4 resolution), 3 (1/8 resolution), 4 (1/16 resolution) and 5 (1/32 resolution) of the backbone. Differently from the original paper, for the sake of portability, we removed the deformable attention modules in the pixel decoder, speeding up the inference while only marginally affecting the accuracy.</li> <li>the transformer decoder is a extremely light version of the original, having only 1 decoder layer (instead of 9) and 100 learnable queries.</li> </ul>"},{"location":"models/fai_cls/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_ce: Cross-entropy loss for the classification of the classes</li> <li>loss_dice: Dice loss for the segmentation of the classes</li> <li>loss_mask: A binary cross-entropy loss applied to the predicted segmentation masks</li> </ul> <p>Please refer to the Mask2Former paper for more details.</p>"},{"location":"models/fai_cls/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of masks with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 100 elements containing the class id associated with each mask (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 100 elements containing the corresponding probability of the class_id</li> <li>masks: a tensor of shape (100, H, W) where H and W are the height and width of the input image and the values represent the index of the class_id associated with the pixel</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of masks with associated class probabilities and has been trained to avoid overlapping masks.</p> <p>After the post-processing, the output is a Focoos Detections object containing the predicted masks with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai_detr/","title":"fai-detr","text":""},{"location":"models/fai_detr/#overview","title":"Overview","text":"<p>The models is the reimplementation of the RT-DETR model by FocoosAI for the COCO dataset. It is a object detection model able to detect 80 thing (dog, cat, car, etc.) classes.</p>"},{"location":"models/fai_detr/#model-details","title":"Model Details","text":"<p>The model is based on the RT-DETR architecture. It is a object detection model that uses a transformer-based encoder-decoder architecture.</p>"},{"location":"models/fai_detr/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>This implementation is a reimplementation of the RT-DETR model by FocoosAI. The original model is fully described in this paper.</p> <p>RT-DETR is a hybrid model that uses three main components: a backbone for extracting features, an encoder for upscaling the features, and a transformer-based decoder for generating the detection output.</p> <p></p> <p>In this implementation:</p> <ul> <li>the backbone is a Resnet-50,that guarantees a good performance while having good efficiency.</li> <li>the encoder is the Hybrid Encoder, as proposed by the paper, and it is a bi-FPN (bilinear feature pyramid network) that includes a transformer encoder on the smaller feature resolution for improving efficiency.</li> <li>The query selection mechanism select the features of the pixels (aka queries) with the highest probability of containing an object and pass them to a transformer decoder head that will generate the final detection output. In this implementation, we select 300 queries and use 6 transformer decoder layers.</li> </ul>"},{"location":"models/fai_detr/#losses","title":"Losses","text":"<p>We use the same losses as the original paper:</p> <ul> <li>loss_vfl: a variant of the binary cross entropy loss for the classification of the classes that is weighted by the correctness of the predicted bounding boxes IoU.</li> <li>loss_bbox: an L1 loss computing the distance between the predicted bounding boxes and the ground truth bounding boxes.</li> <li>loss_giou: a loss minimizing the IoU the predicted bounding boxes and the ground truth bounding boxes. For more details look at GIoU.</li> </ul> <p>These losses are applied to each output of the transformer decoder, meaning that we apply it on the output and on each auxiliary output of the transformer decoder layers. Please refer to the RT-DETR paper for more details.</p>"},{"location":"models/fai_detr/#output-format","title":"Output Format","text":"<p>The pre-processed output of the model is set of bounding boxes with associated class probabilities. In particular, the output is composed by three tensors:</p> <ul> <li>class_ids: a tensor of 300 elements containing the class id associated with each bounding box (such as 1 for wall, 2 for building, etc.)</li> <li>scores: a tensor of 300 elements containing the corresponding probability of the class_id</li> <li>boxes: a tensor of shape (300, 4) where the values represent the coordinates of the bounding boxes in the format [x1, y1, x2, y2]</li> </ul> <p>The model does not need NMS (non-maximum suppression) because the output is already a set of bounding boxes with associated class probabilities and has been trained to avoid overlaps.</p> <p>After the post-processing, the output is a the output is a Focoos Detections object containing the predicted bounding boxes with confidence greather than a specific threshold (0.5 by default).</p>"},{"location":"models/fai_mf/","title":"FAI-MF (FocoosAI MaskFormer)","text":""},{"location":"models/fai_mf/#overview","title":"Overview","text":"<p>The FAI-MF model is a Mask2Former implementation optimized by FocoosAI for semantic and instance segmentation tasks. Unlike traditional segmentation models such as DeepLab, Mask2Former employs a mask-classification approach where predictions consist of segmentation masks paired with class probabilities.</p>"},{"location":"models/fai_mf/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The FAI-MF model is built on the Mask2Former architecture, featuring a transformer-based encoder-decoder design with three main components:</p> <p></p>"},{"location":"models/fai_mf/#backbone","title":"Backbone","text":"<ul> <li>Network: Any backbone that can extract multi-scale features from an image</li> <li>Output: Multi-scale features from stages 2-5 at resolutions 1/4, 1/8, 1/16, and 1/32</li> </ul>"},{"location":"models/fai_mf/#pixel-decoder","title":"Pixel Decoder","text":"<ul> <li>Architecture: Feature Pyramid Network (FPN)</li> <li>Input: Features from backbone stages 2-5</li> <li>Modifications: Deformable attention modules removed for improved portability and inference speed</li> <li>Output: Upscaled multi-scale features for mask generation</li> </ul>"},{"location":"models/fai_mf/#transformer-decoder","title":"Transformer Decoder","text":"<ul> <li>Design: Lightweight version of the original Mask2Former decoder</li> <li>Layers: N decoder layer (depending on the speed/accuracy trade-off)</li> <li>Queries: Q learnable object queries (usually 100)</li> <li>Components:</li> <li>Self-attention layers</li> <li>Masked cross-attention layers</li> <li>Feed-forward networks (FFN)</li> </ul>"},{"location":"models/fai_mf/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"models/fai_mf/#core-model-parameters","title":"Core Model Parameters","text":"<ul> <li><code>num_classes</code> (int): Number of segmentation classes</li> <li><code>num_queries</code> (int, default=100): Number of learnable object queries</li> <li><code>resolution</code> (int, default=640): Input image resolution</li> </ul>"},{"location":"models/fai_mf/#backbone-configuration","title":"Backbone Configuration","text":"<ul> <li><code>backbone_config</code> (BackboneConfig): Backbone network configuration</li> </ul>"},{"location":"models/fai_mf/#architecture-dimensions","title":"Architecture Dimensions","text":"<ul> <li><code>pixel_decoder_out_dim</code> (int, default=256): Pixel decoder output channels</li> <li><code>pixel_decoder_feat_dim</code> (int, default=256): Pixel decoder feature channels</li> <li><code>transformer_predictor_hidden_dim</code> (int, default=256): Transformer hidden dimension</li> <li><code>transformer_predictor_dec_layers</code> (int, default=6): Number of decoder layers</li> <li><code>head_out_dim</code> (int, default=256): Prediction head output dimension</li> </ul>"},{"location":"models/fai_mf/#inference-configuration","title":"Inference Configuration","text":"<ul> <li><code>postprocessing_type</code> (str): Either \"semantic\" or \"instance\" segmentation</li> <li><code>mask_threshold</code> (float, default=0.5): Binary mask threshold</li> <li><code>threshold</code> (float, default=0.5): Confidence threshold for the classification scores</li> <li><code>predict_all_pixels</code> (bool, default=False): Predict class for every pixel, this is usually better for semantic segmentation</li> </ul>"},{"location":"models/fai_mf/#supported-tasks","title":"Supported Tasks","text":""},{"location":"models/fai_mf/#semantic-segmentation","title":"Semantic Segmentation","text":"<ul> <li>Output: Dense pixel-wise class predictions</li> <li>Use case: Scene understanding, medical imaging, autonomous driving</li> <li>Configuration: Set <code>postprocessing_type=\"semantic\"</code></li> </ul>"},{"location":"models/fai_mf/#instance-segmentation","title":"Instance Segmentation","text":"<ul> <li>Output: Individual object instances with masks and bounding boxes</li> <li>Use case: Object detection and counting, robotics, surveillance</li> <li>Configuration: Set <code>postprocessing_type=\"instance\"</code></li> </ul>"},{"location":"models/fai_mf/#model-outputs","title":"Model Outputs","text":""},{"location":"models/fai_mf/#inner-model-output-maskformermodeloutput","title":"Inner Model Output (<code>MaskFormerModelOutput</code>)","text":"<ul> <li><code>masks</code> (torch.Tensor): Shape [B, num_queries, H, W] - Query mask predictions</li> <li><code>logits</code> (torch.Tensor): Shape [B, num_queries, num_classes] - Class predictions</li> <li><code>loss</code> (Optional[dict]): Training losses including:<ul> <li><code>loss_ce</code>: Cross-entropy classification loss</li> <li><code>loss_mask</code>: Binary cross-entropy mask loss</li> <li><code>loss_dice</code>: Dice coefficient loss</li> </ul> </li> </ul>"},{"location":"models/fai_mf/#inference-output-focoosdetections","title":"Inference Output (<code>FocoosDetections</code>)","text":"<p>For each detected object:</p> <ul> <li><code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2]</li> <li><code>conf</code> (float): Confidence score</li> <li><code>cls_id</code> (int): Class identifier</li> <li><code>mask</code> (str): Base64-encoded binary mask</li> <li><code>label</code> (Optional[str]): Human-readable class name</li> </ul>"},{"location":"models/fai_mf/#loss-functions","title":"Loss Functions","text":"<p>The model employs three complementary loss functions as described in the original paper:</p> <ol> <li>Cross-entropy Loss (<code>loss_ce</code>): Classification of object classes</li> <li>Dice Loss (<code>loss_dice</code>): Shape-aware segmentation loss</li> <li>Mask Loss (<code>loss_mask</code>): Binary cross-entropy on predicted masks</li> </ol>"},{"location":"models/fai_mf/#available-models","title":"Available Models","text":"<p>Currently, you can find 5 fai-mf models on the Focoos Hub, 2 for semantic segmentation and 3 for instance-segmentation.</p>"},{"location":"models/fai_mf/#semantic-segmentation-models","title":"Semantic Segmentation Models","text":"Model Name Architecture Dataset Metric FPS Nvidia-T4 fai-mf-l-ade Mask2Former (Resnet-101) ADE20K mIoU: 48.27mAcc: 62.15 73 fai-mf-m-ade Mask2Former (STDC-2) ADE20K mIoU: 45.32mACC: 57.75 127"},{"location":"models/fai_mf/#instance-segmentation-models","title":"Instance Segmentation Models","text":"Model Name Architecture Dataset Metric FPS Nvidia-T4 fai-m2f-s-coco-ins Mask2Former (Resnet-50) COCO segm/AP: 41.45segm/AP50: 64.12 86 fai-m2f-m-coco-ins Mask2Former (Resnet-101) COCO segm/AP: 43.09segm/AP50: 65.87 70 fai-m2f-l-coco-ins Mask2Former (Resnet-101) COCO segm/AP: 44.23segm/AP50: 67.53 55"},{"location":"models/models/","title":"Focoos Models \ud83e\udde0","text":"<p>With the Focoos SDK, you can take advantage of a collection of foundational models that are optimized for a range of computer vision tasks. These pre-trained models, covering detection and semantic segmentation across various domains, provide an excellent starting point for your specific use case. Whether you need to fine-tune for custom requirements or adapt them to your application, these models offer a solid foundation to accelerate your development process.</p>"},{"location":"models/models/#semantic-segmentation","title":"Semantic Segmentation \ud83d\uddbc\ufe0f","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-mf-l-ade Mask2Former (Resnet-101) Common Scene (150) ADE20K mIoU: 48.27mAcc: 62.15 73 fai-mf-m-ade Mask2Former (STDC-2) Common Scene (150) ADE20K mIoU: 45.32mACC: 57.75 127 fai-mf-s-ade Mask2Former (STDC-1) Common Scene (150) ADE20K mIoU: 41.23mAcc: 52.21 189 <p> mIoU = Intersection over Union averaged by class   mAcc = Pixel Accuracy averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"models/models/#object-detection","title":"Object Detection \ud83d\udd75\ufe0f\u200d\u2642\ufe0f","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-detr-l-coco RT-DETR (Resnet-50) Common Objects (80) COCO bbox/AP: 53.06bbox/AP50: 70.91 87 fai-detr-m-coco RT-DETR (STDC-2) Common Objects (80) COCO bbox/AP: 44.69bbox/AP50: 61.63 181 fai-detr-l-obj365 RT-DETR (Resnet50) Common Objects (365) Objects365 bbox/AP: 34.60bbox/AP50: 45.81 87 <p> AP = Average Precision averaged by class   AP50 = Average Precision at IoU threshold 0.50 averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"models/models/#instance-segmentation","title":"Instance Segmentation \ud83c\udfad","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-m2f-s-coco-ins Mask2Former (Resnet-50) Common Objects (80) COCO segm/AP: 41.45segm/AP50: 64.12 86 fai-m2f-m-coco-ins Mask2Former (Resnet-101) Common Objects (80) COCO segm/AP: 43.09segm/AP50: 65.87 70 fai-m2f-l-coco-ins Mask2Former (Resnet-101) Common Objects (80) COCO segm/AP: 44.23segm/AP50: 67.53 55 <p> AP = Average Precision averaged by class   AP50 = Average Precision at IoU threshold 0.50 averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"}]}