{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the Focoos Library","text":"<p>Focoos is a comprehensive library for training and deploying efficient open-source computer vision models. It enables seamless model customization on your datasets, integrates with FocoosHub for experiment tracking and model weight management, and provides easy deployment across multiple devices through simple function calls.</p>"},{"location":"#why-choose-focoos","title":"Why choose Focoos?","text":"<ul> <li>\ud83d\udd39 Performance: Achieve up to 10x faster inference speeds compared to traditional computer vision models, enabling real-time processing even on edge devices.</li> <li>\ud83d\udcb0 Cost Efficiency: Reduce computational requirements by 75%, significantly lowering cloud infrastructure and hardware costs while maintaining high accuracy.</li> <li>\u26a1 Developer Experience: Streamline your workflow with an intuitive API that makes model deployment, fine-tuning, and integration seamless and straightforward.</li> </ul>"},{"location":"#start-now","title":"Start now!","text":"<p>By choosing Focoos AI, you can save time, reduce costs, and achieve superior model performance, all while ensuring the privacy and efficiency of your deployments.</p> <p>Reach out to us for any question or go to Focoos AI to register and start using the hub.</p> <p>Otherwise Book A Meeting with us to see Focoos in action.</p>"},{"location":"cli/","title":"Focoos CLI","text":"<p>A modern, comprehensive command-line interface for the Focoos computer vision framework. The CLI provides streamlined commands for training, validation, inference, model management, and hub interactions with rich terminal output and extensive configuration options.</p>"},{"location":"cli/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>\u2705 Modern CLI Design: Built with Typer for rich, interactive command-line experience</li> <li>\u2705 Comprehensive Coverage: Complete access to all Focoos functionality</li> <li>\u2705 Type Safety: Built-in validation for arguments and options with helpful error messages</li> <li>\u2705 Rich Output: Colored output, progress indicators, and detailed feedback</li> <li>\u2705 Flexible Configuration: Extensive customization options for all operations</li> <li>\u2705 Multi-Platform: Cross-platform support (Linux, macOS, Windows)</li> <li>\u2705 Programmatic API: Commands can be used both via CLI and Python imports</li> <li>\u2705 Hub Integration: Seamless integration with Focoos Hub for model and dataset management</li> </ul>"},{"location":"cli/#installation","title":"\ud83d\udce6 Installation","text":"<pre><code>uv pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n</code></pre>"},{"location":"cli/#quick-start","title":"\ud83c\udfaf Quick Start","text":"<pre><code># Check installation\nfocoos version\n\n# Run system checks\nfocoos checks\n\n# Train a model\nfocoos train --model fai-detr-m-coco --dataset mydataset.zip --im-size 640\n\n# Run inference\nfocoos predict --model fai-detr-m-coco --source image.jpg\n\n# Export model\nfocoos export --model fai-detr-m-coco --format onnx\n\n# Launch gradio interface\nfocoos gradio\n</code></pre>"},{"location":"cli/#usage","title":"\ud83d\udcda Usage","text":"<p>The Focoos CLI uses a clean, modern command syntax:</p> <pre><code>focoos COMMAND [OPTIONS]\n</code></pre> <p>Where: - COMMAND: Main operations like <code>train</code>, <code>val</code>, <code>predict</code>, <code>export</code>, <code>benchmark</code>, <code>gradio</code>, <code>hub</code> - OPTIONS: Command-specific flags and parameters with intelligent defaults</p>"},{"location":"cli/#available-commands","title":"\ud83d\udee0\ufe0f Available Commands","text":""},{"location":"cli/#core-commands","title":"Core Commands","text":"Command Description Example <code>train</code> Train models on datasets <code>focoos train --model fai-detr-m-coco --dataset data.zip</code> <code>val</code> Validate model performance <code>focoos val --model fai-detr-m-coco --dataset data.zip</code> <code>predict</code> Run inference on images <code>focoos predict --model fai-detr-m-coco --source image.jpg</code> <code>export</code> Export models to different formats <code>focoos export --model fai-detr-m-coco --format onnx</code> <code>benchmark</code> Benchmark model performance <code>focoos benchmark --model fai-detr-m-coco --iterations 100</code> <code>gradio</code> Launch interactive web interface <code>focoos gradio</code>"},{"location":"cli/#hub-commands","title":"Hub Commands","text":"Command Description Example <code>hub models</code> List available pretrained models <code>focoos hub models</code> <code>hub datasets</code> List available datasets <code>focoos hub datasets --include-shared</code>"},{"location":"cli/#utility-commands","title":"Utility Commands","text":"Command Description Example <code>version</code> Show Focoos version information <code>focoos version</code> <code>checks</code> Run system diagnostics <code>focoos checks</code> <code>settings</code> Show current configuration <code>focoos settings</code>"},{"location":"cli/#detailed-examples","title":"\ud83d\udcd6 Detailed Examples","text":""},{"location":"cli/#training","title":"\ud83c\udfcb\ufe0f Training","text":"<pre><code># Basic training\nfocoos train --model fai-detr-m-coco --dataset mydataset.zip --im-size 640\n\n# Advanced training with custom hyperparameters\nfocoos train \\\n  --model fai-detr-m-coco \\\n  --dataset mydataset.zip \\\n  --im-size 640 \\\n  --batch-size 16 \\\n  --max-iters 5000 \\\n  --learning-rate 1e-3 \\\n  --scheduler COSINE \\\n  --optimizer ADAMW \\\n  --early-stop \\\n  --patience 20 \\\n  --ema-enabled\n\n# Multi-GPU distributed training\nfocoos train \\\n  --model fai-detr-m-coco \\\n  --dataset mydataset.zip \\\n  --num-gpus 4 \\\n  --batch-size 32 \\\n  --workers 8\n\n# Resume training from checkpoint\nfocoos train \\\n  --model fai-detr-m-coco \\\n  --dataset mydataset.zip \\\n  --resume \\\n  --init-checkpoint path/to/checkpoint.pth\n</code></pre>"},{"location":"cli/#validation","title":"\ud83d\udd0d Validation","text":"<pre><code># Basic validation\nfocoos val --model fai-detr-m-coco --dataset mydataset.zip --im-size 640\n\n# Validation with specific checkpoint\nfocoos val \\\n  --model fai-detr-m-coco \\\n  --dataset mydataset.zip \\\n  --init-checkpoint path/to/best_model.pth \\\n  --batch-size 32\n\n# Multi-GPU validation\nfocoos val \\\n  --model fai-detr-m-coco \\\n  --dataset mydataset.zip \\\n  --num-gpus 2 \\\n  --batch-size 64\n</code></pre>"},{"location":"cli/#prediction-inference","title":"\ud83c\udfaf Prediction &amp; Inference","text":"<pre><code># Single image inference\nfocoos predict --model fai-detr-m-coco --source image.jpg\n\n\n# URL inference with custom runtime\nfocoos predict \\\n  --model fai-detr-m-coco \\\n  --source https://example.com/image.jpg \\\n  --runtime onnx_cuda32 \\\n  --output-dir ./url_results\n</code></pre>"},{"location":"cli/#model-export","title":"\ud83d\udce4 Model Export","text":"<pre><code># Export to ONNX (default)\nfocoos export --model fai-detr-m-coco --im-size 640\n\n# Export to TorchScript\nfocoos export \\\n  --model fai-detr-m-coco \\\n  --format torchscript \\\n  --device cuda \\\n  --im-size 1024\n\n# Export with custom settings\nfocoos export \\\n  --model fai-detr-m-coco \\\n  --format onnx \\\n  --output-dir ./exports \\\n  --onnx-opset 16 \\\n  --overwrite\n\n# CPU-optimized export\nfocoos export \\\n  --model fai-detr-m-coco \\\n  --format onnx \\\n  --device cpu \\\n  --output-dir ./cpu_models\n</code></pre>"},{"location":"cli/#benchmarking","title":"\u26a1 Benchmarking","text":"<pre><code># Basic GPU benchmark\nfocoos benchmark --model fai-detr-m-coco --iterations 100 --device cuda\n\n# Comprehensive benchmark with custom settings\nfocoos benchmark \\\n  --model fai-detr-m-coco \\\n  --im-size 1024 \\\n  --iterations 200 \\\n  --device cuda\n\n# CPU benchmark\nfocoos benchmark \\\n  --model fai-detr-m-coco \\\n  --device cpu \\\n  --iterations 50 \\\n  --im-size 640\n\n# Memory-constrained benchmark\nfocoos benchmark \\\n  --model fai-detr-m-coco \\\n  --im-size 512 \\\n  --iterations 30\n</code></pre>"},{"location":"cli/#hub-integration","title":"\ud83c\udf10 Hub Integration","text":"<pre><code># List all available models\nfocoos hub models\n\n# List your private datasets\nfocoos hub datasets\n\n# List both private and shared datasets\nfocoos hub datasets --include-shared\n</code></pre>"},{"location":"cli/#interactive-web-interface","title":"\ud83d\udda5\ufe0f Interactive Web Interface","text":"<pre><code># Launch Gradio web interface\nfocoos gradio\n</code></pre> <p>The Gradio interface provides an interactive web-based experience for running inference with Focoos models:</p> <ul> <li>Image Inference: Upload images and run detection/segmentation with real-time results</li> <li>Video Inference: Process video files with object detection and tracking</li> <li>Model Selection: Choose from available pretrained models</li> <li>Confidence Tuning: Adjust detection thresholds interactively</li> <li>Visual Results: View annotated outputs with bounding boxes and masks</li> </ul> <p>The interface will automatically open in your default web browser, typically at <code>http://localhost:7860</code>.</p>"},{"location":"cli/#configuration-options","title":"\u2699\ufe0f Configuration Options","text":""},{"location":"cli/#common-parameters","title":"Common Parameters","text":"Parameter Description Default Options <code>--model</code> Model name or path Required <code>fai-detr-m-coco</code>, <code>path/to/model</code> <code>--dataset</code> Dataset name or path Required <code>mydataset.zip</code>, <code>path/to/data/</code> <code>--source</code> Input source (predict only) Required <code>image.jpg</code> <code>--im-size</code> Input image size 640 Any positive integer <code>--batch-size</code> Batch size 16 Powers of 2 recommended <code>--device</code> Compute device <code>cuda</code> <code>cuda</code>, <code>cpu</code> <code>--workers</code> Data loading workers 4 0-16 recommended <code>--output-dir</code> Output directory Auto-generated Any valid path"},{"location":"cli/#training-specific-options","title":"Training-Specific Options","text":"Parameter Description Default Options <code>--max-iters</code> Training iterations 3000 Any positive integer <code>--learning-rate</code> Initial learning rate 5e-4 1e-6 to 1e-1 <code>--scheduler</code> LR scheduler type <code>MULTISTEP</code> <code>MULTISTEP</code>, <code>COSINE</code>, <code>POLY</code>, <code>FIXED</code> <code>--optimizer</code> Optimizer type <code>ADAMW</code> <code>ADAMW</code>, <code>SGD</code>, <code>RMSPROP</code> <code>--early-stop</code> Enable early stopping <code>true</code> <code>true</code>, <code>false</code> <code>--patience</code> Early stopping patience 10 Any positive integer <code>--ema-enabled</code> Enable EMA <code>false</code> <code>true</code>, <code>false</code> <code>--amp-enabled</code> Enable mixed precision <code>true</code> <code>true</code>, <code>false</code>"},{"location":"cli/#export-options","title":"Export Options","text":"Parameter Description Default Options <code>--format</code> Export format <code>onnx</code> <code>onnx</code>, <code>torchscript</code> <code>--onnx-opset</code> ONNX opset version 17 11, 13, 16, 17 <code>--overwrite</code> Overwrite existing files <code>false</code> <code>true</code>, <code>false</code>"},{"location":"cli/#prediction-options","title":"Prediction Options","text":"Parameter Description Default Options <code>--conf</code> Confidence threshold 0.5 0.0 to 1.0 <code>--save</code> Save annotated images <code>true</code> <code>true</code>, <code>false</code> <code>--save-json</code> Save JSON results <code>true</code> <code>true</code>, <code>false</code> <code>--save-masks</code> Save segmentation masks <code>true</code> <code>true</code>, <code>false</code> <code>--runtime</code> Inference runtime Auto <code>onnx_cuda32</code>, <code>torchscript_32</code>, <code>onnx_cpu</code>"},{"location":"cli/#dataset-layout","title":"\ud83d\udcca Dataset Layout","text":"Format Description Usage <code>roboflow_coco</code> Roboflow COCO format (Detection, Instance Segmentation) <code>--dataset-layout roboflow_coco</code> <code>roboflow_seg</code> Roboflow segmentation format (Semantic Segmentation) <code>--dataset-layout roboflow_seg</code> <code>catalog</code> Catalog format <code>--dataset-layout catalog</code> <code>cls_folder</code> Classification folder format <code>--dataset-layout cls_folder</code>"},{"location":"cli/#advanced-usage","title":"\ud83d\udd27 Advanced Usage","text":""},{"location":"cli/#environment-variables","title":"Environment Variables","text":"<p>Configure Focoos behavior through environment variables:</p> <pre><code># Set default runtime type\nexport FOCOOS_RUNTIME_TYPE=onnx_cuda32\n\n# Set logging level\nexport FOCOOS_LOG_LEVEL=INFO\n\n# Set API key for Hub access\nexport FOCOOS_API_KEY=your_api_key\n</code></pre>"},{"location":"cli/#programmatic-usage","title":"Programmatic Usage","text":"<p>Use CLI commands programmatically in Python:</p> <pre><code>from focoos.cli.commands import train_command, predict_command, export_command\n\n# Train a model\ntrain_command(\n    model_name=\"fai-detr-m-coco\",\n    dataset_name=\"mydataset.zip\",\n    dataset_layout=\"roboflow_coco\",\n    im_size=640,\n    run_name=\"my_training\",\n    max_iters=5000,\n    batch_size=16\n)\n\n# Run inference\nresults = predict_command(\n    model_name=\"fai-detr-m-coco\",\n    source=\"image.jpg\",\n    conf=0.5,\n    save=True\n)\n\n# Export model\nexport_path = export_command(\n    model_name=\"fai-detr-m-coco\",\n    format=\"onnx\",\n    device=\"cuda\"\n)\n</code></pre>"},{"location":"cli/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"cli/#common-issues","title":"Common Issues","text":"<p>Command not found: <pre><code># Ensure Focoos is installed\npip install focoos\n\n# Check installation\nfocoos version\n</code></pre></p> <p>CUDA out of memory: <pre><code># Reduce batch size\nfocoos train --model fai-detr-m-coco --dataset data.zip --batch-size 8\n\n# Use smaller image size\nfocoos train --model fai-detr-m-coco --dataset data.zip --im-size 480\n</code></pre></p> <p>Dataset not found: <pre><code># Check dataset path\nls -la mydataset.zip\n\n# Use full path\nfocoos train --model fai-detr-m-coco --dataset /full/path/to/mydataset.zip\n</code></pre></p> <p>Model loading errors: <pre><code># Check available models\nfocoos hub models\n\n# Verify model name\nfocoos train --model fai-detr-m-coco --dataset data.zip\n</code></pre></p>"},{"location":"cli/#getting-help","title":"Getting Help","text":"<pre><code># Get general help\nfocoos --help\n\n# Get command-specific help\nfocoos train --help\nfocoos predict --help\nfocoos export --help\n\n# Check system compatibility\nfocoos checks\n\n# View current settings\nfocoos settings\n</code></pre>"},{"location":"cli/#performance-tips","title":"\ud83d\ude80 Performance Tips","text":""},{"location":"cli/#training-optimization","title":"Training Optimization","text":"<ul> <li>Use <code>--amp-enabled</code> for mixed precision training</li> <li>Increase <code>--workers</code> for faster data loading</li> <li>Use <code>--ema-enabled</code> for better model stability</li> <li>Enable <code>--early-stop</code> to prevent overfitting</li> </ul>"},{"location":"cli/#inference-optimization","title":"Inference Optimization","text":"<ul> <li>Use ONNX runtime for faster inference: <code>--runtime onnx_cuda32</code></li> <li>Choose appropriate <code>--im-size</code> for speed vs accuracy trade-off</li> <li>Use <code>--conf</code> threshold to filter low-confidence detections</li> </ul>"},{"location":"cli/#memory-management","title":"Memory Management","text":"<ul> <li>Reduce <code>--batch-size</code> if experiencing OOM errors</li> <li>Use <code>--device cpu</code> for CPU-only processing</li> <li>Consider smaller <code>--im-size</code> for memory-constrained environments</li> </ul>"},{"location":"concepts/","title":"Main Concepts","text":""},{"location":"concepts/#focoosmodel","title":"FocoosModel","text":"<p>The <code>FocoosModel</code> class is the main interface for working with computer vision models in Focoos. It provides high-level methods for training, testing, inference, and model export while handling preprocessing and postprocessing automatically.</p>"},{"location":"concepts/#key-features","title":"Key Features","text":"<ul> <li>End-to-End Inference: Automatic preprocessing and postprocessing</li> <li>Training Support: Built-in training pipeline with distributed training support</li> <li>Model Export: Export to ONNX and TorchScript formats</li> <li>Performance Benchmarking: Built-in latency and throughput measurement</li> <li>Hub Integration: Seamless integration with Focoos Hub for model sharing</li> <li>Multiple Input Formats: Support for PIL Images, NumPy arrays, and PyTorch tensors</li> </ul>"},{"location":"concepts/#loading-strategies","title":"Loading Strategies","text":"<p>The primary method for loading models is using the <code>ModelManager.get()</code> (see <code>ModelManager</code>). It supports multiple loading strategies based on the input parameters. The return value is a Focoos Model.</p> <p>The ModelManager employs different loading strategies based on the input:</p>"},{"location":"concepts/#1-from-focoos-hub","title":"1. From Focoos Hub","text":"<p>The Focoos Hub is a cloud-based model repository where you can store, share, and collaborate on models. This method enables seamless model downloading and caching from the hub using the <code>hub://</code> protocol.</p> <p>When to use: Load models shared by other users, access your own cloud-stored models, or work with models that require authentication.</p> <p>Requirements: Valid API key for private models, internet connection for initial download.</p> <pre><code>from focoos import FocoosHub, ModelManager\n# Loading from hub using hub:// protocol\n# The model is automatically downloaded and cached locally\nhub = FocoosHUB(api_key=\"your_api_key\")\nmodel = ModelManager.get(\"hub://model_reference\", hub=hub)\n\n# Loading with custom configuration override\nmodel = ModelManager.get(\n    \"hub://model_reference\",\n    hub=hub,\n    cache=True,  # Cache for faster subsequent loads\n    config_parameter=your_value # Override single config parameter\n)\n</code></pre>"},{"location":"concepts/#2-from-model-registry","title":"2. From Model Registry","text":"<p>The Model Registry contains curated, pretrained models that are immediately available without download. These models are optimized, tested, and ready for production use across various computer vision tasks.</p> <p>When to use: Start with proven, high-quality pretrained models, baseline experiments, or when you need reliable performance without customization.</p> <p>Requirements: No internet connection needed, models are bundled with the library.</p> <pre><code>from focoos import ModelRegistry, ModelManager\n# Loading pretrained models from registry\n# Object detection model trained on COCO dataset\nmodel = ModelManager.get(\"fai-detr-l-coco\")\n\n# Semantic segmentation model for ADE20K dataset\nmodel = ModelManager.get(\"fai-mf-l-ade\")\n\n# Check available models first\navailable_models = ModelRegistry.list_models()\nprint(\"Available models:\", available_models)\n\n# Get detailed information before loading\nmodel_info = ModelRegistry.get_model_info(\"fai-detr-l-coco\")\nprint(f\"Classes: {len(model_info.classes)}, Task: {model_info.task}\")\n</code></pre> <p>Available Model Categories:</p> <ul> <li>Object Detection: <code>fai-detr-l-coco</code>, <code>fai-detr-m-coco</code>, <code>fai-detr-l-obj365</code></li> <li>Instance Segmentation: <code>fai-mf-l-coco-ins</code>, <code>fai-mf-m-coco-ins</code>, <code>fai-mf-s-coco-ins</code></li> <li>Semantic Segmentation: <code>fai-mf-l-ade</code>, <code>fai-mf-m-ade</code>, <code>bisenetformer-l-ade</code>, <code>bisenetformer-m-ade</code>, <code>bisenetformer-s-ade</code></li> </ul>"},{"location":"concepts/#3-from-local-directory","title":"3. From Local Directory","text":"<p>Load models from your local filesystem, whether they're custom-trained models or models stored in non-standard locations. This method provides maximum flexibility for local development and deployment scenarios.</p> <p>When to use: Load custom-trained models, work with locally stored models, integrate with existing model storage systems, or work in offline environments.</p> <p>Requirements: Valid model directory containing model artifacts (weights, configuration, metadata).</p> <pre><code># Loading with custom models directory\nmodel = ModelManager.get(\"/custom/models/dir/my_model\")\n\n# Expected directory structure:\n# /path/to/local/model/\n# \u251c\u2500\u2500 model_info.json     # Model metadata and configuration\n# \u251c\u2500\u2500 model_final.pth     # Model weights (optional)\n\n# Loading with configuration override\nmodel = ModelManager.get(\n    \"/custom/models/dir/local_model\",\n    arg1=value1,\n    arg2=value2,\n)\n</code></pre>"},{"location":"concepts/#4-from-modelinfo-object","title":"4. From ModelInfo Object","text":"<p>The <code>ModelInfo</code> class represents comprehensive model metadata including architecture specifications, training configuration, class definitions, and performance metrics. This method provides the most programmatic control over model instantiation.</p> <p>When to use: Programmatically construct models, work with dynamic configurations, integrate with custom model management systems, or when you need fine-grained control over model instantiation.</p> <p>Requirements: Properly constructed ModelInfo object with valid configuration parameters.</p> <pre><code>from focoos import ModelInfo, ModelFamily, Task\n# Loading from JSON file\nmodel_info = ModelInfo.from_json(\"path/to/model_info.json\")\nmodel = ModelManager.get(\"any_name\", model_info=model_info)\n\n# Programmatically creating ModelInfo\n\nmodel_info = ModelInfo(\n    name=\"custom_detector\",\n    model_family=ModelFamily.DETR,\n    classes=[\"person\", \"car\", \"bicycle\"],\n    im_size=640,\n    task=Task.DETECTION,\n    config={\n        \"num_classes\": 3,\n        \"backbone_config\": {\"depth\": 50, \"model_type\": \"resnet\"},\n        \"threshold\": 0.5\n    },\n    weights_uri=\"path/to/weights.pth\",  # Optional\n    description=\"Custom object detector\"\n)\n\nmodel = ModelManager.get(\"custom_detector\", model_info=model_info)\n</code></pre>"},{"location":"concepts/#inference","title":"Inference","text":"<p>Performs end-to-end inference on input images with automatic preprocessing and postprocessing. The model accepts input images in various formats including:</p> <ul> <li>PIL Image objects (<code>PIL.Image.Image</code>)</li> <li>NumPy arrays (<code>numpy.ndarray</code>)</li> <li>PyTorch tensors (<code>torch.Tensor</code>)</li> </ul> <p>The input images are automatically preprocessed to the correct size and format required by the model. After inference, the raw model outputs are postprocessed into a standardized <code>FocoosDetections</code> format that provides easy access to:</p> <ul> <li>Detected object classes and confidence scores</li> <li>Bounding box coordinates</li> <li>Segmentation masks (for segmentation models)</li> <li>Additional model-specific outputs</li> </ul> <p>This provides a simple, unified interface for running inference regardless of the underlying model architecture or task.</p> <p>Parameters: - <code>image</code>: Input image in various supported formats (<code>PIL.Image.Image</code>, <code>numpy.ndarray</code>, <code>torch.Tensor</code>, local or remote path) - <code>threshold</code>: detections threshold - <code>annotate</code>: if you want to annotate detections on provided image - <code>**kwargs</code>: Additional arguments passed to postprocessing</p> <p>Returns: <code>FocoosDetections</code> containing detection/segmentation results</p> <p>Example: <pre><code>from PIL import Image\n\n# Load an image\nim_path = \"example.jpg\"\n\n# Run inference\ndetections = model.infer(im_path,threshold=0.5,annotate=True)\n\n# Access results\nfor detection in detections.detections:\n    print(f\"Class: {detection.label}, Confidence: {detection.conf}\")\n    print(f\"Bounding box: {detection.bbox}\")\n\nImage.fromarray(detections.image)\n</code></pre></p>"},{"location":"concepts/#training","title":"Training","text":"<p>Trains the model on provided datasets. The training function accepts:</p> <ul> <li><code>args</code>: Training configuration (TrainerArgs) specifying the main hyperparameters, among which:</li> <li><code>run_name</code>: Name for the training run</li> <li><code>output_dir</code>: Name for the output folder</li> <li><code>num_gpus</code>: Number of GPUs to use (must be &gt;= 1)</li> <li><code>sync_to_hub</code>: For tracking the experiment on the Focoos Hub.   -<code>batch_size</code>, <code>learning_rate</code>, <code>max_iters</code> and other hyperparameters</li> <li><code>data_train</code>: Training dataset (MapDataset)</li> <li><code>data_val</code>: Validation dataset (MapDataset)</li> <li><code>hub</code>: Optional FocoosHUB instance for experiment tracking</li> </ul> <p>The data can be obtained using the AutoDataset helper.</p> <p>After the training is complete, the model will have updated weights and can be used for inference or export. Furthermore, in the <code>output_dir</code> can be found the model metadata (<code>model_info.json</code>) and the PyTorch weights (<code>model_final.pth</code>).</p> <p>Example: <pre><code>from focoos import TrainerArgs\nfrom focoos.data import MapDataset\n\n# Configure training\ntrain_args = TrainerArgs(\n    run_name=\"my_custom_model\",\n    max_iters=5000,\n    batch_size=16,\n    learning_rate=1e-4,\n    num_gpus=2,\n    sync_to_hub=True,\n)\n\n# Train the model\nmodel.train(train_args, train_dataset, val_dataset, hub=hub)\n</code></pre></p> <p>Here you can find an extensive training tutorial.</p>"},{"location":"concepts/#model-export","title":"Model Export","text":"<p>Exports the model to different runtime formats for optimized inference. The main function arguments are:  - <code>runtime_type</code>: specify the target runtime and must be one of the supported (see RuntimeType)  - <code>out_dir</code>: the destination folder for the exported model  - <code>image_size</code>: the target image size, as an optional integer</p> <p>The function returns an <code>InferModel</code> instance for the exported model.</p> <p>Example: <pre><code># Export to ONNX with TensorRT optimization\ninfer_model = model.export(\n    runtime_type=RuntimeType.ONNX_TRT16,\n    out_dir=\"./exported_models\",\n    overwrite=True\n)\n\n# Use exported model for fast inference\nfast_detections = infer_model(image)\n</code></pre></p>"},{"location":"concepts/#infer-model","title":"Infer Model","text":"<p>The <code>InferModel</code> class represents an optimized model for inference, typically created through the export process of a <code>FocoosModel</code>. It provides a streamlined interface focused on fast and efficient inference while maintaining the same input/output format as the original model.</p>"},{"location":"concepts/#key-features_1","title":"Key Features","text":"<ul> <li>Optimized Performance: Models are optimized for the target runtime (e.g., TensorRT, ONNX)</li> <li>Consistent Interface: Uses the same input/output format as FocoosModel</li> <li>Resource Management: Proper cleanup of runtime resources when no longer needed</li> <li>Multiple Input Formats: Support for PIL Images, NumPy arrays, and PyTorch tensors</li> </ul>"},{"location":"concepts/#initialization","title":"Initialization","text":"<p>InferModel instances are typically created through the <code>export()</code> method of a FocoosModel, which handles the model optimization and conversion process. This method allows you to specify the target runtime (see the availables in <code>Runtimetypes</code>) and the output directory for the exported model. The <code>export()</code> method returns an <code>InferModel</code> instance that is optimized for fast and efficient inference.</p> <p>Example: <pre><code># Export the model to ONNX format\ninfer_model = model.export(\n    runtime_type=RuntimeType.TORCHSCRIPT_32,\n    out_dir=\"./exported_models\"\n)\n\n# Use the exported model for inference\nresults = infer_model(input_image)\n</code></pre></p>"},{"location":"concepts/#inference_1","title":"Inference","text":"<p>Performs end-to-end inference on input images with automatic preprocessing and postprocessing on the selected runtime. The model accepts input images in various formats including:</p> <ul> <li>PIL Image objects (<code>PIL.Image.Image</code>)</li> <li>NumPy arrays (<code>numpy.ndarray</code>)</li> <li>PyTorch tensors (<code>torch.Tensor</code>)</li> </ul> <p>The input images are automatically preprocessed to the correct size and format required by the model. After inference, the raw model outputs are postprocessed into a standardized <code>FocoosDetections</code> format that provides easy access to:</p> <ul> <li>Detected object classes and confidence scores</li> <li>Bounding box coordinates</li> <li>Segmentation masks (for segmentation models)</li> <li>Additional model-specific outputs</li> </ul> <p>This provides a simple, unified interface for running inference regardless of the underlying model architecture or task.</p> <p>Parameters: - <code>image</code>: Input image in various supported formats (<code>PIL.Image.Image</code>, <code>numpy.ndarray</code>, <code>torch.Tensor</code>, local or remote path) - <code>threshold</code>: detections threshold - <code>annotate</code>: if you want to annotate detections on provided image - <code>**kwargs</code>: Additional arguments passed to postprocessing</p> <p>Returns: <code>FocoosDetections</code> containing detection/segmentation results</p> <p>Example: <pre><code>from PIL import Image\n\n# Load an image\nimage_path = \"example.jpg\"\n\n# Run inference\ninfer_model = model.export(\n    runtime_type=RuntimeType.TORCHSCRIPT_32,\n    out_dir=\"./exported_models\"\n)\ndetections = infer_model.infer(image_path,threshold=0.5, annotate = True)\n\n# Access results\nfor detection in detections.detections:\n    print(f\"Class: {detection.label}, Confidence: {detection.conf}\")\n    print(f\"Bounding box: {detection.bbox}\")\n</code></pre></p>"},{"location":"inference/","title":"Inference with Focoos Models","text":"<p>Focoos provides a powerful inference framework that makes it easy to deploy and use state-of-the-art computer vision models in production. Whether you're working on object detection, image classification, or other vision tasks, Focoos offers flexible deployment options that adapt to your specific needs.</p> <p></p> <p>Key features of the Focoos inference framework include:</p> <ul> <li>Multiple Deployment Options: Choose between cloud-based inference, local PyTorch deployment, or optimized runtime deployment</li> <li>Easy Model Loading: Seamlessly load models from the Focoos Hub or your local environment</li> <li>Production-Ready Features:<ul> <li>Optimized inference performance</li> <li>Hardware acceleration compatibility</li> <li>Memory-efficient execution</li> </ul> </li> <li>Simple Integration: Easy-to-use APIs that work seamlessly with your existing applications</li> </ul> <p>In the following sections, we'll guide you through the different ways to use Focoos models for inference, from cloud deployment to local optimization.</p>"},{"location":"inference/#there-are-three-ways-to-use-a-model","title":"\ud83c\udfa8 There are three ways to use a model:","text":"<ol> <li>\ud83c\udf0d Remote Inference</li> <li>\ud83d\udd25 Pytorch Inference</li> <li>\ud83d\udd28 Optimized Inference</li> </ol>"},{"location":"inference/#0-optional-connect-to-the-focoos-hub","title":"0. [Optional] Connect to the Focoos Hub","text":"<p>Focoos can be used without having an accont on the Focoos Hub. With it, you will unlock additional functionalities, as we will see below. If you have it, just connect to the HUB. <pre><code>from focoos import FocoosHUB\n\nFOCOOS_API_KEY = os.getenv(\"FOCOOS_API_KEY\")  # write here your API key os set env variable FOCOOS_API_KEY, will be used as default\nhub = FocoosHUB(api_key=FOCOOS_API_KEY)\n</code></pre></p> <p>You can see the models available for you on the platform with an intuitive user interface. However, you can also list them using the Hub functionalities.</p> <pre><code>models = hub.list_remote_models()\n</code></pre>"},{"location":"inference/#1-remote-inference","title":"1. \ud83c\udf0d Remote Inference","text":"<p>In this section, you'll run a model on the Focoos' servers instead of on your machine. The image will be packed and sent on the network to the servers, where it is processed and the results is retured to your machine, all in few milliseconds. If you want an example model, you can try <code>fai-detr-l-obj365</code>.</p> <pre><code>model_ref = \"&lt;YOUR-MODEL-REF&gt;\"\nmodel = hub.get_remote_model(model_ref)\n</code></pre> <p>Using the model is as simple as it could! Just call it with an image.</p> <pre><code>from PIL import Image\nimage = Image.open(\"&lt;PATH-TO-IMAGE&gt;\")\ndetections = model.infer(image)\n</code></pre> <p><code>detections</code> is a FocoosDetections object, containing a list of FocoosDet objects and optionally a dict of information about the latency of the inference. The <code>FocoosDet</code> object contains the following attributes:</p> <ul> <li><code>bbox</code>: Bounding box coordinates in x1y1x2y2 absolute format.</li> <li><code>conf</code>: Confidence score (from 0 to 1).</li> <li><code>cls_id</code>: Class ID (0-indexed).</li> <li><code>label</code>: Label (name of the class).</li> <li><code>mask</code>: Mask (base64 encoded string having origin in the top left corner of bbox and the same width and height of the bbox).</li> <li><code>keypoints</code>: keypoints detected</li> </ul> <p>If you want to visualize the result on the image,just set annotate=true</p> <pre><code>from PIL import Image\ndetections = model.infer(image,annotate=True)\nImage.fromarray(detections.image)\n</code></pre>"},{"location":"inference/#2-pytorch-inference","title":"2. \ud83d\udd25 PyTorch Inference","text":"<p>This section demonstrates how to perform local inference using a plain Pytorch model. We will load a model and then run inference on a sample image.</p> <p>First, let's get a model. We need to use the <code>ModelManager</code> that will take care of instaciating the right model starting from a model reference (for example, the <code>fai-detr-l-obj365</code>). If you want to use a model from the Hub, please remember to add <code>hub://</code> as prefix to the model reference.</p> Pretrained ModelHUB Model <pre><code>from focoos.model_manager import ModelManager\nmodel_name = \"fai-detr-l-obj365\"\n\nmodel = ModelManager.get(model_name)\n</code></pre> <pre><code>from focoos import ModelManager\n\nmodel_ref = \"&lt;YOUR-MODEL-REF&gt;\"\n\n\nmodel = ModelManager.get(f\"hub://{model_ref}\")\n</code></pre> Local Model  <pre><code>from focoos import ModelManager\n\nmodel_path = \"/path/to/model\"\n\n\nmodel = ModelManager.get(model_path)\n</code></pre> <p>Now, again, you can now run the model by simply passing it an image and visualize the results.</p> <pre><code>from focoos import annotate_image\n\ndetections = model.infer(image,annotate=True)\n\nImage.fromarray(detections.image)\n</code></pre> <p><code>detections</code> is a FocoosDetections object.</p> <p>How fast is this model locally? We can compute it's speed by using the benchmark utility.</p> <pre><code>model.benchmark(iterations=10, size=640)\n</code></pre>"},{"location":"inference/#3-optimized-inference","title":"3. \ud83d\udd28 Optimized Inference","text":"<p>As you can see, using the torch model is great, but we can achieve better performance by exporting and running it with a optimized runtime, such as Torchscript, TensorRT, CoreML or the ones available on ONNXRuntime.</p> <p>In the following cells, we will export the previous model for one of these and run it.</p>"},{"location":"inference/#torchscript","title":"Torchscript","text":"<p>We already provide multiple inference runtime, that you can see on the <code>RuntimeTypes</code> enum. Let's select Torchscript as an example.</p> <pre><code>from focoos import RuntimeType\n\nruntime = RuntimeType.TORCHSCRIPT_32\n</code></pre> <p>It's time to export the model. We can use the export method of the models.</p> <pre><code>optimized_model = model.export(runtime_type=runtime, image_size=512)\n</code></pre> <p>Let's visualize the output. As you will see, there are not differences from the model in pure torch.</p> <pre><code>from focoos import annotate_image\n\ndetections = optimized_model(image, annotate = True)\nImage.fromarray(detections.image)\n</code></pre> <p><code>detections</code> is a FocoosDetections object.</p> <p>But, let's see its latency, that should be substantially lower than the pure pytorch model. <pre><code>optimized_model.benchmark(iterations=10)\n</code></pre></p> <p>You can use different runtimes that may fit better your device, such as TensorRT. See the list of available Runtimes at <code>RuntimeTypes</code>. Please note that you need to install the relative packages for onnx and tensorRT for using them.</p>"},{"location":"inference/#onnx-with-tensorrt","title":"ONNX with TensorRT","text":"<pre><code>from focoos import RuntimeType, annotate_image\n\nruntime = RuntimeType.ONNX_TRT16\noptimized_model = model.export(runtime_type=runtime)\n\ndetections = optimized_model(image)\ndisplay(annotate_image(image, detections, task=model.model_info.task, classes=model.model_info.classes))\n\noptimized_model.benchmark(iterations=10, size=640)\n</code></pre>"},{"location":"quantization/","title":"Quantization (Beta)","text":"<p>The quantization of Focoos models is currently in working in progress stage.</p> <p>currently tested and working for classification models.</p>"},{"location":"quantization/#example","title":"Example","text":"<pre><code>from focoos import ModelManager,ASSETS_DIR, MODELS_DIR, RuntimeType\nfrom focoos.infer.quantizer import OnnxQuantizer, QuantizationCfg\nimport os\n\nimage_size = 224  # 224px input size\nmodel_name = \"fai-cls-m-coco\" # you can also take model from focoos hub with \"hub://YOUR_MODEL_REF\"\nim = ASSETS_DIR / \"federer.jpg\"\n\nmodel = ModelManager.get(model_name)\n\nexported_model = model.export(runtime_type=RuntimeType.ONNX_CPU, # optimized for edge or cpu\n    image_size=image_size,\n    dynamic_axes=False,  # quantization need static axes!\n    simplify_onnx=False, # simplify and optimize onnx model graph\n    onnx_opset=18,\n    out_dir=os.path.join(MODELS_DIR, \"my_edge_model\")) # save to models dir\n\n# benchmark onnx model\nexported_model.benchmark(iterations=100)\n\n# test onnx model\n\nresult = exported_model.infer(im,annotate=True)\nImage.fromarray(result.image)\n\n\nquantization_cfg = QuantizationCfg(\n  size = image_size, # input size: must be same as exported model\n  calibration_images_folder = str(ASSETS_DIR), # Calibration images folder: It is strongly recommended\n                                               # to use the dataset validation split on which the model was trained.\n                                               # Here, for example, we will use the assets folder.\n  format=\"QDQ\", # QO (QOperator): All the quantized operators have their own ONNX definitions, like QLinearConv, MatMulInteger etc.\n                # QDQ (Quantize-DeQuantize): inserts DeQuantizeLinear(QuantizeLinear(tensor)) between the original operators to simulate the quantization and dequantization process.\n  per_channel=True,      # Per-channel quantization: each channel has its own scale/zero-point \u2192 more accurate,\n                         # especially for convolutions, at the cost of extra memory and computation.\n  normalize_images=True, # normalize images during preprocessing: some models have normalization outside of model forward\n)\n\nquantizer = OnnxQuantizer(\n    input_model_path=exported_model.model_path,\n    cfg=quantization_cfg\n)\nmodel_path = quantizer.quantize(\n  benchmark=True # benchmark bot fp32 and int8 models\n)\n\nquantized_model = InferModel(model_path, runtime_type=RuntimeType.ONNX_CPU)\n\nres = quantized_model.infer(im,annotate=True)\nImage.fromarray(res.image)\n</code></pre>"},{"location":"setup/","title":"Python SDK Setup \ud83d\udc0d","text":""},{"location":"setup/#install-the-focoos-sdk","title":"Install the Focoos SDK","text":"<p>The Focoos SDK can be installed with different package managers using python 3.10 and above.</p> <p>We recommend using UV (how to install uv) as a package manager and environment manager for a streamlined dependency management experience. however the installation process is the same if you use pip or conda.</p> <p>You can easily create a new virtual environment with UV using the following command: <pre><code>uv venv --python 3.12\nsource .venv/bin/activate\n</code></pre></p> DefaultONNX Runtime (CUDA) ONNX Runtime (Tensorrt) CPU ONNX Runtime <p>The default installation provides compatibility with both CPU and GPU environments, utilizing PyTorch as the default runtime. you can perfom training and inference with PyTorch</p> <pre><code>uv pip install 'focoos @ git+https://github.com/FocoosAI/focoos'\n</code></pre> <p>To perform inference using ONNX Runtime with GPU (CUDA) acceleration Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.22.0. To install cuDNN 9:</p> <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> <pre><code>uv pip install 'focoos[onnx] @ git+https://github.com/FocoosAI/focoos'\n</code></pre> <p>To perform inference using ONNX Runtime with GPU (Tensorrt) acceleration Additional requirements: Ensure that you have CUDA 12 and cuDNN 9 installed, as they are required for onnxruntime version 1.22.0. To install cuDNN 9:</p> <pre><code>apt-get -y install cudnn9-cuda-12\n</code></pre> <p>To perform inference using TensorRT, ensure you have TensorRT version 10.5 installed.</p> <pre><code>uv pip install 'focoos[onnx,tensorrt] @ git+https://github.com/FocoosAI/focoos'\n</code></pre> <pre><code>uv pip install 'focoos[onnx-cpu] @ git+https://github.com/FocoosAI/focoos'\n</code></pre> <p>Note</p> <p>\ud83e\udd16 Multiple Runtimes: You can install multiple extras by running <code>pip install 'focoos[onnx,tensorrt] @ git+https://github.com/FocoosAI/focoos.git'</code>. Note that you can't use <code>onnx-cpu</code> and <code>onnx</code> or <code>tensorrt</code> at the same time.</p> <p>Note</p> <p>\ud83d\udee0\ufe0f Installation Tip: If you want to install a specific version, for example <code>v0.18.0</code>, use:</p> <pre><code>uv pip install 'focoos @ git+https://github.com/FocoosAI/focoos@v0.18.0'\n</code></pre> <p>\ud83d\udccb Check Versions: Visit https://github.com/FocoosAI/focoos/tags for available versions.</p>"},{"location":"setup/#inference-runtime-support","title":"Inference Runtime support","text":"<p>Focoos models support multiple inference runtimes. The library can be used without any extras for training and inference using the PyTorch runtime. Additional extras are only needed if you want to use ONNX or TensorRT runtimes for optimized inference.</p> RuntimeType Extra Runtime Compatible Devices Available ExecutionProvider TORCHSCRIPT_32 - torchscript CPU, NVIDIA GPUs - ONNX_CUDA32 <code>[onnx]</code> onnxruntime GPU NVIDIA GPUs CUDAExecutionProvider ONNX_TRT32 <code>[tensorrt]</code> onnxruntime TRT NVIDIA GPUs (Optimized) CUDAExecutionProvider, TensorrtExecutionProvider ONNX_TRT16 <code>[tensorrt]</code> onnxruntime TRT NVIDIA GPUs (Optimized) CUDAExecutionProvider, TensorrtExecutionProvider ONNX_CPU <code>[onnx-cpu]</code> onnxruntime CPU CPU (x86, ARM), M1, M2, M3 (Apple Silicon) CPUExecutionProvider, CoreMLExecutionProvider, AzureExecutionProvider ONNX_COREML <code>[onnx-cpu]</code> onnxruntime CPU M1, M2, M3 (Apple Silicon) CoreMLExecutionProvider, CPUExecutionProvider"},{"location":"setup/#docker-and-devcontainers","title":"Docker and Devcontainers","text":"<p>For container support, Focoos offers different Docker images:</p> <ul> <li><code>focoos-gpu</code>: Includes ONNX Runtime (CUDA) support</li> <li><code>focoos-tensorrt</code>: Includes ONNX and TensorRT support</li> <li><code>focoos-cpu</code>: only CPU</li> </ul> <p>to use the docker images, you can run the following command:</p> <pre><code>docker build -t focoos-gpu . --target=focoos-gpu\ndocker run -it focoos-gpu\n</code></pre> <p>This repository also includes a devcontainer configuration for each of the above images. You can launch these devcontainers in Visual Studio Code for a seamless development experience.</p>"},{"location":"training/","title":"How to Train a Computer Vision Model with Focoos","text":"<p>Focoos provides a comprehensive training framework that makes it easy to train state-of-the-art computer vision models on your own datasets. Whether you're working on object detection, image classification, or other vision tasks, Focoos offers an intuitive training pipeline that handles everything from data preparation to model optimization.</p> <p></p> <p>Key features of the Focoos training framework include:</p> <ul> <li>Easy Dataset Integration: Seamlessly import and prepare your datasets using Focoos' data loading utilities</li> <li>Flexible Model Architecture: Choose from a variety of pre-built model architectures or customize your own</li> <li>Advanced Training Features:<ul> <li>Mixed precision training for faster training and reduced memory usage</li> <li>Automatic learning rate scheduling</li> <li>Early stopping and model checkpointing</li> <li>Distributed training support</li> </ul> </li> <li>Experiment Tracking: Monitor training progress, visualize metrics, and compare experiments through the Focoos Hub</li> </ul> <p>In the following sections, we'll guide you through the process of training a model with Focoos, from setting up your environment to deploying your trained model.</p>"},{"location":"training/#fine-tune-a-model-in-3-steps","title":"\ud83c\udfa8 Fine-tune a model in 3 steps","text":"<p>In this guide, we will perform the following steps:</p> <ol> <li>\ud83d\udce6 Select dataset</li> <li>\ud83c\udfc3\u200d\u2642\ufe0f Train model</li> <li>\ud83e\uddea Test model</li> </ol>"},{"location":"training/#0-optional-connect-to-the-focoos-hub","title":"0. [Optional] Connect to the Focoos Hub","text":"<p>Focoos can be used without having an accont on the Focoos Hub. With it, you will unlock additional functionalities, as we will see below. If you have it, just connect to the HUB.</p> <pre><code>from focoos import FocoosHUB\n\nFOCOOS_API_KEY = None  # write here your API key\nhub = FocoosHUB(api_key=FOCOOS_API_KEY)\n</code></pre>"},{"location":"training/#1-select-dataset","title":"1. Select dataset","text":"<p>Before starting the training, we need to get a dataset. You can either use a local dataset or you can download one from the hub.</p>"},{"location":"training/#optional-download-the-data-from-the-hub","title":"[Optional] Download the data from the Hub","text":"<p>If you want to download a dataset from the hub, you can use it to directly store it in your local environment. Check the reference of your dataset on the platform and use it in the following cell. If you want to try an example dataset, just use one of the many available on the Focoos Hub.</p> <pre><code>dataset_ref = \"&lt;YOUR-DATASET-REFERENCE&gt;\"\ndataset = hub.get_remote_dataset(dataset_ref)\nprint(dataset)\n\ndataset_path = dataset.download_data()\n</code></pre>"},{"location":"training/#get-the-training-dataset","title":"Get the training dataset","text":"<p>Now that we downloaded the dataset, we can magically \ud83e\ude84 instanciate the dataset using the <code>AutoDataset</code> as will be used in the training. You can optionally specify aumgentations for the training using the <code>DatasetAugmentation</code> dataclass.</p> <pre><code>from focoos.data import AutoDataset,DatasetAugmentations\n\nfrom focoos import DatasetSplitType\n\ntask = dataset.task  # see ports.Task for more information\nlayout = dataset.layout  # see ports.DatasetLayout for more information\nauto_dataset = AutoDataset(dataset_name=dataset_path, task=task, layout=layout)\n\naugs = DatasetAugmentations(resolution=512).get_augmentations()\n\ntrain_dataset = auto_dataset.get_split(augs=augs, split=DatasetSplitType.TRAIN)\nvalid_dataset = auto_dataset.get_split(augs=augs, split=DatasetSplitType.VAL)\n</code></pre>"},{"location":"training/#2-train-the-model","title":"2. Train the Model","text":""},{"location":"training/#instanciate-a-model","title":"Instanciate a model","text":"<p>The first step to personalize your model is to instance a model. You can get a model using the ModelManager by specifying a model name. Optionally, you can also get one of your trained models on the hub. If you want to follow the example, just use <code>fai-detr-m-coco</code> as the model reference.</p> <pre><code>from focoos import ModelManager\n\nmodel_ref = \"&lt;YOUR-MODEL-REF&gt;\"\nmodel = ModelManager.get(\"hub://\" + model_ref, hub=hub)\n</code></pre>"},{"location":"training/#select-the-hyper-parameters","title":"Select the hyper-parameters","text":"<p>The next step is to create a <code>TrainerArgs</code> with the hyper-parameters such as the learning rate, the number of iterations and so on. Optionally, if you are using the hub, you can specify <code>sync_to_hub=True</code> to track the experiment on the Focoos Hub.</p> <pre><code>from focoos import TrainerArgs\n\nargs = TrainerArgs(\n    run_name=f\"{model.name}_{train_dataset.name}\",  # the name of the experiment\n    output_dir=\"./experiments\",  # the folder where the model is saved, DEFAULT  ~/FocoosAI/models\"\n    batch_size=16,  # how many images in each iteration\n    max_iters=500,  # how many iterations lasts the training\n    eval_period=100,  # period after we eval the model on the validation (in iterations)\n    learning_rate=0.0001,  # learning rate\n    weight_decay=0.0001,  # regularization strenght (set it properly to avoid under/over fitting)\n    sync_to_hub=True, # Use this to sync model info, weights and metrics on the platform\n)\n</code></pre>"},{"location":"training/#train-the-model","title":"Train the model","text":"<p>Now we are set up. We can directly call the train function of the model.</p> <pre><code>model.train(args, train_dataset, valid_dataset, hub=hub)\n</code></pre>"},{"location":"training/#3-test-the-model","title":"3. Test the Model","text":"<p>Now that the model is ready, let's see how it behaves.</p> <pre><code>import random\nfrom PIL import Image\nfrom focoos import annotate_image\n\nindex = random.randint(0, len(valid_dataset))\n\nground_truth = valid_dataset.preview(index, use_augmentations=False).save(\"ground_truth.jpg\")\n\nimage = Image.open(valid_dataset[index][\"file_name\"])\noutputs = model.infer(image,annotate=True)\n\nImage.fromarray(detections.image)\n</code></pre>"},{"location":"api/auto_dataset/","title":"AutoDataset","text":""},{"location":"api/auto_dataset/#focoos.data.auto_dataset.AutoDataset","title":"<code>AutoDataset</code>","text":"Source code in <code>focoos/data/auto_dataset.py</code> <pre><code>class AutoDataset:\n    def __init__(\n        self,\n        dataset_name: str,\n        task: Task,\n        layout: DatasetLayout,\n        datasets_dir: str = DATASETS_DIR,\n    ):\n        self.task = task\n        self.layout = layout\n        self.datasets_dir = datasets_dir\n        self.dataset_name = dataset_name\n\n        if self.layout is not DatasetLayout.CATALOG:\n            dataset_path = os.path.join(self.datasets_dir, dataset_name)\n        else:\n            dataset_path = self.datasets_dir\n\n        if dataset_path.endswith(\".zip\") or dataset_path.endswith(\".gz\"):\n            # compressed path: datasets_root_dir/dataset_compressed/{dataset_name}.zip\n            # _dest_path = os.path.join(self.datasets_root_dir, dataset_name.split(\".\")[0])\n            assert not (self.layout == DatasetLayout.CATALOG and not is_inside_sagemaker()), (\n                \"Catalog layout does not support compressed datasets externally to Sagemaker.\"\n            )\n            if self.layout == DatasetLayout.CATALOG:\n                dataset_path = extract_archive(dataset_path)\n                logger.info(f\"Extracted archive: {dataset_path}, {os.listdir(dataset_path)}\")\n            else:\n                dataset_name = dataset_name.split(\".\")[0]\n                _dest_path = os.path.join(self.datasets_dir, dataset_name)\n                dataset_path = extract_archive(dataset_path, _dest_path)\n                logger.info(f\"Extracted archive: {dataset_path}, {os.listdir(dataset_path)}\")\n\n        self.dataset_path = str(dataset_path)\n        self.dataset_name = dataset_name\n        logger.info(\n            f\"\ud83d\udd04 Loading dataset {self.dataset_name}, \ud83d\udcc1 Dataset Path: {self.dataset_path}, \ud83d\uddc2\ufe0f Dataset Layout: {self.layout}\"\n        )\n\n    def _load_split(self, dataset_name: str, split: DatasetSplitType) -&gt; DictDataset:\n        if self.layout == DatasetLayout.CATALOG:\n            return DictDataset.from_catalog(ds_name=dataset_name, split_type=split, root=self.dataset_path)\n        else:\n            ds_root = self.dataset_path\n            if not check_folder_exists(ds_root):\n                raise FileNotFoundError(f\"Dataset {ds_root} not found\")\n            split_path = self._get_split_path(dataset_root=ds_root, split_type=split)\n            if self.layout == DatasetLayout.ROBOFLOW_SEG:\n                return DictDataset.from_roboflow_seg(ds_dir=split_path, task=self.task, split_type=split)\n            elif self.layout == DatasetLayout.CLS_FOLDER:\n                return DictDataset.from_folder(root_dir=split_path, split_type=split)\n            elif self.layout == DatasetLayout.ROBOFLOW_COCO:\n                return DictDataset.from_roboflow_coco(ds_dir=split_path, task=self.task, split_type=split)\n            else:  # Focoos\n                raise NotImplementedError(f\"Dataset layout {self.layout} not implemented\")\n\n    def _load_mapper(\n        self,\n        augs: List[T.Transform],\n        is_validation_split: bool,\n    ) -&gt; DatasetMapper:\n        if self.task == Task.SEMSEG:\n            return SemanticDatasetMapper(\n                image_format=\"RGB\",\n                ignore_label=255,\n                augmentations=augs,\n                is_train=not is_validation_split,\n            )\n        elif self.task == Task.DETECTION:\n            return DetectionDatasetMapper(\n                image_format=\"RGB\",\n                is_train=not is_validation_split,\n                augmentations=augs,\n            )\n        elif self.task == Task.INSTANCE_SEGMENTATION:\n            return DetectionDatasetMapper(\n                image_format=\"RGB\",\n                is_train=not is_validation_split,\n                augmentations=augs,\n                use_instance_mask=True,\n            )\n        elif self.task == Task.CLASSIFICATION:\n            return ClassificationDatasetMapper(\n                image_format=\"RGB\",\n                is_train=not is_validation_split,\n                augmentations=augs,\n            )\n        elif self.task == Task.KEYPOINT:\n            return KeypointDatasetMapper(\n                image_format=\"RGB\",\n                augmentations=augs,\n                is_train=not is_validation_split,\n                # keypoint_hflip_indices=np.array(keypoint_hflip_indices),\n            )\n        else:\n            raise NotImplementedError(f\"Task {self.task} not found in autodataset _load_mapper()\")\n\n    def _get_split_path(self, dataset_root: str, split_type: DatasetSplitType) -&gt; str:\n        if split_type == DatasetSplitType.TRAIN:\n            possible_names = [\"train\", \"training\"]\n            for name in possible_names:\n                split_path = os.path.join(dataset_root, name)\n                if check_folder_exists(split_path):\n                    return split_path\n            raise FileNotFoundError(f\"Train split not found in {dataset_root}\")\n        elif split_type == DatasetSplitType.VAL:\n            possible_names = [\"valid\", \"val\", \"validation\"]\n            for name in possible_names:\n                split_path = os.path.join(dataset_root, name)\n                if check_folder_exists(split_path):\n                    return split_path\n            raise FileNotFoundError(f\"Validation split not found in {dataset_root}\")\n        else:\n            raise ValueError(f\"Invalid split type: {split_type}\")\n\n    def get_split(\n        self,\n        augs: List[T.Transform],\n        split: DatasetSplitType = DatasetSplitType.TRAIN,\n    ) -&gt; MapDataset:\n        \"\"\"\n        Generate a dataset for a given dataset name with optional augmentations.\n\n        Parameters:\n            short_edge_length (int): The length of the shorter edge of the images.\n            max_size (int): The maximum size of the images.\n            extra_augs (List[Transform]): Extra augmentations to apply.\n\n        Returns:\n            MapDataset: A DictDataset with DatasetMapper for training.\n        \"\"\"\n        dict_split = self._load_split(dataset_name=self.dataset_name, split=split)\n        assert dict_split.metadata.num_classes &gt; 0, \"Number of dataset classes must be greater than 0\"\n\n        return MapDataset(\n            dataset=dict_split,\n            mapper=self._load_mapper(\n                augs=augs,\n                is_validation_split=(split == DatasetSplitType.VAL),\n            ),\n        )  # type: ignore\n</code></pre>"},{"location":"api/auto_dataset/#focoos.data.auto_dataset.AutoDataset.get_split","title":"<code>get_split(augs, split=DatasetSplitType.TRAIN)</code>","text":"<p>Generate a dataset for a given dataset name with optional augmentations.</p> <p>Parameters:</p> Name Type Description Default <code>short_edge_length</code> <code>int</code> <p>The length of the shorter edge of the images.</p> required <code>max_size</code> <code>int</code> <p>The maximum size of the images.</p> required <code>extra_augs</code> <code>List[Transform]</code> <p>Extra augmentations to apply.</p> required <p>Returns:</p> Name Type Description <code>MapDataset</code> <code>MapDataset</code> <p>A DictDataset with DatasetMapper for training.</p> Source code in <code>focoos/data/auto_dataset.py</code> <pre><code>def get_split(\n    self,\n    augs: List[T.Transform],\n    split: DatasetSplitType = DatasetSplitType.TRAIN,\n) -&gt; MapDataset:\n    \"\"\"\n    Generate a dataset for a given dataset name with optional augmentations.\n\n    Parameters:\n        short_edge_length (int): The length of the shorter edge of the images.\n        max_size (int): The maximum size of the images.\n        extra_augs (List[Transform]): Extra augmentations to apply.\n\n    Returns:\n        MapDataset: A DictDataset with DatasetMapper for training.\n    \"\"\"\n    dict_split = self._load_split(dataset_name=self.dataset_name, split=split)\n    assert dict_split.metadata.num_classes &gt; 0, \"Number of dataset classes must be greater than 0\"\n\n    return MapDataset(\n        dataset=dict_split,\n        mapper=self._load_mapper(\n            augs=augs,\n            is_validation_split=(split == DatasetSplitType.VAL),\n        ),\n    )  # type: ignore\n</code></pre>"},{"location":"api/auto_dataset/#focoos.data.default_aug.DatasetAugmentations","title":"<code>DatasetAugmentations</code>  <code>dataclass</code>","text":"<p>Configuration class for dataset augmentations.</p> <p>This class defines parameters for various image transformations used in training and validation pipelines for computer vision tasks. It provides a comprehensive set of options for both color and geometric augmentations.</p> <p>Attributes:</p> Name Type Description <code>resolution</code> <code>int</code> <p>Target image size for resizing operations. Range [256, 1024]. Default: 640.</p> <code>color_augmentation</code> <code>float</code> <p>Strenght of color augmentations. Range [0,1]. Default: 0.0.</p> <code>horizontal_flip</code> <code>float</code> <p>Probability of applying horizontal flip. Range [0,1]. Default: 0.0.</p> <code>vertical_flip</code> <code>float</code> <p>Probability of applying vertical flip. Range [0,1]. Default: 0.0.</p> <code>zoom_out</code> <code>float</code> <p>Probability of applying RandomZoomOut. Range [0,1]. Default: 0.0.</p> <code>zoom_out_side</code> <code>float</code> <p>Zoom out side range. Range [1,5]. Default: 4.0.</p> <code>rotation</code> <code>float</code> <p>Probability of applying RandomRotation. 1 equals +/-180 degrees. Range [0,1]. Default: 0.0.</p> <code>square</code> <code>bool</code> <p>Whether to Square the image. Default: False.</p> <code>aspect_ratio</code> <code>float</code> <p>Aspect ratio for resizing (actual scale range is (2 ** -aspect_ratio, 2 ** aspect_ratio). Range [0,1]. Default: 0.0.</p> <code>scale_ratio</code> <code>Optional[float]</code> <p>scale factor for resizing (actual scale range is (2 ** -scale_ratio, 2 ** scale_ratio). Range [0,1]. Default: None.</p> <code>max_size</code> <code>Optional[int]</code> <p>Maximum allowed dimension after resizing. Range [256, sys.maxsize]. Default: sys.maxsize.</p> <code>crop</code> <code>bool</code> <p>Whether to apply RandomCrop. Default: False.</p> <code>crop_size_min</code> <code>Optional[int]</code> <p>Minimum crop size for RandomCrop. Range [256, 1024]. Default: None.</p> <code>crop_size_max</code> <code>Optional[int]</code> <p>Maximum crop size for RandomCrop. Range [256, 1024]. Default: None.</p> Source code in <code>focoos/data/default_aug.py</code> <pre><code>@dataclass\nclass DatasetAugmentations:\n    \"\"\"\n    Configuration class for dataset augmentations.\n\n    This class defines parameters for various image transformations used in training and validation\n    pipelines for computer vision tasks. It provides a comprehensive set of options for both\n    color and geometric augmentations.\n\n    Attributes:\n        resolution (int): Target image size for resizing operations.\n            Range [256, 1024]. Default: 640.\n        ==\n        color_augmentation (float): Strenght of color augmentations.\n            Range [0,1]. Default: 0.0.\n        ==\n        horizontal_flip (float): Probability of applying horizontal flip.\n            Range [0,1]. Default: 0.0.\n        vertical_flip (float): Probability of applying vertical flip.\n            Range [0,1]. Default: 0.0.\n        zoom_out (float): Probability of applying RandomZoomOut.\n            Range [0,1]. Default: 0.0.\n        zoom_out_side (float): Zoom out side range.\n            Range [1,5]. Default: 4.0.\n        rotation (float): Probability of applying RandomRotation. 1 equals +/-180 degrees.\n            Range [0,1]. Default: 0.0.\n        ==\n        square (bool): Whether to Square the image.\n            Default: False.\n        aspect_ratio (float): Aspect ratio for resizing (actual scale range is (2 ** -aspect_ratio, 2 ** aspect_ratio).\n            Range [0,1]. Default: 0.0.\n        scale_ratio (Optional[float]): scale factor for resizing (actual scale range is (2 ** -scale_ratio, 2 ** scale_ratio).\n            Range [0,1]. Default: None.\n        max_size (Optional[int]): Maximum allowed dimension after resizing.\n            Range [256, sys.maxsize]. Default: sys.maxsize.\n        ==\n        crop (bool): Whether to apply RandomCrop.\n            Default: False.\n        crop_size_min (Optional[int]): Minimum crop size for RandomCrop.\n            Range [256, 1024]. Default: None.\n        crop_size_max (Optional[int]): Maximum crop size for RandomCrop.\n            Range [256, 1024]. Default: None.\n    \"\"\"\n\n    # Resolution for resizing\n    resolution: int = 640\n\n    # Color augmentation parameters\n    color_augmentation: float = 0.0\n    color_base_brightness: int = 32\n    color_base_saturation: float = 0.5\n    color_base_contrast: float = 0.5\n    color_base_hue: float = 18\n    # blur: float = 0.0\n    # noise: float = 0.0\n\n    # Geometric augmentation\n    horizontal_flip: float = 0.0\n    vertical_flip: float = 0.0\n    zoom_out: float = 0.0\n    zoom_out_side: float = 4.0\n    rotation: float = 0.0\n    aspect_ratio: float = 0.0\n\n    ## Rescaling\n    square: float = 0.0\n    scale_ratio: float = 0.0\n    max_size: int = 4096\n\n    # Cropping\n    crop: bool = False\n    crop_size: Optional[int] = None\n\n    # TODO: Add more augmentations like:\n    # - GaussianBlur\n    # - RandomNoise\n    # - RandomResizedCrop\n\n    def override(self, args):\n        if not isinstance(args, dict):\n            args = vars(args)\n        for key, value in args.items():\n            if hasattr(self, key) and value is not None:\n                setattr(self, key, value)\n        return self\n\n    def get_augmentations(self, img_format=\"RGB\", task: Optional[Task] = None) -&gt; List[T.Transform]:\n        \"\"\"Generate augmentation pipeline based on configuration.\"\"\"\n        augs = []\n        self.max_size = self.max_size if self.max_size else sys.maxsize\n\n        ### Add color augmentation if configured\n        if self.color_augmentation &gt; 0:\n            brightness_delta = int(self.color_base_brightness * self.color_augmentation)\n            contrast_delta = self.color_base_contrast * self.color_augmentation\n            saturation_delta = self.color_base_saturation * self.color_augmentation\n            hue_delta = int(self.color_base_hue * self.color_augmentation)\n            augs.append(\n                T.ColorAugSSDTransform(\n                    img_format=img_format,\n                    brightness_delta=brightness_delta,\n                    contrast_low=(1 - contrast_delta),\n                    contrast_high=(1 + contrast_delta),\n                    saturation_low=(1 - saturation_delta),\n                    saturation_high=(1 + saturation_delta),\n                    hue_delta=hue_delta,\n                ),\n            )\n\n        ### Add geometric augmentations\n        # Add flipping augmentations if configured\n        if self.horizontal_flip &gt; 0:\n            augs.append(A.RandomFlip(prob=self.horizontal_flip, horizontal=True))\n        if self.vertical_flip &gt; 0:\n            augs.append(A.RandomFlip(prob=self.vertical_flip, horizontal=False, vertical=True))\n\n        # Add zoom out augmentations if configured\n        if self.zoom_out &gt; 0.0:\n            seg_pad_value = 255 if task == Task.SEMSEG else 0\n            augs.append(\n                A.RandomApply(\n                    A.RandomZoomOut(side_range=(1.0, self.zoom_out_side), pad_value=0, seg_pad_value=seg_pad_value),\n                    prob=self.zoom_out,\n                )\n            )\n\n        ### Add AspectRatio augmentations based on configuration\n        if self.square &gt; 0.0:\n            augs.append(A.RandomApply(A.Resize(shape=(self.resolution, self.resolution)), prob=self.square))\n        elif self.aspect_ratio &gt; 0.0:\n            augs.append(A.RandomAspectRatio(aspect_ratio=self.aspect_ratio))\n\n        ### Add Resizing augmentations based on configuration\n        min_scale, max_scale = 2 ** (-self.scale_ratio), 2**self.scale_ratio\n        augs.append(\n            A.ResizeShortestEdge(\n                short_edge_length=[int(x * self.resolution) for x in [min_scale, max_scale]],\n                sample_style=\"range\",\n                max_size=self.max_size,\n            )\n        )\n\n        ### Add rotation augmentations if configured\n        if self.rotation &gt; 0:\n            angle = self.rotation * 180\n            augs.append(A.RandomRotation(angle=(-angle, angle), expand=False))\n\n        # Add cropping if configured\n        if self.crop:\n            crop_range = (self.crop_size or self.resolution, self.crop_size or self.resolution)\n            augs.append(A.RandomCrop(crop_type=\"absolute_range\", crop_size=crop_range))\n\n        return augs\n</code></pre>"},{"location":"api/auto_dataset/#focoos.data.default_aug.DatasetAugmentations.get_augmentations","title":"<code>get_augmentations(img_format='RGB', task=None)</code>","text":"<p>Generate augmentation pipeline based on configuration.</p> Source code in <code>focoos/data/default_aug.py</code> <pre><code>def get_augmentations(self, img_format=\"RGB\", task: Optional[Task] = None) -&gt; List[T.Transform]:\n    \"\"\"Generate augmentation pipeline based on configuration.\"\"\"\n    augs = []\n    self.max_size = self.max_size if self.max_size else sys.maxsize\n\n    ### Add color augmentation if configured\n    if self.color_augmentation &gt; 0:\n        brightness_delta = int(self.color_base_brightness * self.color_augmentation)\n        contrast_delta = self.color_base_contrast * self.color_augmentation\n        saturation_delta = self.color_base_saturation * self.color_augmentation\n        hue_delta = int(self.color_base_hue * self.color_augmentation)\n        augs.append(\n            T.ColorAugSSDTransform(\n                img_format=img_format,\n                brightness_delta=brightness_delta,\n                contrast_low=(1 - contrast_delta),\n                contrast_high=(1 + contrast_delta),\n                saturation_low=(1 - saturation_delta),\n                saturation_high=(1 + saturation_delta),\n                hue_delta=hue_delta,\n            ),\n        )\n\n    ### Add geometric augmentations\n    # Add flipping augmentations if configured\n    if self.horizontal_flip &gt; 0:\n        augs.append(A.RandomFlip(prob=self.horizontal_flip, horizontal=True))\n    if self.vertical_flip &gt; 0:\n        augs.append(A.RandomFlip(prob=self.vertical_flip, horizontal=False, vertical=True))\n\n    # Add zoom out augmentations if configured\n    if self.zoom_out &gt; 0.0:\n        seg_pad_value = 255 if task == Task.SEMSEG else 0\n        augs.append(\n            A.RandomApply(\n                A.RandomZoomOut(side_range=(1.0, self.zoom_out_side), pad_value=0, seg_pad_value=seg_pad_value),\n                prob=self.zoom_out,\n            )\n        )\n\n    ### Add AspectRatio augmentations based on configuration\n    if self.square &gt; 0.0:\n        augs.append(A.RandomApply(A.Resize(shape=(self.resolution, self.resolution)), prob=self.square))\n    elif self.aspect_ratio &gt; 0.0:\n        augs.append(A.RandomAspectRatio(aspect_ratio=self.aspect_ratio))\n\n    ### Add Resizing augmentations based on configuration\n    min_scale, max_scale = 2 ** (-self.scale_ratio), 2**self.scale_ratio\n    augs.append(\n        A.ResizeShortestEdge(\n            short_edge_length=[int(x * self.resolution) for x in [min_scale, max_scale]],\n            sample_style=\"range\",\n            max_size=self.max_size,\n        )\n    )\n\n    ### Add rotation augmentations if configured\n    if self.rotation &gt; 0:\n        angle = self.rotation * 180\n        augs.append(A.RandomRotation(angle=(-angle, angle), expand=False))\n\n    # Add cropping if configured\n    if self.crop:\n        crop_range = (self.crop_size or self.resolution, self.crop_size or self.resolution)\n        augs.append(A.RandomCrop(crop_type=\"absolute_range\", crop_size=crop_range))\n\n    return augs\n</code></pre>"},{"location":"api/base_model/","title":"BaseModelNN","text":""},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN","title":"<code>BaseModelNN</code>","text":"<p>               Bases: <code>ABC</code>, <code>Module</code></p> <p>Abstract base class for neural network models in Focoos.</p> <p>This class provides a common interface for all neural network models, defining abstract methods that must be implemented by concrete model classes. It extends both ABC (Abstract Base Class) and nn.Module from PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelConfig</code> <p>Model configuration containing hyperparameters and settings.</p> required Source code in <code>focoos/models/base_model.py</code> <pre><code>class BaseModelNN(ABC, nn.Module):\n    \"\"\"Abstract base class for neural network models in Focoos.\n\n    This class provides a common interface for all neural network models,\n    defining abstract methods that must be implemented by concrete model classes.\n    It extends both ABC (Abstract Base Class) and nn.Module from PyTorch.\n\n    Args:\n        config: Model configuration containing hyperparameters and settings.\n    \"\"\"\n\n    def __init__(self, config: ModelConfig):\n        \"\"\"Initialize the base model.\n\n        Args:\n            config: Model configuration object containing model parameters\n                and settings.\n        \"\"\"\n        super().__init__()\n\n    @property\n    @abstractmethod\n    def device(self) -&gt; torch.device:\n        \"\"\"Get the device where the model is located.\n\n        Returns:\n            The PyTorch device (CPU or CUDA) where the model parameters\n            are stored.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(\"Device is not implemented for this model.\")\n\n    @property\n    @abstractmethod\n    def dtype(self) -&gt; torch.dtype:\n        \"\"\"Get the data type of the model parameters.\n\n        Returns:\n            The PyTorch data type (e.g., float32, float16) of the model\n            parameters.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(\"Dtype is not implemented for this model.\")\n\n    @abstractmethod\n    def forward(\n        self,\n        inputs: Union[\n            torch.Tensor,\n            np.ndarray,\n            Image.Image,\n            list[Image.Image],\n            list[np.ndarray],\n            list[torch.Tensor],\n            list[DatasetEntry],\n        ],\n    ) -&gt; ModelOutput:\n        \"\"\"Perform forward pass through the model.\n\n        Args:\n            inputs: Input data in various supported formats:\n                - torch.Tensor: Single tensor input\n                - np.ndarray: Single numpy array input\n                - Image.Image: Single PIL Image input\n                - list[Image.Image]: List of PIL Images\n                - list[np.ndarray]: List of numpy arrays\n                - list[torch.Tensor]: List of tensors\n                - list[DatasetEntry]: List of dataset entries\n\n        Returns:\n            Model output containing predictions and any additional metadata.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(\"Forward is not implemented for this model.\")\n\n    def load_state_dict(self, checkpoint_state_dict: dict, strict: bool = True) -&gt; IncompatibleKeys:\n        \"\"\"Load model state dictionary from checkpoint with preprocessing.\n\n        This method handles common issues when loading checkpoints:\n        - Removes \"module.\" prefix from DataParallel/DistributedDataParallel models\n        - Handles shape mismatches by removing incompatible parameters\n        - Logs incompatible keys for debugging\n\n        Args:\n            checkpoint_state_dict: Dictionary containing model parameters from\n                a saved checkpoint.\n            strict: Whether to strictly enforce that the keys in checkpoint_state_dict\n                match the keys returned by this module's state_dict() function.\n                Defaults to True.\n\n        Returns:\n            IncompatibleKeys object containing information about missing keys,\n            unexpected keys, and parameters with incorrect shapes.\n        \"\"\"\n        # if the state_dict comes from a model that was wrapped in a\n        # DataParallel or DistributedDataParallel during serialization,\n        # remove the \"module\" prefix before performing the matching.\n        strip_prefix_if_present(checkpoint_state_dict, \"module.\")\n\n        # workaround https://github.com/pytorch/pytorch/issues/24139\n        model_state_dict = self.state_dict()\n        incorrect_shapes = []\n        for k in list(checkpoint_state_dict.keys()):\n            if k in model_state_dict:\n                model_param = model_state_dict[k]\n                shape_model = tuple(model_param.shape)\n                shape_checkpoint = tuple(checkpoint_state_dict[k].shape)\n                if shape_model != shape_checkpoint:\n                    incorrect_shapes.append((k, shape_checkpoint, shape_model))\n                    checkpoint_state_dict.pop(k)\n\n        incompatible = super().load_state_dict(checkpoint_state_dict, strict=strict)\n        incompatible = IncompatibleKeys(\n            missing_keys=incompatible.missing_keys,\n            unexpected_keys=incompatible.unexpected_keys,\n            incorrect_shapes=incorrect_shapes,\n        )\n\n        incompatible.log_incompatible_keys()\n\n        return incompatible\n\n    def benchmark(self, iterations: int = 50, size: Tuple[int, int] = (640, 640)) -&gt; LatencyMetrics:\n        \"\"\"Benchmark model inference latency and throughput.\n\n        Performs multiple inference runs on random data to measure model\n        performance metrics including FPS, mean latency, and latency statistics.\n        Uses CUDA events for precise timing when running on GPU.\n\n        Args:\n            iterations: Number of inference runs to perform for benchmarking.\n                Defaults to 50.\n            size: Input image size as (height, width) tuple. Defaults to (640, 640).\n\n        Returns:\n            LatencyMetrics object containing:\n                - fps: Frames per second (throughput)\n                - engine: Hardware/framework used for inference\n                - mean: Mean inference time in milliseconds\n                - max: Maximum inference time in milliseconds\n                - min: Minimum inference time in milliseconds\n                - std: Standard deviation of inference times\n                - im_size: Input image size\n                - device: Device used for inference\n\n        Note:\n            This method assumes the model is running on CUDA for timing.\n            Input data is randomly generated for benchmarking purposes.\n        \"\"\"\n        if self.device.type == \"cpu\":\n            device_name = get_cpu_name()\n        else:\n            device_name = get_device_name()\n\n        logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device_name} ({self.device}), size: {size}x{size}..\")\n        # warmup\n        data = 128 * torch.randn(1, 3, size[0], size[1]).to(self.device)\n        durations = []\n        for _ in range(iterations):\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            start.record(stream=torch.cuda.Stream())\n            _ = self(data)\n            end.record(stream=torch.cuda.Stream())\n            torch.cuda.synchronize()\n            durations.append(start.elapsed_time(end))\n\n        durations = np.array(durations)\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean()),\n            engine=f\"torch.{self.device}\",\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n            device=device_name,\n        )\n        logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n\n    def switch_to_export(self, test_cfg: Optional[dict] = None, device: str = \"cuda\"):\n        pass\n</code></pre>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.device","title":"<code>device</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the device where the model is located.</p> <p>Returns:</p> Type Description <code>device</code> <p>The PyTorch device (CPU or CUDA) where the model parameters</p> <code>device</code> <p>are stored.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.dtype","title":"<code>dtype</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the data type of the model parameters.</p> <p>Returns:</p> Type Description <code>dtype</code> <p>The PyTorch data type (e.g., float32, float16) of the model</p> <code>dtype</code> <p>parameters.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the base model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelConfig</code> <p>Model configuration object containing model parameters and settings.</p> required Source code in <code>focoos/models/base_model.py</code> <pre><code>def __init__(self, config: ModelConfig):\n    \"\"\"Initialize the base model.\n\n    Args:\n        config: Model configuration object containing model parameters\n            and settings.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.benchmark","title":"<code>benchmark(iterations=50, size=(640, 640))</code>","text":"<p>Benchmark model inference latency and throughput.</p> <p>Performs multiple inference runs on random data to measure model performance metrics including FPS, mean latency, and latency statistics. Uses CUDA events for precise timing when running on GPU.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of inference runs to perform for benchmarking. Defaults to 50.</p> <code>50</code> <code>size</code> <code>Tuple[int, int]</code> <p>Input image size as (height, width) tuple. Defaults to (640, 640).</p> <code>(640, 640)</code> <p>Returns:</p> Type Description <code>LatencyMetrics</code> <p>LatencyMetrics object containing: - fps: Frames per second (throughput) - engine: Hardware/framework used for inference - mean: Mean inference time in milliseconds - max: Maximum inference time in milliseconds - min: Minimum inference time in milliseconds - std: Standard deviation of inference times - im_size: Input image size - device: Device used for inference</p> Note <p>This method assumes the model is running on CUDA for timing. Input data is randomly generated for benchmarking purposes.</p> Source code in <code>focoos/models/base_model.py</code> <pre><code>def benchmark(self, iterations: int = 50, size: Tuple[int, int] = (640, 640)) -&gt; LatencyMetrics:\n    \"\"\"Benchmark model inference latency and throughput.\n\n    Performs multiple inference runs on random data to measure model\n    performance metrics including FPS, mean latency, and latency statistics.\n    Uses CUDA events for precise timing when running on GPU.\n\n    Args:\n        iterations: Number of inference runs to perform for benchmarking.\n            Defaults to 50.\n        size: Input image size as (height, width) tuple. Defaults to (640, 640).\n\n    Returns:\n        LatencyMetrics object containing:\n            - fps: Frames per second (throughput)\n            - engine: Hardware/framework used for inference\n            - mean: Mean inference time in milliseconds\n            - max: Maximum inference time in milliseconds\n            - min: Minimum inference time in milliseconds\n            - std: Standard deviation of inference times\n            - im_size: Input image size\n            - device: Device used for inference\n\n    Note:\n        This method assumes the model is running on CUDA for timing.\n        Input data is randomly generated for benchmarking purposes.\n    \"\"\"\n    if self.device.type == \"cpu\":\n        device_name = get_cpu_name()\n    else:\n        device_name = get_device_name()\n\n    logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device_name} ({self.device}), size: {size}x{size}..\")\n    # warmup\n    data = 128 * torch.randn(1, 3, size[0], size[1]).to(self.device)\n    durations = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record(stream=torch.cuda.Stream())\n        _ = self(data)\n        end.record(stream=torch.cuda.Stream())\n        torch.cuda.synchronize()\n        durations.append(start.elapsed_time(end))\n\n    durations = np.array(durations)\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean()),\n        engine=f\"torch.{self.device}\",\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n        device=device_name,\n    )\n    logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.forward","title":"<code>forward(inputs)</code>  <code>abstractmethod</code>","text":"<p>Perform forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor], list[DatasetEntry]]</code> <p>Input data in various supported formats: - torch.Tensor: Single tensor input - np.ndarray: Single numpy array input - Image.Image: Single PIL Image input - list[Image.Image]: List of PIL Images - list[np.ndarray]: List of numpy arrays - list[torch.Tensor]: List of tensors - list[DatasetEntry]: List of dataset entries</p> required <p>Returns:</p> Type Description <code>ModelOutput</code> <p>Model output containing predictions and any additional metadata.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>focoos/models/base_model.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    inputs: Union[\n        torch.Tensor,\n        np.ndarray,\n        Image.Image,\n        list[Image.Image],\n        list[np.ndarray],\n        list[torch.Tensor],\n        list[DatasetEntry],\n    ],\n) -&gt; ModelOutput:\n    \"\"\"Perform forward pass through the model.\n\n    Args:\n        inputs: Input data in various supported formats:\n            - torch.Tensor: Single tensor input\n            - np.ndarray: Single numpy array input\n            - Image.Image: Single PIL Image input\n            - list[Image.Image]: List of PIL Images\n            - list[np.ndarray]: List of numpy arrays\n            - list[torch.Tensor]: List of tensors\n            - list[DatasetEntry]: List of dataset entries\n\n    Returns:\n        Model output containing predictions and any additional metadata.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(\"Forward is not implemented for this model.\")\n</code></pre>"},{"location":"api/base_model/#focoos.models.base_model.BaseModelNN.load_state_dict","title":"<code>load_state_dict(checkpoint_state_dict, strict=True)</code>","text":"<p>Load model state dictionary from checkpoint with preprocessing.</p> <p>This method handles common issues when loading checkpoints: - Removes \"module.\" prefix from DataParallel/DistributedDataParallel models - Handles shape mismatches by removing incompatible parameters - Logs incompatible keys for debugging</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_state_dict</code> <code>dict</code> <p>Dictionary containing model parameters from a saved checkpoint.</p> required <code>strict</code> <code>bool</code> <p>Whether to strictly enforce that the keys in checkpoint_state_dict match the keys returned by this module's state_dict() function. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>IncompatibleKeys</code> <p>IncompatibleKeys object containing information about missing keys,</p> <code>IncompatibleKeys</code> <p>unexpected keys, and parameters with incorrect shapes.</p> Source code in <code>focoos/models/base_model.py</code> <pre><code>def load_state_dict(self, checkpoint_state_dict: dict, strict: bool = True) -&gt; IncompatibleKeys:\n    \"\"\"Load model state dictionary from checkpoint with preprocessing.\n\n    This method handles common issues when loading checkpoints:\n    - Removes \"module.\" prefix from DataParallel/DistributedDataParallel models\n    - Handles shape mismatches by removing incompatible parameters\n    - Logs incompatible keys for debugging\n\n    Args:\n        checkpoint_state_dict: Dictionary containing model parameters from\n            a saved checkpoint.\n        strict: Whether to strictly enforce that the keys in checkpoint_state_dict\n            match the keys returned by this module's state_dict() function.\n            Defaults to True.\n\n    Returns:\n        IncompatibleKeys object containing information about missing keys,\n        unexpected keys, and parameters with incorrect shapes.\n    \"\"\"\n    # if the state_dict comes from a model that was wrapped in a\n    # DataParallel or DistributedDataParallel during serialization,\n    # remove the \"module\" prefix before performing the matching.\n    strip_prefix_if_present(checkpoint_state_dict, \"module.\")\n\n    # workaround https://github.com/pytorch/pytorch/issues/24139\n    model_state_dict = self.state_dict()\n    incorrect_shapes = []\n    for k in list(checkpoint_state_dict.keys()):\n        if k in model_state_dict:\n            model_param = model_state_dict[k]\n            shape_model = tuple(model_param.shape)\n            shape_checkpoint = tuple(checkpoint_state_dict[k].shape)\n            if shape_model != shape_checkpoint:\n                incorrect_shapes.append((k, shape_checkpoint, shape_model))\n                checkpoint_state_dict.pop(k)\n\n    incompatible = super().load_state_dict(checkpoint_state_dict, strict=strict)\n    incompatible = IncompatibleKeys(\n        missing_keys=incompatible.missing_keys,\n        unexpected_keys=incompatible.unexpected_keys,\n        incorrect_shapes=incorrect_shapes,\n    )\n\n    incompatible.log_incompatible_keys()\n\n    return incompatible\n</code></pre>"},{"location":"api/cli/","title":"cli","text":"<p>Focoos Command Line Interface.</p> <p>The Focoos CLI provides both Typer-style commands and a streamlined interface for computer vision tasks.</p> <p>This module contains the main CLI application with commands for training, validation, prediction, export, benchmarking, and system utilities.</p> Usage <pre><code>focoos COMMAND [OPTIONS]\n</code></pre> Available Commands <ul> <li><code>train</code>: Train a model on a dataset</li> <li><code>val</code>: Validate a model on a dataset</li> <li><code>predict</code>: Run inference on images</li> <li><code>export</code>: Export a model to different formats</li> <li><code>benchmark</code>: Benchmark model performance</li> <li><code>version</code>: Show Focoos version information</li> <li><code>checks</code>: Run system checks and display system information</li> <li><code>settings</code>: Show current Focoos configuration settings</li> <li><code>hub</code>: Focoos Hub commands</li> </ul> <p>Examples:</p> <p>Training a model: <pre><code>focoos train --model fai-detr-m-coco --dataset mydataset.zip --im-size 640\n</code></pre></p> <p>Running inference: <pre><code>focoos predict --model fai-detr-m-coco --source image.jpg --im-size 640\n</code></pre></p> <p>Validating a model: <pre><code>focoos val --model fai-detr-m-coco --dataset mydataset.zip --im-size 640\n</code></pre></p> <p>Exporting a model: <pre><code>focoos export --model fai-detr-m-coco --format onnx --im-size 640\n</code></pre></p> <p>Benchmarking performance: <pre><code>focoos benchmark --model fai-detr-m-coco --iterations 100 --device cuda\n</code></pre></p> <p>System information: <pre><code>focoos checks\nfocoos settings\nfocoos version\n</code></pre></p> <p>Focoos Hub commands: <pre><code>focoos hub\nfocoos hub models\nfocoos hub datasets\nfocoos hub dataset download --ref my-dataset --path ./data\nfocoos hub dataset upload --ref my-dataset --path ./data.zip\n</code></pre></p> Note <p>For command-specific help, use: <code>focoos COMMAND --help</code></p> See Also <ul> <li>Training Guide</li> <li>Inference Guide</li> <li>Focoos Hub</li> </ul>"},{"location":"api/cli/#focoos.cli.cli.benchmark","title":"<code>benchmark(model, im_size=None, iterations=None, device='cuda')</code>","text":"<p>Benchmark model performance with detailed metrics.</p> <p>This command performs comprehensive performance benchmarking of a model, measuring inference speed, memory usage, and throughput across multiple iterations. Provides detailed statistics including mean, median, and percentile measurements for performance analysis.</p> Benchmark Metrics <ul> <li>Inference Time: Per-image processing time (ms)</li> <li>Throughput: Frames per second (FPS)</li> <li>Memory Usage: Peak GPU/CPU memory consumption</li> <li>Latency Statistics: P50, P95, P99 percentiles</li> <li>Model Loading Time: Initialization overhead</li> <li>Warmup Performance: Cold vs. warm inference speeds</li> </ul> Performance Factors <ul> <li>Image Size: Larger images \u2192 slower inference</li> <li>Model Complexity: More parameters \u2192 higher latency</li> <li>Device Type: GPU vs. CPU performance differences</li> <li>Batch Size: Single vs. batch inference comparison</li> <li>Runtime Backend: ONNX vs. PyTorch performance</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model or path to model file for benchmarking. Can be specified in several ways: - Pretrained model: Simple model name like 'fai-detr-m-coco' - Hub model: Format 'hub://' for models from Focoos Hub - Local model: Model name with --models-dir pointing to custom directory - Default directory model: Model name for models in default Focoos directory required <code>im_size</code> <code>Optional[int]</code> <p>Input image size for benchmarking. If not specified, uses the model's default input size. Larger sizes typically result in slower inference but may improve accuracy.</p> <code>None</code> <code>iterations</code> <code>Optional[int]</code> <p>Number of benchmark iterations to run. If not specified, uses a default number of iterations suitable for reliable statistics. More iterations provide more accurate timing measurements.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for benchmarking ('cuda' or 'cpu'). Defaults to 'cuda'. GPU benchmarking typically shows better performance but requires CUDA availability.</p> <code>'cuda'</code> <p>Examples:</p> <p>Basic benchmarking with pretrained model: <pre><code>focoos benchmark --model fai-detr-m-coco\n</code></pre></p> <p>Benchmark model from Focoos Hub: <pre><code>focoos benchmark --model hub://&lt;model-ref&gt; --iterations 100\n</code></pre></p> <p>Benchmark local model from custom directory: <pre><code>focoos benchmark --model my-model --models-dir ./custom_models --im-size 640\n</code></pre></p> <p>Benchmark local model from default Focoos directory: <pre><code>focoos benchmark --model my-exported-model --iterations 200\n</code></pre></p> <p>Benchmark with specific parameters: <pre><code>focoos benchmark --model fai-detr-m-coco --im-size 800 --iterations 100\n</code></pre></p> <p>CPU benchmarking: <pre><code>focoos benchmark --model fai-detr-m-coco --device cpu --iterations 50\n</code></pre></p> <p>Comprehensive benchmark suite: <pre><code># Test different image sizes\nfocoos benchmark --model fai-detr-m-coco --im-size 416\nfocoos benchmark --model fai-detr-m-coco --im-size 640\nfocoos benchmark --model fai-detr-m-coco --im-size 800\n</code></pre></p> Output <p>The command prints benchmark results to the console:</p> <pre><code>Benchmark Results:\n==================\nModel: fai-detr-m-coco\nDevice: CUDA (GeForce RTX 3080)\nImage Size: 640x640\nIterations: 100\n\nPerformance Metrics:\n- Average Inference Time: 12.5ms\n- Throughput (FPS): 80.0\n- P50 Latency: 11.8ms\n- P95 Latency: 15.2ms\n- P99 Latency: 18.1ms\n- Peak GPU Memory: 2.1GB\n- Model Loading Time: 1.2s\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If device parameter is not valid.</p> <code>FileNotFoundError</code> <p>If model file is not found.</p> <code>RuntimeError</code> <p>If device is unavailable or out of memory.</p> <code>ImportError</code> <p>If required runtime dependencies are missing.</p> Note <p>Benchmark results can vary based on system load, thermal throttling, and hardware specifications. For consistent results, ensure the system is not under heavy load during benchmarking. Results are automatically saved to a benchmark report file for later analysis.</p> See Also <ul> <li><code>focoos predict</code>: For actual inference</li> <li><code>focoos export</code>: For optimized model formats</li> </ul> Source code in <code>focoos/cli/cli.py</code> <pre><code>@app.command(\"benchmark\")\ndef benchmark(\n    model: Annotated[str, typer.Option(help=\"Model name or path (required)\")],\n    im_size: Annotated[Optional[int], typer.Option(help=\"Image size for benchmarking\")] = None,\n    iterations: Annotated[Optional[int], typer.Option(help=\"Number of benchmark iterations\")] = None,\n    device: Annotated[str, typer.Option(help=\"Device for benchmarking (cuda or cpu)\")] = \"cuda\",\n):\n    \"\"\"Benchmark model performance with detailed metrics.\n\n    This command performs comprehensive performance benchmarking of a model,\n    measuring inference speed, memory usage, and throughput across multiple\n    iterations. Provides detailed statistics including mean, median, and\n    percentile measurements for performance analysis.\n\n    Benchmark Metrics:\n        - **Inference Time**: Per-image processing time (ms)\n        - **Throughput**: Frames per second (FPS)\n        - **Memory Usage**: Peak GPU/CPU memory consumption\n        - **Latency Statistics**: P50, P95, P99 percentiles\n        - **Model Loading Time**: Initialization overhead\n        - **Warmup Performance**: Cold vs. warm inference speeds\n\n    Performance Factors:\n        - **Image Size**: Larger images \u2192 slower inference\n        - **Model Complexity**: More parameters \u2192 higher latency\n        - **Device Type**: GPU vs. CPU performance differences\n        - **Batch Size**: Single vs. batch inference comparison\n        - **Runtime Backend**: ONNX vs. PyTorch performance\n\n    Args:\n        model (str): Name of the model or path to model file for benchmarking.\n            Can be specified in several ways:\n            - **Pretrained model**: Simple model name like 'fai-detr-m-coco'\n            - **Hub model**: Format 'hub://&lt;model-ref&gt;' for models from Focoos Hub\n            - **Local model**: Model name with --models-dir pointing to custom directory\n            - **Default directory model**: Model name for models in default Focoos directory\n        im_size (Optional[int]): Input image size for benchmarking. If not specified,\n            uses the model's default input size. Larger sizes typically\n            result in slower inference but may improve accuracy.\n        iterations (Optional[int]): Number of benchmark iterations to run. If not specified,\n            uses a default number of iterations suitable for reliable statistics.\n            More iterations provide more accurate timing measurements.\n        device (str): Device to use for benchmarking ('cuda' or 'cpu'). Defaults to 'cuda'.\n            GPU benchmarking typically shows better performance but requires CUDA availability.\n\n    Examples:\n        Basic benchmarking with pretrained model:\n        ```bash\n        focoos benchmark --model fai-detr-m-coco\n        ```\n\n        Benchmark model from Focoos Hub:\n        ```bash\n        focoos benchmark --model hub://&lt;model-ref&gt; --iterations 100\n        ```\n\n        Benchmark local model from custom directory:\n        ```bash\n        focoos benchmark --model my-model --models-dir ./custom_models --im-size 640\n        ```\n\n        Benchmark local model from default Focoos directory:\n        ```bash\n        focoos benchmark --model my-exported-model --iterations 200\n        ```\n\n        Benchmark with specific parameters:\n        ```bash\n        focoos benchmark --model fai-detr-m-coco --im-size 800 --iterations 100\n        ```\n\n        CPU benchmarking:\n        ```bash\n        focoos benchmark --model fai-detr-m-coco --device cpu --iterations 50\n        ```\n\n        Comprehensive benchmark suite:\n        ```bash\n        # Test different image sizes\n        focoos benchmark --model fai-detr-m-coco --im-size 416\n        focoos benchmark --model fai-detr-m-coco --im-size 640\n        focoos benchmark --model fai-detr-m-coco --im-size 800\n        ```\n\n    Output:\n        The command prints benchmark results to the console:\n\n        ```\n        Benchmark Results:\n        ==================\n        Model: fai-detr-m-coco\n        Device: CUDA (GeForce RTX 3080)\n        Image Size: 640x640\n        Iterations: 100\n\n        Performance Metrics:\n        - Average Inference Time: 12.5ms\n        - Throughput (FPS): 80.0\n        - P50 Latency: 11.8ms\n        - P95 Latency: 15.2ms\n        - P99 Latency: 18.1ms\n        - Peak GPU Memory: 2.1GB\n        - Model Loading Time: 1.2s\n        ```\n\n    Raises:\n        AssertionError: If device parameter is not valid.\n        FileNotFoundError: If model file is not found.\n        RuntimeError: If device is unavailable or out of memory.\n        ImportError: If required runtime dependencies are missing.\n\n    Note:\n        Benchmark results can vary based on system load, thermal throttling,\n        and hardware specifications. For consistent results, ensure the system\n        is not under heavy load during benchmarking. Results are automatically\n        saved to a benchmark report file for later analysis.\n\n    See Also:\n        - [`focoos predict`][focoos.cli.cli.predict]: For actual inference\n        - [`focoos export`][focoos.cli.cli.export]: For optimized model formats\n    \"\"\"\n    typer.echo(f\"\ud83d\udce6 Starting benchmark - Model: {model}, Iterations: {iterations}, Size: {im_size}, Device: {device}\")\n    try:\n        validated_device = cast(DeviceType, device)\n        assert device in get_args(DeviceType)\n        benchmark_command(\n            model_name=model,\n            iterations=iterations,\n            im_size=im_size,\n            device=validated_device,\n        )\n    except Exception as e:\n        typer.echo(f\"\u274c Benchmark failed: {e}\")\n</code></pre>"},{"location":"api/cli/#focoos.cli.cli.checks","title":"<code>checks()</code>","text":"<p>Run system checks and display system information.</p> <p>This command performs comprehensive system checks including hardware information, GPU availability, dependencies, and other system-level diagnostics relevant to Focoos operation.</p> The system checks include <ul> <li>Hardware information (CPU, memory, GPU)</li> <li>CUDA availability and version</li> <li>Python environment details</li> <li>Focoos dependencies status</li> <li>System compatibility verification</li> </ul> <p>Examples:</p> <pre><code>focoos checks\n</code></pre> <p>Output: <pre><code>Running system checks...\n\nSystem Information:\n==================\nOS: Linux 5.4.0-74-generic\nPython: 3.8.10\nCUDA: 11.2.2\nGPU: NVIDIA GeForce RTX 3080\nMemory: 32GB\n</code></pre></p> <p>Raises:</p> Type Description <code>Exit</code> <p>If system checks fail or encounter errors.</p> See Also <ul> <li><code>focoos version</code>: For version info only</li> <li><code>focoos settings</code>: For configuration details</li> </ul> Source code in <code>focoos/cli/cli.py</code> <pre><code>@app.command(\"checks\")\ndef checks():\n    \"\"\"Run system checks and display system information.\n\n    This command performs comprehensive system checks including hardware\n    information, GPU availability, dependencies, and other system-level\n    diagnostics relevant to Focoos operation.\n\n    The system checks include:\n        - Hardware information (CPU, memory, GPU)\n        - CUDA availability and version\n        - Python environment details\n        - Focoos dependencies status\n        - System compatibility verification\n\n    Examples:\n        ```bash\n        focoos checks\n        ```\n\n        Output:\n        ```\n        Running system checks...\n\n        System Information:\n        ==================\n        OS: Linux 5.4.0-74-generic\n        Python: 3.8.10\n        CUDA: 11.2.2\n        GPU: NVIDIA GeForce RTX 3080\n        Memory: 32GB\n        ```\n\n    Raises:\n        typer.Exit: If system checks fail or encounter errors.\n\n    See Also:\n        - [`focoos version`][focoos.cli.cli.version]: For version info only\n        - [`focoos settings`][focoos.cli.cli.settings]: For configuration details\n    \"\"\"\n    typer.echo(\"Running system checks...\")\n    try:\n        from focoos.utils.system import get_system_info\n\n        system_info = get_system_info()\n        system_info.pprint()\n    except Exception as e:\n        logger.error(f\"Error running checks: {e}\")\n        raise typer.Exit(1)\n</code></pre>"},{"location":"api/cli/#focoos.cli.cli.export","title":"<code>export(model, format=ExportFormat.ONNX, output_dir=None, device='cuda', onnx_opset=17, im_size=640, overwrite=False)</code>","text":"<p>Export a trained model to various deployment formats.</p> <p>This command exports a Focoos model to different formats suitable for deployment in various environments. Currently supports ONNX and TorchScript formats for flexible deployment across different platforms and frameworks.</p> Supported Export Formats <ul> <li>ONNX: Cross-platform neural network format for interoperability</li> <li>TorchScript: PyTorch's native serialization format for production</li> </ul> Platform Compatibility Format Linux Windows macOS Mobile Edge Description ONNX \u2705 \u2705 \u2705 \u2705 \u2705 Universal format TorchScript \u2705 \u2705 \u2705 \u2705 \u2705 PyTorch native <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the Focoos model to export (e.g., 'fai-detr-m-coco'). Can be specified in several ways: - Pretrained model: Simple model name like 'fai-detr-m-coco' - Hub model: Format 'hub://' for models from Focoos Hub - Local model: Model name or model directory path (e.g., 'my-model' or '/path/to/my-model') - Default directory model: Model name for models in default Focoos directory required <code>format</code> <code>Optional[ExportFormat]</code> <p>Target export format. Defaults to ONNX. Available formats: 'onnx', 'torchscript'.</p> <code>ONNX</code> <code>output_dir</code> <code>Optional[str]</code> <p>Directory to save the exported model files. If not specified, uses a default export directory.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for export process ('cuda' or 'cpu'). Defaults to 'cuda'.</p> <code>'cuda'</code> <code>onnx_opset</code> <code>Optional[int]</code> <p>ONNX opset version for ONNX exports. Defaults to 17. Higher versions support more operations but may have compatibility issues. Only applies to ONNX format exports.</p> <code>17</code> <code>im_size</code> <code>Optional[int]</code> <p>Input image size for the exported model. Defaults to 640. This determines the input tensor shape for the exported model.</p> <code>640</code> <code>overwrite</code> <code>Optional[bool]</code> <p>Whether to overwrite existing exported files. Defaults to False. If False, export will fail if output files already exist.</p> <code>False</code> <p>Examples:</p> <p>Basic ONNX export with pretrained model: <pre><code>focoos export --model fai-detr-m-coco\n</code></pre></p> <p>Export model from Focoos Hub: <pre><code>focoos export --model hub://&lt;model-ref&gt; --format onnx\n</code></pre></p> <p>Export local model from custom directory: <pre><code>focoos export --model my-fine-tuned-model --models-dir ./trained_models\n</code></pre></p> <p>Export local model from default Focoos directory: <pre><code>focoos export --model my-checkpoint-model --format torchscript\n</code></pre></p> <p>Export to TorchScript: <pre><code>focoos export --model fai-detr-m-coco --format torchscript\n</code></pre></p> <p>Export with custom image size: <pre><code>focoos export --model fai-detr-m-coco --format onnx --im-size 800\n</code></pre></p> <p>Export to custom directory with overwrite: <pre><code>focoos export --model fai-detr-m-coco --format onnx                       --output-dir ./exported_models --overwrite\n</code></pre></p> <p>CPU-based export: <pre><code>focoos export --model fai-detr-m-coco --format torchscript                       --device cpu --im-size 416\n</code></pre></p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If device parameter is not valid.</p> <code>ValueError</code> <p>If export format is not supported.</p> <code>RuntimeError</code> <p>If export process fails due to model incompatibility.</p> <code>FileExistsError</code> <p>If output files exist and overwrite is disabled.</p> Note <p>ONNX format provides broader compatibility across different frameworks and deployment environments. TorchScript format is optimized for PyTorch-based deployment scenarios and may offer better performance in PyTorch environments.</p> See Also <ul> <li><code>focoos predict</code>: For using exported models</li> <li><code>focoos benchmark</code>: For testing exported models</li> </ul> Source code in <code>focoos/cli/cli.py</code> <pre><code>@app.command(\"export\")\ndef export(\n    model: Annotated[str, typer.Option(help=\"Model name (required)\")],\n    format: Annotated[Optional[ExportFormat], typer.Option(help=\"Export format\")] = ExportFormat.ONNX,\n    output_dir: Annotated[Optional[str], typer.Option(help=\"Output directory\")] = None,\n    device: Annotated[Optional[str], typer.Option(help=\"Device (cuda or cpu)\")] = \"cuda\",\n    onnx_opset: Annotated[Optional[int], typer.Option(help=\"ONNX opset version\")] = 17,\n    im_size: Annotated[Optional[int], typer.Option(help=\"Image size for export\")] = 640,\n    overwrite: Annotated[Optional[bool], typer.Option(help=\"Overwrite existing files\")] = False,\n):\n    \"\"\"Export a trained model to various deployment formats.\n\n    This command exports a Focoos model to different formats suitable for\n    deployment in various environments. Currently supports ONNX and TorchScript\n    formats for flexible deployment across different platforms and frameworks.\n\n    Supported Export Formats:\n        - **ONNX**: Cross-platform neural network format for interoperability\n        - **TorchScript**: PyTorch's native serialization format for production\n\n    Platform Compatibility:\n        | Format | Linux | Windows | macOS | Mobile | Edge | Description |\n        |--------|-------|---------|-------|--------|------|-------------|\n        | ONNX   | \u2705    | \u2705      | \u2705    | \u2705     | \u2705   | Universal format |\n        | TorchScript | \u2705 | \u2705     | \u2705    | \u2705     | \u2705   | PyTorch native |\n\n    Args:\n        model (str): Name of the Focoos model to export (e.g., 'fai-detr-m-coco').\n            Can be specified in several ways:\n            - **Pretrained model**: Simple model name like 'fai-detr-m-coco'\n            - **Hub model**: Format 'hub://&lt;model-ref&gt;' for models from Focoos Hub\n            - **Local model**: Model name or model directory path (e.g., 'my-model' or '/path/to/my-model')\n            - **Default directory model**: Model name for models in default Focoos directory\n        format (Optional[ExportFormat]): Target export format. Defaults to ONNX.\n            Available formats: 'onnx', 'torchscript'.\n        output_dir (Optional[str]): Directory to save the exported model files.\n            If not specified, uses a default export directory.\n        device (Optional[str]): Device to use for export process ('cuda' or 'cpu').\n            Defaults to 'cuda'.\n        onnx_opset (Optional[int]): ONNX opset version for ONNX exports. Defaults to 17.\n            Higher versions support more operations but may have compatibility issues.\n            Only applies to ONNX format exports.\n        im_size (Optional[int]): Input image size for the exported model. Defaults to 640.\n            This determines the input tensor shape for the exported model.\n        overwrite (Optional[bool]): Whether to overwrite existing exported files.\n            Defaults to False. If False, export will fail if output files already exist.\n\n    Examples:\n        Basic ONNX export with pretrained model:\n        ```bash\n        focoos export --model fai-detr-m-coco\n        ```\n\n        Export model from Focoos Hub:\n        ```bash\n        focoos export --model hub://&lt;model-ref&gt; --format onnx\n        ```\n\n        Export local model from custom directory:\n        ```bash\n        focoos export --model my-fine-tuned-model --models-dir ./trained_models\n        ```\n\n        Export local model from default Focoos directory:\n        ```bash\n        focoos export --model my-checkpoint-model --format torchscript\n        ```\n\n        Export to TorchScript:\n        ```bash\n        focoos export --model fai-detr-m-coco --format torchscript\n        ```\n\n        Export with custom image size:\n        ```bash\n        focoos export --model fai-detr-m-coco --format onnx --im-size 800\n        ```\n\n        Export to custom directory with overwrite:\n        ```bash\n        focoos export --model fai-detr-m-coco --format onnx \\\n                      --output-dir ./exported_models --overwrite\n        ```\n\n        CPU-based export:\n        ```bash\n        focoos export --model fai-detr-m-coco --format torchscript \\\n                      --device cpu --im-size 416\n        ```\n\n    Raises:\n        AssertionError: If device parameter is not valid.\n        ValueError: If export format is not supported.\n        RuntimeError: If export process fails due to model incompatibility.\n        FileExistsError: If output files exist and overwrite is disabled.\n\n    Note:\n        ONNX format provides broader compatibility across different frameworks and\n        deployment environments. TorchScript format is optimized for PyTorch-based\n        deployment scenarios and may offer better performance in PyTorch environments.\n\n    See Also:\n        - [`focoos predict`][focoos.cli.cli.predict]: For using exported models\n        - [`focoos benchmark`][focoos.cli.cli.benchmark]: For testing exported models\n    \"\"\"\n    typer.echo(\n        f\"\ud83d\udce6 Starting export - Model: {model}, Format: {format}, Output dir: {output_dir}, Device: {device}, ONNX opset: {onnx_opset}, Image size: {im_size}, Overwrite: {overwrite}\"\n    )\n    try:\n        validated_device = cast(DeviceType, device)\n        assert device in get_args(DeviceType)\n        export_command(\n            model_name=model,\n            format=format,\n            output_dir=output_dir,\n            device=validated_device,\n            onnx_opset=onnx_opset,\n            im_size=im_size,\n            overwrite=overwrite,\n        )\n    except Exception as e:\n        typer.echo(f\"\u274c Export failed: {e}\")\n</code></pre>"},{"location":"api/cli/#focoos.cli.cli.gradio","title":"<code>gradio()</code>","text":"<p>Launch the Focoos Gradio app.</p> Source code in <code>focoos/cli/cli.py</code> <pre><code>@app.command(\"gradio\")\ndef gradio():\n    \"\"\"Launch the Focoos Gradio app.\"\"\"\n    from focoos.cli.gradio_app import launch_gradio\n\n    launch_gradio(share=False)\n</code></pre>"},{"location":"api/cli/#focoos.cli.cli.predict","title":"<code>predict(source, model='fai-detr-l-obj365', runtime=None, im_size=640, conf=0.5, output_dir=PREDICTIONS_DIR, save=True, save_json=True, save_masks=True)</code>","text":"<p>Run inference on images with flexible output options.</p> <p>This command performs inference (prediction) on input images using a trained model. Supports various output formats including annotated images, JSON results, and mask files. Results are always printed to the console.</p> Supported Input <ul> <li>Single Images: Local image files (loaded via <code>image_loader()</code>)</li> <li>URLs: Remote image files accessible via URL</li> </ul> Output Options <ul> <li>Console Output: Detection results printed to terminal (always enabled)</li> <li>Annotated Images: Visual results with bounding boxes and labels (if <code>save=True</code>)</li> <li>JSON Files: Machine-readable detection results with coordinates and confidence (if <code>save_json=True</code>)</li> <li>Mask Images: Individual mask files as PNG images (if <code>save_masks=True</code> and model produces masks)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model or path to model file for inference. Can be specified in several ways: - Pretrained model: Simple model name like 'fai-detr-m-coco' - Hub model: Format 'hub://' for models from Focoos Hub - Default directory model: Model name for models in default Focoos directory <code>'fai-detr-l-obj365'</code> <code>source</code> <code>str</code> <p>Path to input image file or URL. Must be a single image file.</p> required <code>runtime</code> <code>RuntimeType</code> <p>Runtime backend for inference. Defaults to ONNX_CUDA32. Options include ONNX_CUDA32, ONNX_CPU, PYTORCH, etc.</p> <code>None</code> <code>im_size</code> <code>Optional[int]</code> <p>Input image size for inference. Defaults to 640. Images will be resized to this size while maintaining aspect ratio.</p> <code>640</code> <code>conf</code> <code>Optional[float]</code> <p>Confidence threshold for detections. Defaults to 0.25. Only detections above this threshold will be reported.</p> <code>0.5</code> <code>save</code> <code>Optional[bool]</code> <p>Whether to save annotated images with detection overlays. Defaults to True.</p> <code>True</code> <code>output_dir</code> <code>Optional[str]</code> <p>Directory to save all inference results.</p> <code>PREDICTIONS_DIR</code> <code>save_json</code> <code>Optional[bool]</code> <p>Whether to save detection results in JSON format. Defaults to True.</p> <code>True</code> <code>save_masks</code> <code>Optional[bool]</code> <p>Whether to save segmentation masks as separate PNG images. Defaults to True. Only applies if the model produces masks.</p> <code>True</code> <p>Examples:</p> <p>Basic image inference with pretrained model: <pre><code>focoos predict --model fai-detr-m-coco --source image.jpg\n</code></pre></p> <p>Inference with model from Focoos Hub: <pre><code>focoos predict --model hub://&lt;model-ref&gt; --source image.jpg\n</code></pre></p> <p>Inference with local model in default Focoos directory: <pre><code>focoos predict --model my-trained-model --source image.jpg\n</code></pre></p> <p>Image inference with custom confidence: <pre><code>focoos predict --model fai-detr-m-coco --source image.jpg --conf 0.5\n</code></pre></p> <p>Inference with custom output directory: <pre><code>focoos predict --model fai-detr-m-coco --source image.jpg                        --output-dir ./my_results --runtime ONNX_CPU\n</code></pre></p> <p>URL inference: <pre><code>focoos predict --model fai-detr-m-coco                        --source https://example.com/image.jpg\n</code></pre></p> <p>Only console output (no file saving): <pre><code>focoos predict --model fai-detr-m-coco --source image.jpg                        --save false --save-json false --save-masks false\n</code></pre></p> Output <p>The command prints detection results to the console and optionally saves files:</p> <p>Console output example: <pre><code>==================================================\nDETECTION RESULTS\n==================================================\nFound 2 detections:\n\n  1. person\n     Confidence: 0.856\n     Bbox: [245, 120, 380, 450]\n     Size: 135 x 330\n\n  2. car\n     Confidence: 0.742\n     Bbox: [50, 200, 200, 300]\n     Size: 150 x 100\n==================================================\n</code></pre></p> <p>File outputs (if enabled): - <code>{source_name}_annotated.{ext}</code>: Annotated image with bounding boxes - <code>{source_name}_detections.json</code>: Structured detection data - <code>{source_name}_masks/mask_N.png</code>: Individual mask files (if applicable)</p> Note <p>The function currently processes single images only. For batch processing, the command needs to be run multiple times. All detection results are always printed to the console regardless of save options.</p> See Also <ul> <li><code>focoos val</code>: For model validation</li> <li><code>focoos benchmark</code>: For performance testing</li> <li>Inference Guide: For detailed inference documentation</li> </ul> Source code in <code>focoos/cli/cli.py</code> <pre><code>@app.command(\"predict\")\ndef predict(\n    source: Annotated[str, typer.Option(help=\"Image path or URL (required)\")],\n    model: Annotated[str, typer.Option(help=\"Model name or path (required)\")] = \"fai-detr-l-obj365\",\n    runtime: Annotated[\n        Optional[RuntimeType], typer.Option(help=\"Runtime type (If not provided, torch will be used)\")\n    ] = None,\n    im_size: Annotated[Optional[int], typer.Option(help=\"Image size\")] = 640,\n    conf: Annotated[Optional[float], typer.Option(help=\"Confidence threshold\")] = 0.5,\n    output_dir: Annotated[\n        Optional[str], typer.Option(help=\"Directory to save results (default: ~/FocoosAI/predictions/)\")\n    ] = PREDICTIONS_DIR,\n    save: Annotated[Optional[bool], typer.Option(help=\"Save annotated image results\")] = True,\n    save_json: Annotated[Optional[bool], typer.Option(help=\"Save detections as JSON\")] = True,\n    save_masks: Annotated[Optional[bool], typer.Option(help=\"Save masks as separate images\")] = True,\n):\n    \"\"\"Run inference on images with flexible output options.\n\n    This command performs inference (prediction) on input images using a trained model.\n    Supports various output formats including annotated images, JSON results, and mask files.\n    Results are always printed to the console.\n\n    Supported Input:\n        - **Single Images**: Local image files (loaded via `image_loader()`)\n        - **URLs**: Remote image files accessible via URL\n\n    Output Options:\n        - **Console Output**: Detection results printed to terminal (always enabled)\n        - **Annotated Images**: Visual results with bounding boxes and labels (if `save=True`)\n        - **JSON Files**: Machine-readable detection results with coordinates and confidence (if `save_json=True`)\n        - **Mask Images**: Individual mask files as PNG images (if `save_masks=True` and model produces masks)\n\n    Args:\n        model (str): Name of the model or path to model file for inference.\n            Can be specified in several ways:\n            - **Pretrained model**: Simple model name like 'fai-detr-m-coco'\n            - **Hub model**: Format 'hub://&lt;model-ref&gt;' for models from Focoos Hub\n            - **Default directory model**: Model name for models in default Focoos directory\n        source (str): Path to input image file or URL. Must be a single image file.\n        runtime (RuntimeType): Runtime backend for inference. Defaults to ONNX_CUDA32.\n            Options include ONNX_CUDA32, ONNX_CPU, PYTORCH, etc.\n        im_size (Optional[int]): Input image size for inference. Defaults to 640.\n            Images will be resized to this size while maintaining aspect ratio.\n        conf (Optional[float]): Confidence threshold for detections. Defaults to 0.25.\n            Only detections above this threshold will be reported.\n        save (Optional[bool]): Whether to save annotated images with detection overlays.\n            Defaults to True.\n        output_dir (Optional[str]): Directory to save all inference results.\n        save_json (Optional[bool]): Whether to save detection results in JSON format.\n            Defaults to True.\n        save_masks (Optional[bool]): Whether to save segmentation masks as separate PNG images.\n            Defaults to True. Only applies if the model produces masks.\n\n    Examples:\n        Basic image inference with pretrained model:\n        ```bash\n        focoos predict --model fai-detr-m-coco --source image.jpg\n        ```\n\n        Inference with model from Focoos Hub:\n        ```bash\n        focoos predict --model hub://&lt;model-ref&gt; --source image.jpg\n        ```\n\n        Inference with local model in default Focoos directory:\n        ```bash\n        focoos predict --model my-trained-model --source image.jpg\n        ```\n\n        Image inference with custom confidence:\n        ```bash\n        focoos predict --model fai-detr-m-coco --source image.jpg --conf 0.5\n        ```\n\n        Inference with custom output directory:\n        ```bash\n        focoos predict --model fai-detr-m-coco --source image.jpg \\\n                       --output-dir ./my_results --runtime ONNX_CPU\n        ```\n\n        URL inference:\n        ```bash\n        focoos predict --model fai-detr-m-coco \\\n                       --source https://example.com/image.jpg\n        ```\n\n        Only console output (no file saving):\n        ```bash\n        focoos predict --model fai-detr-m-coco --source image.jpg \\\n                       --save false --save-json false --save-masks false\n        ```\n\n    Output:\n        The command prints detection results to the console and optionally saves files:\n\n        Console output example:\n        ```\n        ==================================================\n        DETECTION RESULTS\n        ==================================================\n        Found 2 detections:\n\n          1. person\n             Confidence: 0.856\n             Bbox: [245, 120, 380, 450]\n             Size: 135 x 330\n\n          2. car\n             Confidence: 0.742\n             Bbox: [50, 200, 200, 300]\n             Size: 150 x 100\n        ==================================================\n        ```\n\n        File outputs (if enabled):\n        - `{source_name}_annotated.{ext}`: Annotated image with bounding boxes\n        - `{source_name}_detections.json`: Structured detection data\n        - `{source_name}_masks/mask_N.png`: Individual mask files (if applicable)\n\n    Note:\n        The function currently processes single images only. For batch processing,\n        the command needs to be run multiple times. All detection results are\n        always printed to the console regardless of save options.\n\n    See Also:\n        - [`focoos val`][focoos.cli.cli.val]: For model validation\n        - [`focoos benchmark`][focoos.cli.cli.benchmark]: For performance testing\n        - [Inference Guide](../inference.md): For detailed inference documentation\n    \"\"\"\n    typer.echo(\n        f\"\ud83d\udd2e Starting predict - Model: {model}, Source: {source}, Runtime: {runtime}, Image size: {im_size}, Conf: {conf}, Save: {save}, Output dir: {output_dir}, Save json: {save_json}, Save masks: {save_masks}\"\n    )\n    try:\n        predict_command(\n            model_name=model,\n            source=source,\n            runtime=runtime,\n            im_size=im_size,\n            conf=conf,\n            save=save,\n            output_dir=output_dir,\n            save_json=save_json,\n            save_masks=save_masks,\n        )\n    except Exception as e:\n        typer.echo(f\"\u274c Predict failed: {e}\")\n</code></pre>"},{"location":"api/cli/#focoos.cli.cli.settings","title":"<code>settings()</code>","text":"<p>Show current Focoos configuration settings.</p> <p>This command displays the current configuration settings for Focoos, including runtime type, host URL, API key status, log level, and other configuration parameters.</p> Configuration includes <ul> <li>Runtime Type: Backend engine for inference</li> <li>Host URL: API endpoint for Focoos services</li> <li>API Key: Authentication status (masked for security)</li> <li>Log Level: Current logging verbosity</li> <li>Warmup Iterations: Performance optimization setting</li> </ul> <p>Examples:</p> <pre><code>focoos settings\n</code></pre> <p>Output: <pre><code>Current Focoos settings:\nFOCOOS_RUNTIME_TYPE=ONNX_CUDA32\nFOCOOS_HOST_URL=https://api.focoos.ai\nFOCOOS_API_KEY=********************\nFOCOOS_LOG_LEVEL=INFO\nFOCOOS_WARMUP_ITER=3\n</code></pre></p> <p>Raises:</p> Type Description <code>Exit</code> <p>If there's an error accessing configuration settings.</p> See Also <ul> <li><code>focoos checks</code>: For system diagnostics</li> </ul> Source code in <code>focoos/cli/cli.py</code> <pre><code>@app.command(\"settings\")\ndef settings():\n    \"\"\"Show current Focoos configuration settings.\n\n    This command displays the current configuration settings for Focoos,\n    including runtime type, host URL, API key status, log level, and\n    other configuration parameters.\n\n    Configuration includes:\n        - **Runtime Type**: Backend engine for inference\n        - **Host URL**: API endpoint for Focoos services\n        - **API Key**: Authentication status (masked for security)\n        - **Log Level**: Current logging verbosity\n        - **Warmup Iterations**: Performance optimization setting\n\n    Examples:\n        ```bash\n        focoos settings\n        ```\n\n        Output:\n        ```\n        Current Focoos settings:\n        FOCOOS_RUNTIME_TYPE=ONNX_CUDA32\n        FOCOOS_HOST_URL=https://api.focoos.ai\n        FOCOOS_API_KEY=********************\n        FOCOOS_LOG_LEVEL=INFO\n        FOCOOS_WARMUP_ITER=3\n        ```\n\n    Raises:\n        typer.Exit: If there's an error accessing configuration settings.\n\n    See Also:\n        - [`focoos checks`][focoos.cli.cli.checks]: For system diagnostics\n    \"\"\"\n    try:\n        from focoos.config import FOCOOS_CONFIG\n\n        typer.echo(\"Current Focoos settings:\")\n        typer.echo(f\"FOCOOS_RUNTIME_TYPE={FOCOOS_CONFIG.runtime_type}\")\n        typer.echo(f\"FOCOOS_HOST_URL={FOCOOS_CONFIG.default_host_url}\")\n        typer.echo(f\"FOCOOS_API_KEY={'*' * 20 if FOCOOS_CONFIG.focoos_api_key else 'Not set'}\")\n        typer.echo(f\"FOCOOS_LOG_LEVEL={FOCOOS_CONFIG.focoos_log_level}\")\n        typer.echo(f\"FOCOOS_WARMUP_ITER={FOCOOS_CONFIG.warmup_iter}\")\n    except Exception as e:\n        logger.error(f\"Error showing settings: {e}\")\n        raise typer.Exit(1)\n</code></pre>"},{"location":"api/cli/#focoos.cli.cli.train","title":"<code>train(model, dataset, run_name=None, datasets_dir=None, dataset_layout=DatasetLayout.ROBOFLOW_COCO, im_size=640, output_dir=None, ckpt_dir=None, init_checkpoint=None, resume=False, num_gpus=get_gpus_count(), device='cuda', workers=4, amp_enabled=True, ddp_broadcast_buffers=False, ddp_find_unused=True, checkpointer_period=1000, checkpointer_max_to_keep=1, eval_period=50, log_period=20, samples=9, seed=42, early_stop=True, patience=10, ema_enabled=False, ema_decay=0.999, ema_warmup=2000, learning_rate=0.0005, weight_decay=0.02, max_iters=3000, batch_size=16, scheduler='MULTISTEP', optimizer='ADAMW', weight_decay_norm=0.0, weight_decay_embed=0.0, backbone_multiplier=0.1, decoder_multiplier=1.0, head_multiplier=1.0, freeze_bn=False, clip_gradients=0.1, size_divisibility=0, gather_metric_period=1, zero_grad_before_forward=False, sync_to_hub=False)</code>","text":"<p>Train a model with comprehensive configuration options.</p> <p>This command initiates model training with extensive customization options for dataset handling, model architecture, optimization, and training dynamics. Supports distributed training, mixed precision, and various optimization strategies.</p> Training Features <ul> <li>Multi-GPU Training: Automatic distributed training support</li> <li>Mixed Precision: Faster training with AMP (Automatic Mixed Precision)</li> <li>Early Stopping: Prevent overfitting with validation-based stopping</li> <li>EMA (Exponential Moving Average): Improved model stability</li> <li>Flexible Scheduling: Multiple learning rate schedules</li> <li>Checkpoint Management: Automatic saving and resuming</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model architecture to train (e.g., 'fai-detr-m-coco'). Can be specified in several ways: - Pretrained model: Simple model name like 'fai-detr-m-coco' - Hub model: Format 'hub://' for models from Focoos Hub - Default directory model: Model name for models in default Focoos directory required <code>dataset</code> <code>str</code> <p>Name of the dataset to train on (e.g., 'mydataset.zip').</p> required <code>run_name</code> <code>Optional[str]</code> <p>Optional name for the training run. If not provided, generates a unique name using model name and UUID.</p> <code>None</code> <code>datasets_dir</code> <code>Optional[str]</code> <p>Custom directory for datasets.</p> <code>None</code> <code>dataset_layout</code> <code>DatasetLayout</code> <p>Layout format of the dataset. Defaults to ROBOFLOW_COCO.</p> <code>ROBOFLOW_COCO</code> <code>im_size</code> <code>int</code> <p>Input image size for training. Defaults to 640.</p> <code>640</code> <code>output_dir</code> <code>Optional[str]</code> <p>Directory to save training outputs and logs.</p> <code>None</code> <code>ckpt_dir</code> <code>Optional[str]</code> <p>Directory to save model checkpoints.</p> <code>None</code> <code>init_checkpoint</code> <code>Optional[str]</code> <p>Path to initial checkpoint for transfer learning.</p> <code>None</code> <code>resume</code> <code>bool</code> <p>Whether to resume training from the latest checkpoint. Defaults to False.</p> <code>False</code> <code>num_gpus</code> <code>int</code> <p>Number of GPUs to use for training. Defaults to auto-detected count.</p> <code>get_gpus_count()</code> <code>device</code> <code>str</code> <p>Device type for training ('cuda' or 'cpu'). Defaults to 'cuda'.</p> <code>'cuda'</code> <code>workers</code> <code>int</code> <p>Number of data loading workers. Defaults to 4.</p> <code>4</code> <code>amp_enabled</code> <code>bool</code> <p>Enable Automatic Mixed Precision for faster training. Defaults to True.</p> <code>True</code> <code>ddp_broadcast_buffers</code> <code>bool</code> <p>Whether to broadcast buffers in DistributedDataParallel. Defaults to False.</p> <code>False</code> <code>ddp_find_unused</code> <code>bool</code> <p>Whether to find unused parameters in DDP. Defaults to True.</p> <code>True</code> <code>checkpointer_period</code> <code>int</code> <p>Frequency of checkpoint saving (in iterations). Defaults to 1000.</p> <code>1000</code> <code>checkpointer_max_to_keep</code> <code>int</code> <p>Maximum number of checkpoints to retain. Defaults to 1.</p> <code>1</code> <code>eval_period</code> <code>int</code> <p>Frequency of model evaluation (in iterations). Defaults to 50.</p> <code>50</code> <code>log_period</code> <code>int</code> <p>Frequency of logging metrics (in iterations). Defaults to 20.</p> <code>20</code> <code>samples</code> <code>int</code> <p>Number of sample images to log during training. Defaults to 9.</p> <code>9</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible training. Defaults to 42.</p> <code>42</code> <code>early_stop</code> <code>bool</code> <p>Enable early stopping based on validation metrics. Defaults to True.</p> <code>True</code> <code>patience</code> <code>int</code> <p>Number of evaluations to wait before early stopping. Defaults to 10.</p> <code>10</code> <code>ema_enabled</code> <code>bool</code> <p>Enable Exponential Moving Average of model weights. Defaults to False.</p> <code>False</code> <code>ema_decay</code> <code>float</code> <p>Decay rate for EMA. Defaults to 0.999.</p> <code>0.999</code> <code>ema_warmup</code> <code>int</code> <p>Number of warmup steps for EMA. Defaults to 2000.</p> <code>2000</code> <code>learning_rate</code> <code>float</code> <p>Initial learning rate for optimization. Defaults to 5e-4.</p> <code>0.0005</code> <code>weight_decay</code> <code>float</code> <p>L2 regularization weight decay. Defaults to 0.02.</p> <code>0.02</code> <code>max_iters</code> <code>int</code> <p>Maximum number of training iterations. Defaults to 3000.</p> <code>3000</code> <code>batch_size</code> <code>int</code> <p>Training batch size. Defaults to 16.</p> <code>16</code> <code>scheduler</code> <code>str</code> <p>Learning rate scheduler type ('MULTISTEP', 'COSINE', etc.). Defaults to 'MULTISTEP'.</p> <code>'MULTISTEP'</code> <code>optimizer</code> <code>str</code> <p>Optimizer type ('ADAMW', 'SGD', etc.). Defaults to 'ADAMW'.</p> <code>'ADAMW'</code> <code>weight_decay_norm</code> <code>float</code> <p>Weight decay for normalization layers. Defaults to 0.0.</p> <code>0.0</code> <code>weight_decay_embed</code> <code>float</code> <p>Weight decay for embedding layers. Defaults to 0.0.</p> <code>0.0</code> <code>backbone_multiplier</code> <code>float</code> <p>Learning rate multiplier for backbone layers. Defaults to 0.1.</p> <code>0.1</code> <code>decoder_multiplier</code> <code>float</code> <p>Learning rate multiplier for decoder layers. Defaults to 1.0.</p> <code>1.0</code> <code>head_multiplier</code> <code>float</code> <p>Learning rate multiplier for head layers. Defaults to 1.0.</p> <code>1.0</code> <code>freeze_bn</code> <code>bool</code> <p>Whether to freeze batch normalization layers. Defaults to False.</p> <code>False</code> <code>clip_gradients</code> <code>float</code> <p>Gradient clipping threshold. Defaults to 0.1.</p> <code>0.1</code> <code>size_divisibility</code> <code>int</code> <p>Image size divisibility constraint. Defaults to 0.</p> <code>0</code> <code>gather_metric_period</code> <code>int</code> <p>Frequency of metric gathering (in iterations). Defaults to 1.</p> <code>1</code> <code>zero_grad_before_forward</code> <code>bool</code> <p>Whether to zero gradients before forward pass. Defaults to False.</p> <code>False</code> <code>sync_to_hub</code> <code>bool</code> <p>Whether to sync model to Focoos Hub. Defaults to False.</p> <code>False</code> <p>Examples:</p> <p>Basic training with pretrained model: <pre><code>focoos train --model fai-detr-m-coco --dataset mydataset.zip\n</code></pre></p> <p>Training with model from Focoos Hub: <pre><code>focoos train --model hub://&lt;model-ref&gt; --dataset mydataset.zip\n</code></pre></p> <p>Training with local model in default Focoos directory: <pre><code>focoos train --model my-saved-model --dataset mydataset.zip\n</code></pre></p> <p>Advanced training with custom parameters: <pre><code>focoos train --model fai-detr-m-coco --dataset mydataset.zip                      --im-size 800 --batch-size 32 --learning-rate 1e-4                      --max-iters 5000 --early-stop --patience 20\n</code></pre></p> <p>Multi-GPU training with mixed precision: <pre><code>focoos train --model fai-detr-m-coco --dataset custom_dataset                      --num-gpus 4 --amp-enabled --batch-size 64\n</code></pre></p> <p>Resume training from checkpoint: <pre><code>focoos train --model fai-detr-m-coco --dataset mydataset.zip                      --resume --ckpt-dir ./checkpoints\n</code></pre></p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If device, scheduler, or optimizer parameters are invalid.</p> <code>FileNotFoundError</code> <p>If dataset or checkpoint files are not found.</p> <code>RuntimeError</code> <p>If GPU resources are insufficient or CUDA is unavailable.</p> See Also <ul> <li><code>focoos val</code>: For model validation</li> <li><code>focoos export</code>: For model export</li> <li>Training Guide: For detailed training documentation</li> </ul> Source code in <code>focoos/cli/cli.py</code> <pre><code>@app.command(\"train\")\ndef train(\n    model: Annotated[str, typer.Option(help=\"Model name (required)\")],\n    dataset: Annotated[str, typer.Option(help=\"Dataset name (required)\")],\n    run_name: Annotated[Optional[str], typer.Option(help=\"Run name\")] = None,\n    datasets_dir: Annotated[\n        Optional[str], typer.Option(help=\"Datasets directory (default: ~/FocoosAI/datasets/)\")\n    ] = None,\n    dataset_layout: Annotated[DatasetLayout, typer.Option(help=\"Dataset layout\")] = DatasetLayout.ROBOFLOW_COCO,\n    im_size: Annotated[int, typer.Option(help=\"Image size\")] = 640,\n    output_dir: Annotated[Optional[str], typer.Option(help=\"Output directory\")] = None,\n    ckpt_dir: Annotated[Optional[str], typer.Option(help=\"Checkpoint directory\")] = None,\n    init_checkpoint: Annotated[Optional[str], typer.Option(help=\"Initial checkpoint path\")] = None,\n    resume: Annotated[bool, typer.Option(help=\"Resume training from checkpoint\")] = False,\n    num_gpus: Annotated[int, typer.Option(help=\"Number of GPUs to use\")] = get_gpus_count(),\n    device: Annotated[str, typer.Option(help=\"Device to use\")] = \"cuda\",\n    workers: Annotated[int, typer.Option(help=\"Number of workers\")] = 4,\n    amp_enabled: Annotated[bool, typer.Option(help=\"Enable automatic mixed precision\")] = True,\n    ddp_broadcast_buffers: Annotated[bool, typer.Option(help=\"Broadcast buffers in DDP\")] = False,\n    ddp_find_unused: Annotated[bool, typer.Option(help=\"Find unused parameters in DDP\")] = True,\n    checkpointer_period: Annotated[int, typer.Option(help=\"Checkpoint save period\")] = 1000,\n    checkpointer_max_to_keep: Annotated[int, typer.Option(help=\"Maximum checkpoints to keep\")] = 1,\n    eval_period: Annotated[int, typer.Option(help=\"Evaluation period\")] = 50,\n    log_period: Annotated[int, typer.Option(help=\"Logging period\")] = 20,\n    samples: Annotated[int, typer.Option(help=\"Number of samples to log\")] = 9,\n    seed: Annotated[int, typer.Option(help=\"Random seed\")] = 42,\n    early_stop: Annotated[bool, typer.Option(help=\"Enable early stopping\")] = True,\n    patience: Annotated[int, typer.Option(help=\"Early stopping patience\")] = 10,\n    ema_enabled: Annotated[bool, typer.Option(help=\"Enable EMA\")] = False,\n    ema_decay: Annotated[float, typer.Option(help=\"EMA decay rate\")] = 0.999,\n    ema_warmup: Annotated[int, typer.Option(help=\"EMA warmup steps\")] = 2000,\n    learning_rate: Annotated[float, typer.Option(help=\"Learning rate\")] = 5e-4,\n    weight_decay: Annotated[float, typer.Option(help=\"Weight decay\")] = 0.02,\n    max_iters: Annotated[int, typer.Option(help=\"Maximum iterations\")] = 3000,\n    batch_size: Annotated[int, typer.Option(help=\"Batch size\")] = 16,\n    scheduler: Annotated[str, typer.Option(help=\"Learning rate scheduler\")] = \"MULTISTEP\",\n    optimizer: Annotated[str, typer.Option(help=\"Optimizer type\")] = \"ADAMW\",\n    weight_decay_norm: Annotated[float, typer.Option(help=\"Weight decay for normalization layers\")] = 0.0,\n    weight_decay_embed: Annotated[float, typer.Option(help=\"Weight decay for embedding layers\")] = 0.0,\n    backbone_multiplier: Annotated[float, typer.Option(help=\"Learning rate multiplier for backbone\")] = 0.1,\n    decoder_multiplier: Annotated[float, typer.Option(help=\"Learning rate multiplier for decoder\")] = 1.0,\n    head_multiplier: Annotated[float, typer.Option(help=\"Learning rate multiplier for head\")] = 1.0,\n    freeze_bn: Annotated[bool, typer.Option(help=\"Freeze batch normalization layers\")] = False,\n    clip_gradients: Annotated[float, typer.Option(help=\"Gradient clipping value\")] = 0.1,\n    size_divisibility: Annotated[int, typer.Option(help=\"Size divisibility\")] = 0,\n    gather_metric_period: Annotated[int, typer.Option(help=\"Metric gathering period\")] = 1,\n    zero_grad_before_forward: Annotated[bool, typer.Option(help=\"Zero gradients before forward pass\")] = False,\n    sync_to_hub: Annotated[bool, typer.Option(help=\"Sync to Focoos Hub\")] = False,\n):\n    \"\"\"Train a model with comprehensive configuration options.\n\n    This command initiates model training with extensive customization options\n    for dataset handling, model architecture, optimization, and training dynamics.\n    Supports distributed training, mixed precision, and various optimization strategies.\n\n    Training Features:\n        - **Multi-GPU Training**: Automatic distributed training support\n        - **Mixed Precision**: Faster training with AMP (Automatic Mixed Precision)\n        - **Early Stopping**: Prevent overfitting with validation-based stopping\n        - **EMA (Exponential Moving Average)**: Improved model stability\n        - **Flexible Scheduling**: Multiple learning rate schedules\n        - **Checkpoint Management**: Automatic saving and resuming\n\n    Args:\n        model (str): Name of the model architecture to train (e.g., 'fai-detr-m-coco').\n            Can be specified in several ways:\n            - **Pretrained model**: Simple model name like 'fai-detr-m-coco'\n            - **Hub model**: Format 'hub://&lt;model-ref&gt;' for models from Focoos Hub\n            - **Default directory model**: Model name for models in default Focoos directory\n        dataset (str): Name of the dataset to train on (e.g., 'mydataset.zip').\n        run_name (Optional[str]): Optional name for the training run. If not provided,\n            generates a unique name using model name and UUID.\n        datasets_dir (Optional[str]): Custom directory for datasets.\n        dataset_layout (DatasetLayout): Layout format of the dataset. Defaults to ROBOFLOW_COCO.\n        im_size (int): Input image size for training. Defaults to 640.\n        output_dir (Optional[str]): Directory to save training outputs and logs.\n        ckpt_dir (Optional[str]): Directory to save model checkpoints.\n        init_checkpoint (Optional[str]): Path to initial checkpoint for transfer learning.\n        resume (bool): Whether to resume training from the latest checkpoint. Defaults to False.\n        num_gpus (int): Number of GPUs to use for training. Defaults to auto-detected count.\n        device (str): Device type for training ('cuda' or 'cpu'). Defaults to 'cuda'.\n        workers (int): Number of data loading workers. Defaults to 4.\n        amp_enabled (bool): Enable Automatic Mixed Precision for faster training. Defaults to True.\n        ddp_broadcast_buffers (bool): Whether to broadcast buffers in DistributedDataParallel.\n            Defaults to False.\n        ddp_find_unused (bool): Whether to find unused parameters in DDP. Defaults to True.\n        checkpointer_period (int): Frequency of checkpoint saving (in iterations). Defaults to 1000.\n        checkpointer_max_to_keep (int): Maximum number of checkpoints to retain. Defaults to 1.\n        eval_period (int): Frequency of model evaluation (in iterations). Defaults to 50.\n        log_period (int): Frequency of logging metrics (in iterations). Defaults to 20.\n        samples (int): Number of sample images to log during training. Defaults to 9.\n        seed (int): Random seed for reproducible training. Defaults to 42.\n        early_stop (bool): Enable early stopping based on validation metrics. Defaults to True.\n        patience (int): Number of evaluations to wait before early stopping. Defaults to 10.\n        ema_enabled (bool): Enable Exponential Moving Average of model weights. Defaults to False.\n        ema_decay (float): Decay rate for EMA. Defaults to 0.999.\n        ema_warmup (int): Number of warmup steps for EMA. Defaults to 2000.\n        learning_rate (float): Initial learning rate for optimization. Defaults to 5e-4.\n        weight_decay (float): L2 regularization weight decay. Defaults to 0.02.\n        max_iters (int): Maximum number of training iterations. Defaults to 3000.\n        batch_size (int): Training batch size. Defaults to 16.\n        scheduler (str): Learning rate scheduler type ('MULTISTEP', 'COSINE', etc.).\n            Defaults to 'MULTISTEP'.\n        optimizer (str): Optimizer type ('ADAMW', 'SGD', etc.). Defaults to 'ADAMW'.\n        weight_decay_norm (float): Weight decay for normalization layers. Defaults to 0.0.\n        weight_decay_embed (float): Weight decay for embedding layers. Defaults to 0.0.\n        backbone_multiplier (float): Learning rate multiplier for backbone layers. Defaults to 0.1.\n        decoder_multiplier (float): Learning rate multiplier for decoder layers. Defaults to 1.0.\n        head_multiplier (float): Learning rate multiplier for head layers. Defaults to 1.0.\n        freeze_bn (bool): Whether to freeze batch normalization layers. Defaults to False.\n        clip_gradients (float): Gradient clipping threshold. Defaults to 0.1.\n        size_divisibility (int): Image size divisibility constraint. Defaults to 0.\n        gather_metric_period (int): Frequency of metric gathering (in iterations). Defaults to 1.\n        zero_grad_before_forward (bool): Whether to zero gradients before forward pass.\n            Defaults to False.\n        sync_to_hub (bool): Whether to sync model to Focoos Hub. Defaults to False.\n\n    Examples:\n        Basic training with pretrained model:\n        ```bash\n        focoos train --model fai-detr-m-coco --dataset mydataset.zip\n        ```\n\n        Training with model from Focoos Hub:\n        ```bash\n        focoos train --model hub://&lt;model-ref&gt; --dataset mydataset.zip\n        ```\n\n        Training with local model in default Focoos directory:\n        ```bash\n        focoos train --model my-saved-model --dataset mydataset.zip\n        ```\n\n        Advanced training with custom parameters:\n        ```bash\n        focoos train --model fai-detr-m-coco --dataset mydataset.zip \\\n                     --im-size 800 --batch-size 32 --learning-rate 1e-4 \\\n                     --max-iters 5000 --early-stop --patience 20\n        ```\n\n        Multi-GPU training with mixed precision:\n        ```bash\n        focoos train --model fai-detr-m-coco --dataset custom_dataset \\\n                     --num-gpus 4 --amp-enabled --batch-size 64\n        ```\n\n        Resume training from checkpoint:\n        ```bash\n        focoos train --model fai-detr-m-coco --dataset mydataset.zip \\\n                     --resume --ckpt-dir ./checkpoints\n        ```\n\n    Raises:\n        AssertionError: If device, scheduler, or optimizer parameters are invalid.\n        FileNotFoundError: If dataset or checkpoint files are not found.\n        RuntimeError: If GPU resources are insufficient or CUDA is unavailable.\n\n    See Also:\n        - [`focoos val`][focoos.cli.cli.val]: For model validation\n        - [`focoos export`][focoos.cli.cli.export]: For model export\n        - [Training Guide](../training.md): For detailed training documentation\n    \"\"\"\n    typer.echo(\"\ud83d\udd0d Training arguments:\")\n    typer.echo(f\"  Model: {model}\")\n    typer.echo(f\"  Dataset: {dataset}\")\n    typer.echo(f\"  Dataset layout: {dataset_layout}\")\n    typer.echo(f\"  Image size: {im_size}\")\n    typer.echo(f\"  Run name: {run_name}\")\n    typer.echo(f\"  Output dir: {output_dir}\")\n    typer.echo(f\"  Checkpoint dir: {ckpt_dir}\")\n    typer.echo(f\"  Init checkpoint: {init_checkpoint}\")\n    typer.echo(f\"  Resume: {resume}\")\n    typer.echo(f\"  Num GPUs: {num_gpus}\")\n    typer.echo(f\"  Device: {device}\")\n    typer.echo(f\"  Workers: {workers}\")\n    typer.echo(f\"  Batch size: {batch_size}\")\n    typer.echo(f\"  Scheduler: {scheduler}\")\n    typer.echo(f\"  Optimizer: {optimizer}\")\n    typer.echo(f\"  Weight decay norm: {weight_decay_norm}\")\n    typer.echo(f\"  Weight decay embed: {weight_decay_embed}\")\n    typer.echo(f\"  Backbone multiplier: {backbone_multiplier}\")\n    typer.echo(f\"  Decoder multiplier: {decoder_multiplier}\")\n    typer.echo(f\"  Head multiplier: {head_multiplier}\")\n\n    try:\n        # Cast to proper literal types\n        validated_device = cast(DeviceType, device)\n        assert device in get_args(DeviceType)\n        validated_scheduler = cast(SchedulerType, scheduler.upper())\n        assert scheduler in get_args(SchedulerType)\n        validated_optimizer = cast(OptimizerType, optimizer.upper())\n        assert optimizer in get_args(OptimizerType)\n\n        train_command(\n            model_name=model,\n            dataset_name=dataset,\n            dataset_layout=dataset_layout,\n            im_size=im_size,\n            run_name=run_name or f\"{model}-{uuid.uuid4()}\",\n            output_dir=output_dir,\n            ckpt_dir=ckpt_dir,\n            init_checkpoint=init_checkpoint,\n            resume=resume,\n            num_gpus=num_gpus,\n            device=validated_device,\n            workers=workers,\n            amp_enabled=amp_enabled,\n            ddp_broadcast_buffers=ddp_broadcast_buffers,\n            ddp_find_unused=ddp_find_unused,\n            checkpointer_period=checkpointer_period,\n            checkpointer_max_to_keep=checkpointer_max_to_keep,\n            eval_period=eval_period,\n            log_period=log_period,\n            samples=samples,\n            seed=seed,\n            early_stop=early_stop,\n            patience=patience,\n            ema_enabled=ema_enabled,\n            ema_decay=ema_decay,\n            ema_warmup=ema_warmup,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            max_iters=max_iters,\n            batch_size=batch_size,\n            scheduler=validated_scheduler,\n            optimizer=validated_optimizer,\n            weight_decay_norm=weight_decay_norm,\n            weight_decay_embed=weight_decay_embed,\n            backbone_multiplier=backbone_multiplier,\n            decoder_multiplier=decoder_multiplier,\n            head_multiplier=head_multiplier,\n            freeze_bn=freeze_bn,\n            clip_gradients=clip_gradients,\n            size_divisibility=size_divisibility,\n            gather_metric_period=gather_metric_period,\n            zero_grad_before_forward=zero_grad_before_forward,\n            sync_to_hub=sync_to_hub,\n            datasets_dir=datasets_dir,\n        )\n    except Exception as e:\n        typer.echo(f\"\u274c Training failed: {e}\")\n</code></pre>"},{"location":"api/cli/#focoos.cli.cli.val","title":"<code>val(model, dataset, datasets_dir=None, run_name=None, dataset_layout=DatasetLayout.ROBOFLOW_COCO, im_size=640, output_dir=None, ckpt_dir=None, init_checkpoint=None, resume=False, num_gpus=get_gpus_count(), device='cuda', workers=4, amp_enabled=True, ddp_broadcast_buffers=False, ddp_find_unused=True, checkpointer_period=1000, checkpointer_max_to_keep=1, eval_period=50, log_period=20, samples=9, seed=42, early_stop=True, patience=10, ema_enabled=False, ema_decay=0.999, ema_warmup=2000, learning_rate=0.0005, weight_decay=0.02, max_iters=3000, batch_size=16, scheduler='MULTISTEP', optimizer='ADAMW', weight_decay_norm=0.0, weight_decay_embed=0.0, backbone_multiplier=0.1, decoder_multiplier=1.0, head_multiplier=1.0, freeze_bn=False, clip_gradients=0.1, size_divisibility=0, gather_metric_period=1, zero_grad_before_forward=False)</code>","text":"<p>Validate a model on a dataset with comprehensive evaluation metrics.</p> <p>This command performs model validation/evaluation on a specified dataset, computing various metrics such as mAP, precision, recall, and other task-specific evaluation measures. Supports the same configuration options as training for consistency in evaluation setup.</p> Validation Metrics <ul> <li>mAP (mean Average Precision): Overall detection accuracy</li> <li>Precision/Recall: Per-class and overall performance</li> <li>F1-Score: Harmonic mean of precision and recall</li> <li>IoU (Intersection over Union): Bounding box accuracy</li> <li>Inference Speed: FPS and latency measurements</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model to validate (e.g., 'fai-detr-m-coco'). Can be specified in several ways: - Pretrained model: Simple model name like 'fai-detr-m-coco' - Hub model: Format 'hub://' for models from Focoos Hub - Default directory model: Model name for models in default Focoos directory required <code>dataset</code> <code>str</code> <p>Name of the dataset for validation (e.g., 'mydataset.zip').</p> required <code>run_name</code> <code>Optional[str]</code> <p>Optional name for the validation run. If not provided, generates a unique name using model name and UUID.</p> <code>None</code> <code>dataset_layout</code> <code>DatasetLayout</code> <p>Layout format of the dataset. Defaults to ROBOFLOW_COCO.</p> <code>ROBOFLOW_COCO</code> <code>im_size</code> <code>int</code> <p>Input image size for validation. Defaults to 640.</p> <code>640</code> <code>output_dir</code> <code>Optional[str]</code> <p>Directory to save validation outputs and results.</p> <code>None</code> <code>ckpt_dir</code> <code>Optional[str]</code> <p>Directory containing model checkpoints.</p> <code>None</code> <code>init_checkpoint</code> <code>Optional[str]</code> <p>Path to specific checkpoint for validation.</p> <code>None</code> <code>resume</code> <code>bool</code> <p>Whether to resume from checkpoint (typically not used in validation).</p> <code>False</code> <code>num_gpus</code> <code>int</code> <p>Number of GPUs to use for validation. Defaults to auto-detected count.</p> <code>get_gpus_count()</code> <code>device</code> <code>str</code> <p>Device type for validation ('cuda' or 'cpu'). Defaults to 'cuda'.</p> <code>'cuda'</code> <code>workers</code> <code>int</code> <p>Number of data loading workers. Defaults to 4.</p> <code>4</code> <code>amp_enabled</code> <code>bool</code> <p>Enable Automatic Mixed Precision for faster inference. Defaults to True.</p> <code>True</code> <code>ddp_broadcast_buffers</code> <code>bool</code> <p>Whether to broadcast buffers in DistributedDataParallel. Defaults to False.</p> <code>False</code> <code>ddp_find_unused</code> <code>bool</code> <p>Whether to find unused parameters in DDP. Defaults to True.</p> <code>True</code> <code>checkpointer_period</code> <code>int</code> <p>Checkpoint saving frequency (not typically used in validation).</p> <code>1000</code> <code>checkpointer_max_to_keep</code> <code>int</code> <p>Maximum checkpoints to keep.</p> <code>1</code> <code>eval_period</code> <code>int</code> <p>Frequency of evaluation logging (in iterations). Defaults to 50.</p> <code>50</code> <code>log_period</code> <code>int</code> <p>Frequency of metric logging (in iterations). Defaults to 20.</p> <code>20</code> <code>samples</code> <code>int</code> <p>Number of sample images to visualize during validation. Defaults to 9.</p> <code>9</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible validation. Defaults to 42.</p> <code>42</code> <code>early_stop</code> <code>bool</code> <p>Enable early stopping (not typically used in validation).</p> <code>True</code> <code>patience</code> <code>int</code> <p>Early stopping patience.</p> <code>10</code> <code>ema_enabled</code> <code>bool</code> <p>Use Exponential Moving Average weights if available.</p> <code>False</code> <code>ema_decay</code> <code>float</code> <p>EMA decay rate.</p> <code>0.999</code> <code>ema_warmup</code> <code>int</code> <p>EMA warmup steps.</p> <code>2000</code> <code>learning_rate</code> <code>float</code> <p>Learning rate (not used in validation).</p> <code>0.0005</code> <code>weight_decay</code> <code>float</code> <p>Weight decay (not used in validation).</p> <code>0.02</code> <code>max_iters</code> <code>int</code> <p>Maximum validation iterations.</p> <code>3000</code> <code>batch_size</code> <code>int</code> <p>Validation batch size. Defaults to 16.</p> <code>16</code> <code>scheduler</code> <code>str</code> <p>Scheduler type (not used in validation).</p> <code>'MULTISTEP'</code> <code>optimizer</code> <code>str</code> <p>Optimizer type (not used in validation).</p> <code>'ADAMW'</code> <code>weight_decay_norm</code> <code>float</code> <p>Weight decay for normalization layers.</p> <code>0.0</code> <code>weight_decay_embed</code> <code>float</code> <p>Weight decay for embedding layers.</p> <code>0.0</code> <code>backbone_multiplier</code> <code>float</code> <p>Backbone learning rate multiplier.</p> <code>0.1</code> <code>decoder_multiplier</code> <code>float</code> <p>Decoder learning rate multiplier.</p> <code>1.0</code> <code>head_multiplier</code> <code>float</code> <p>Head learning rate multiplier.</p> <code>1.0</code> <code>freeze_bn</code> <code>bool</code> <p>Whether to freeze batch normalization layers.</p> <code>False</code> <code>clip_gradients</code> <code>float</code> <p>Gradient clipping threshold.</p> <code>0.1</code> <code>size_divisibility</code> <code>int</code> <p>Image size divisibility constraint.</p> <code>0</code> <code>gather_metric_period</code> <code>int</code> <p>Frequency of metric gathering.</p> <code>1</code> <code>zero_grad_before_forward</code> <code>bool</code> <p>Whether to zero gradients before forward pass.</p> <code>False</code> <code>datasets_dir</code> <code>Optional[str]</code> <p>Custom directory for datasets.</p> <code>None</code> <p>Examples:</p> <p>Basic validation with pretrained model: <pre><code>focoos val --model fai-detr-m-coco --dataset mydataset.zip\n</code></pre></p> <p>Validation with model from Focoos Hub: <pre><code>focoos val --model hub://&lt;model-ref&gt; --dataset mydataset.zip\n</code></pre></p> <p>Validation with local model in default Focoos directory: <pre><code>focoos val --model my-checkpoint-model --dataset mydataset.zip\n</code></pre></p> <p>Validation with specific checkpoint: <pre><code>focoos val --model fai-detr-m-coco --dataset mydataset.zip                    --init-checkpoint path/to/checkpoint.pth --im-size 800\n</code></pre></p> <p>Multi-GPU validation: <pre><code>focoos val --model fai-detr-m-coco --dataset large_dataset                    --num-gpus 2 --batch-size 32\n</code></pre></p> <p>Validation with custom output directory: <pre><code>focoos val --model fai-detr-m-coco --dataset mydataset.zip                    --output-dir ./validation_results\n</code></pre></p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If device, scheduler, or optimizer parameters are invalid.</p> <code>FileNotFoundError</code> <p>If dataset or checkpoint files are not found.</p> <code>RuntimeError</code> <p>If GPU resources are insufficient or model loading fails.</p> See Also <ul> <li><code>focoos train</code>: For model training</li> <li><code>focoos predict</code>: For inference</li> <li>Validation Guide: For detailed validation documentation</li> </ul> Source code in <code>focoos/cli/cli.py</code> <pre><code>@app.command(\"val\")\ndef val(\n    model: Annotated[str, typer.Option(help=\"Model name (required)\")],\n    dataset: Annotated[str, typer.Option(help=\"Dataset name (required)\")],\n    datasets_dir: Annotated[\n        Optional[str], typer.Option(help=\"Datasets directory (default: ~/FocoosAI/datasets/)\")\n    ] = None,\n    run_name: Annotated[Optional[str], typer.Option(help=\"Run name\")] = None,\n    dataset_layout: Annotated[DatasetLayout, typer.Option(help=\"Dataset layout\")] = DatasetLayout.ROBOFLOW_COCO,\n    im_size: Annotated[int, typer.Option(help=\"Image size\")] = 640,\n    output_dir: Annotated[Optional[str], typer.Option(help=\"Output directory\")] = None,\n    ckpt_dir: Annotated[Optional[str], typer.Option(help=\"Checkpoint directory\")] = None,\n    init_checkpoint: Annotated[Optional[str], typer.Option(help=\"Initial checkpoint\")] = None,\n    resume: Annotated[bool, typer.Option(help=\"Resume training\")] = False,\n    num_gpus: Annotated[int, typer.Option(help=\"Number of GPUs\")] = get_gpus_count(),\n    device: Annotated[str, typer.Option(help=\"Device\")] = \"cuda\",\n    workers: Annotated[int, typer.Option(help=\"Number of workers\")] = 4,\n    amp_enabled: Annotated[bool, typer.Option(help=\"Enable AMP\")] = True,\n    ddp_broadcast_buffers: Annotated[bool, typer.Option(help=\"DDP broadcast buffers\")] = False,\n    ddp_find_unused: Annotated[bool, typer.Option(help=\"DDP find unused\")] = True,\n    checkpointer_period: Annotated[int, typer.Option(help=\"Checkpointer period\")] = 1000,\n    checkpointer_max_to_keep: Annotated[int, typer.Option(help=\"Checkpointer max to keep\")] = 1,\n    eval_period: Annotated[int, typer.Option(help=\"Evaluation period\")] = 50,\n    log_period: Annotated[int, typer.Option(help=\"Log period\")] = 20,\n    samples: Annotated[int, typer.Option(help=\"Number of samples\")] = 9,\n    seed: Annotated[int, typer.Option(help=\"Random seed\")] = 42,\n    early_stop: Annotated[bool, typer.Option(help=\"Enable early stopping\")] = True,\n    patience: Annotated[int, typer.Option(help=\"Early stopping patience\")] = 10,\n    ema_enabled: Annotated[bool, typer.Option(help=\"Enable EMA\")] = False,\n    ema_decay: Annotated[float, typer.Option(help=\"EMA decay\")] = 0.999,\n    ema_warmup: Annotated[int, typer.Option(help=\"EMA warmup\")] = 2000,\n    learning_rate: Annotated[float, typer.Option(help=\"Learning rate\")] = 5e-4,\n    weight_decay: Annotated[float, typer.Option(help=\"Weight decay\")] = 0.02,\n    max_iters: Annotated[int, typer.Option(help=\"Maximum iterations\")] = 3000,\n    batch_size: Annotated[int, typer.Option(help=\"Batch size\")] = 16,\n    scheduler: Annotated[str, typer.Option(help=\"Scheduler type\")] = \"MULTISTEP\",\n    optimizer: Annotated[str, typer.Option(help=\"Optimizer type\")] = \"ADAMW\",\n    weight_decay_norm: Annotated[float, typer.Option(help=\"Weight decay for normalization layers\")] = 0.0,\n    weight_decay_embed: Annotated[float, typer.Option(help=\"Weight decay for embedding layers\")] = 0.0,\n    backbone_multiplier: Annotated[float, typer.Option(help=\"Backbone learning rate multiplier\")] = 0.1,\n    decoder_multiplier: Annotated[float, typer.Option(help=\"Decoder learning rate multiplier\")] = 1.0,\n    head_multiplier: Annotated[float, typer.Option(help=\"Head learning rate multiplier\")] = 1.0,\n    freeze_bn: Annotated[bool, typer.Option(help=\"Freeze batch normalization\")] = False,\n    clip_gradients: Annotated[float, typer.Option(help=\"Gradient clipping value\")] = 0.1,\n    size_divisibility: Annotated[int, typer.Option(help=\"Size divisibility\")] = 0,\n    gather_metric_period: Annotated[int, typer.Option(help=\"Gather metric period\")] = 1,\n    zero_grad_before_forward: Annotated[bool, typer.Option(help=\"Zero gradients before forward pass\")] = False,\n):\n    \"\"\"Validate a model on a dataset with comprehensive evaluation metrics.\n\n    This command performs model validation/evaluation on a specified dataset,\n    computing various metrics such as mAP, precision, recall, and other\n    task-specific evaluation measures. Supports the same configuration options\n    as training for consistency in evaluation setup.\n\n    Validation Metrics:\n        - **mAP (mean Average Precision)**: Overall detection accuracy\n        - **Precision/Recall**: Per-class and overall performance\n        - **F1-Score**: Harmonic mean of precision and recall\n        - **IoU (Intersection over Union)**: Bounding box accuracy\n        - **Inference Speed**: FPS and latency measurements\n\n    Args:\n        model (str): Name of the model to validate (e.g., 'fai-detr-m-coco').\n            Can be specified in several ways:\n            - **Pretrained model**: Simple model name like 'fai-detr-m-coco'\n            - **Hub model**: Format 'hub://&lt;model-ref&gt;' for models from Focoos Hub\n            - **Default directory model**: Model name for models in default Focoos directory\n        dataset (str): Name of the dataset for validation (e.g., 'mydataset.zip').\n        run_name (Optional[str]): Optional name for the validation run. If not provided,\n            generates a unique name using model name and UUID.\n        dataset_layout (DatasetLayout): Layout format of the dataset. Defaults to ROBOFLOW_COCO.\n        im_size (int): Input image size for validation. Defaults to 640.\n        output_dir (Optional[str]): Directory to save validation outputs and results.\n        ckpt_dir (Optional[str]): Directory containing model checkpoints.\n        init_checkpoint (Optional[str]): Path to specific checkpoint for validation.\n        resume (bool): Whether to resume from checkpoint (typically not used in validation).\n        num_gpus (int): Number of GPUs to use for validation. Defaults to auto-detected count.\n        device (str): Device type for validation ('cuda' or 'cpu'). Defaults to 'cuda'.\n        workers (int): Number of data loading workers. Defaults to 4.\n        amp_enabled (bool): Enable Automatic Mixed Precision for faster inference. Defaults to True.\n        ddp_broadcast_buffers (bool): Whether to broadcast buffers in DistributedDataParallel.\n            Defaults to False.\n        ddp_find_unused (bool): Whether to find unused parameters in DDP. Defaults to True.\n        checkpointer_period (int): Checkpoint saving frequency (not typically used in validation).\n        checkpointer_max_to_keep (int): Maximum checkpoints to keep.\n        eval_period (int): Frequency of evaluation logging (in iterations). Defaults to 50.\n        log_period (int): Frequency of metric logging (in iterations). Defaults to 20.\n        samples (int): Number of sample images to visualize during validation. Defaults to 9.\n        seed (int): Random seed for reproducible validation. Defaults to 42.\n        early_stop (bool): Enable early stopping (not typically used in validation).\n        patience (int): Early stopping patience.\n        ema_enabled (bool): Use Exponential Moving Average weights if available.\n        ema_decay (float): EMA decay rate.\n        ema_warmup (int): EMA warmup steps.\n        learning_rate (float): Learning rate (not used in validation).\n        weight_decay (float): Weight decay (not used in validation).\n        max_iters (int): Maximum validation iterations.\n        batch_size (int): Validation batch size. Defaults to 16.\n        scheduler (str): Scheduler type (not used in validation).\n        optimizer (str): Optimizer type (not used in validation).\n        weight_decay_norm (float): Weight decay for normalization layers.\n        weight_decay_embed (float): Weight decay for embedding layers.\n        backbone_multiplier (float): Backbone learning rate multiplier.\n        decoder_multiplier (float): Decoder learning rate multiplier.\n        head_multiplier (float): Head learning rate multiplier.\n        freeze_bn (bool): Whether to freeze batch normalization layers.\n        clip_gradients (float): Gradient clipping threshold.\n        size_divisibility (int): Image size divisibility constraint.\n        gather_metric_period (int): Frequency of metric gathering.\n        zero_grad_before_forward (bool): Whether to zero gradients before forward pass.\n        datasets_dir (Optional[str]): Custom directory for datasets.\n\n    Examples:\n        Basic validation with pretrained model:\n        ```bash\n        focoos val --model fai-detr-m-coco --dataset mydataset.zip\n        ```\n\n        Validation with model from Focoos Hub:\n        ```bash\n        focoos val --model hub://&lt;model-ref&gt; --dataset mydataset.zip\n        ```\n\n        Validation with local model in default Focoos directory:\n        ```bash\n        focoos val --model my-checkpoint-model --dataset mydataset.zip\n        ```\n\n        Validation with specific checkpoint:\n        ```bash\n        focoos val --model fai-detr-m-coco --dataset mydataset.zip \\\n                   --init-checkpoint path/to/checkpoint.pth --im-size 800\n        ```\n\n        Multi-GPU validation:\n        ```bash\n        focoos val --model fai-detr-m-coco --dataset large_dataset \\\n                   --num-gpus 2 --batch-size 32\n        ```\n\n        Validation with custom output directory:\n        ```bash\n        focoos val --model fai-detr-m-coco --dataset mydataset.zip \\\n                   --output-dir ./validation_results\n        ```\n\n    Raises:\n        AssertionError: If device, scheduler, or optimizer parameters are invalid.\n        FileNotFoundError: If dataset or checkpoint files are not found.\n        RuntimeError: If GPU resources are insufficient or model loading fails.\n\n    See Also:\n        - [`focoos train`][focoos.cli.cli.train]: For model training\n        - [`focoos predict`][focoos.cli.cli.predict]: For inference\n        - [Validation Guide](../validation.md): For detailed validation documentation\n    \"\"\"\n\n    typer.echo(\"\ud83d\udd0d Validation arguments:\")\n    typer.echo(f\"  Model: {model}\")\n    typer.echo(f\"  Dataset: {dataset}\")\n    typer.echo(f\"  Dataset layout: {dataset_layout}\")\n    typer.echo(f\"  Image size: {im_size}\")\n    typer.echo(f\"  Run name: {run_name}\")\n    typer.echo(f\"  Output dir: {output_dir}\")\n    typer.echo(f\"  Checkpoint dir: {ckpt_dir}\")\n    typer.echo(f\"  Init checkpoint: {init_checkpoint}\")\n    typer.echo(f\"  Resume: {resume}\")\n    typer.echo(f\"  Num GPUs: {num_gpus}\")\n    typer.echo(f\"  Device: {device}\")\n    typer.echo(f\"  Workers: {workers}\")\n    typer.echo(f\"  Batch size: {batch_size}\")\n    typer.echo(f\"  Learning rate: {learning_rate}\")\n    typer.echo(f\"  Weight decay: {weight_decay}\")\n    typer.echo(f\"  Max iterations: {max_iters}\")\n    typer.echo(f\"  Scheduler: {scheduler}\")\n    typer.echo(f\"  Optimizer: {optimizer}\")\n    typer.echo(f\"  Weight decay norm: {weight_decay_norm}\")\n    typer.echo(f\"  Weight decay embed: {weight_decay_embed}\")\n    typer.echo(f\"  Backbone multiplier: {backbone_multiplier}\")\n    typer.echo(f\"  Decoder multiplier: {decoder_multiplier}\")\n    typer.echo(f\"  Head multiplier: {head_multiplier}\")\n    typer.echo(f\"  Freeze BN: {freeze_bn}\")\n\n    try:\n        validated_device = cast(DeviceType, device)\n        assert device in get_args(DeviceType)\n        validated_scheduler = cast(SchedulerType, scheduler.upper())\n        assert scheduler in get_args(SchedulerType)\n        validated_optimizer = cast(OptimizerType, optimizer.upper())\n        assert optimizer in get_args(OptimizerType)\n\n        val_command(\n            model_name=model,\n            dataset_name=dataset,\n            dataset_layout=dataset_layout,\n            im_size=im_size,\n            run_name=run_name or f\"{model}-{uuid.uuid4()}\",\n            output_dir=output_dir,\n            ckpt_dir=ckpt_dir,\n            init_checkpoint=init_checkpoint,\n            resume=resume,\n            num_gpus=num_gpus,\n            device=validated_device,\n            workers=workers,\n            amp_enabled=amp_enabled,\n            ddp_broadcast_buffers=ddp_broadcast_buffers,\n            ddp_find_unused=ddp_find_unused,\n            checkpointer_period=checkpointer_period,\n            checkpointer_max_to_keep=checkpointer_max_to_keep,\n            eval_period=eval_period,\n            log_period=log_period,\n            samples=samples,\n            seed=seed,\n            early_stop=early_stop,\n            patience=patience,\n            ema_enabled=ema_enabled,\n            ema_decay=ema_decay,\n            ema_warmup=ema_warmup,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            max_iters=max_iters,\n            batch_size=batch_size,\n            scheduler=validated_scheduler,\n            optimizer=validated_optimizer,\n            weight_decay_norm=weight_decay_norm,\n            weight_decay_embed=weight_decay_embed,\n            backbone_multiplier=backbone_multiplier,\n            decoder_multiplier=decoder_multiplier,\n            head_multiplier=head_multiplier,\n            freeze_bn=freeze_bn,\n            clip_gradients=clip_gradients,\n            size_divisibility=size_divisibility,\n            gather_metric_period=gather_metric_period,\n            zero_grad_before_forward=zero_grad_before_forward,\n            datasets_dir=datasets_dir,\n        )\n    except Exception as e:\n        typer.echo(f\"\u274c Validation failed: {e}\")\n</code></pre> <p>Hub command implementation.</p> <p>This module implements the hub-related commands for the Focoos CLI. It provides functionality to interact with the Focoos Hub, including listing available models and datasets, both private and shared resources.</p> <p>The Hub commands allow users to: - Browse and discover available pretrained models - List private and shared datasets - Get detailed information about model specifications - Access dataset metadata and statistics</p> <p>Examples:</p> <p>List available models: <pre><code>focoos hub models\n</code></pre></p> <p>List datasets including shared ones: <pre><code>focoos hub datasets --include-shared\n</code></pre></p> <p>Download a dataset: <pre><code>focoos hub dataset download --ref my-dataset --path ./data\n</code></pre></p> <p>Upload a dataset: <pre><code>focoos hub dataset upload --ref my-dataset --path ./data.zip\n</code></pre></p> See Also <ul> <li><code>focoos.hub.focoos_hub.FocoosHUB</code>: Core hub functionality</li> </ul>"},{"location":"api/cli/#focoos.cli.cli.version","title":"<code>version()</code>","text":"<p>Show Focoos version information.</p> <p>This command displays the current version of the Focoos library installed on the system. If the version cannot be determined, it shows \"Unknown\".</p> <p>Examples:</p> <pre><code>focoos version\n</code></pre> <p>Output: <pre><code>Focoos version: 0.15.0\n</code></pre></p> <p>Raises:</p> Type Description <code>Exit</code> <p>If there's an error retrieving version information.</p> See Also <ul> <li><code>focoos checks</code>: For system diagnostics</li> <li><code>focoos settings</code>: For configuration info</li> </ul> Source code in <code>focoos/cli/cli.py</code> <pre><code>@app.command(\"version\")\ndef version():\n    \"\"\"Show Focoos version information.\n\n    This command displays the current version of the Focoos library installed\n    on the system. If the version cannot be determined, it shows \"Unknown\".\n\n    Examples:\n        ```bash\n        focoos version\n        ```\n\n        Output:\n        ```\n        Focoos version: 0.15.0\n        ```\n\n    Raises:\n        typer.Exit: If there's an error retrieving version information.\n\n    See Also:\n        - [`focoos checks`][focoos.cli.cli.checks]: For system diagnostics\n        - [`focoos settings`][focoos.cli.cli.settings]: For configuration info\n    \"\"\"\n    try:\n        from focoos.utils.system import get_focoos_version\n\n        version = get_focoos_version()\n        typer.echo(f\"Focoos version: {version}\")\n    except Exception:\n        typer.echo(\"Focoos version: Unknown\")\n</code></pre>"},{"location":"api/cli/#focoos.cli.commands.hub.datasets","title":"<code>datasets(include_shared=typer.Option(False, help='Include shared/public datasets in addition to private ones'))</code>","text":"<p>List available datasets from the Focoos Hub.</p> <p>Retrieves and displays datasets available on the Focoos Hub. By default, only shows private datasets associated with your account. Use the <code>--include-shared</code> flag to also include publicly shared datasets.</p> <p>For each dataset, displays comprehensive information including: - Name: Human-readable dataset name - Reference: Unique dataset identifier for CLI usage - Task: Computer vision task type - Layout: Dataset format/structure - Description: Dataset description and details - Statistics: Training/validation split sizes and total size</p> <p>Parameters:</p> Name Type Description Default <code>include_shared</code> <code>bool</code> <p>Whether to include shared/public datasets in addition to private ones. Defaults to False.</p> <code>Option(False, help='Include shared/public datasets in addition to private ones')</code> <p>Examples:</p> <pre><code># List only your private datasets\nfocoos hub datasets\n\n# List both private and shared datasets\nfocoos hub datasets --include-shared\n</code></pre> <p>Output example: <pre><code>\ud83d\udd0d Found 3 dataset(s):\n\n\ud83d\udce6 Dataset #1\n   \ud83c\udff7\ufe0f  Name: My Custom Dataset\n   \ud83d\udd17 Reference: my-custom-dataset\n   \ud83c\udfaf Task: object_detection\n   \ud83d\udccb Layout: roboflow_coco\n   \ud83d\udcdd Description: Custom dataset for object detection\n   \ud83e\udd16 Train Length: 1000\n   \ud83e\udd16 Val Length: 200\n   \ud83e\udd16 Size MB: 150.5\n</code></pre></p> Note <ul> <li>This command requires an internet connection to access the Focoos Hub.</li> <li>This command requires a Focoos API Key to be set in the environment variable <code>FOCOOS_API_KEY</code>.</li> <li>Shared datasets may require appropriate permissions to access.</li> </ul> Source code in <code>focoos/cli/commands/hub.py</code> <pre><code>@app.command()\ndef datasets(\n    include_shared: bool = typer.Option(False, help=\"Include shared/public datasets in addition to private ones\"),\n):\n    \"\"\"List available datasets from the Focoos Hub.\n\n    Retrieves and displays datasets available on the Focoos Hub. By default,\n    only shows private datasets associated with your account. Use the\n    `--include-shared` flag to also include publicly shared datasets.\n\n    For each dataset, displays comprehensive information including:\n    - **Name**: Human-readable dataset name\n    - **Reference**: Unique dataset identifier for CLI usage\n    - **Task**: Computer vision task type\n    - **Layout**: Dataset format/structure\n    - **Description**: Dataset description and details\n    - **Statistics**: Training/validation split sizes and total size\n\n    Args:\n        include_shared (bool, optional): Whether to include shared/public datasets\n            in addition to private ones. Defaults to False.\n\n    Examples:\n        ```bash\n        # List only your private datasets\n        focoos hub datasets\n\n        # List both private and shared datasets\n        focoos hub datasets --include-shared\n        ```\n\n        Output example:\n        ```\n        \ud83d\udd0d Found 3 dataset(s):\n\n        \ud83d\udce6 Dataset #1\n           \ud83c\udff7\ufe0f  Name: My Custom Dataset\n           \ud83d\udd17 Reference: my-custom-dataset\n           \ud83c\udfaf Task: object_detection\n           \ud83d\udccb Layout: roboflow_coco\n           \ud83d\udcdd Description: Custom dataset for object detection\n           \ud83e\udd16 Train Length: 1000\n           \ud83e\udd16 Val Length: 200\n           \ud83e\udd16 Size MB: 150.5\n        ```\n\n    Note:\n        - This command requires an internet connection to access the Focoos Hub.\n        - This command requires a Focoos API Key to be set in the environment variable `FOCOOS_API_KEY`.\n        - Shared datasets may require appropriate permissions to access.\n    \"\"\"\n    typer.echo(\"Listing datasets...\")\n    try:\n        focoos_hub = FocoosHUB()\n        datasets = focoos_hub.list_remote_datasets(include_shared=include_shared)\n        if not datasets:\n            dataset_type = \"shared and private\" if include_shared else \"private\"\n            typer.echo(f\"\u274c No {dataset_type} datasets found.\")\n            return\n\n        dataset_type_desc = \"shared and private\" if include_shared else \"private\"\n        typer.echo(f\"\ud83d\udd0d Found {len(datasets)} {dataset_type_desc} dataset(s):\\n\")\n\n        for i, dataset in enumerate(datasets, 1):\n            typer.echo(f\"\ud83d\udce6 Dataset #{i}\")\n            typer.echo(f\"   \ud83c\udff7\ufe0f  Name: {dataset.name}\")\n            typer.echo(f\"   \ud83d\udd17 Reference: {dataset.ref}\")\n            typer.echo(f\"   \ud83c\udfaf Task: {dataset.task}\")\n            typer.echo(f\"   \ud83d\udccb Layout: {dataset.layout}\")\n            typer.echo(f\"   \ud83d\udcdd Description: {dataset.description}\")\n            if dataset.spec:\n                typer.echo(f\"   \ud83d\udcca Train Length: {dataset.spec.train_length}\")\n                typer.echo(f\"   \ud83d\udcca Val Length: {dataset.spec.valid_length}\")\n                typer.echo(f\"   \ud83d\udcca Size MB: {dataset.spec.size_mb}\")\n\n            if i &lt; len(datasets):\n                typer.echo(\"   \" + \"\u2500\" * 50)\n                typer.echo(\"\")\n    except Exception as e:\n        typer.echo(f\"\u274c Failed to list datasets: {e}\")\n        raise typer.Exit(1)\n</code></pre>"},{"location":"api/cli/#focoos.cli.commands.hub.download","title":"<code>download(ref, path=None)</code>","text":"<p>Download a dataset from the Focoos Hub.</p> <p>Downloads a dataset from the Focoos Hub to a specified local path.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>The reference ID of the dataset to download</p> required <code>path</code> <code>str</code> <p>Path to download the dataset to</p> <code>None</code> <p>Examples:</p> <pre><code># Download a dataset to a specific path\nfocoos hub dataset download --ref my-dataset-ref --path ./data\n</code></pre> Notes <ul> <li>This command requires an internet connection to access the Focoos Hub.</li> <li>This command requires a Focoos API Key to be set in the environment variable <code>FOCOOS_API_KEY</code>.</li> </ul> Source code in <code>focoos/cli/commands/hub.py</code> <pre><code>@dataset_app.command()\ndef download(\n    ref: Annotated[str, typer.Option(..., help=\"The reference ID of the dataset to download\")],\n    path: Annotated[Optional[str], typer.Option(help=\"Path to download the dataset to\")] = None,\n):\n    \"\"\"Download a dataset from the Focoos Hub.\n\n    Downloads a dataset from the Focoos Hub to a specified local path.\n\n    Args:\n        ref (str): The reference ID of the dataset to download\n        path (str): Path to download the dataset to\n\n    Examples:\n        ```bash\n        # Download a dataset to a specific path\n        focoos hub dataset download --ref my-dataset-ref --path ./data\n        ```\n\n    Notes:\n        - This command requires an internet connection to access the Focoos Hub.\n        - This command requires a Focoos API Key to be set in the environment variable `FOCOOS_API_KEY`.\n    \"\"\"\n    typer.echo(f\"Downloading dataset from {ref} to {path}...\")\n    try:\n        focoos_hub = FocoosHUB()\n        dataset = focoos_hub.get_remote_dataset(ref)\n        if path:\n            dataset.download_data(path)\n        else:\n            dataset.download_data()\n    except Exception as e:\n        typer.echo(f\"\u274c Failed to download dataset: {e}\")\n        raise typer.Exit(1)\n</code></pre>"},{"location":"api/cli/#focoos.cli.commands.hub.main","title":"<code>main()</code>","text":"<p>Hub commands for interacting with the Focoos Hub.</p> <p>The Focoos Hub provides access to pretrained models and datasets. Use subcommands to list and discover available resources.</p> <p>Available subcommands: - <code>models</code>: List the available user models - <code>datasets</code>: List the available user datasets (private and optionally shared) - <code>dataset</code>: Dataset operations (download, upload)</p> Notes <ul> <li>This command requires an internet connection to access the Focoos Hub.</li> <li>This command requires a Focoos API Key to be set in the environment variable <code>FOCOOS_API_KEY</code>.</li> </ul> <p>Examples:</p> <pre><code># List all available models\nfocoos hub models\n\n# List your datasets\nfocoos hub datasets\n\n# List datasets including shared ones\nfocoos hub datasets --include-shared\n\n# Download a dataset\nfocoos hub dataset download --ref my-dataset --path ./data\n\n# Upload a dataset\nfocoos hub dataset upload --ref my-dataset --path ./data.zip\n</code></pre> Source code in <code>focoos/cli/commands/hub.py</code> <pre><code>@app.callback()\ndef main():\n    \"\"\"Hub commands for interacting with the Focoos Hub.\n\n    The Focoos Hub provides access to pretrained models and datasets.\n    Use subcommands to list and discover available resources.\n\n    Available subcommands:\n    - `models`: List the available user models\n    - `datasets`: List the available user datasets (private and optionally shared)\n    - `dataset`: Dataset operations (download, upload)\n\n    Notes:\n        - This command requires an internet connection to access the Focoos Hub.\n        - This command requires a Focoos API Key to be set in the environment variable `FOCOOS_API_KEY`.\n\n    Examples:\n\n        # List all available models\n        focoos hub models\n\n        # List your datasets\n        focoos hub datasets\n\n        # List datasets including shared ones\n        focoos hub datasets --include-shared\n\n        # Download a dataset\n        focoos hub dataset download --ref my-dataset --path ./data\n\n        # Upload a dataset\n        focoos hub dataset upload --ref my-dataset --path ./data.zip\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cli/#focoos.cli.commands.hub.models","title":"<code>models()</code>","text":"<p>List all available pretrained models from the Focoos Hub.</p> <p>Retrieves and displays a comprehensive list of all pretrained models available on the Focoos Hub. For each model, shows detailed information including name, reference, task type, description, status, and model type.</p> <p>The displayed information includes: - Name: Human-readable model name - Reference: Unique model identifier for CLI usage - Task: Computer vision task (detection, segmentation, classification) - Description: Detailed model description and capabilities - Status: Model availability status - Focoos Model: Model architecture/family information</p> <p>Examples:</p> <pre><code># List all available models\nfocoos hub models\n</code></pre> <p>Output example: <pre><code>\ud83d\udd0d Found 12 model(s):\n\n\ud83d\udce6 Model #1\n   \ud83c\udff7\ufe0f  Name: FAI DETR Medium COCO\n   \ud83d\udd17 Reference: fai-detr-m-coco\n   \ud83c\udfaf Task: object_detection\n   \ud83d\udcdd Description: Medium-sized DETR model trained on COCO dataset\n   \u26a1 Status: ready\n   \ud83e\udd16 Focoos Model: fai_detr\n</code></pre></p> Note <ul> <li>This command requires an internet connection to access the Focoos Hub.</li> <li>This command requires a Focoos API Key to be set in the environment variable <code>FOCOOS_API_KEY</code>.</li> </ul> Source code in <code>focoos/cli/commands/hub.py</code> <pre><code>@app.command()\ndef models():\n    \"\"\"List all available pretrained models from the Focoos Hub.\n\n    Retrieves and displays a comprehensive list of all pretrained models\n    available on the Focoos Hub. For each model, shows detailed information\n    including name, reference, task type, description, status, and model type.\n\n    The displayed information includes:\n    - **Name**: Human-readable model name\n    - **Reference**: Unique model identifier for CLI usage\n    - **Task**: Computer vision task (detection, segmentation, classification)\n    - **Description**: Detailed model description and capabilities\n    - **Status**: Model availability status\n    - **Focoos Model**: Model architecture/family information\n\n    Examples:\n        ```bash\n        # List all available models\n        focoos hub models\n        ```\n\n        Output example:\n        ```\n        \ud83d\udd0d Found 12 model(s):\n\n        \ud83d\udce6 Model #1\n           \ud83c\udff7\ufe0f  Name: FAI DETR Medium COCO\n           \ud83d\udd17 Reference: fai-detr-m-coco\n           \ud83c\udfaf Task: object_detection\n           \ud83d\udcdd Description: Medium-sized DETR model trained on COCO dataset\n           \u26a1 Status: ready\n           \ud83e\udd16 Focoos Model: fai_detr\n        ```\n\n    Note:\n        - This command requires an internet connection to access the Focoos Hub.\n        - This command requires a Focoos API Key to be set in the environment variable `FOCOOS_API_KEY`.\n    \"\"\"\n    typer.echo(\"Listing models...\")\n    try:\n        focoos_hub = FocoosHUB()\n        models = focoos_hub.list_remote_models()\n        if not models:\n            typer.echo(\"\u274c No models found.\")\n            return\n\n        typer.echo(f\"\ud83d\udd0d Found {len(models)} model(s):\\n\")\n\n        for i, model in enumerate(models, 1):\n            typer.echo(f\"\ud83d\udce6 Model #{i}\")\n            typer.echo(f\"   \ud83c\udff7\ufe0f  Name: {model.name}\")\n            typer.echo(f\"   \ud83d\udd17 Reference: {model.ref}\")\n            typer.echo(f\"   \ud83c\udfaf Task: {model.task}\")\n            typer.echo(f\"   \ud83d\udcdd Description: {model.description}\")\n            typer.echo(f\"   \u26a1 Status: {model.status}\")\n            typer.echo(f\"   \ud83e\udd16 Focoos Model: {model.focoos_model}\")\n\n            if i &lt; len(models):\n                typer.echo(\"   \" + \"\u2500\" * 50)\n                typer.echo(\"\")\n    except Exception as e:\n        typer.echo(f\"\u274c Failed to list models: {e}\")\n        raise typer.Exit(1)\n</code></pre>"},{"location":"api/cli/#focoos.cli.commands.hub.upload","title":"<code>upload(ref, path)</code>","text":"<p>Upload a dataset to the Focoos Hub (You must first create a dataset on the Focoos hub).</p> <p>Uploads a dataset to the Focoos Hub from a specified local path.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>The reference ID of the dataset to upload</p> required <code>path</code> <code>str</code> <p>Path to upload the dataset from</p> required <p>Examples:</p> <pre><code># Upload a dataset from a specific path\nfocoos hub dataset upload --ref my-dataset-ref --path ./data.zip\n</code></pre> <p>For more information on how the dataset should be structured look at the <code>documentation</code>.</p> Notes <ul> <li>This command requires an internet connection to access the Focoos Hub.</li> <li>This command requires a Focoos API Key to be set in the environment variable <code>FOCOOS_API_KEY</code>.</li> <li>You must first create a dataset on the Focoos hub.</li> </ul> Source code in <code>focoos/cli/commands/hub.py</code> <pre><code>@dataset_app.command()\ndef upload(\n    ref: Annotated[str, typer.Option(..., help=\"The reference ID of the dataset to upload\")],\n    path: Annotated[str, typer.Option(..., help=\"Path to upload the dataset from\")],\n):\n    \"\"\"Upload a dataset to the Focoos Hub (You must first create a dataset on the Focoos hub).\n\n    Uploads a dataset to the Focoos Hub from a specified local path.\n\n    Args:\n        ref (str): The reference ID of the dataset to upload\n        path (str): Path to upload the dataset from\n\n    Examples:\n        ```bash\n        # Upload a dataset from a specific path\n        focoos hub dataset upload --ref my-dataset-ref --path ./data.zip\n        ```\n\n    For more information on how the dataset should be structured look at the [`documentation`](../api/ports.md#focoos.ports.DatasetLayout).\n\n    Notes:\n        - This command requires an internet connection to access the Focoos Hub.\n        - This command requires a Focoos API Key to be set in the environment variable `FOCOOS_API_KEY`.\n        - You must first create a dataset on the Focoos hub.\n    \"\"\"\n    typer.echo(f\"Uploading dataset from {path} to {ref}...\")\n\n    try:\n        focoos_hub = FocoosHUB()\n        dataset = focoos_hub.get_remote_dataset(ref)\n        spec = dataset.upload_data(path)\n        dataset_info = dataset.get_info()\n        if not spec:\n            raise Exception(\"Failed to upload dataset.\")\n        typer.echo(\"\u2705 Dataset uploaded successfully!\")\n        typer.echo(f\"   \ud83c\udff7\ufe0f  Name: {dataset_info.name}\")\n        typer.echo(f\"   \ud83d\udd17 Reference: {dataset_info.ref}\")\n        typer.echo(f\"   \ud83c\udfaf Task: {dataset_info.task}\")\n        typer.echo(f\"   \ud83d\udccb Layout: {dataset_info.layout}\")\n        typer.echo(f\"   \ud83d\udcdd Description: {dataset_info.description}\")\n        typer.echo(f\"   \ud83d\udcca Train Length: {spec.train_length}\")\n        typer.echo(f\"   \ud83d\udcca Val Length: {spec.valid_length}\")\n        typer.echo(f\"   \ud83d\udcca Size MB: {spec.size_mb}\")\n    except Exception as e:\n        typer.echo(f\"\u274c Failed to upload dataset: {e}\")\n        raise typer.Exit(1)\n</code></pre>"},{"location":"api/config/","title":"config","text":"<p>Configuration module for Focoos AI SDK.</p> <p>This module defines the configuration settings for the Focoos AI SDK, including API credentials, logging levels, default endpoints, and runtime preferences. It provides a centralized way to manage SDK behavior through environment variables.</p> <p>Classes:</p> Name Description <code>FocoosConfig</code> <p>Pydantic settings class for Focoos SDK configuration.</p> Constants <p>LogLevel: Type definition for supported logging levels. FOCOOS_CONFIG: Global configuration instance used throughout the SDK.</p>"},{"location":"api/config/#focoos.config.FocoosConfig","title":"<code>FocoosConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration settings for the Focoos AI SDK.</p> <p>This class uses pydantic_settings to manage configuration values, supporting environment variable overrides and validation. All attributes can be configured via environment variables with the same name.</p> <p>Attributes:</p> Name Type Description <code>focoos_api_key</code> <code>Optional[str]</code> <p>API key for authenticating with Focoos services. Required for accessing protected API endpoints. Defaults to None.</p> <code>focoos_log_level</code> <code>LogLevel</code> <p>Logging level for the SDK to control verbosity. Defaults to \"DEBUG\".</p> <code>default_host_url</code> <code>str</code> <p>Default API endpoint URL for all service requests. Defaults to the production API URL.</p> <code>runtime_type</code> <code>RuntimeTypes</code> <p>Default runtime type for model inference engines. Defaults to TORCHSCRIPT_32 for optimal performance.</p> <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations for model initialization to stabilize performance metrics. Defaults to 2.</p> Example <p>Setting configuration via environment variables in bash: <pre><code># Set API key and change log level\nexport FOCOOS_API_KEY=\"your-api-key-here\"\nexport FOCOOS_LOG_LEVEL=\"INFO\"\nexport DEFAULT_HOST_URL=\"https://custom-api-endpoint.com\"\nexport RUNTIME_TYPE=\"ONNX_CUDA32\"\nexport WARMUP_ITER=\"3\"\n\n# Then run your Python application\npython your_app.py\n</code></pre></p> Source code in <code>focoos/config.py</code> <pre><code>class FocoosConfig(BaseSettings):\n    \"\"\"\n    Configuration settings for the Focoos AI SDK.\n\n    This class uses pydantic_settings to manage configuration values,\n    supporting environment variable overrides and validation. All attributes\n    can be configured via environment variables with the same name.\n\n    Attributes:\n        focoos_api_key (Optional[str]): API key for authenticating with Focoos services.\n            Required for accessing protected API endpoints. Defaults to None.\n        focoos_log_level (LogLevel): Logging level for the SDK to control verbosity.\n            Defaults to \"DEBUG\".\n        default_host_url (str): Default API endpoint URL for all service requests.\n            Defaults to the production API URL.\n        runtime_type (RuntimeTypes): Default runtime type for model inference engines.\n            Defaults to TORCHSCRIPT_32 for optimal performance.\n        warmup_iter (int): Number of warmup iterations for model initialization to\n            stabilize performance metrics. Defaults to 2.\n\n    Example:\n        Setting configuration via environment variables in bash:\n        ```bash\n        # Set API key and change log level\n        export FOCOOS_API_KEY=\"your-api-key-here\"\n        export FOCOOS_LOG_LEVEL=\"INFO\"\n        export DEFAULT_HOST_URL=\"https://custom-api-endpoint.com\"\n        export RUNTIME_TYPE=\"ONNX_CUDA32\"\n        export WARMUP_ITER=\"3\"\n\n        # Then run your Python application\n        python your_app.py\n        ```\n    \"\"\"\n\n    focoos_api_key: Optional[str] = None\n    focoos_log_level: LogLevel = \"DEBUG\"\n    default_host_url: str = PROD_API_URL\n    runtime_type: RuntimeType = RuntimeType.TORCHSCRIPT_32\n    warmup_iter: int = 2\n</code></pre>"},{"location":"api/focoos_model/","title":"FocoosModel","text":""},{"location":"api/focoos_model/#focoos.models.focoos_model.ExportableModel","title":"<code>ExportableModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>A wrapper class for making models exportable to different formats.</p> <p>This class wraps a BaseModelNN model to make it compatible with export formats like ONNX and TorchScript by handling the output formatting.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModelNN</code> <p>The base model to wrap for export.</p> required <code>device</code> <p>The device to move the model to. Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>class ExportableModel(torch.nn.Module):\n    \"\"\"A wrapper class for making models exportable to different formats.\n\n    This class wraps a BaseModelNN model to make it compatible with export formats\n    like ONNX and TorchScript by handling the output formatting.\n\n    Args:\n        model: The base model to wrap for export.\n        device: The device to move the model to. Defaults to \"cuda\".\n    \"\"\"\n\n    def __init__(self, model: BaseModelNN, device=\"cuda\", input_size: Optional[Union[int, Tuple[int, int]]] = None):\n        \"\"\"Initialize the ExportableModel.\n\n        Args:\n            model: The base model to wrap for export.\n            device: The device to move the model to. Defaults to \"cuda\".\n            input_size: Input image size for export optimization. Can be int (square) or tuple (height, width).\n        \"\"\"\n        super().__init__()\n\n        # Configure export mode with correct input size\n        test_cfg = None\n        if input_size is not None:\n            if isinstance(input_size, int):\n                # Square image: convert int to tuple\n                test_cfg = {\"input_size\": (input_size, input_size)}\n            else:\n                # Already a tuple (height, width)\n                test_cfg = {\"input_size\": input_size}\n\n        # Use BaseModelNN's switch_to_export method which accepts test_cfg\n\n        self.model = model.eval().to(device)\n        self.model.switch_to_export(test_cfg=test_cfg, device=device)\n\n    def forward(self, x):\n        \"\"\"Forward pass through the wrapped model.\n\n        Args:\n            x: Input tensor to pass through the model.\n\n        Returns:\n            Model output converted to tuple format for export compatibility.\n        \"\"\"\n        return self.model(x).to_tuple()\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.ExportableModel.__init__","title":"<code>__init__(model, device='cuda', input_size=None)</code>","text":"<p>Initialize the ExportableModel.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModelNN</code> <p>The base model to wrap for export.</p> required <code>device</code> <p>The device to move the model to. Defaults to \"cuda\".</p> <code>'cuda'</code> <code>input_size</code> <code>Optional[Union[int, Tuple[int, int]]]</code> <p>Input image size for export optimization. Can be int (square) or tuple (height, width).</p> <code>None</code> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def __init__(self, model: BaseModelNN, device=\"cuda\", input_size: Optional[Union[int, Tuple[int, int]]] = None):\n    \"\"\"Initialize the ExportableModel.\n\n    Args:\n        model: The base model to wrap for export.\n        device: The device to move the model to. Defaults to \"cuda\".\n        input_size: Input image size for export optimization. Can be int (square) or tuple (height, width).\n    \"\"\"\n    super().__init__()\n\n    # Configure export mode with correct input size\n    test_cfg = None\n    if input_size is not None:\n        if isinstance(input_size, int):\n            # Square image: convert int to tuple\n            test_cfg = {\"input_size\": (input_size, input_size)}\n        else:\n            # Already a tuple (height, width)\n            test_cfg = {\"input_size\": input_size}\n\n    # Use BaseModelNN's switch_to_export method which accepts test_cfg\n\n    self.model = model.eval().to(device)\n    self.model.switch_to_export(test_cfg=test_cfg, device=device)\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.ExportableModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the wrapped model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input tensor to pass through the model.</p> required <p>Returns:</p> Type Description <p>Model output converted to tuple format for export compatibility.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass through the wrapped model.\n\n    Args:\n        x: Input tensor to pass through the model.\n\n    Returns:\n        Model output converted to tuple format for export compatibility.\n    \"\"\"\n    return self.model(x).to_tuple()\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel","title":"<code>FocoosModel</code>","text":"<p>Main model class for Focoos computer vision models.</p> <p>This class provides a high-level interface for training, testing, exporting, and running inference with Focoos models. It handles model configuration, weight loading, preprocessing, and postprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModelNN</code> <p>The underlying neural network model.</p> required <code>model_info</code> <code>ModelInfo</code> <p>Metadata and configuration information for the model.</p> required Source code in <code>focoos/models/focoos_model.py</code> <pre><code>class FocoosModel:\n    \"\"\"Main model class for Focoos computer vision models.\n\n    This class provides a high-level interface for training, testing, exporting,\n    and running inference with Focoos models. It handles model configuration,\n    weight loading, preprocessing, and postprocessing.\n\n    Args:\n        model: The underlying neural network model.\n        model_info: Metadata and configuration information for the model.\n    \"\"\"\n\n    def __init__(self, model: BaseModelNN, model_info: ModelInfo):\n        \"\"\"Initialize the FocoosModel.\n\n        Args:\n            model: The underlying neural network model.\n            model_info: Metadata and configuration information for the model.\n        \"\"\"\n\n        self.model_info = model_info\n        self.processor = ProcessorManager.get_processor(\n            self.model_info.model_family,\n            self.model_info.config,  # type: ignore\n            self.model_info.im_size,\n        )\n        self.processor.eval()\n        self.model = model.eval()\n\n        if torch.cuda.is_available():\n            try:\n                self.model = self.model.cuda()\n            except Exception:\n                logger.warning(\"Unable to use CUDA\")\n\n        if torch.backends.mps.is_available():\n            try:\n                self.model = self.model.to(device=\"mps\")\n            except Exception:\n                logger.warning(\"Unable to use MPS\")\n\n        if self.model_info.weights_uri:\n            self._load_weights()\n        else:\n            logger.warning(f\"\u26a0\ufe0f Model {self.model_info.name} has no pretrained weights\")\n\n    def __str__(self):\n        \"\"\"Return string representation of the model.\n\n        Returns:\n            String containing model name and family.\n        \"\"\"\n        return f\"{self.model_info.name} ({self.model_info.model_family.value})\"\n\n    def __repr__(self):\n        \"\"\"Return detailed string representation of the model.\n\n        Returns:\n            String containing model name and family.\n        \"\"\"\n        return f\"{self.model_info.name} ({self.model_info.model_family.value})\"\n\n    def _setup_model_for_training(self, train_args: TrainerArgs, data_train: MapDataset, data_val: MapDataset):\n        \"\"\"Set up the model and metadata for training.\n\n        This method configures the model information with training parameters,\n        device information, dataset metadata, and initializes training status.\n\n        Args:\n            train_args: Training configuration arguments.\n            data_train: Training dataset.\n            data_val: Validation dataset.\n        \"\"\"\n        device = get_cpu_name()\n        system_info = get_system_info()\n        if (\n            train_args.device == \"cuda\"\n            and system_info.gpu_info\n            and system_info.gpu_info.devices\n            and len(system_info.gpu_info.devices) &gt; 0\n        ):\n            device = system_info.gpu_info.devices[0].gpu_name\n        elif train_args.device == \"mps\" and torch.backends.mps.is_available():\n            device = \"mps\"\n        else:\n            device = \"cpu\"\n        self.model_info.ref = None\n\n        self.model_info.train_args = train_args  # type: ignore\n        self.model_info.val_dataset = data_val.dataset.metadata.name\n        self.model_info.val_metrics = None\n        self.model_info.classes = data_val.dataset.metadata.classes\n        self.model_info.focoos_version = get_focoos_version()\n        self.model_info.status = ModelStatus.TRAINING_STARTING\n        self.model_info.updated_at = datetime.now().isoformat()\n        self.model_info.latency = []\n        self.model_info.metrics = None\n        self.model_info.training_info = TrainingInfo(\n            instance_device=device,\n            main_status=ModelStatus.TRAINING_STARTING,\n            start_time=datetime.now().isoformat(),\n            status_transitions=[\n                dict(\n                    status=ModelStatus.TRAINING_STARTING,\n                    timestamp=datetime.now().isoformat(),\n                )\n            ],\n        )\n\n        self.model_info.classes = data_train.dataset.metadata.classes\n        self.model_info.config[\"num_classes\"] = len(data_train.dataset.metadata.classes)\n        if data_train.dataset.metadata.keypoints is not None:\n            self.model_info.config[\"keypoints\"] = data_train.dataset.metadata.keypoints\n            self.model_info.config[\"num_keypoints\"] = len(data_train.dataset.metadata.keypoints)\n        if data_train.dataset.metadata.keypoints_skeleton is not None:\n            self.model_info.config[\"skeleton\"] = data_train.dataset.metadata.keypoints_skeleton\n        self._reload_model()\n        self.model_info.name = train_args.run_name.strip()\n        self.processor = ProcessorManager.get_processor(self.model_info.model_family, self.model_info.config)  # type: ignore\n        self.model = self.model.train()\n        assert self.model_info.task == data_train.dataset.metadata.task, \"Task mismatch between model and dataset.\"\n\n    def train(self, args: TrainerArgs, data_train: MapDataset, data_val: MapDataset, hub: Optional[FocoosHUB] = None):\n        \"\"\"Train the model on the provided datasets.\n\n        This method handles both single-GPU and multi-GPU distributed training.\n        It sets up the model for training, optionally syncs with Focoos Hub,\n        and manages the training process.\n\n        Args:\n            args: Training configuration and hyperparameters.\n            data_train: Training dataset containing images and annotations.\n            data_val: Validation dataset for model evaluation.\n            hub: Optional Focoos Hub instance for model syncing.\n\n        Raises:\n            AssertionError: If task mismatch between model and dataset.\n            AssertionError: If number of classes mismatch between model and dataset.\n            AssertionError: If num_gpus is 0 (GPU training is required).\n            FileNotFoundError: If training artifacts are not found after completion.\n        \"\"\"\n        from focoos.trainer.trainer import run_train\n\n        assert data_train.dataset.metadata.num_classes &gt; 0, \"Number of dataset classes must be greater than 0\"\n        self._setup_model_for_training(args, data_train, data_val)\n\n        assert self.model_info.task == data_train.dataset.metadata.task, \"Task mismatch between model and dataset.\"\n        assert self.model_info.config[\"num_classes\"] == data_train.dataset.metadata.num_classes, (\n            \"Number of classes mismatch between model and dataset.\"\n        )\n        assert args.num_gpus, \"Training without GPUs is not supported. num_gpus must be greater than 0\"\n        if args.num_gpus &gt; 1:\n            launch(\n                run_train,\n                args.num_gpus,\n                dist_url=\"auto\",\n                args=(args, data_train, data_val, self.model, self.processor, self.model_info, hub),\n            )\n\n            logger.info(\"Training done, resuming main process.\")\n            # here i should restore the best model and config since in DDP it is not updated\n            final_folder = os.path.join(args.output_dir, args.run_name)\n            model_path = os.path.join(final_folder, ArtifactName.WEIGHTS)\n            metadata_path = os.path.join(final_folder, ArtifactName.INFO)\n\n            if not os.path.exists(model_path):\n                raise FileNotFoundError(f\"Training did not end correctly, model file not found at {model_path}\")\n            if not os.path.exists(metadata_path):\n                raise FileNotFoundError(f\"Training did not end correctly, metadata file not found at {metadata_path}\")\n            self.model_info = ModelInfo.from_json(metadata_path)\n\n            logger.info(f\"Reloading weights from {self.model_info.weights_uri}\")\n            self._reload_model()\n        else:\n            run_train(args, data_train, data_val, self.model, self.processor, self.model_info, hub)\n\n    def eval(self, args: TrainerArgs, data_test: MapDataset, save_json: bool = True):\n        \"\"\"evaluate the model on the provided test dataset.\n\n        This method evaluates the model performance on a test dataset,\n        supporting both single-GPU and multi-GPU testing.\n\n        Args:\n            args: Test configuration arguments.\n            data_test: Test dataset for model evaluation.\n\n        Raises:\n            AssertionError: If task mismatch between model and dataset.\n            AssertionError: If num_gpus is 0 (GPU testing is required).\n        \"\"\"\n        from focoos.trainer.trainer import run_eval\n\n        self.model_info.val_dataset = data_test.dataset.metadata.name\n        self.model_info.val_metrics = None\n        self.model_info.classes = data_test.dataset.metadata.classes\n        self.model_info.config[\"num_classes\"] = data_test.dataset.metadata.num_classes\n        assert self.model_info.task == data_test.dataset.metadata.task, \"Task mismatch between model and dataset.\"\n\n        assert args.num_gpus, \"Testing without GPUs is not supported. num_gpus must be greater than 0\"\n        if args.num_gpus &gt; 1:\n            launch(\n                run_eval,\n                args.num_gpus,\n                dist_url=\"auto\",\n                args=(args, data_test, self.model, self.processor, self.model_info, save_json),\n            )\n            logger.info(\"Testing done, resuming main process.\")\n            # here i should restore the best model and config since in DDP it is not updated\n            final_folder = os.path.join(args.output_dir, args.run_name)\n            metadata_path = os.path.join(final_folder, ArtifactName.INFO)\n            self.model_info = ModelInfo.from_json(metadata_path)\n        else:\n            run_eval(\n                train_args=args,\n                data_val=data_test,\n                image_model=self.model,\n                processor=self.processor,\n                model_info=self.model_info,\n                save_json=save_json,\n            )\n\n    @property\n    def name(self):\n        return self.model_info.name\n\n    @property\n    def device(self):\n        \"\"\"Get the device where the model is located.\n\n        Returns:\n            The device (CPU or CUDA) where the model is currently located.\n        \"\"\"\n        return self.model.device\n\n    @property\n    def resolution(self):\n        \"\"\"Get the input resolution of the model.\n\n        Returns:\n            The input image resolution expected by the model.\n        \"\"\"\n        return self.model_info.config[\"resolution\"]\n\n    @property\n    def config(self) -&gt; dict:\n        \"\"\"Get the model configuration.\n\n        Returns:\n            Dictionary containing the model configuration parameters.\n        \"\"\"\n        return self.model_info.config\n\n    @property\n    def classes(self):\n        \"\"\"Get the class names the model can predict.\n\n        Returns:\n            List of class names that the model was trained to recognize.\n        \"\"\"\n        return self.model_info.classes\n\n    @property\n    def task(self):\n        \"\"\"Get the computer vision task type.\n\n        Returns:\n            The type of computer vision task (e.g., detection, classification).\n        \"\"\"\n        return self.model_info.task\n\n    def infer(\n        self,\n        image: Union[bytes, str, Path, np.ndarray, Image.Image],\n        threshold: float = 0.5,\n        annotate: bool = False,\n        keypoints_threshold: float = 0.5,\n    ) -&gt; FocoosDetections:\n        \"\"\"\n        Perform inference on an input image and optionally annotate the results.\n\n        This method processes the input image, runs it through the model, and returns the detections.\n        Optionally, it can also annotate the image with the detection results.\n\n        Args:\n            image: The input image to run inference on. Accepts a file path, bytes, PIL Image, or numpy array.\n            threshold: Minimum confidence score for a detection to be included in the results. Default is 0.5.\n            annotate: If True, annotate the image with detection results and include it in the output.\n            keypoints_threshold: Minimum confidence score for a keypoint to be included in the results. Default is 0.5.\n        Returns:\n            FocoosDetections: An object containing the detection results, optional annotated image, and latency metrics.\n\n        Usage:\n            Use this method to obtain detection results from a local model, with optional annotation for visualization or further processing.\n        \"\"\"\n        t0 = perf_counter()\n        im = image_loader(image)\n        t1 = perf_counter()\n\n        focoos_det = self.__call__(inputs=im, threshold=threshold)\n        if focoos_det.latency is not None:\n            focoos_det.latency.imload = round(t1 - t0, 3)\n        if annotate:\n            t2 = perf_counter()\n            skeleton = self.model_info.config.get(\"skeleton\", None)\n            focoos_det.image = annotate_image(\n                im,\n                focoos_det,\n                task=self.model_info.task,\n                classes=self.model_info.classes,\n                keypoints_skeleton=skeleton,\n                keypoints_threshold=keypoints_threshold,\n            )\n            t3 = perf_counter()\n            if focoos_det.latency is not None:\n                focoos_det.latency.annotate = round(t3 - t2, 3)\n        focoos_det.infer_print()\n        return focoos_det\n\n    def export(\n        self,\n        runtime_type: RuntimeType = RuntimeType.TORCHSCRIPT_32,\n        onnx_opset: int = 18,\n        out_dir: Optional[str] = None,\n        device: Literal[\"cuda\", \"cpu\", \"mps\", \"auto\"] = \"auto\",\n        simplify_onnx: bool = True,\n        overwrite: bool = True,\n        image_size: Optional[Union[int, Tuple[int, int]]] = None,\n        dynamic_axes: bool = True,\n    ) -&gt; InferModel:\n        \"\"\"Export the model to different runtime formats.\n\n        This method exports the model to formats like ONNX or TorchScript\n        for deployment and inference optimization.\n\n        Args:\n            runtime_type: Target runtime format for export.\n            onnx_opset: ONNX opset version to use for ONNX export.\n            out_dir: Output directory for exported model. If None, uses default location.\n            device: Device to use for export (\"cuda\", \"cpu\", \"auto\").\n            simplify_onnx: Whether to simplify the ONNX model. Default is True.\n            overwrite: Whether to overwrite existing exported model files.\n            image_size: Custom image size for export. Can be int (square) or tuple (height, width). If None, uses model's default size.\n\n        Returns:\n            InferModel instance for the exported model.\n\n        Raises:\n            ValueError: If unsupported PyTorch version or export format.\n        \"\"\"\n        if device == \"auto\":\n            if runtime_type == RuntimeType.ONNX_CPU:\n                device = \"cpu\"\n            else:\n                device = get_device_type()  # type: ignore\n        else:\n            device = device\n\n        logger.info(f\"\ud83d\udd27 Export Device: {device}\")\n        if out_dir is None:\n            out_dir = os.path.join(MODELS_DIR, self.model_info.ref or self.model_info.name)\n\n        format = runtime_type.to_export_format()\n        export_image_size = image_size if image_size is not None else self.model_info.im_size\n\n        exportable_model = ExportableModel(\n            model=copy.deepcopy(self.model),\n            device=device,\n            input_size=export_image_size,\n        )\n        os.makedirs(out_dir, exist_ok=True)\n        if image_size is None:\n            data = 128 * torch.randn(1, 3, self.model_info.im_size, self.model_info.im_size).to(device)\n        else:\n            if isinstance(image_size, int):\n                # Square image\n                data = 128 * torch.randn(1, 3, image_size, image_size).to(device)\n                self.model_info.im_size = image_size\n            else:\n                # Tuple (height, width)\n                height, width = image_size\n                data = 128 * torch.randn(1, 3, height, width).to(device)\n                self.model_info.im_size = max(height, width)  # Use max dimension for compatibility\n\n        export_model_name = ArtifactName.ONNX if format == ExportFormat.ONNX else ArtifactName.PT\n        _out_file = os.path.join(out_dir, export_model_name)\n\n        axes = self.processor.get_dynamic_axes()\n\n        # Hack to warm up the model and record the spacial shapes if needed\n        exportable_model(data)\n\n        if not overwrite and os.path.exists(_out_file):\n            logger.info(f\"Model file {_out_file} already exists. Set overwrite to True to overwrite.\")\n            return InferModel(model_path=out_dir, runtime_type=runtime_type)\n\n        if format == \"onnx\":\n            import onnx\n\n            with torch.no_grad():\n                logger.info(\"\ud83d\ude80 Exporting ONNX model with Optimum..\")\n                # Try to use Optimum for enhanced ONNX export with additional optimizations\n                import shutil\n\n                # First export using standard torch.onnx.export\n                torch.onnx.export(\n                    exportable_model,\n                    (data,),\n                    f=_out_file,\n                    opset_version=onnx_opset,\n                    verbose=False,\n                    verify=True,\n                    dynamo=False,\n                    external_data=False,  # model weights external to model\n                    input_names=axes.input_names,\n                    output_names=axes.output_names,\n                    dynamic_axes=axes.dynamic_axes if dynamic_axes else None,\n                    do_constant_folding=True,\n                    export_params=True,\n                )\n                # Load original model to count nodes before optimization\n                original_model = onnx.load(_out_file)\n                original_nodes = len(original_model.graph.node)\n                logger.info(f\"\ud83d\udcca Nodes in graph: {original_nodes}\")\n\n                logger.info(\"\u2705 ONNX export completed \")\n\n                if simplify_onnx:\n                    from onnxruntime.transformers.optimizer import optimize_model as ort_optimize_model\n\n                    opt_level = 99 if device == \"cuda\" else 1  # quantization on cpu fail otherwise\n\n                    logger.info(\"\ud83d\udd27 Applying ONNX Simplify: Run Optimum graph optimizations...\")\n                    optimized_model_path = _out_file.replace(\".onnx\", \"_optimized.onnx\")\n\n                    optimized_model = ort_optimize_model(\n                        input=_out_file,\n                        model_type=\"bert\",  # Generic model type for optimization\n                        num_heads=0,  # Auto-detected\n                        hidden_size=0,  # Auto-detected\n                        opt_level=opt_level,  # Maximum optimization level\n                        use_gpu=(device == \"cuda\"),\n                        only_onnxruntime=False,\n                    )\n\n                    optimized_model.save_model_to_file(optimized_model_path)\n\n                    if os.path.exists(optimized_model_path):\n                        import shutil\n\n                        shutil.move(optimized_model_path, _out_file)\n\n                        # Load optimized model to count nodes and log comparison\n                        optimized_onnx_model = onnx.load(_out_file)\n                        optimized_nodes = len(optimized_onnx_model.graph.node)\n                        reduction_pct = round((original_nodes - optimized_nodes) / original_nodes * 100, 1)\n\n                        logger.info(f\"\ud83d\udcca After ONNX Runtime optimizations: {optimized_nodes} nodes in graph\")\n                        logger.info(f\"\ud83d\udcc8 Reduction: ~{reduction_pct}% nodes removed!\")\n                        logger.info(\"\u2705 Onnx model successfully simplified.\")\n                    else:\n                        raise RuntimeError(\"ONNX Runtime optimization output not found\")\n                    logger.info(f\"\u2705 Exported {format}  model to {_out_file}\")\n\n        elif format == \"torchscript\":\n            with torch.no_grad():\n                logger.info(\"\ud83d\ude80 Exporting TorchScript model..\")\n                exp_program = torch.jit.trace(exportable_model, data)\n                if exp_program is not None:\n                    _out_file = os.path.join(out_dir, ArtifactName.PT)\n                    torch.jit.save(exp_program, _out_file)\n                    logger.info(f\"\u2705 Exported {format} model to {_out_file} \")\n                else:\n                    raise ValueError(f\"Failed to export {format} model\")\n\n        # Fixme: this may override the model_info with the one from the exportable model\n        self.model_info.dump_json(os.path.join(out_dir, ArtifactName.INFO))\n        return InferModel(model_path=_out_file, runtime_type=runtime_type, device=device)\n\n    def __call__(\n        self,\n        inputs: Union[\n            torch.Tensor,\n            np.ndarray,\n            Image.Image,\n            list[Image.Image],\n            list[np.ndarray],\n            list[torch.Tensor],\n        ],\n        **kwargs,\n    ) -&gt; FocoosDetections:\n        \"\"\"Run inference on input images.\n\n        This method performs end-to-end inference including preprocessing,\n        model forward pass, and postprocessing to return detections.\n\n        Args:\n            inputs: Input images in various formats (PIL, numpy, torch tensor, or lists).\n            **kwargs: Additional arguments passed to postprocessing.\n\n        Returns:\n            FocoosDetections containing the detection results.\n        \"\"\"\n        t0 = perf_counter()\n        images, _ = self.processor.preprocess(\n            inputs, device=self.model.device, dtype=self.model.dtype\n        )  # second output is targets that we're not using\n        t1 = perf_counter()\n        with torch.no_grad():\n            try:\n                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                    output = self.model.forward(images)\n            except Exception:\n                output = self.model.forward(images)\n        t2 = perf_counter()\n        class_names = self.model_info.classes\n        output_fdet = self.processor.postprocess(output, inputs, class_names=class_names, **kwargs)\n        t3 = perf_counter()\n\n        # FIXME: we don't support batching yet\n        output_fdet[0].latency = InferLatency(\n            preprocess=round(t1 - t0, 3),\n            inference=round(t2 - t1, 3),\n            postprocess=round(t3 - t2, 3),\n        )\n        return output_fdet[0]\n\n    def _reload_model(self):\n        \"\"\"Reload the model with updated configuration.\n\n        This method recreates the model instance with the current configuration\n        and reloads the weights. Used when configuration changes during training.\n        \"\"\"\n        from focoos.model_manager import ConfigManager  # here to avoid circular import\n\n        torch.cuda.empty_cache()\n        model_class = self.model.__class__\n        # without the next line, the inner config may be not a ModelConfig but a dict\n        config = ConfigManager.from_dict(self.model_info.model_family, self.model_info.config)\n        self.model_info.config = config\n        model = model_class(config)\n        self.model = model\n        self._load_weights()\n\n    def _load_weights(self) -&gt; int:\n        \"\"\"Load model weights from the specified URI.\n\n        This method loads the model weights from either a local path or a remote URL,\n        depending on the value of `self.model_info.weights_uri`. If the weights are remote,\n        they are downloaded to a local directory. The method then loads the weights into\n        the model, allowing for missing or unexpected keys (non-strict loading).\n\n        Returns:\n            The total number of missing or unexpected keys encountered during loading.\n            Returns 0 if no weights are loaded or an error occurs.\n\n        Raises:\n            FileNotFoundError: If the weights file cannot be found at the specified path.\n        \"\"\"\n        if not self.model_info.weights_uri:\n            logger.warning(f\"\u26a0\ufe0f Model {self.model_info.name} has no pretrained weights\")\n            return 0\n\n        # Determine if weights are remote or local\n        parsed_uri = urlparse(self.model_info.weights_uri)\n        is_remote = bool(parsed_uri.scheme and parsed_uri.netloc)\n\n        # Get weights path\n        if is_remote:\n            model_dir = Path(MODELS_DIR) / self.model_info.name\n            local_path = model_dir / \"model_final.pth\"\n            if not local_path.exists():\n                logger.info(f\"Downloading weights from remote URL: {self.model_info.weights_uri}\")\n                weights_path = ApiClient().download_ext_file(\n                    self.model_info.weights_uri, str(model_dir), skip_if_exists=False\n                )\n            else:\n                weights_path = local_path\n                logger.info(f\"Skipping download, using weights from local path: {weights_path}\")\n        else:\n            logger.info(f\"Loading weights from local path: {self.model_info.weights_uri}\")\n            weights_path = self.model_info.weights_uri\n\n        try:\n            if not os.path.exists(weights_path):\n                raise FileNotFoundError(f\"Weights file not found: {weights_path}\")\n\n            # Load weights and extract model state if needed\n            state_dict = torch.load(weights_path, map_location=\"cpu\", weights_only=True)\n            weights_dict = state_dict.get(\"model\", state_dict) if isinstance(state_dict, dict) else state_dict\n\n        except Exception as e:\n            logger.error(f\"Error loading weights for {self.model_info.name}: {str(e)}\")\n            return 0\n\n        incompatible = self.model.load_state_dict(weights_dict, strict=False)\n        return len(incompatible.missing_keys) + len(incompatible.unexpected_keys)\n\n    def benchmark(\n        self,\n        iterations: int = 50,\n        size: Optional[Union[int, Tuple[int, int]]] = None,\n        device: Literal[\"cuda\", \"cpu\"] = \"cuda\",\n    ) -&gt; LatencyMetrics:\n        \"\"\"Benchmark the model's inference performance.\n\n        This method measures the raw model inference latency without\n        preprocessing and postprocessing overhead.\n\n        Args:\n            iterations: Number of iterations to run for benchmarking.\n            size: Input image size. If None, uses model's default size.\n            device: Device to run benchmarking on (\"cuda\" or \"cpu\").\n\n        Returns:\n            LatencyMetrics containing performance statistics.\n        \"\"\"\n        self.model.eval()\n\n        if size is None:\n            size = self.model_info.im_size\n        if isinstance(size, int):\n            size = (size, size)\n        model = self.model.to(device)\n        metrics = model.benchmark(size=size, iterations=iterations)\n        return metrics\n\n    def end2end_benchmark(self, iterations: int = 50, size: Optional[int] = None) -&gt; LatencyMetrics:\n        \"\"\"Benchmark the complete end-to-end inference pipeline.\n\n        This method measures the full inference latency including preprocessing,\n        model forward pass, and postprocessing steps.\n\n        Args:\n            iterations: Number of iterations to run for benchmarking.\n            size: Input image size. If None, uses model's default size.\n            device: Device to run benchmarking on (\"cuda\" or \"cpu\").\n\n        Returns:\n            LatencyMetrics containing end-to-end performance statistics.\n        \"\"\"\n        if size is None:\n            size = self.model_info.im_size\n        if self.model.device.type == \"cpu\":\n            device_name = get_cpu_name()\n        else:\n            device_name = get_device_name()\n        try:\n            model = self.model.cuda()\n        except Exception:\n            logger.warning(\"Unable to use CUDA\")\n        logger.info(f\"\u23f1\ufe0f Benchmarking End-to-End latency on {device_name} ({self.model.device}), size: {size}x{size}..\")\n        # warmup\n        data = 128 * torch.randn(1, 3, size, size).to(model.device)\n\n        durations = []\n        for _ in range(iterations):\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            start.record(stream=torch.cuda.Stream())\n            _ = self(data)\n            end.record(stream=torch.cuda.Stream())\n            torch.cuda.synchronize()\n            durations.append(start.elapsed_time(end))\n\n        durations = np.array(durations)\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean()),\n            engine=f\"torch.{self.model.device}\",\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size,\n            device=str(self.model.device),\n        )\n        logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.classes","title":"<code>classes</code>  <code>property</code>","text":"<p>Get the class names the model can predict.</p> <p>Returns:</p> Type Description <p>List of class names that the model was trained to recognize.</p>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.config","title":"<code>config</code>  <code>property</code>","text":"<p>Get the model configuration.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the model configuration parameters.</p>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.device","title":"<code>device</code>  <code>property</code>","text":"<p>Get the device where the model is located.</p> <p>Returns:</p> Type Description <p>The device (CPU or CUDA) where the model is currently located.</p>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.resolution","title":"<code>resolution</code>  <code>property</code>","text":"<p>Get the input resolution of the model.</p> <p>Returns:</p> Type Description <p>The input image resolution expected by the model.</p>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.task","title":"<code>task</code>  <code>property</code>","text":"<p>Get the computer vision task type.</p> <p>Returns:</p> Type Description <p>The type of computer vision task (e.g., detection, classification).</p>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.__call__","title":"<code>__call__(inputs, **kwargs)</code>","text":"<p>Run inference on input images.</p> <p>This method performs end-to-end inference including preprocessing, model forward pass, and postprocessing to return detections.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Input images in various formats (PIL, numpy, torch tensor, or lists).</p> required <code>**kwargs</code> <p>Additional arguments passed to postprocessing.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FocoosDetections</code> <p>FocoosDetections containing the detection results.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def __call__(\n    self,\n    inputs: Union[\n        torch.Tensor,\n        np.ndarray,\n        Image.Image,\n        list[Image.Image],\n        list[np.ndarray],\n        list[torch.Tensor],\n    ],\n    **kwargs,\n) -&gt; FocoosDetections:\n    \"\"\"Run inference on input images.\n\n    This method performs end-to-end inference including preprocessing,\n    model forward pass, and postprocessing to return detections.\n\n    Args:\n        inputs: Input images in various formats (PIL, numpy, torch tensor, or lists).\n        **kwargs: Additional arguments passed to postprocessing.\n\n    Returns:\n        FocoosDetections containing the detection results.\n    \"\"\"\n    t0 = perf_counter()\n    images, _ = self.processor.preprocess(\n        inputs, device=self.model.device, dtype=self.model.dtype\n    )  # second output is targets that we're not using\n    t1 = perf_counter()\n    with torch.no_grad():\n        try:\n            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                output = self.model.forward(images)\n        except Exception:\n            output = self.model.forward(images)\n    t2 = perf_counter()\n    class_names = self.model_info.classes\n    output_fdet = self.processor.postprocess(output, inputs, class_names=class_names, **kwargs)\n    t3 = perf_counter()\n\n    # FIXME: we don't support batching yet\n    output_fdet[0].latency = InferLatency(\n        preprocess=round(t1 - t0, 3),\n        inference=round(t2 - t1, 3),\n        postprocess=round(t3 - t2, 3),\n    )\n    return output_fdet[0]\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.__init__","title":"<code>__init__(model, model_info)</code>","text":"<p>Initialize the FocoosModel.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModelNN</code> <p>The underlying neural network model.</p> required <code>model_info</code> <code>ModelInfo</code> <p>Metadata and configuration information for the model.</p> required Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def __init__(self, model: BaseModelNN, model_info: ModelInfo):\n    \"\"\"Initialize the FocoosModel.\n\n    Args:\n        model: The underlying neural network model.\n        model_info: Metadata and configuration information for the model.\n    \"\"\"\n\n    self.model_info = model_info\n    self.processor = ProcessorManager.get_processor(\n        self.model_info.model_family,\n        self.model_info.config,  # type: ignore\n        self.model_info.im_size,\n    )\n    self.processor.eval()\n    self.model = model.eval()\n\n    if torch.cuda.is_available():\n        try:\n            self.model = self.model.cuda()\n        except Exception:\n            logger.warning(\"Unable to use CUDA\")\n\n    if torch.backends.mps.is_available():\n        try:\n            self.model = self.model.to(device=\"mps\")\n        except Exception:\n            logger.warning(\"Unable to use MPS\")\n\n    if self.model_info.weights_uri:\n        self._load_weights()\n    else:\n        logger.warning(f\"\u26a0\ufe0f Model {self.model_info.name} has no pretrained weights\")\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.__repr__","title":"<code>__repr__()</code>","text":"<p>Return detailed string representation of the model.</p> <p>Returns:</p> Type Description <p>String containing model name and family.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return detailed string representation of the model.\n\n    Returns:\n        String containing model name and family.\n    \"\"\"\n    return f\"{self.model_info.name} ({self.model_info.model_family.value})\"\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation of the model.</p> <p>Returns:</p> Type Description <p>String containing model name and family.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def __str__(self):\n    \"\"\"Return string representation of the model.\n\n    Returns:\n        String containing model name and family.\n    \"\"\"\n    return f\"{self.model_info.name} ({self.model_info.model_family.value})\"\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.benchmark","title":"<code>benchmark(iterations=50, size=None, device='cuda')</code>","text":"<p>Benchmark the model's inference performance.</p> <p>This method measures the raw model inference latency without preprocessing and postprocessing overhead.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of iterations to run for benchmarking.</p> <code>50</code> <code>size</code> <code>Optional[Union[int, Tuple[int, int]]]</code> <p>Input image size. If None, uses model's default size.</p> <code>None</code> <code>device</code> <code>Literal['cuda', 'cpu']</code> <p>Device to run benchmarking on (\"cuda\" or \"cpu\").</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>LatencyMetrics</code> <p>LatencyMetrics containing performance statistics.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def benchmark(\n    self,\n    iterations: int = 50,\n    size: Optional[Union[int, Tuple[int, int]]] = None,\n    device: Literal[\"cuda\", \"cpu\"] = \"cuda\",\n) -&gt; LatencyMetrics:\n    \"\"\"Benchmark the model's inference performance.\n\n    This method measures the raw model inference latency without\n    preprocessing and postprocessing overhead.\n\n    Args:\n        iterations: Number of iterations to run for benchmarking.\n        size: Input image size. If None, uses model's default size.\n        device: Device to run benchmarking on (\"cuda\" or \"cpu\").\n\n    Returns:\n        LatencyMetrics containing performance statistics.\n    \"\"\"\n    self.model.eval()\n\n    if size is None:\n        size = self.model_info.im_size\n    if isinstance(size, int):\n        size = (size, size)\n    model = self.model.to(device)\n    metrics = model.benchmark(size=size, iterations=iterations)\n    return metrics\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.end2end_benchmark","title":"<code>end2end_benchmark(iterations=50, size=None)</code>","text":"<p>Benchmark the complete end-to-end inference pipeline.</p> <p>This method measures the full inference latency including preprocessing, model forward pass, and postprocessing steps.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of iterations to run for benchmarking.</p> <code>50</code> <code>size</code> <code>Optional[int]</code> <p>Input image size. If None, uses model's default size.</p> <code>None</code> <code>device</code> <p>Device to run benchmarking on (\"cuda\" or \"cpu\").</p> required <p>Returns:</p> Type Description <code>LatencyMetrics</code> <p>LatencyMetrics containing end-to-end performance statistics.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def end2end_benchmark(self, iterations: int = 50, size: Optional[int] = None) -&gt; LatencyMetrics:\n    \"\"\"Benchmark the complete end-to-end inference pipeline.\n\n    This method measures the full inference latency including preprocessing,\n    model forward pass, and postprocessing steps.\n\n    Args:\n        iterations: Number of iterations to run for benchmarking.\n        size: Input image size. If None, uses model's default size.\n        device: Device to run benchmarking on (\"cuda\" or \"cpu\").\n\n    Returns:\n        LatencyMetrics containing end-to-end performance statistics.\n    \"\"\"\n    if size is None:\n        size = self.model_info.im_size\n    if self.model.device.type == \"cpu\":\n        device_name = get_cpu_name()\n    else:\n        device_name = get_device_name()\n    try:\n        model = self.model.cuda()\n    except Exception:\n        logger.warning(\"Unable to use CUDA\")\n    logger.info(f\"\u23f1\ufe0f Benchmarking End-to-End latency on {device_name} ({self.model.device}), size: {size}x{size}..\")\n    # warmup\n    data = 128 * torch.randn(1, 3, size, size).to(model.device)\n\n    durations = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record(stream=torch.cuda.Stream())\n        _ = self(data)\n        end.record(stream=torch.cuda.Stream())\n        torch.cuda.synchronize()\n        durations.append(start.elapsed_time(end))\n\n    durations = np.array(durations)\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean()),\n        engine=f\"torch.{self.model.device}\",\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size,\n        device=str(self.model.device),\n    )\n    logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.eval","title":"<code>eval(args, data_test, save_json=True)</code>","text":"<p>evaluate the model on the provided test dataset.</p> <p>This method evaluates the model performance on a test dataset, supporting both single-GPU and multi-GPU testing.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>TrainerArgs</code> <p>Test configuration arguments.</p> required <code>data_test</code> <code>MapDataset</code> <p>Test dataset for model evaluation.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If task mismatch between model and dataset.</p> <code>AssertionError</code> <p>If num_gpus is 0 (GPU testing is required).</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def eval(self, args: TrainerArgs, data_test: MapDataset, save_json: bool = True):\n    \"\"\"evaluate the model on the provided test dataset.\n\n    This method evaluates the model performance on a test dataset,\n    supporting both single-GPU and multi-GPU testing.\n\n    Args:\n        args: Test configuration arguments.\n        data_test: Test dataset for model evaluation.\n\n    Raises:\n        AssertionError: If task mismatch between model and dataset.\n        AssertionError: If num_gpus is 0 (GPU testing is required).\n    \"\"\"\n    from focoos.trainer.trainer import run_eval\n\n    self.model_info.val_dataset = data_test.dataset.metadata.name\n    self.model_info.val_metrics = None\n    self.model_info.classes = data_test.dataset.metadata.classes\n    self.model_info.config[\"num_classes\"] = data_test.dataset.metadata.num_classes\n    assert self.model_info.task == data_test.dataset.metadata.task, \"Task mismatch between model and dataset.\"\n\n    assert args.num_gpus, \"Testing without GPUs is not supported. num_gpus must be greater than 0\"\n    if args.num_gpus &gt; 1:\n        launch(\n            run_eval,\n            args.num_gpus,\n            dist_url=\"auto\",\n            args=(args, data_test, self.model, self.processor, self.model_info, save_json),\n        )\n        logger.info(\"Testing done, resuming main process.\")\n        # here i should restore the best model and config since in DDP it is not updated\n        final_folder = os.path.join(args.output_dir, args.run_name)\n        metadata_path = os.path.join(final_folder, ArtifactName.INFO)\n        self.model_info = ModelInfo.from_json(metadata_path)\n    else:\n        run_eval(\n            train_args=args,\n            data_val=data_test,\n            image_model=self.model,\n            processor=self.processor,\n            model_info=self.model_info,\n            save_json=save_json,\n        )\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.export","title":"<code>export(runtime_type=RuntimeType.TORCHSCRIPT_32, onnx_opset=18, out_dir=None, device='auto', simplify_onnx=True, overwrite=True, image_size=None, dynamic_axes=True)</code>","text":"<p>Export the model to different runtime formats.</p> <p>This method exports the model to formats like ONNX or TorchScript for deployment and inference optimization.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_type</code> <code>RuntimeType</code> <p>Target runtime format for export.</p> <code>TORCHSCRIPT_32</code> <code>onnx_opset</code> <code>int</code> <p>ONNX opset version to use for ONNX export.</p> <code>18</code> <code>out_dir</code> <code>Optional[str]</code> <p>Output directory for exported model. If None, uses default location.</p> <code>None</code> <code>device</code> <code>Literal['cuda', 'cpu', 'mps', 'auto']</code> <p>Device to use for export (\"cuda\", \"cpu\", \"auto\").</p> <code>'auto'</code> <code>simplify_onnx</code> <code>bool</code> <p>Whether to simplify the ONNX model. Default is True.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing exported model files.</p> <code>True</code> <code>image_size</code> <code>Optional[Union[int, Tuple[int, int]]]</code> <p>Custom image size for export. Can be int (square) or tuple (height, width). If None, uses model's default size.</p> <code>None</code> <p>Returns:</p> Type Description <code>InferModel</code> <p>InferModel instance for the exported model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unsupported PyTorch version or export format.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def export(\n    self,\n    runtime_type: RuntimeType = RuntimeType.TORCHSCRIPT_32,\n    onnx_opset: int = 18,\n    out_dir: Optional[str] = None,\n    device: Literal[\"cuda\", \"cpu\", \"mps\", \"auto\"] = \"auto\",\n    simplify_onnx: bool = True,\n    overwrite: bool = True,\n    image_size: Optional[Union[int, Tuple[int, int]]] = None,\n    dynamic_axes: bool = True,\n) -&gt; InferModel:\n    \"\"\"Export the model to different runtime formats.\n\n    This method exports the model to formats like ONNX or TorchScript\n    for deployment and inference optimization.\n\n    Args:\n        runtime_type: Target runtime format for export.\n        onnx_opset: ONNX opset version to use for ONNX export.\n        out_dir: Output directory for exported model. If None, uses default location.\n        device: Device to use for export (\"cuda\", \"cpu\", \"auto\").\n        simplify_onnx: Whether to simplify the ONNX model. Default is True.\n        overwrite: Whether to overwrite existing exported model files.\n        image_size: Custom image size for export. Can be int (square) or tuple (height, width). If None, uses model's default size.\n\n    Returns:\n        InferModel instance for the exported model.\n\n    Raises:\n        ValueError: If unsupported PyTorch version or export format.\n    \"\"\"\n    if device == \"auto\":\n        if runtime_type == RuntimeType.ONNX_CPU:\n            device = \"cpu\"\n        else:\n            device = get_device_type()  # type: ignore\n    else:\n        device = device\n\n    logger.info(f\"\ud83d\udd27 Export Device: {device}\")\n    if out_dir is None:\n        out_dir = os.path.join(MODELS_DIR, self.model_info.ref or self.model_info.name)\n\n    format = runtime_type.to_export_format()\n    export_image_size = image_size if image_size is not None else self.model_info.im_size\n\n    exportable_model = ExportableModel(\n        model=copy.deepcopy(self.model),\n        device=device,\n        input_size=export_image_size,\n    )\n    os.makedirs(out_dir, exist_ok=True)\n    if image_size is None:\n        data = 128 * torch.randn(1, 3, self.model_info.im_size, self.model_info.im_size).to(device)\n    else:\n        if isinstance(image_size, int):\n            # Square image\n            data = 128 * torch.randn(1, 3, image_size, image_size).to(device)\n            self.model_info.im_size = image_size\n        else:\n            # Tuple (height, width)\n            height, width = image_size\n            data = 128 * torch.randn(1, 3, height, width).to(device)\n            self.model_info.im_size = max(height, width)  # Use max dimension for compatibility\n\n    export_model_name = ArtifactName.ONNX if format == ExportFormat.ONNX else ArtifactName.PT\n    _out_file = os.path.join(out_dir, export_model_name)\n\n    axes = self.processor.get_dynamic_axes()\n\n    # Hack to warm up the model and record the spacial shapes if needed\n    exportable_model(data)\n\n    if not overwrite and os.path.exists(_out_file):\n        logger.info(f\"Model file {_out_file} already exists. Set overwrite to True to overwrite.\")\n        return InferModel(model_path=out_dir, runtime_type=runtime_type)\n\n    if format == \"onnx\":\n        import onnx\n\n        with torch.no_grad():\n            logger.info(\"\ud83d\ude80 Exporting ONNX model with Optimum..\")\n            # Try to use Optimum for enhanced ONNX export with additional optimizations\n            import shutil\n\n            # First export using standard torch.onnx.export\n            torch.onnx.export(\n                exportable_model,\n                (data,),\n                f=_out_file,\n                opset_version=onnx_opset,\n                verbose=False,\n                verify=True,\n                dynamo=False,\n                external_data=False,  # model weights external to model\n                input_names=axes.input_names,\n                output_names=axes.output_names,\n                dynamic_axes=axes.dynamic_axes if dynamic_axes else None,\n                do_constant_folding=True,\n                export_params=True,\n            )\n            # Load original model to count nodes before optimization\n            original_model = onnx.load(_out_file)\n            original_nodes = len(original_model.graph.node)\n            logger.info(f\"\ud83d\udcca Nodes in graph: {original_nodes}\")\n\n            logger.info(\"\u2705 ONNX export completed \")\n\n            if simplify_onnx:\n                from onnxruntime.transformers.optimizer import optimize_model as ort_optimize_model\n\n                opt_level = 99 if device == \"cuda\" else 1  # quantization on cpu fail otherwise\n\n                logger.info(\"\ud83d\udd27 Applying ONNX Simplify: Run Optimum graph optimizations...\")\n                optimized_model_path = _out_file.replace(\".onnx\", \"_optimized.onnx\")\n\n                optimized_model = ort_optimize_model(\n                    input=_out_file,\n                    model_type=\"bert\",  # Generic model type for optimization\n                    num_heads=0,  # Auto-detected\n                    hidden_size=0,  # Auto-detected\n                    opt_level=opt_level,  # Maximum optimization level\n                    use_gpu=(device == \"cuda\"),\n                    only_onnxruntime=False,\n                )\n\n                optimized_model.save_model_to_file(optimized_model_path)\n\n                if os.path.exists(optimized_model_path):\n                    import shutil\n\n                    shutil.move(optimized_model_path, _out_file)\n\n                    # Load optimized model to count nodes and log comparison\n                    optimized_onnx_model = onnx.load(_out_file)\n                    optimized_nodes = len(optimized_onnx_model.graph.node)\n                    reduction_pct = round((original_nodes - optimized_nodes) / original_nodes * 100, 1)\n\n                    logger.info(f\"\ud83d\udcca After ONNX Runtime optimizations: {optimized_nodes} nodes in graph\")\n                    logger.info(f\"\ud83d\udcc8 Reduction: ~{reduction_pct}% nodes removed!\")\n                    logger.info(\"\u2705 Onnx model successfully simplified.\")\n                else:\n                    raise RuntimeError(\"ONNX Runtime optimization output not found\")\n                logger.info(f\"\u2705 Exported {format}  model to {_out_file}\")\n\n    elif format == \"torchscript\":\n        with torch.no_grad():\n            logger.info(\"\ud83d\ude80 Exporting TorchScript model..\")\n            exp_program = torch.jit.trace(exportable_model, data)\n            if exp_program is not None:\n                _out_file = os.path.join(out_dir, ArtifactName.PT)\n                torch.jit.save(exp_program, _out_file)\n                logger.info(f\"\u2705 Exported {format} model to {_out_file} \")\n            else:\n                raise ValueError(f\"Failed to export {format} model\")\n\n    # Fixme: this may override the model_info with the one from the exportable model\n    self.model_info.dump_json(os.path.join(out_dir, ArtifactName.INFO))\n    return InferModel(model_path=_out_file, runtime_type=runtime_type, device=device)\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.infer","title":"<code>infer(image, threshold=0.5, annotate=False, keypoints_threshold=0.5)</code>","text":"<p>Perform inference on an input image and optionally annotate the results.</p> <p>This method processes the input image, runs it through the model, and returns the detections. Optionally, it can also annotate the image with the detection results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[bytes, str, Path, ndarray, Image]</code> <p>The input image to run inference on. Accepts a file path, bytes, PIL Image, or numpy array.</p> required <code>threshold</code> <code>float</code> <p>Minimum confidence score for a detection to be included in the results. Default is 0.5.</p> <code>0.5</code> <code>annotate</code> <code>bool</code> <p>If True, annotate the image with detection results and include it in the output.</p> <code>False</code> <code>keypoints_threshold</code> <code>float</code> <p>Minimum confidence score for a keypoint to be included in the results. Default is 0.5.</p> <code>0.5</code> <p>Returns:     FocoosDetections: An object containing the detection results, optional annotated image, and latency metrics.</p> Usage <p>Use this method to obtain detection results from a local model, with optional annotation for visualization or further processing.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def infer(\n    self,\n    image: Union[bytes, str, Path, np.ndarray, Image.Image],\n    threshold: float = 0.5,\n    annotate: bool = False,\n    keypoints_threshold: float = 0.5,\n) -&gt; FocoosDetections:\n    \"\"\"\n    Perform inference on an input image and optionally annotate the results.\n\n    This method processes the input image, runs it through the model, and returns the detections.\n    Optionally, it can also annotate the image with the detection results.\n\n    Args:\n        image: The input image to run inference on. Accepts a file path, bytes, PIL Image, or numpy array.\n        threshold: Minimum confidence score for a detection to be included in the results. Default is 0.5.\n        annotate: If True, annotate the image with detection results and include it in the output.\n        keypoints_threshold: Minimum confidence score for a keypoint to be included in the results. Default is 0.5.\n    Returns:\n        FocoosDetections: An object containing the detection results, optional annotated image, and latency metrics.\n\n    Usage:\n        Use this method to obtain detection results from a local model, with optional annotation for visualization or further processing.\n    \"\"\"\n    t0 = perf_counter()\n    im = image_loader(image)\n    t1 = perf_counter()\n\n    focoos_det = self.__call__(inputs=im, threshold=threshold)\n    if focoos_det.latency is not None:\n        focoos_det.latency.imload = round(t1 - t0, 3)\n    if annotate:\n        t2 = perf_counter()\n        skeleton = self.model_info.config.get(\"skeleton\", None)\n        focoos_det.image = annotate_image(\n            im,\n            focoos_det,\n            task=self.model_info.task,\n            classes=self.model_info.classes,\n            keypoints_skeleton=skeleton,\n            keypoints_threshold=keypoints_threshold,\n        )\n        t3 = perf_counter()\n        if focoos_det.latency is not None:\n            focoos_det.latency.annotate = round(t3 - t2, 3)\n    focoos_det.infer_print()\n    return focoos_det\n</code></pre>"},{"location":"api/focoos_model/#focoos.models.focoos_model.FocoosModel.train","title":"<code>train(args, data_train, data_val, hub=None)</code>","text":"<p>Train the model on the provided datasets.</p> <p>This method handles both single-GPU and multi-GPU distributed training. It sets up the model for training, optionally syncs with Focoos Hub, and manages the training process.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>TrainerArgs</code> <p>Training configuration and hyperparameters.</p> required <code>data_train</code> <code>MapDataset</code> <p>Training dataset containing images and annotations.</p> required <code>data_val</code> <code>MapDataset</code> <p>Validation dataset for model evaluation.</p> required <code>hub</code> <code>Optional[FocoosHUB]</code> <p>Optional Focoos Hub instance for model syncing.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If task mismatch between model and dataset.</p> <code>AssertionError</code> <p>If number of classes mismatch between model and dataset.</p> <code>AssertionError</code> <p>If num_gpus is 0 (GPU training is required).</p> <code>FileNotFoundError</code> <p>If training artifacts are not found after completion.</p> Source code in <code>focoos/models/focoos_model.py</code> <pre><code>def train(self, args: TrainerArgs, data_train: MapDataset, data_val: MapDataset, hub: Optional[FocoosHUB] = None):\n    \"\"\"Train the model on the provided datasets.\n\n    This method handles both single-GPU and multi-GPU distributed training.\n    It sets up the model for training, optionally syncs with Focoos Hub,\n    and manages the training process.\n\n    Args:\n        args: Training configuration and hyperparameters.\n        data_train: Training dataset containing images and annotations.\n        data_val: Validation dataset for model evaluation.\n        hub: Optional Focoos Hub instance for model syncing.\n\n    Raises:\n        AssertionError: If task mismatch between model and dataset.\n        AssertionError: If number of classes mismatch between model and dataset.\n        AssertionError: If num_gpus is 0 (GPU training is required).\n        FileNotFoundError: If training artifacts are not found after completion.\n    \"\"\"\n    from focoos.trainer.trainer import run_train\n\n    assert data_train.dataset.metadata.num_classes &gt; 0, \"Number of dataset classes must be greater than 0\"\n    self._setup_model_for_training(args, data_train, data_val)\n\n    assert self.model_info.task == data_train.dataset.metadata.task, \"Task mismatch between model and dataset.\"\n    assert self.model_info.config[\"num_classes\"] == data_train.dataset.metadata.num_classes, (\n        \"Number of classes mismatch between model and dataset.\"\n    )\n    assert args.num_gpus, \"Training without GPUs is not supported. num_gpus must be greater than 0\"\n    if args.num_gpus &gt; 1:\n        launch(\n            run_train,\n            args.num_gpus,\n            dist_url=\"auto\",\n            args=(args, data_train, data_val, self.model, self.processor, self.model_info, hub),\n        )\n\n        logger.info(\"Training done, resuming main process.\")\n        # here i should restore the best model and config since in DDP it is not updated\n        final_folder = os.path.join(args.output_dir, args.run_name)\n        model_path = os.path.join(final_folder, ArtifactName.WEIGHTS)\n        metadata_path = os.path.join(final_folder, ArtifactName.INFO)\n\n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Training did not end correctly, model file not found at {model_path}\")\n        if not os.path.exists(metadata_path):\n            raise FileNotFoundError(f\"Training did not end correctly, metadata file not found at {metadata_path}\")\n        self.model_info = ModelInfo.from_json(metadata_path)\n\n        logger.info(f\"Reloading weights from {self.model_info.weights_uri}\")\n        self._reload_model()\n    else:\n        run_train(args, data_train, data_val, self.model, self.processor, self.model_info, hub)\n</code></pre>"},{"location":"api/hub/","title":"FocoosHUB","text":"<p>Focoos Module</p> <p>This module provides a Python interface for interacting with Focoos APIs, allowing users to manage machine learning models and datasets in the Focoos ecosystem. The module supports operations such as retrieving model metadata, downloading models, and listing shared datasets.</p> <p>Classes:</p> Name Description <code>FocoosHUB</code> <p>Main class to interface with Focoos APIs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised for invalid API responses or missing parameters.</p> <p>RemoteModel Module</p> <p>This module provides a class to manage remote models in the Focoos ecosystem. It supports various functionalities including model training, deployment, inference, and monitoring.</p> <p>Classes:</p> Name Description <code>RemoteModel</code> <p>A class for interacting with remote models, managing their lifecycle,          and performing inference.</p> <p>Functions:</p> Name Description <code>__init__</code> <p>Initializes the RemoteModel instance.</p> <code>get_info</code> <p>Retrieves model metadata.</p> <code>train</code> <p>Initiates model training.</p> <code>train_info</code> <p>Retrieves training status.</p> <code>train_logs</code> <p>Retrieves training logs.</p> <code>metrics</code> <p>Retrieves model metrics.</p>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB","title":"<code>FocoosHUB</code>","text":"<p>Main class to interface with Focoos APIs.</p> <p>This class provides methods to interact with Focoos-hosted models and datasets. It supports functionalities such as listing models, retrieving model metadata, downloading models, and creating new models.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key for authentication.</p> <code>api_client</code> <code>ApiClient</code> <p>HTTP client for making API requests.</p> <code>user_info</code> <code>User</code> <p>Information about the currently authenticated user.</p> <code>host_url</code> <code>str</code> <p>Base URL for the Focoos API.</p> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>class FocoosHUB:\n    \"\"\"\n    Main class to interface with Focoos APIs.\n\n    This class provides methods to interact with Focoos-hosted models and datasets.\n    It supports functionalities such as listing models, retrieving model metadata,\n    downloading models, and creating new models.\n\n    Attributes:\n        api_key (str): The API key for authentication.\n        api_client (ApiClient): HTTP client for making API requests.\n        user_info (User): Information about the currently authenticated user.\n        host_url (str): Base URL for the Focoos API.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        host_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes the FocoosHUB client.\n\n        This client provides authenticated access to the Focoos API, enabling various operations\n        through the configured HTTP client. It retrieves user information upon initialization and\n        logs the environment details.\n\n        Args:\n            api_key (Optional[str]): API key for authentication. Defaults to the `focoos_api_key`\n                specified in the FOCOOS_CONFIG.\n            host_url (Optional[str]): Base URL for the Focoos API. Defaults to the `default_host_url`\n                specified in the FOCOOS_CONFIG.\n\n        Raises:\n            ValueError: If the API key is not provided, or if the host URL is not specified in the\n                arguments or the configuration.\n\n        Attributes:\n            api_key (str): The API key used for authentication.\n            api_client (ApiClient): An HTTP client instance configured with the API key and host URL.\n            user_info (User): Information about the authenticated user retrieved from the API.\n            host_url (str): The base URL used for API requests.\n\n        Logs:\n            - Error if the API key or host URL is missing.\n            - Info about the authenticated user and environment upon successful initialization.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            ```\n        \"\"\"\n        self.api_key = api_key or FOCOOS_CONFIG.focoos_api_key\n        if not self.api_key:\n            logger.error(\"API key is required \ud83e\udd16\")\n            raise ValueError(\"API key is required \ud83e\udd16\")\n\n        self.host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n        self.api_client = ApiClient(api_key=self.api_key, host_url=self.host_url)\n        self.user_info = self.get_user_info()\n        logger.info(f\"Currently logged as: {self.user_info.email} environment: {self.host_url}\")\n\n    def get_user_info(self) -&gt; User:\n        \"\"\"\n        Retrieves information about the authenticated user.\n\n        Returns:\n            User: User object containing account information and usage quotas.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            user_info = focoos.get_user_info()\n\n            # Access user info fields\n            print(f\"Email: {user_info.email}\")\n            print(f\"Created at: {user_info.created_at}\")\n            print(f\"Updated at: {user_info.updated_at}\")\n            print(f\"Company: {user_info.company}\")\n            print(f\"API key: {user_info.api_key.key}\")\n\n            # Access quotas\n            quotas = user_info.quotas\n            print(f\"Total inferences: {quotas.total_inferences}\")\n            print(f\"Max inferences: {quotas.max_inferences}\")\n            print(f\"Used storage (GB): {quotas.used_storage_gb}\")\n            print(f\"Max storage (GB): {quotas.max_storage_gb}\")\n            print(f\"Active training jobs: {quotas.active_training_jobs}\")\n            print(f\"Max active training jobs: {quotas.max_active_training_jobs}\")\n            print(f\"Used MLG4DNXLarge training jobs hours: {quotas.used_mlg4dnxlarge_training_jobs_hours}\")\n            print(f\"Max MLG4DNXLarge training jobs hours: {quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n            ```\n        \"\"\"\n        res = self.api_client.get(\"user/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get user info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get user info: {res.status_code} {res.text}\")\n        return User.from_json(res.json())\n\n    def get_model_info(self, model_ref: str) -&gt; RemoteModelInfo:\n        \"\"\"\n        Retrieves metadata for a specific model.\n\n        Args:\n            model_ref (str): Reference identifier for the model.\n\n        Returns:\n            RemoteModelInfo: Metadata of the specified model.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            model_info = focoos.get_model_info(model_ref=\"user-or-fai-model-ref\")\n            ```\n        \"\"\"\n        res = self.api_client.get(f\"models/{model_ref}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n        return RemoteModelInfo.from_json(res.json())\n\n    def list_remote_models(self) -&gt; list[ModelPreview]:\n        \"\"\"\n        Lists all models owned by the user.\n\n        Returns:\n            list[ModelPreview]: List of model previews.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            models = focoos.list_remote_models()\n            ```\n        \"\"\"\n        res = self.api_client.get(\"models/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list models: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list models: {res.status_code} {res.text}\")\n        return [ModelPreview.from_json(r) for r in res.json()]\n\n    def get_remote_model(self, model_ref: str) -&gt; RemoteModel:\n        \"\"\"\n        Retrieves a remote model instance for cloud-based inference.\n\n        Args:\n            model_ref (str): Reference identifier for the model.\n\n        Returns:\n            RemoteModel: The remote model instance configured for cloud-based inference.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            model = focoos.get_remote_model(model_ref=\"fai-model-ref\")\n            results = model.infer(\"image.jpg\", threshold=0.5)  # inference is remote!\n            ```\n        \"\"\"\n        return RemoteModel(model_ref, self.api_client)\n\n    def download_model_pth(self, model_ref: str, skip_if_exists: bool = True) -&gt; str:\n        \"\"\"\n        Downloads a model from the Focoos API.\n\n        Args:\n            model_ref (str): Reference identifier for the model.\n            skip_if_exists (bool): If True, skips the download if the model file already exists.\n                Defaults to True.\n\n        Returns:\n            str: Path to the downloaded model file.\n\n        Raises:\n            ValueError: If the API request fails or the download fails.\n        \"\"\"\n        model_dir = os.path.join(MODELS_DIR, model_ref)\n        model_pth_path = os.path.join(model_dir, ArtifactName.WEIGHTS)\n        if os.path.exists(model_pth_path) and skip_if_exists:\n            logger.info(\"\ud83d\udce5 Model already downloaded\")\n            return model_pth_path\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir)\n        ## download model metadata\n        res = self.api_client.get(f\"models/{model_ref}/download?format=pth\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n\n        download_data = res.json()\n\n        download_uri = download_data.get(\"download_uri\")\n        if download_uri is None:\n            logger.error(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n        ## download model from Focoos Cloud\n        logger.debug(f\"Model URI: {download_uri}\")\n        logger.info(\"\ud83d\udce5 Downloading model from Focoos Cloud.. \")\n        try:\n            model_pth_path = self.api_client.download_ext_file(download_uri, model_dir, skip_if_exists=skip_if_exists)\n        except Exception as e:\n            logger.error(f\"Failed to download model: {e}\")\n            raise ValueError(f\"Failed to download model: {e}\")\n        if model_pth_path is None:\n            logger.error(f\"Failed to download model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to download model: {res.status_code} {res.text}\")\n\n        return model_pth_path\n\n    def list_remote_datasets(self, include_shared: bool = False) -&gt; list[DatasetPreview]:\n        \"\"\"\n        Lists all datasets available to the user.\n\n        This method retrieves all datasets owned by the user and optionally includes\n        shared datasets as well.\n\n        Args:\n            include_shared (bool): If True, includes datasets shared with the user.\n                Defaults to False.\n\n        Returns:\n            list[DatasetPreview]: A list of DatasetPreview objects representing the available datasets.\n\n        Raises:\n            ValueError: If the API request to list datasets fails.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n\n            # List only user's datasets\n            datasets = focoos.list_remote_datasets()\n\n            # List user's datasets and shared datasets\n            all_datasets = focoos.list_remote_datasets(include_shared=True)\n\n            for dataset in all_datasets:\n                print(f\"Dataset: {dataset.name}, Task: {dataset.task}\")\n            ```\n        \"\"\"\n        res = self.api_client.get(\"datasets/\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        datasets = [DatasetPreview.from_json(r) for r in res.json()]\n        if include_shared:\n            res = self.api_client.get(\"datasets/shared\")\n            if res.status_code != 200:\n                logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n                raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            datasets.extend([DatasetPreview.from_json(sh_dataset) for sh_dataset in res.json()])\n        return datasets\n\n    def get_remote_dataset(self, ref: str) -&gt; RemoteDataset:\n        \"\"\"\n        Retrieves a remote dataset by its reference ID.\n\n        Args:\n            ref (str): The reference ID of the dataset to retrieve.\n\n        Returns:\n            RemoteDataset: A RemoteDataset instance for the specified reference.\n\n        Example:\n            ```python\n            from focoos import FocoosHUB\n\n            focoos = FocoosHUB()\n            dataset = focoos.get_remote_dataset(ref=\"my-dataset-ref\")\n            ```\n        \"\"\"\n        return RemoteDataset(ref, self.api_client)\n\n    def new_model(self, model_info: ModelInfo) -&gt; Optional[RemoteModel]:\n        \"\"\"\n        Creates a new model in the Focoos platform.\n\n        Args:\n            name (str): Name of the new model.\n            focoos_model (str): Reference to the base Focoos model.\n            description (str): Description of the new model.\n\n        Returns:\n            Optional[RemoteModel]: The created model instance, or None if creation fails.\n\n        Raises:\n            ValueError: If the API request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n\n            focoos = Focoos()\n            model = focoos.new_model(name=\"my-model\", focoos_model=\"fai-model-ref\", description=\"my-model-description\")\n            ```\n        \"\"\"\n        if model_info.task == Task.KEYPOINT:\n            logger.warning(\n                \"Unfortunatelly keypoint models are not supported in the hub yet. Use them only locally temporarily.\"\n            )\n            return None\n        if model_info.model_family not in SUPPORTED_MODEL_FAMILIES:\n            logger.warning(\n                f\"Unfortunatelly model family {model_info.model_family} is not supported in the hub yet. Use one of {SUPPORTED_MODEL_FAMILIES}.\"\n            )\n            return None\n\n        res = self.api_client.post(\n            \"models/local-model\",\n            data={\n                \"name\": model_info.name,\n                \"focoos_model\": model_info.focoos_model,\n                \"description\": model_info.description,\n                \"config\": model_info.config if model_info.config else {},\n                \"task\": model_info.task,\n                \"classes\": model_info.classes,\n                \"im_size\": model_info.im_size,\n                \"train_args\": asdict(model_info.train_args) if model_info.train_args else None,\n                \"focoos_version\": model_info.focoos_version,\n            },\n        )\n        if res.status_code in [200, 201]:\n            return RemoteModel(res.json()[\"ref\"], self.api_client)\n        if res.status_code == 409:\n            logger.warning(f\"Model already exists: {model_info.name}\")\n            raise ValueError(f\"Failed to create new model: {res.status_code} {res.text}\")\n        else:\n            logger.warning(f\"Failed to create new model: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to create new model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.__init__","title":"<code>__init__(api_key=None, host_url=None)</code>","text":"<p>Initializes the FocoosHUB client.</p> <p>This client provides authenticated access to the Focoos API, enabling various operations through the configured HTTP client. It retrieves user information upon initialization and logs the environment details.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>API key for authentication. Defaults to the <code>focoos_api_key</code> specified in the FOCOOS_CONFIG.</p> <code>None</code> <code>host_url</code> <code>Optional[str]</code> <p>Base URL for the Focoos API. Defaults to the <code>default_host_url</code> specified in the FOCOOS_CONFIG.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API key is not provided, or if the host URL is not specified in the arguments or the configuration.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key used for authentication.</p> <code>api_client</code> <code>ApiClient</code> <p>An HTTP client instance configured with the API key and host URL.</p> <code>user_info</code> <code>User</code> <p>Information about the authenticated user retrieved from the API.</p> <code>host_url</code> <code>str</code> <p>The base URL used for API requests.</p> Logs <ul> <li>Error if the API key or host URL is missing.</li> <li>Info about the authenticated user and environment upon successful initialization.</li> </ul> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = None,\n    host_url: Optional[str] = None,\n):\n    \"\"\"\n    Initializes the FocoosHUB client.\n\n    This client provides authenticated access to the Focoos API, enabling various operations\n    through the configured HTTP client. It retrieves user information upon initialization and\n    logs the environment details.\n\n    Args:\n        api_key (Optional[str]): API key for authentication. Defaults to the `focoos_api_key`\n            specified in the FOCOOS_CONFIG.\n        host_url (Optional[str]): Base URL for the Focoos API. Defaults to the `default_host_url`\n            specified in the FOCOOS_CONFIG.\n\n    Raises:\n        ValueError: If the API key is not provided, or if the host URL is not specified in the\n            arguments or the configuration.\n\n    Attributes:\n        api_key (str): The API key used for authentication.\n        api_client (ApiClient): An HTTP client instance configured with the API key and host URL.\n        user_info (User): Information about the authenticated user retrieved from the API.\n        host_url (str): The base URL used for API requests.\n\n    Logs:\n        - Error if the API key or host URL is missing.\n        - Info about the authenticated user and environment upon successful initialization.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        ```\n    \"\"\"\n    self.api_key = api_key or FOCOOS_CONFIG.focoos_api_key\n    if not self.api_key:\n        logger.error(\"API key is required \ud83e\udd16\")\n        raise ValueError(\"API key is required \ud83e\udd16\")\n\n    self.host_url = host_url or FOCOOS_CONFIG.default_host_url\n\n    self.api_client = ApiClient(api_key=self.api_key, host_url=self.host_url)\n    self.user_info = self.get_user_info()\n    logger.info(f\"Currently logged as: {self.user_info.email} environment: {self.host_url}\")\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.download_model_pth","title":"<code>download_model_pth(model_ref, skip_if_exists=True)</code>","text":"<p>Downloads a model from the Focoos API.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference identifier for the model.</p> required <code>skip_if_exists</code> <code>bool</code> <p>If True, skips the download if the model file already exists. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the downloaded model file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails or the download fails.</p> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def download_model_pth(self, model_ref: str, skip_if_exists: bool = True) -&gt; str:\n    \"\"\"\n    Downloads a model from the Focoos API.\n\n    Args:\n        model_ref (str): Reference identifier for the model.\n        skip_if_exists (bool): If True, skips the download if the model file already exists.\n            Defaults to True.\n\n    Returns:\n        str: Path to the downloaded model file.\n\n    Raises:\n        ValueError: If the API request fails or the download fails.\n    \"\"\"\n    model_dir = os.path.join(MODELS_DIR, model_ref)\n    model_pth_path = os.path.join(model_dir, ArtifactName.WEIGHTS)\n    if os.path.exists(model_pth_path) and skip_if_exists:\n        logger.info(\"\ud83d\udce5 Model already downloaded\")\n        return model_pth_path\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    ## download model metadata\n    res = self.api_client.get(f\"models/{model_ref}/download?format=pth\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n\n    download_data = res.json()\n\n    download_uri = download_data.get(\"download_uri\")\n    if download_uri is None:\n        logger.error(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to retrieve download url for model: {res.status_code} {res.text}\")\n    ## download model from Focoos Cloud\n    logger.debug(f\"Model URI: {download_uri}\")\n    logger.info(\"\ud83d\udce5 Downloading model from Focoos Cloud.. \")\n    try:\n        model_pth_path = self.api_client.download_ext_file(download_uri, model_dir, skip_if_exists=skip_if_exists)\n    except Exception as e:\n        logger.error(f\"Failed to download model: {e}\")\n        raise ValueError(f\"Failed to download model: {e}\")\n    if model_pth_path is None:\n        logger.error(f\"Failed to download model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to download model: {res.status_code} {res.text}\")\n\n    return model_pth_path\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.get_model_info","title":"<code>get_model_info(model_ref)</code>","text":"<p>Retrieves metadata for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference identifier for the model.</p> required <p>Returns:</p> Name Type Description <code>RemoteModelInfo</code> <code>RemoteModelInfo</code> <p>Metadata of the specified model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\nmodel_info = focoos.get_model_info(model_ref=\"user-or-fai-model-ref\")\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def get_model_info(self, model_ref: str) -&gt; RemoteModelInfo:\n    \"\"\"\n    Retrieves metadata for a specific model.\n\n    Args:\n        model_ref (str): Reference identifier for the model.\n\n    Returns:\n        RemoteModelInfo: Metadata of the specified model.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        model_info = focoos.get_model_info(model_ref=\"user-or-fai-model-ref\")\n        ```\n    \"\"\"\n    res = self.api_client.get(f\"models/{model_ref}\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n    return RemoteModelInfo.from_json(res.json())\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.get_remote_dataset","title":"<code>get_remote_dataset(ref)</code>","text":"<p>Retrieves a remote dataset by its reference ID.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>The reference ID of the dataset to retrieve.</p> required <p>Returns:</p> Name Type Description <code>RemoteDataset</code> <code>RemoteDataset</code> <p>A RemoteDataset instance for the specified reference.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\ndataset = focoos.get_remote_dataset(ref=\"my-dataset-ref\")\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def get_remote_dataset(self, ref: str) -&gt; RemoteDataset:\n    \"\"\"\n    Retrieves a remote dataset by its reference ID.\n\n    Args:\n        ref (str): The reference ID of the dataset to retrieve.\n\n    Returns:\n        RemoteDataset: A RemoteDataset instance for the specified reference.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        dataset = focoos.get_remote_dataset(ref=\"my-dataset-ref\")\n        ```\n    \"\"\"\n    return RemoteDataset(ref, self.api_client)\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.get_remote_model","title":"<code>get_remote_model(model_ref)</code>","text":"<p>Retrieves a remote model instance for cloud-based inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference identifier for the model.</p> required <p>Returns:</p> Name Type Description <code>RemoteModel</code> <code>RemoteModel</code> <p>The remote model instance configured for cloud-based inference.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\nmodel = focoos.get_remote_model(model_ref=\"fai-model-ref\")\nresults = model.infer(\"image.jpg\", threshold=0.5)  # inference is remote!\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def get_remote_model(self, model_ref: str) -&gt; RemoteModel:\n    \"\"\"\n    Retrieves a remote model instance for cloud-based inference.\n\n    Args:\n        model_ref (str): Reference identifier for the model.\n\n    Returns:\n        RemoteModel: The remote model instance configured for cloud-based inference.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        model = focoos.get_remote_model(model_ref=\"fai-model-ref\")\n        results = model.infer(\"image.jpg\", threshold=0.5)  # inference is remote!\n        ```\n    \"\"\"\n    return RemoteModel(model_ref, self.api_client)\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.get_user_info","title":"<code>get_user_info()</code>","text":"<p>Retrieves information about the authenticated user.</p> <p>Returns:</p> Name Type Description <code>User</code> <code>User</code> <p>User object containing account information and usage quotas.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\nuser_info = focoos.get_user_info()\n\n# Access user info fields\nprint(f\"Email: {user_info.email}\")\nprint(f\"Created at: {user_info.created_at}\")\nprint(f\"Updated at: {user_info.updated_at}\")\nprint(f\"Company: {user_info.company}\")\nprint(f\"API key: {user_info.api_key.key}\")\n\n# Access quotas\nquotas = user_info.quotas\nprint(f\"Total inferences: {quotas.total_inferences}\")\nprint(f\"Max inferences: {quotas.max_inferences}\")\nprint(f\"Used storage (GB): {quotas.used_storage_gb}\")\nprint(f\"Max storage (GB): {quotas.max_storage_gb}\")\nprint(f\"Active training jobs: {quotas.active_training_jobs}\")\nprint(f\"Max active training jobs: {quotas.max_active_training_jobs}\")\nprint(f\"Used MLG4DNXLarge training jobs hours: {quotas.used_mlg4dnxlarge_training_jobs_hours}\")\nprint(f\"Max MLG4DNXLarge training jobs hours: {quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def get_user_info(self) -&gt; User:\n    \"\"\"\n    Retrieves information about the authenticated user.\n\n    Returns:\n        User: User object containing account information and usage quotas.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        user_info = focoos.get_user_info()\n\n        # Access user info fields\n        print(f\"Email: {user_info.email}\")\n        print(f\"Created at: {user_info.created_at}\")\n        print(f\"Updated at: {user_info.updated_at}\")\n        print(f\"Company: {user_info.company}\")\n        print(f\"API key: {user_info.api_key.key}\")\n\n        # Access quotas\n        quotas = user_info.quotas\n        print(f\"Total inferences: {quotas.total_inferences}\")\n        print(f\"Max inferences: {quotas.max_inferences}\")\n        print(f\"Used storage (GB): {quotas.used_storage_gb}\")\n        print(f\"Max storage (GB): {quotas.max_storage_gb}\")\n        print(f\"Active training jobs: {quotas.active_training_jobs}\")\n        print(f\"Max active training jobs: {quotas.max_active_training_jobs}\")\n        print(f\"Used MLG4DNXLarge training jobs hours: {quotas.used_mlg4dnxlarge_training_jobs_hours}\")\n        print(f\"Max MLG4DNXLarge training jobs hours: {quotas.max_mlg4dnxlarge_training_jobs_hours}\")\n        ```\n    \"\"\"\n    res = self.api_client.get(\"user/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get user info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get user info: {res.status_code} {res.text}\")\n    return User.from_json(res.json())\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.list_remote_datasets","title":"<code>list_remote_datasets(include_shared=False)</code>","text":"<p>Lists all datasets available to the user.</p> <p>This method retrieves all datasets owned by the user and optionally includes shared datasets as well.</p> <p>Parameters:</p> Name Type Description Default <code>include_shared</code> <code>bool</code> <p>If True, includes datasets shared with the user. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[DatasetPreview]</code> <p>list[DatasetPreview]: A list of DatasetPreview objects representing the available datasets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request to list datasets fails.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\n\n# List only user's datasets\ndatasets = focoos.list_remote_datasets()\n\n# List user's datasets and shared datasets\nall_datasets = focoos.list_remote_datasets(include_shared=True)\n\nfor dataset in all_datasets:\n    print(f\"Dataset: {dataset.name}, Task: {dataset.task}\")\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def list_remote_datasets(self, include_shared: bool = False) -&gt; list[DatasetPreview]:\n    \"\"\"\n    Lists all datasets available to the user.\n\n    This method retrieves all datasets owned by the user and optionally includes\n    shared datasets as well.\n\n    Args:\n        include_shared (bool): If True, includes datasets shared with the user.\n            Defaults to False.\n\n    Returns:\n        list[DatasetPreview]: A list of DatasetPreview objects representing the available datasets.\n\n    Raises:\n        ValueError: If the API request to list datasets fails.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n\n        # List only user's datasets\n        datasets = focoos.list_remote_datasets()\n\n        # List user's datasets and shared datasets\n        all_datasets = focoos.list_remote_datasets(include_shared=True)\n\n        for dataset in all_datasets:\n            print(f\"Dataset: {dataset.name}, Task: {dataset.task}\")\n        ```\n    \"\"\"\n    res = self.api_client.get(\"datasets/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n    datasets = [DatasetPreview.from_json(r) for r in res.json()]\n    if include_shared:\n        res = self.api_client.get(\"datasets/shared\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to list datasets: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to list datasets: {res.status_code} {res.text}\")\n        datasets.extend([DatasetPreview.from_json(sh_dataset) for sh_dataset in res.json()])\n    return datasets\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.list_remote_models","title":"<code>list_remote_models()</code>","text":"<p>Lists all models owned by the user.</p> <p>Returns:</p> Type Description <code>list[ModelPreview]</code> <p>list[ModelPreview]: List of model previews.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import FocoosHUB\n\nfocoos = FocoosHUB()\nmodels = focoos.list_remote_models()\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def list_remote_models(self) -&gt; list[ModelPreview]:\n    \"\"\"\n    Lists all models owned by the user.\n\n    Returns:\n        list[ModelPreview]: List of model previews.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import FocoosHUB\n\n        focoos = FocoosHUB()\n        models = focoos.list_remote_models()\n        ```\n    \"\"\"\n    res = self.api_client.get(\"models/\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to list models: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to list models: {res.status_code} {res.text}\")\n    return [ModelPreview.from_json(r) for r in res.json()]\n</code></pre>"},{"location":"api/hub/#focoos.hub.focoos_hub.FocoosHUB.new_model","title":"<code>new_model(model_info)</code>","text":"<p>Creates a new model in the Focoos platform.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the new model.</p> required <code>focoos_model</code> <code>str</code> <p>Reference to the base Focoos model.</p> required <code>description</code> <code>str</code> <p>Description of the new model.</p> required <p>Returns:</p> Type Description <code>Optional[RemoteModel]</code> <p>Optional[RemoteModel]: The created model instance, or None if creation fails.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails.</p> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos()\nmodel = focoos.new_model(name=\"my-model\", focoos_model=\"fai-model-ref\", description=\"my-model-description\")\n</code></pre> Source code in <code>focoos/hub/focoos_hub.py</code> <pre><code>def new_model(self, model_info: ModelInfo) -&gt; Optional[RemoteModel]:\n    \"\"\"\n    Creates a new model in the Focoos platform.\n\n    Args:\n        name (str): Name of the new model.\n        focoos_model (str): Reference to the base Focoos model.\n        description (str): Description of the new model.\n\n    Returns:\n        Optional[RemoteModel]: The created model instance, or None if creation fails.\n\n    Raises:\n        ValueError: If the API request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos()\n        model = focoos.new_model(name=\"my-model\", focoos_model=\"fai-model-ref\", description=\"my-model-description\")\n        ```\n    \"\"\"\n    if model_info.task == Task.KEYPOINT:\n        logger.warning(\n            \"Unfortunatelly keypoint models are not supported in the hub yet. Use them only locally temporarily.\"\n        )\n        return None\n    if model_info.model_family not in SUPPORTED_MODEL_FAMILIES:\n        logger.warning(\n            f\"Unfortunatelly model family {model_info.model_family} is not supported in the hub yet. Use one of {SUPPORTED_MODEL_FAMILIES}.\"\n        )\n        return None\n\n    res = self.api_client.post(\n        \"models/local-model\",\n        data={\n            \"name\": model_info.name,\n            \"focoos_model\": model_info.focoos_model,\n            \"description\": model_info.description,\n            \"config\": model_info.config if model_info.config else {},\n            \"task\": model_info.task,\n            \"classes\": model_info.classes,\n            \"im_size\": model_info.im_size,\n            \"train_args\": asdict(model_info.train_args) if model_info.train_args else None,\n            \"focoos_version\": model_info.focoos_version,\n        },\n    )\n    if res.status_code in [200, 201]:\n        return RemoteModel(res.json()[\"ref\"], self.api_client)\n    if res.status_code == 409:\n        logger.warning(f\"Model already exists: {model_info.name}\")\n        raise ValueError(f\"Failed to create new model: {res.status_code} {res.text}\")\n    else:\n        logger.warning(f\"Failed to create new model: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to create new model: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_dataset.RemoteDataset","title":"<code>RemoteDataset</code>","text":"<p>A class to manage remote datasets through the Focoos API.</p> <p>This class provides functionality to interact with datasets stored remotely, including uploading, downloading, and managing dataset data.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>The reference identifier for the dataset.</p> required <code>api_client</code> <code>ApiClient</code> <p>The API client instance for making requests.</p> required <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>The dataset reference identifier.</p> <code>api_client</code> <code>ApiClient</code> <p>The API client instance.</p> <code>metadata</code> <code>DatasetPreview</code> <p>The dataset metadata.</p> Source code in <code>focoos/hub/remote_dataset.py</code> <pre><code>class RemoteDataset:\n    \"\"\"\n    A class to manage remote datasets through the Focoos API.\n\n    This class provides functionality to interact with datasets stored remotely,\n    including uploading, downloading, and managing dataset data.\n\n    Args:\n        ref (str): The reference identifier for the dataset.\n        api_client (ApiClient): The API client instance for making requests.\n\n    Attributes:\n        ref (str): The dataset reference identifier.\n        api_client (ApiClient): The API client instance.\n        metadata (DatasetPreview): The dataset metadata.\n    \"\"\"\n\n    def __init__(self, ref: str, api_client: ApiClient):\n        self.ref = ref\n        self.api_client = api_client\n        self.metadata: DatasetPreview = self.get_info()\n\n    def get_info(self) -&gt; DatasetPreview:\n        \"\"\"\n        Retrieves the dataset information from the API.\n\n        Returns:\n            DatasetPreview: The dataset preview information.\n        \"\"\"\n        res = self.api_client.get(f\"datasets/{self.ref}\")\n        if res.status_code != 200:\n            raise ValueError(f\"Failed to get dataset info: {res.status_code} {res.text}\")\n        return DatasetPreview.from_json(res.json())\n\n    def upload_data(self, path: str) -&gt; Optional[DatasetSpec]:\n        \"\"\"\n        Uploads dataset data from a local zip file to the remote storage.\n\n        Args:\n            path (str): Local path to the zip file containing dataset data.\n\n        Returns:\n            Optional[DatasetSpec]: The dataset specification after successful upload.\n\n        Raises:\n            FileNotFoundError: If the specified file does not exist.\n            ValueError: If the file is not a zip file or upload fails.\n        \"\"\"\n        if not path.endswith(\".zip\"):\n            raise ValueError(\"Dataset must be .zip compressed\")\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"File not found: {path}\")\n\n        file_name = os.path.basename(path)\n        file_size = os.path.getsize(path)\n        file_size_mb = file_size / (1024 * 1024)\n        logger.info(f\"\ud83d\udd17 Requesting upload url for {file_name} of size {file_size_mb:.2f} MB\")\n        presigned_url = self.api_client.post(\n            f\"datasets/{self.ref}/generate-upload-url\",\n            data={\"file_size_bytes\": file_size, \"file_name\": file_name},\n        )\n        if presigned_url.status_code != 200:\n            raise ValueError(f\"Failed to generate upload url: {presigned_url.status_code} {presigned_url.text}\")\n        presigned_url = presigned_url.json()\n        fields = {k: v for k, v in presigned_url[\"fields\"].items()}\n        logger.info(f\"\ud83d\udce4 Uploading file {file_name}..\")\n\n        # Use context manager to properly handle file closure\n        with open(path, \"rb\") as file_obj:\n            fields[\"file\"] = (file_name, file_obj, \"application/zip\")\n\n            res = self.api_client.external_post(\n                presigned_url[\"url\"],\n                files=fields,\n                data=presigned_url[\"fields\"],\n                stream=True,\n            )\n\n        logger.info(\"\u2705 Upload file done.\")\n        if res.status_code not in [200, 201, 204]:\n            raise ValueError(f\"Failed to upload dataset: {res.status_code} {res.text}\")\n\n        logger.info(\"\ud83d\udd17 Validating dataset..\")\n        complete_upload = self.api_client.post(\n            f\"datasets/{self.ref}/complete-upload\",\n        )\n        if complete_upload.status_code not in [200, 201, 204]:\n            raise ValueError(f\"Failed to validate dataset: {complete_upload.status_code} {complete_upload.text}\")\n        self.metadata = self.get_info()\n        logger.info(f\"\u2705 Dataset validated! =&gt; {self.metadata.spec}\")\n        return self.metadata.spec\n\n    @property\n    def name(self):\n        return self.metadata.name\n\n    @property\n    def task(self):\n        return self.metadata.task\n\n    @property\n    def layout(self):\n        return self.metadata.layout\n\n    def download_data(self, path: str = DATASETS_DIR):\n        \"\"\"\n        Downloads the dataset data to a local path.\n\n        Args:\n            path (str): Local path where the dataset should be downloaded.\n\n        Returns:\n            str: The path where the file was downloaded.\n\n        Raises:\n            ValueError: If the download fails.\n        \"\"\"\n        res = self.api_client.get(f\"datasets/{self.ref}/download\")\n        if res.status_code != 200:\n            raise ValueError(f\"Failed to download dataset data: {res.status_code} {res.text}\")\n        url = res.json()[\"download_uri\"]\n\n        path = self.api_client.download_ext_file(url, path, skip_if_exists=True)\n        logger.info(f\"\u2705 Dataset data downloaded to {path}\")\n        return path\n\n    def __str__(self):\n        return f\"RemoteDataset(ref={self.ref}, name={self.name}, task={self.task}, layout={self.layout})\"\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_dataset.RemoteDataset.download_data","title":"<code>download_data(path=DATASETS_DIR)</code>","text":"<p>Downloads the dataset data to a local path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Local path where the dataset should be downloaded.</p> <code>DATASETS_DIR</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The path where the file was downloaded.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the download fails.</p> Source code in <code>focoos/hub/remote_dataset.py</code> <pre><code>def download_data(self, path: str = DATASETS_DIR):\n    \"\"\"\n    Downloads the dataset data to a local path.\n\n    Args:\n        path (str): Local path where the dataset should be downloaded.\n\n    Returns:\n        str: The path where the file was downloaded.\n\n    Raises:\n        ValueError: If the download fails.\n    \"\"\"\n    res = self.api_client.get(f\"datasets/{self.ref}/download\")\n    if res.status_code != 200:\n        raise ValueError(f\"Failed to download dataset data: {res.status_code} {res.text}\")\n    url = res.json()[\"download_uri\"]\n\n    path = self.api_client.download_ext_file(url, path, skip_if_exists=True)\n    logger.info(f\"\u2705 Dataset data downloaded to {path}\")\n    return path\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_dataset.RemoteDataset.get_info","title":"<code>get_info()</code>","text":"<p>Retrieves the dataset information from the API.</p> <p>Returns:</p> Name Type Description <code>DatasetPreview</code> <code>DatasetPreview</code> <p>The dataset preview information.</p> Source code in <code>focoos/hub/remote_dataset.py</code> <pre><code>def get_info(self) -&gt; DatasetPreview:\n    \"\"\"\n    Retrieves the dataset information from the API.\n\n    Returns:\n        DatasetPreview: The dataset preview information.\n    \"\"\"\n    res = self.api_client.get(f\"datasets/{self.ref}\")\n    if res.status_code != 200:\n        raise ValueError(f\"Failed to get dataset info: {res.status_code} {res.text}\")\n    return DatasetPreview.from_json(res.json())\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_dataset.RemoteDataset.upload_data","title":"<code>upload_data(path)</code>","text":"<p>Uploads dataset data from a local zip file to the remote storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Local path to the zip file containing dataset data.</p> required <p>Returns:</p> Type Description <code>Optional[DatasetSpec]</code> <p>Optional[DatasetSpec]: The dataset specification after successful upload.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>ValueError</code> <p>If the file is not a zip file or upload fails.</p> Source code in <code>focoos/hub/remote_dataset.py</code> <pre><code>def upload_data(self, path: str) -&gt; Optional[DatasetSpec]:\n    \"\"\"\n    Uploads dataset data from a local zip file to the remote storage.\n\n    Args:\n        path (str): Local path to the zip file containing dataset data.\n\n    Returns:\n        Optional[DatasetSpec]: The dataset specification after successful upload.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        ValueError: If the file is not a zip file or upload fails.\n    \"\"\"\n    if not path.endswith(\".zip\"):\n        raise ValueError(\"Dataset must be .zip compressed\")\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File not found: {path}\")\n\n    file_name = os.path.basename(path)\n    file_size = os.path.getsize(path)\n    file_size_mb = file_size / (1024 * 1024)\n    logger.info(f\"\ud83d\udd17 Requesting upload url for {file_name} of size {file_size_mb:.2f} MB\")\n    presigned_url = self.api_client.post(\n        f\"datasets/{self.ref}/generate-upload-url\",\n        data={\"file_size_bytes\": file_size, \"file_name\": file_name},\n    )\n    if presigned_url.status_code != 200:\n        raise ValueError(f\"Failed to generate upload url: {presigned_url.status_code} {presigned_url.text}\")\n    presigned_url = presigned_url.json()\n    fields = {k: v for k, v in presigned_url[\"fields\"].items()}\n    logger.info(f\"\ud83d\udce4 Uploading file {file_name}..\")\n\n    # Use context manager to properly handle file closure\n    with open(path, \"rb\") as file_obj:\n        fields[\"file\"] = (file_name, file_obj, \"application/zip\")\n\n        res = self.api_client.external_post(\n            presigned_url[\"url\"],\n            files=fields,\n            data=presigned_url[\"fields\"],\n            stream=True,\n        )\n\n    logger.info(\"\u2705 Upload file done.\")\n    if res.status_code not in [200, 201, 204]:\n        raise ValueError(f\"Failed to upload dataset: {res.status_code} {res.text}\")\n\n    logger.info(\"\ud83d\udd17 Validating dataset..\")\n    complete_upload = self.api_client.post(\n        f\"datasets/{self.ref}/complete-upload\",\n    )\n    if complete_upload.status_code not in [200, 201, 204]:\n        raise ValueError(f\"Failed to validate dataset: {complete_upload.status_code} {complete_upload.text}\")\n    self.metadata = self.get_info()\n    logger.info(f\"\u2705 Dataset validated! =&gt; {self.metadata.spec}\")\n    return self.metadata.spec\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel","title":"<code>RemoteModel</code>","text":"<p>Represents a remote model in the Focoos platform.</p> <p>Attributes:</p> Name Type Description <code>model_ref</code> <code>str</code> <p>Reference ID for the model.</p> <code>api_client</code> <code>ApiClient</code> <p>Client for making HTTP requests.</p> <code>model_info</code> <code>RemoteModelInfo</code> <p>Model information of the model.</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>class RemoteModel:\n    \"\"\"\n    Represents a remote model in the Focoos platform.\n\n    Attributes:\n        model_ref (str): Reference ID for the model.\n        api_client (ApiClient): Client for making HTTP requests.\n        model_info (RemoteModelInfo): Model information of the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_ref: str,\n        api_client: ApiClient,\n    ):\n        \"\"\"\n        Initialize the RemoteModel instance.\n\n        Args:\n            model_ref (str): Reference ID for the model.\n            api_client (ApiClient): HTTP client instance for communication.\n\n        Raises:\n            ValueError: If model metadata retrieval fails.\n        \"\"\"\n        self.model_ref = model_ref\n        self.api_client = api_client\n        self.model_info: RemoteModelInfo = self.get_info()\n\n        logger.info(\n            f\"[RemoteModel]: ref: {self.model_ref} name: {self.model_info.name} description: {self.model_info.description} status: {self.model_info.status}\"\n        )\n\n    @property\n    def ref(self) -&gt; str:\n        return self.model_ref\n\n    def get_info(self) -&gt; RemoteModelInfo:\n        \"\"\"\n        Retrieve model metadata.\n\n        Returns:\n            ModelMetadata: Metadata of the model.\n\n        Raises:\n            ValueError: If the request fails.\n\n        Example:\n            ```python\n            from focoos import Focoos, RemoteModel\n\n            focoos = Focoos()\n            model = focoos.get_remote_model(model_ref=\"&lt;model_ref&gt;\")\n            model_info = model.get_info()\n            ```\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n        self.metadata = RemoteModelInfo(**res.json())\n        return self.metadata\n\n    def sync_local_training_job(\n        self, local_training_info: HubSyncLocalTraining, dir: str, upload_artifacts: Optional[List[ArtifactName]] = None\n    ) -&gt; None:\n        if not os.path.exists(os.path.join(dir, ArtifactName.INFO)):\n            logger.warning(f\"Model info not found in {dir}\")\n            raise ValueError(f\"Model info not found in {dir}\")\n        metrics = parse_metrics(os.path.join(dir, ArtifactName.METRICS))\n        local_training_info.metrics = metrics\n        logger.debug(\n            f\"[Syncing Training] iter: {metrics.iterations} {self.metadata.name} status: {local_training_info.status} ref: {self.model_ref}\"\n        )\n\n        ## Update metrics\n        res = self.api_client.patch(\n            f\"models/{self.model_ref}/sync-local-training\",\n            data=asdict(local_training_info),\n        )\n        if res.status_code != 200:\n            logger.error(f\"Failed to sync local training: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to sync local training: {res.status_code} {res.text}\")\n\n        if upload_artifacts:\n            for artifact in upload_artifacts:\n                file_path = os.path.join(dir, artifact.value)\n                if os.path.isfile(file_path):\n                    try:\n                        self._upload_model_artifact(file_path)\n                    except Exception as e:\n                        logger.error(f\"Failed to upload artifact: {artifact.value} path: {file_path} error -&gt; {str(e)}\")\n                        pass\n                else:\n                    logger.warning(f\"Artifact {artifact.value} not found in {dir}\")\n\n    def _upload_model_artifact(self, path: str) -&gt; None:\n        \"\"\"\n        Uploads an model artifact to the Focoos platform.\n        \"\"\"\n        if not os.path.exists(path):\n            raise ValueError(f\"File not found: {path}\")\n        file_ext = os.path.splitext(path)[1]\n        if file_ext not in [\".pt\", \".onnx\", \".pth\", \".json\", \".txt\"]:\n            raise ValueError(f\"Unsupported file extension: {file_ext}\")\n        file_name = os.path.basename(path)\n        file_size = os.path.getsize(path)\n        file_size *= 1.1  # 10% buffer\n        file_size_mb = file_size / (1024 * 1024)\n\n        logger.debug(f\"\ud83d\udd17 Requesting upload url for {file_name} of size {file_size_mb:.2f} MB\")\n\n        presigned_url = self.api_client.post(\n            f\"models/{self.model_ref}/generate-upload-url\",\n            data={\"file_size_bytes\": int(file_size), \"file_name\": file_name},\n        )\n        if presigned_url.status_code != 200:\n            raise ValueError(f\"Failed to generate upload url: {presigned_url.status_code} {presigned_url.text}\")\n        presigned_url = presigned_url.json()\n        fields = {k: v for k, v in presigned_url[\"fields\"].items()}\n        logger.info(f\"\ud83d\udce4 Uploading file {file_name} to HUB, size: {file_size_mb:.2f} MB\")\n        fields[\"file\"] = (file_name, open(path, \"rb\"))\n        res = self.api_client.external_post(\n            presigned_url[\"url\"],\n            files=fields,\n            data=presigned_url[\"fields\"],\n        )\n        if res.status_code not in [200, 201, 204]:\n            raise ValueError(f\"Failed to upload model artifact: {res.status_code} {res.text}\")\n        logger.info(f\"\u2705 Model artifact {file_name} uploaded to HUB.\")\n\n    def train_info(self) -&gt; Optional[TrainingInfo]:\n        \"\"\"\n        Retrieve the current status of the model training.\n\n        Sends a request to check the training status of the model referenced by `self.model_ref`.\n\n        Returns:\n            dict: A dictionary containing the training status information.\n\n        Raises:\n            ValueError: If the request to get training status fails.\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}/train/status\")\n        if res.status_code != 200:\n            logger.error(f\"Failed to get train info: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to get train info: {res.status_code} {res.text}\")\n        dct = {k: v for k, v in res.json().items() if k in TrainingInfo.__dataclass_fields__}\n        return TrainingInfo(**dct)\n\n    def train_logs(self) -&gt; list[str]:\n        \"\"\"\n        Retrieve the training logs for the model.\n\n        This method sends a request to fetch the logs of the model's training process. If the request\n        is successful (status code 200), it returns the logs as a list of strings. If the request fails,\n        it logs a warning and returns an empty list.\n\n        Returns:\n            list[str]: A list of training logs as strings.\n\n        Raises:\n            None: Returns an empty list if the request fails.\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}/train/logs\")\n        if res.status_code != 200:\n            logger.warning(f\"Failed to get train logs: {res.status_code} {res.text}\")\n            return []\n        return res.json()\n\n    def metrics(self) -&gt; Metrics:  # noqa: F821\n        \"\"\"\n        Retrieve the metrics of the model.\n\n        This method sends a request to fetch the metrics of the model identified by `model_ref`.\n        If the request is successful (status code 200), it returns the metrics as a `Metrics` object.\n        If the request fails, it logs a warning and returns an empty `Metrics` object.\n\n        Returns:\n            Metrics: An object containing the metrics of the model.\n\n        Raises:\n            None: Returns an empty `Metrics` object if the request fails.\n        \"\"\"\n        res = self.api_client.get(f\"models/{self.model_ref}/metrics\")\n        if res.status_code != 200:\n            logger.warning(f\"Failed to get metrics: {res.status_code} {res.text}\")\n            return Metrics()  # noqa: F821\n        return Metrics(**{k: v for k, v in res.json().items() if k in Metrics.__dataclass_fields__})\n\n    def __call__(\n        self, image: Union[str, Path, np.ndarray, bytes, Image.Image], threshold: float = 0.5\n    ) -&gt; FocoosDetections:\n        return self.infer(image, threshold)\n\n    def infer(\n        self,\n        image: Union[str, Path, np.ndarray, bytes, Image.Image],\n        threshold: float = 0.5,\n        annotate: bool = False,\n    ) -&gt; FocoosDetections:\n        \"\"\"\n        Run inference on an image using the remote model and return detection results.\n\n        This method uploads an image to the remote model for inference. The image can be provided as a file path,\n        a string path, a NumPy array, raw bytes, or a PIL Image. The method returns detection results, including\n        class IDs, confidence scores, bounding boxes, and segmentation masks if available. Optionally, the results\n        can be returned with the image annotated with detections.\n\n        Args:\n            image (Union[str, Path, np.ndarray, bytes, Image.Image]): The image to run inference on.\n            threshold (float, optional): Minimum confidence threshold for detections. Detections below this\n                threshold are filtered out. Default is 0.5.\n            annotate (bool, optional): If True, returns the image with detections drawn on it. Default is False.\n\n        Returns:\n            FocoosDetections: Detection results, including a list of detections and optional annotated image.\n\n        Raises:\n            FileNotFoundError: If the image file path does not exist or cannot be loaded.\n            ValueError: If the inference request to the remote model fails.\n\n        Example:\n            ```python\n            from focoos import Focoos\n            from PIL import Image\n\n            focoos = Focoos()\n            model = focoos.get_remote_model(\"my-model\")\n            results = model.infer(\"image.jpg\", threshold=0.5, annotate=True)\n\n            for det in results.detections:\n                print(f\"Label: {det.label}, Confidence: {det.conf:.2f}, BBox: {det.bbox}\")\n                if det.mask is not None:\n                    print(\"Segmentation mask available\")\n            if results.image is not None:\n                Image.fromarray(results.image)\n            ```\n        \"\"\"\n\n        image = image_loader(image)\n        _, buffer = cv2.imencode(\".jpg\", image)\n        image_bytes = buffer.tobytes()\n        files = {\"file\": (\"image\", image_bytes, \"image/jpeg\")}\n        t0 = time.time()\n        res = self.api_client.post(\n            f\"models/{self.model_ref}/inference?confidence_threshold={threshold}\",\n            files=files,\n        )\n        t1 = time.time()\n        if res.status_code == 200:\n            detections = FocoosDetections(\n                detections=[FocoosDet.from_json(d) for d in res.json().get(\"detections\", [])],\n                latency=InferLatency(**res.json().get(\"latency\", {})),\n            )\n            detections.infer_print()\n            print(f\"Request time: {(t1 - t0) * 1000:.0f}ms\")\n            if annotate:\n                detections.image = annotate_image(image, detections, self.model_info.task, self.model_info.classes)\n            return detections\n        else:\n            logger.error(f\"Failed to infer: {res.status_code} {res.text}\")\n            raise ValueError(f\"Failed to infer: {res.status_code} {res.text}\")\n\n    def notebook_monitor_train(self, interval: int = 30, plot_metrics: bool = False, max_runtime: int = 36000) -&gt; None:\n        \"\"\"\n        Monitor the training process in a Jupyter notebook and display metrics.\n\n        Periodically checks the training status and displays metrics in a notebook cell.\n        Clears previous output to maintain a clean view.\n\n        Args:\n            interval (int): Time between status checks in seconds. Must be 30-240. Default: 30\n            plot_metrics (bool): Whether to plot metrics graphs. Default: False\n            max_runtime (int): Maximum monitoring time in seconds. Default: 36000 (10 hours)\n\n        Returns:\n            None\n        \"\"\"\n        from IPython.display import clear_output\n\n        if not 30 &lt;= interval &lt;= 240:\n            raise ValueError(\"Interval must be between 30 and 240 seconds\")\n\n        last_update = self.get_info().updated_at\n        start_time = time.time()\n        status_history = []\n\n        while True:\n            # Get current status\n            model_info = self.get_info()\n            status = model_info.status\n\n            # Clear and display status\n            clear_output(wait=True)\n            status_msg = f\"[Live Monitor {self.metadata.name}] {status.value}\"\n            status_history.append(status_msg)\n            for msg in status_history:\n                logger.info(msg)\n\n            # Show metrics if training completed\n            if status == ModelStatus.TRAINING_COMPLETED:\n                metrics = self.metrics()\n                if metrics.best_valid_metric:\n                    logger.info(f\"Best Checkpoint (iter: {metrics.best_valid_metric.get('iteration', 'N/A')}):\")\n                    for k, v in metrics.best_valid_metric.items():\n                        logger.info(f\"  {k}: {v}\")\n                    visualizer = MetricsVisualizer(metrics)\n                    visualizer.log_metrics()\n                    if plot_metrics:\n                        visualizer.notebook_plot_training_metrics()\n\n            # Update metrics during training\n            if status == ModelStatus.TRAINING_RUNNING and model_info.updated_at &gt; last_update:\n                last_update = model_info.updated_at\n                metrics = self.metrics()\n                visualizer = MetricsVisualizer(metrics)\n                visualizer.log_metrics()\n                if plot_metrics:\n                    visualizer.notebook_plot_training_metrics()\n\n            # Check exit conditions\n            if status not in [ModelStatus.CREATED, ModelStatus.TRAINING_RUNNING, ModelStatus.TRAINING_STARTING]:\n                return\n\n            if time.time() - start_time &gt; max_runtime:\n                logger.warning(f\"Monitoring exceeded {max_runtime} seconds limit\")\n                return\n\n            sleep(interval)\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.__init__","title":"<code>__init__(model_ref, api_client)</code>","text":"<p>Initialize the RemoteModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_ref</code> <code>str</code> <p>Reference ID for the model.</p> required <code>api_client</code> <code>ApiClient</code> <p>HTTP client instance for communication.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If model metadata retrieval fails.</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def __init__(\n    self,\n    model_ref: str,\n    api_client: ApiClient,\n):\n    \"\"\"\n    Initialize the RemoteModel instance.\n\n    Args:\n        model_ref (str): Reference ID for the model.\n        api_client (ApiClient): HTTP client instance for communication.\n\n    Raises:\n        ValueError: If model metadata retrieval fails.\n    \"\"\"\n    self.model_ref = model_ref\n    self.api_client = api_client\n    self.model_info: RemoteModelInfo = self.get_info()\n\n    logger.info(\n        f\"[RemoteModel]: ref: {self.model_ref} name: {self.model_info.name} description: {self.model_info.description} status: {self.model_info.status}\"\n    )\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.get_info","title":"<code>get_info()</code>","text":"<p>Retrieve model metadata.</p> <p>Returns:</p> Name Type Description <code>ModelMetadata</code> <code>RemoteModelInfo</code> <p>Metadata of the model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request fails.</p> Example <pre><code>from focoos import Focoos, RemoteModel\n\nfocoos = Focoos()\nmodel = focoos.get_remote_model(model_ref=\"&lt;model_ref&gt;\")\nmodel_info = model.get_info()\n</code></pre> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def get_info(self) -&gt; RemoteModelInfo:\n    \"\"\"\n    Retrieve model metadata.\n\n    Returns:\n        ModelMetadata: Metadata of the model.\n\n    Raises:\n        ValueError: If the request fails.\n\n    Example:\n        ```python\n        from focoos import Focoos, RemoteModel\n\n        focoos = Focoos()\n        model = focoos.get_remote_model(model_ref=\"&lt;model_ref&gt;\")\n        model_info = model.get_info()\n        ```\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get model info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get model info: {res.status_code} {res.text}\")\n    self.metadata = RemoteModelInfo(**res.json())\n    return self.metadata\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.infer","title":"<code>infer(image, threshold=0.5, annotate=False)</code>","text":"<p>Run inference on an image using the remote model and return detection results.</p> <p>This method uploads an image to the remote model for inference. The image can be provided as a file path, a string path, a NumPy array, raw bytes, or a PIL Image. The method returns detection results, including class IDs, confidence scores, bounding boxes, and segmentation masks if available. Optionally, the results can be returned with the image annotated with detections.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Path, ndarray, bytes, Image]</code> <p>The image to run inference on.</p> required <code>threshold</code> <code>float</code> <p>Minimum confidence threshold for detections. Detections below this threshold are filtered out. Default is 0.5.</p> <code>0.5</code> <code>annotate</code> <code>bool</code> <p>If True, returns the image with detections drawn on it. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>FocoosDetections</code> <code>FocoosDetections</code> <p>Detection results, including a list of detections and optional annotated image.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the image file path does not exist or cannot be loaded.</p> <code>ValueError</code> <p>If the inference request to the remote model fails.</p> Example <pre><code>from focoos import Focoos\nfrom PIL import Image\n\nfocoos = Focoos()\nmodel = focoos.get_remote_model(\"my-model\")\nresults = model.infer(\"image.jpg\", threshold=0.5, annotate=True)\n\nfor det in results.detections:\n    print(f\"Label: {det.label}, Confidence: {det.conf:.2f}, BBox: {det.bbox}\")\n    if det.mask is not None:\n        print(\"Segmentation mask available\")\nif results.image is not None:\n    Image.fromarray(results.image)\n</code></pre> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def infer(\n    self,\n    image: Union[str, Path, np.ndarray, bytes, Image.Image],\n    threshold: float = 0.5,\n    annotate: bool = False,\n) -&gt; FocoosDetections:\n    \"\"\"\n    Run inference on an image using the remote model and return detection results.\n\n    This method uploads an image to the remote model for inference. The image can be provided as a file path,\n    a string path, a NumPy array, raw bytes, or a PIL Image. The method returns detection results, including\n    class IDs, confidence scores, bounding boxes, and segmentation masks if available. Optionally, the results\n    can be returned with the image annotated with detections.\n\n    Args:\n        image (Union[str, Path, np.ndarray, bytes, Image.Image]): The image to run inference on.\n        threshold (float, optional): Minimum confidence threshold for detections. Detections below this\n            threshold are filtered out. Default is 0.5.\n        annotate (bool, optional): If True, returns the image with detections drawn on it. Default is False.\n\n    Returns:\n        FocoosDetections: Detection results, including a list of detections and optional annotated image.\n\n    Raises:\n        FileNotFoundError: If the image file path does not exist or cannot be loaded.\n        ValueError: If the inference request to the remote model fails.\n\n    Example:\n        ```python\n        from focoos import Focoos\n        from PIL import Image\n\n        focoos = Focoos()\n        model = focoos.get_remote_model(\"my-model\")\n        results = model.infer(\"image.jpg\", threshold=0.5, annotate=True)\n\n        for det in results.detections:\n            print(f\"Label: {det.label}, Confidence: {det.conf:.2f}, BBox: {det.bbox}\")\n            if det.mask is not None:\n                print(\"Segmentation mask available\")\n        if results.image is not None:\n            Image.fromarray(results.image)\n        ```\n    \"\"\"\n\n    image = image_loader(image)\n    _, buffer = cv2.imencode(\".jpg\", image)\n    image_bytes = buffer.tobytes()\n    files = {\"file\": (\"image\", image_bytes, \"image/jpeg\")}\n    t0 = time.time()\n    res = self.api_client.post(\n        f\"models/{self.model_ref}/inference?confidence_threshold={threshold}\",\n        files=files,\n    )\n    t1 = time.time()\n    if res.status_code == 200:\n        detections = FocoosDetections(\n            detections=[FocoosDet.from_json(d) for d in res.json().get(\"detections\", [])],\n            latency=InferLatency(**res.json().get(\"latency\", {})),\n        )\n        detections.infer_print()\n        print(f\"Request time: {(t1 - t0) * 1000:.0f}ms\")\n        if annotate:\n            detections.image = annotate_image(image, detections, self.model_info.task, self.model_info.classes)\n        return detections\n    else:\n        logger.error(f\"Failed to infer: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to infer: {res.status_code} {res.text}\")\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.metrics","title":"<code>metrics()</code>","text":"<p>Retrieve the metrics of the model.</p> <p>This method sends a request to fetch the metrics of the model identified by <code>model_ref</code>. If the request is successful (status code 200), it returns the metrics as a <code>Metrics</code> object. If the request fails, it logs a warning and returns an empty <code>Metrics</code> object.</p> <p>Returns:</p> Name Type Description <code>Metrics</code> <code>Metrics</code> <p>An object containing the metrics of the model.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns an empty <code>Metrics</code> object if the request fails.</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def metrics(self) -&gt; Metrics:  # noqa: F821\n    \"\"\"\n    Retrieve the metrics of the model.\n\n    This method sends a request to fetch the metrics of the model identified by `model_ref`.\n    If the request is successful (status code 200), it returns the metrics as a `Metrics` object.\n    If the request fails, it logs a warning and returns an empty `Metrics` object.\n\n    Returns:\n        Metrics: An object containing the metrics of the model.\n\n    Raises:\n        None: Returns an empty `Metrics` object if the request fails.\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}/metrics\")\n    if res.status_code != 200:\n        logger.warning(f\"Failed to get metrics: {res.status_code} {res.text}\")\n        return Metrics()  # noqa: F821\n    return Metrics(**{k: v for k, v in res.json().items() if k in Metrics.__dataclass_fields__})\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.notebook_monitor_train","title":"<code>notebook_monitor_train(interval=30, plot_metrics=False, max_runtime=36000)</code>","text":"<p>Monitor the training process in a Jupyter notebook and display metrics.</p> <p>Periodically checks the training status and displays metrics in a notebook cell. Clears previous output to maintain a clean view.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>int</code> <p>Time between status checks in seconds. Must be 30-240. Default: 30</p> <code>30</code> <code>plot_metrics</code> <code>bool</code> <p>Whether to plot metrics graphs. Default: False</p> <code>False</code> <code>max_runtime</code> <code>int</code> <p>Maximum monitoring time in seconds. Default: 36000 (10 hours)</p> <code>36000</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def notebook_monitor_train(self, interval: int = 30, plot_metrics: bool = False, max_runtime: int = 36000) -&gt; None:\n    \"\"\"\n    Monitor the training process in a Jupyter notebook and display metrics.\n\n    Periodically checks the training status and displays metrics in a notebook cell.\n    Clears previous output to maintain a clean view.\n\n    Args:\n        interval (int): Time between status checks in seconds. Must be 30-240. Default: 30\n        plot_metrics (bool): Whether to plot metrics graphs. Default: False\n        max_runtime (int): Maximum monitoring time in seconds. Default: 36000 (10 hours)\n\n    Returns:\n        None\n    \"\"\"\n    from IPython.display import clear_output\n\n    if not 30 &lt;= interval &lt;= 240:\n        raise ValueError(\"Interval must be between 30 and 240 seconds\")\n\n    last_update = self.get_info().updated_at\n    start_time = time.time()\n    status_history = []\n\n    while True:\n        # Get current status\n        model_info = self.get_info()\n        status = model_info.status\n\n        # Clear and display status\n        clear_output(wait=True)\n        status_msg = f\"[Live Monitor {self.metadata.name}] {status.value}\"\n        status_history.append(status_msg)\n        for msg in status_history:\n            logger.info(msg)\n\n        # Show metrics if training completed\n        if status == ModelStatus.TRAINING_COMPLETED:\n            metrics = self.metrics()\n            if metrics.best_valid_metric:\n                logger.info(f\"Best Checkpoint (iter: {metrics.best_valid_metric.get('iteration', 'N/A')}):\")\n                for k, v in metrics.best_valid_metric.items():\n                    logger.info(f\"  {k}: {v}\")\n                visualizer = MetricsVisualizer(metrics)\n                visualizer.log_metrics()\n                if plot_metrics:\n                    visualizer.notebook_plot_training_metrics()\n\n        # Update metrics during training\n        if status == ModelStatus.TRAINING_RUNNING and model_info.updated_at &gt; last_update:\n            last_update = model_info.updated_at\n            metrics = self.metrics()\n            visualizer = MetricsVisualizer(metrics)\n            visualizer.log_metrics()\n            if plot_metrics:\n                visualizer.notebook_plot_training_metrics()\n\n        # Check exit conditions\n        if status not in [ModelStatus.CREATED, ModelStatus.TRAINING_RUNNING, ModelStatus.TRAINING_STARTING]:\n            return\n\n        if time.time() - start_time &gt; max_runtime:\n            logger.warning(f\"Monitoring exceeded {max_runtime} seconds limit\")\n            return\n\n        sleep(interval)\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.train_info","title":"<code>train_info()</code>","text":"<p>Retrieve the current status of the model training.</p> <p>Sends a request to check the training status of the model referenced by <code>self.model_ref</code>.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[TrainingInfo]</code> <p>A dictionary containing the training status information.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the request to get training status fails.</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def train_info(self) -&gt; Optional[TrainingInfo]:\n    \"\"\"\n    Retrieve the current status of the model training.\n\n    Sends a request to check the training status of the model referenced by `self.model_ref`.\n\n    Returns:\n        dict: A dictionary containing the training status information.\n\n    Raises:\n        ValueError: If the request to get training status fails.\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}/train/status\")\n    if res.status_code != 200:\n        logger.error(f\"Failed to get train info: {res.status_code} {res.text}\")\n        raise ValueError(f\"Failed to get train info: {res.status_code} {res.text}\")\n    dct = {k: v for k, v in res.json().items() if k in TrainingInfo.__dataclass_fields__}\n    return TrainingInfo(**dct)\n</code></pre>"},{"location":"api/hub/#focoos.hub.remote_model.RemoteModel.train_logs","title":"<code>train_logs()</code>","text":"<p>Retrieve the training logs for the model.</p> <p>This method sends a request to fetch the logs of the model's training process. If the request is successful (status code 200), it returns the logs as a list of strings. If the request fails, it logs a warning and returns an empty list.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of training logs as strings.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns an empty list if the request fails.</p> Source code in <code>focoos/hub/remote_model.py</code> <pre><code>def train_logs(self) -&gt; list[str]:\n    \"\"\"\n    Retrieve the training logs for the model.\n\n    This method sends a request to fetch the logs of the model's training process. If the request\n    is successful (status code 200), it returns the logs as a list of strings. If the request fails,\n    it logs a warning and returns an empty list.\n\n    Returns:\n        list[str]: A list of training logs as strings.\n\n    Raises:\n        None: Returns an empty list if the request fails.\n    \"\"\"\n    res = self.api_client.get(f\"models/{self.model_ref}/train/logs\")\n    if res.status_code != 200:\n        logger.warning(f\"Failed to get train logs: {res.status_code} {res.text}\")\n        return []\n    return res.json()\n</code></pre>"},{"location":"api/infer_model/","title":"InferModel","text":"<p>InferModel Module</p> <p>This module provides the <code>InferModel</code> class that allows loading, inference, and benchmark testing of models in a local environment. It supports detection and segmentation tasks, and utilizes various runtime backends including ONNXRuntime and TorchScript for model execution.</p> <p>Classes:</p> Name Description <code>InferModel</code> <p>A class for managing and interacting with local models.</p> <p>Functions:</p> Name Description <code>__init__</code> <p>Initializes the InferModel instance, loading the model, metadata,       and setting up the runtime.</p> <code>_read_metadata</code> <p>Reads the model metadata from a JSON file.</p> <code>_annotate</code> <p>Annotates the input image with detection or segmentation results.</p> <code>infer</code> <p>Runs inference on an input image, with optional annotation.</p> <code>benchmark</code> <p>Benchmarks the model's inference performance over a specified        number of iterations and input size.</p>"},{"location":"api/infer_model/#focoos.infer.infer_model.InferModel","title":"<code>InferModel</code>","text":"Source code in <code>focoos/infer/infer_model.py</code> <pre><code>class InferModel:\n    def __init__(\n        self,\n        model_path: Union[str, Path],\n        runtime_type: Optional[RuntimeType] = None,\n        device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n    ):\n        \"\"\"\n        Initialize a LocalModel instance.\n\n        This class sets up a local model for inference by initializing the runtime environment,\n        loading metadata, and preparing annotation utilities.\n\n        Args:\n            model_dir (Union[str, Path]): The path to the directory containing the model files.\n            runtime_type (Optional[RuntimeTypes]): Specifies the runtime type to use for inference.\n                Defaults to the value of `FOCOOS_CONFIG.runtime_type` if not provided.\n\n        Raises:\n            ValueError: If no runtime type is provided and `FOCOOS_CONFIG.runtime_type` is not set.\n            FileNotFoundError: If the specified model directory does not exist.\n\n        Attributes:\n            model_dir (Union[str, Path]): Path to the model directory.\n            metadata (ModelMetadata): Metadata information for the model.\n            model_ref: Reference identifier for the model obtained from metadata.\n            label_annotator (sv.LabelAnnotator): Utility for adding labels to the output,\n                initialized with text padding and border radius.\n            box_annotator (sv.BoxAnnotator): Utility for annotating bounding boxes.\n            mask_annotator (sv.MaskAnnotator): Utility for annotating masks.\n            runtime (ONNXRuntime): Inference runtime initialized with the specified runtime type,\n                model path, metadata, and warmup iterations.\n\n        The method verifies the existence of the model directory, reads the model metadata,\n        and initializes the runtime for inference using the provided runtime type. Annotation\n        utilities are also prepared for visualizing model outputs.\n        \"\"\"\n\n        # Determine runtime type and model format\n        model_extension = pathlib.Path(model_path).suffix\n        runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n        runtime_extension = ModelExtension.from_runtime_type(runtime_type)\n\n        if not model_extension == f\".{runtime_extension.value}\":\n            raise ValueError(\n                f\"Model extension .{model_extension} mismatch with runtime type: {runtime_type} that expects .{runtime_extension.value}\"\n            )\n        self.device: Literal[\"cuda\", \"cpu\"]\n        if device == \"auto\":\n            self.device = get_device_type()\n        elif runtime_type == RuntimeType.ONNX_CPU:\n            self.device = \"cpu\"\n        else:\n            self.device = device\n        # Set model directory and path\n        self.model_path = model_path\n        self.model_dir: Union[str, Path] = os.path.dirname(str(model_path))\n        # self.model_path = os.path.join(model_path, f\"model.{extension.value}\")\n        logger.debug(f\"Runtime type: {runtime_type}, Loading model from {self.model_path}..\")\n\n        # Check if model path exists\n        if not os.path.exists(self.model_path):\n            raise FileNotFoundError(f\"Model path not found: {self.model_path}\")\n\n        # Load metadata and set model reference\n        # self.metadata: RemoteModelInfo = self._read_metadata()\n\n        self.model_info: ModelInfo = self._read_model_info()\n\n        try:\n            from focoos.model_manager import ConfigManager\n\n            model_config = ConfigManager.from_dict(self.model_info.model_family, self.model_info.config)\n            self.processor = ProcessorManager.get_processor(\n                self.model_info.model_family, model_config, self.model_info.im_size\n            ).eval()\n        except Exception as e:\n            logger.error(f\"Error creating model config: {e}\")\n            raise e\n\n        # Initialize annotation utilities\n        self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n        self.box_annotator = sv.BoxAnnotator()\n        self.mask_annotator = sv.MaskAnnotator()\n\n        # Load runtime for inference\n        self.runtime: BaseRuntime = load_runtime(\n            runtime_type=runtime_type,\n            model_path=str(self.model_path),\n            model_info=self.model_info,\n            warmup_iter=FOCOOS_CONFIG.warmup_iter,\n            device=self.device,\n        )\n\n    def _read_model_info(self) -&gt; ModelInfo:\n        \"\"\"\n        Reads the model info from a JSON file.\n        \"\"\"\n        model_info_path = os.path.join(self.model_dir, \"model_info.json\")\n        if not os.path.exists(model_info_path):\n            raise FileNotFoundError(f\"Model info file not found: {model_info_path}\")\n        return ModelInfo.from_json(model_info_path)\n\n    def __call__(\n        self, image: Union[bytes, str, Path, np.ndarray, Image.Image], threshold: float = 0.5, annotate: bool = False\n    ) -&gt; FocoosDetections:\n        return self.infer(image, threshold, annotate)\n\n    def infer(\n        self,\n        image: Union[bytes, str, Path, np.ndarray, Image.Image],\n        threshold: float = 0.5,\n        annotate: bool = False,\n    ) -&gt; FocoosDetections:\n        \"\"\"\n        Perform inference on an input image and optionally return an annotated result.\n\n        This method processes the input image, runs it through the model, and returns the detections.\n        Optionally, it can also annotate the image with the detection results.\n\n        Args:\n            image: The input image to run inference on. Accepts a file path, bytes, PIL Image, or numpy array.\n            threshold: Minimum confidence score for a detection to be included in the results. Default is 0.5.\n            annotate: If True, annotate the image with detection results and include it in the output.\n\n        Returns:\n            FocoosDetections: An object containing the detection results, optional annotated image, and latency metrics.\n\n        Raises:\n            AssertionError: If the model runtime is not initialized.\n\n        Usage:\n            This method is intended for users who want to obtain detection results from a local model,\n            with optional annotation for visualization or further processing.\n        \"\"\"\n        assert self.runtime is not None, \"Model is not deployed (locally)\"\n\n        t0 = perf_counter()\n        im = image_loader(image)\n        t1 = perf_counter()\n        tensors, _ = self.processor.preprocess(inputs=im, device=self.device)\n        # logger.debug(f\"Input image size: {im.shape}\")\n        t2 = perf_counter()\n\n        raw_detections = self.runtime(tensors)\n\n        t3 = perf_counter()\n        detections = self.processor.export_postprocess(\n            raw_detections, im, threshold=threshold, class_names=self.model_info.classes\n        )\n        t4 = perf_counter()\n        if annotate:\n            skeleton = self.model_info.config.get(\"skeleton\", None)\n            detections[0].image = annotate_image(\n                im,\n                detections[0],\n                task=self.model_info.task,\n                classes=self.model_info.classes,\n                keypoints_skeleton=skeleton,\n            )\n        t5 = perf_counter()\n\n        res = detections[0]  #!TODO  check for batching\n        res.latency = InferLatency(\n            imload=round(t1 - t0, 3),\n            preprocess=round(t2 - t1, 3),\n            inference=round(t3 - t2, 3),\n            postprocess=round(t4 - t3, 3),\n            annotate=round(t5 - t4, 3) if annotate else None,\n        )\n\n        res.infer_print()\n        return res\n\n    def benchmark(self, iterations: int = 50, size: Optional[Union[int, Tuple[int, int]]] = None) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model's inference performance over multiple iterations.\n        \"\"\"\n        if size is None:\n            size = self.model_info.im_size\n        if isinstance(size, int):\n            size = (size, size)\n        return self.runtime.benchmark(iterations, size)\n\n    def end2end_benchmark(\n        self, iterations: int = 50, size: Optional[Union[int, Tuple[int, int]]] = None\n    ) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model's inference performance over multiple iterations.\n\n        Args:\n            iterations (int): Number of iterations to run for benchmarking.\n            size (int): The input size for each benchmark iteration.\n\n        Returns:\n            LatencyMetrics: Latency metrics including time taken for inference.\n\n        Example:\n            ```python\n            from focoos import Focoos, LocalModel\n\n            focoos = Focoos()\n            model = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\n            metrics = model.end2end_benchmark(iterations=10, size=640)\n\n            # Access latency metrics\n            print(f\"FPS: {metrics.fps}\")\n            print(f\"Mean latency: {metrics.mean} ms\")\n            print(f\"Engine: {metrics.engine}\")\n            print(f\"Device: {metrics.device}\")\n            print(f\"Input size: {metrics.im_size}x{metrics.im_size}\")\n            ```\n        \"\"\"\n        if size is None:\n            size = self.model_info.im_size\n        if isinstance(size, int):\n            size = (size, size)\n\n        device = get_device_name()\n        if self.runtime.__class__.__name__ == \"ONNXRuntime\":\n            active_provider = self.runtime.active_provider or \"cpu\"  # type: ignore\n            engine = f\"onnx.{active_provider}\"\n            if active_provider in [\"CPUExecutionProvider\"]:\n                device = get_cpu_name()\n        else:\n            engine = \"torchscript\"\n            device = get_device_name()\n        logger.info(f\"\u23f1\ufe0f Benchmarking End-to-End latency on {device}, size: {size}x{size}..\")\n\n        np_input = (255 * np.random.random((size[0], size[1], 3))).astype(np.uint8)\n\n        durations = []\n        for step in range(iterations + 5):\n            start = perf_counter()\n            self.infer(np_input)\n            end = perf_counter()\n\n            if step &gt;= 5:  # Skip first 5 iterations\n                durations.append((end - start) * 1000)\n\n        durations = np.array(durations)\n\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean()),\n            engine=engine,\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n            device=device,\n        )\n        logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n</code></pre>"},{"location":"api/infer_model/#focoos.infer.infer_model.InferModel.__init__","title":"<code>__init__(model_path, runtime_type=None, device='auto')</code>","text":"<p>Initialize a LocalModel instance.</p> <p>This class sets up a local model for inference by initializing the runtime environment, loading metadata, and preparing annotation utilities.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Union[str, Path]</code> <p>The path to the directory containing the model files.</p> required <code>runtime_type</code> <code>Optional[RuntimeTypes]</code> <p>Specifies the runtime type to use for inference. Defaults to the value of <code>FOCOOS_CONFIG.runtime_type</code> if not provided.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no runtime type is provided and <code>FOCOOS_CONFIG.runtime_type</code> is not set.</p> <code>FileNotFoundError</code> <p>If the specified model directory does not exist.</p> <p>Attributes:</p> Name Type Description <code>model_dir</code> <code>Union[str, Path]</code> <p>Path to the model directory.</p> <code>metadata</code> <code>ModelMetadata</code> <p>Metadata information for the model.</p> <code>model_ref</code> <code>ModelMetadata</code> <p>Reference identifier for the model obtained from metadata.</p> <code>label_annotator</code> <code>LabelAnnotator</code> <p>Utility for adding labels to the output, initialized with text padding and border radius.</p> <code>box_annotator</code> <code>BoxAnnotator</code> <p>Utility for annotating bounding boxes.</p> <code>mask_annotator</code> <code>MaskAnnotator</code> <p>Utility for annotating masks.</p> <code>runtime</code> <code>ONNXRuntime</code> <p>Inference runtime initialized with the specified runtime type, model path, metadata, and warmup iterations.</p> <p>The method verifies the existence of the model directory, reads the model metadata, and initializes the runtime for inference using the provided runtime type. Annotation utilities are also prepared for visualizing model outputs.</p> Source code in <code>focoos/infer/infer_model.py</code> <pre><code>def __init__(\n    self,\n    model_path: Union[str, Path],\n    runtime_type: Optional[RuntimeType] = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n):\n    \"\"\"\n    Initialize a LocalModel instance.\n\n    This class sets up a local model for inference by initializing the runtime environment,\n    loading metadata, and preparing annotation utilities.\n\n    Args:\n        model_dir (Union[str, Path]): The path to the directory containing the model files.\n        runtime_type (Optional[RuntimeTypes]): Specifies the runtime type to use for inference.\n            Defaults to the value of `FOCOOS_CONFIG.runtime_type` if not provided.\n\n    Raises:\n        ValueError: If no runtime type is provided and `FOCOOS_CONFIG.runtime_type` is not set.\n        FileNotFoundError: If the specified model directory does not exist.\n\n    Attributes:\n        model_dir (Union[str, Path]): Path to the model directory.\n        metadata (ModelMetadata): Metadata information for the model.\n        model_ref: Reference identifier for the model obtained from metadata.\n        label_annotator (sv.LabelAnnotator): Utility for adding labels to the output,\n            initialized with text padding and border radius.\n        box_annotator (sv.BoxAnnotator): Utility for annotating bounding boxes.\n        mask_annotator (sv.MaskAnnotator): Utility for annotating masks.\n        runtime (ONNXRuntime): Inference runtime initialized with the specified runtime type,\n            model path, metadata, and warmup iterations.\n\n    The method verifies the existence of the model directory, reads the model metadata,\n    and initializes the runtime for inference using the provided runtime type. Annotation\n    utilities are also prepared for visualizing model outputs.\n    \"\"\"\n\n    # Determine runtime type and model format\n    model_extension = pathlib.Path(model_path).suffix\n    runtime_type = runtime_type or FOCOOS_CONFIG.runtime_type\n    runtime_extension = ModelExtension.from_runtime_type(runtime_type)\n\n    if not model_extension == f\".{runtime_extension.value}\":\n        raise ValueError(\n            f\"Model extension .{model_extension} mismatch with runtime type: {runtime_type} that expects .{runtime_extension.value}\"\n        )\n    self.device: Literal[\"cuda\", \"cpu\"]\n    if device == \"auto\":\n        self.device = get_device_type()\n    elif runtime_type == RuntimeType.ONNX_CPU:\n        self.device = \"cpu\"\n    else:\n        self.device = device\n    # Set model directory and path\n    self.model_path = model_path\n    self.model_dir: Union[str, Path] = os.path.dirname(str(model_path))\n    # self.model_path = os.path.join(model_path, f\"model.{extension.value}\")\n    logger.debug(f\"Runtime type: {runtime_type}, Loading model from {self.model_path}..\")\n\n    # Check if model path exists\n    if not os.path.exists(self.model_path):\n        raise FileNotFoundError(f\"Model path not found: {self.model_path}\")\n\n    # Load metadata and set model reference\n    # self.metadata: RemoteModelInfo = self._read_metadata()\n\n    self.model_info: ModelInfo = self._read_model_info()\n\n    try:\n        from focoos.model_manager import ConfigManager\n\n        model_config = ConfigManager.from_dict(self.model_info.model_family, self.model_info.config)\n        self.processor = ProcessorManager.get_processor(\n            self.model_info.model_family, model_config, self.model_info.im_size\n        ).eval()\n    except Exception as e:\n        logger.error(f\"Error creating model config: {e}\")\n        raise e\n\n    # Initialize annotation utilities\n    self.label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n    self.box_annotator = sv.BoxAnnotator()\n    self.mask_annotator = sv.MaskAnnotator()\n\n    # Load runtime for inference\n    self.runtime: BaseRuntime = load_runtime(\n        runtime_type=runtime_type,\n        model_path=str(self.model_path),\n        model_info=self.model_info,\n        warmup_iter=FOCOOS_CONFIG.warmup_iter,\n        device=self.device,\n    )\n</code></pre>"},{"location":"api/infer_model/#focoos.infer.infer_model.InferModel.benchmark","title":"<code>benchmark(iterations=50, size=None)</code>","text":"<p>Benchmark the model's inference performance over multiple iterations.</p> Source code in <code>focoos/infer/infer_model.py</code> <pre><code>def benchmark(self, iterations: int = 50, size: Optional[Union[int, Tuple[int, int]]] = None) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model's inference performance over multiple iterations.\n    \"\"\"\n    if size is None:\n        size = self.model_info.im_size\n    if isinstance(size, int):\n        size = (size, size)\n    return self.runtime.benchmark(iterations, size)\n</code></pre>"},{"location":"api/infer_model/#focoos.infer.infer_model.InferModel.end2end_benchmark","title":"<code>end2end_benchmark(iterations=50, size=None)</code>","text":"<p>Benchmark the model's inference performance over multiple iterations.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of iterations to run for benchmarking.</p> <code>50</code> <code>size</code> <code>int</code> <p>The input size for each benchmark iteration.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Latency metrics including time taken for inference.</p> Example <pre><code>from focoos import Focoos, LocalModel\n\nfocoos = Focoos()\nmodel = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\nmetrics = model.end2end_benchmark(iterations=10, size=640)\n\n# Access latency metrics\nprint(f\"FPS: {metrics.fps}\")\nprint(f\"Mean latency: {metrics.mean} ms\")\nprint(f\"Engine: {metrics.engine}\")\nprint(f\"Device: {metrics.device}\")\nprint(f\"Input size: {metrics.im_size}x{metrics.im_size}\")\n</code></pre> Source code in <code>focoos/infer/infer_model.py</code> <pre><code>def end2end_benchmark(\n    self, iterations: int = 50, size: Optional[Union[int, Tuple[int, int]]] = None\n) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model's inference performance over multiple iterations.\n\n    Args:\n        iterations (int): Number of iterations to run for benchmarking.\n        size (int): The input size for each benchmark iteration.\n\n    Returns:\n        LatencyMetrics: Latency metrics including time taken for inference.\n\n    Example:\n        ```python\n        from focoos import Focoos, LocalModel\n\n        focoos = Focoos()\n        model = focoos.get_local_model(model_ref=\"&lt;model_ref&gt;\")\n        metrics = model.end2end_benchmark(iterations=10, size=640)\n\n        # Access latency metrics\n        print(f\"FPS: {metrics.fps}\")\n        print(f\"Mean latency: {metrics.mean} ms\")\n        print(f\"Engine: {metrics.engine}\")\n        print(f\"Device: {metrics.device}\")\n        print(f\"Input size: {metrics.im_size}x{metrics.im_size}\")\n        ```\n    \"\"\"\n    if size is None:\n        size = self.model_info.im_size\n    if isinstance(size, int):\n        size = (size, size)\n\n    device = get_device_name()\n    if self.runtime.__class__.__name__ == \"ONNXRuntime\":\n        active_provider = self.runtime.active_provider or \"cpu\"  # type: ignore\n        engine = f\"onnx.{active_provider}\"\n        if active_provider in [\"CPUExecutionProvider\"]:\n            device = get_cpu_name()\n    else:\n        engine = \"torchscript\"\n        device = get_device_name()\n    logger.info(f\"\u23f1\ufe0f Benchmarking End-to-End latency on {device}, size: {size}x{size}..\")\n\n    np_input = (255 * np.random.random((size[0], size[1], 3))).astype(np.uint8)\n\n    durations = []\n    for step in range(iterations + 5):\n        start = perf_counter()\n        self.infer(np_input)\n        end = perf_counter()\n\n        if step &gt;= 5:  # Skip first 5 iterations\n            durations.append((end - start) * 1000)\n\n    durations = np.array(durations)\n\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean()),\n        engine=engine,\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n        device=device,\n    )\n    logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/infer_model/#focoos.infer.infer_model.InferModel.infer","title":"<code>infer(image, threshold=0.5, annotate=False)</code>","text":"<p>Perform inference on an input image and optionally return an annotated result.</p> <p>This method processes the input image, runs it through the model, and returns the detections. Optionally, it can also annotate the image with the detection results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[bytes, str, Path, ndarray, Image]</code> <p>The input image to run inference on. Accepts a file path, bytes, PIL Image, or numpy array.</p> required <code>threshold</code> <code>float</code> <p>Minimum confidence score for a detection to be included in the results. Default is 0.5.</p> <code>0.5</code> <code>annotate</code> <code>bool</code> <p>If True, annotate the image with detection results and include it in the output.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>FocoosDetections</code> <code>FocoosDetections</code> <p>An object containing the detection results, optional annotated image, and latency metrics.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the model runtime is not initialized.</p> Usage <p>This method is intended for users who want to obtain detection results from a local model, with optional annotation for visualization or further processing.</p> Source code in <code>focoos/infer/infer_model.py</code> <pre><code>def infer(\n    self,\n    image: Union[bytes, str, Path, np.ndarray, Image.Image],\n    threshold: float = 0.5,\n    annotate: bool = False,\n) -&gt; FocoosDetections:\n    \"\"\"\n    Perform inference on an input image and optionally return an annotated result.\n\n    This method processes the input image, runs it through the model, and returns the detections.\n    Optionally, it can also annotate the image with the detection results.\n\n    Args:\n        image: The input image to run inference on. Accepts a file path, bytes, PIL Image, or numpy array.\n        threshold: Minimum confidence score for a detection to be included in the results. Default is 0.5.\n        annotate: If True, annotate the image with detection results and include it in the output.\n\n    Returns:\n        FocoosDetections: An object containing the detection results, optional annotated image, and latency metrics.\n\n    Raises:\n        AssertionError: If the model runtime is not initialized.\n\n    Usage:\n        This method is intended for users who want to obtain detection results from a local model,\n        with optional annotation for visualization or further processing.\n    \"\"\"\n    assert self.runtime is not None, \"Model is not deployed (locally)\"\n\n    t0 = perf_counter()\n    im = image_loader(image)\n    t1 = perf_counter()\n    tensors, _ = self.processor.preprocess(inputs=im, device=self.device)\n    # logger.debug(f\"Input image size: {im.shape}\")\n    t2 = perf_counter()\n\n    raw_detections = self.runtime(tensors)\n\n    t3 = perf_counter()\n    detections = self.processor.export_postprocess(\n        raw_detections, im, threshold=threshold, class_names=self.model_info.classes\n    )\n    t4 = perf_counter()\n    if annotate:\n        skeleton = self.model_info.config.get(\"skeleton\", None)\n        detections[0].image = annotate_image(\n            im,\n            detections[0],\n            task=self.model_info.task,\n            classes=self.model_info.classes,\n            keypoints_skeleton=skeleton,\n        )\n    t5 = perf_counter()\n\n    res = detections[0]  #!TODO  check for batching\n    res.latency = InferLatency(\n        imload=round(t1 - t0, 3),\n        preprocess=round(t2 - t1, 3),\n        inference=round(t3 - t2, 3),\n        postprocess=round(t4 - t3, 3),\n        annotate=round(t5 - t4, 3) if annotate else None,\n    )\n\n    res.infer_print()\n    return res\n</code></pre>"},{"location":"api/model_manager/","title":"ModelManager","text":""},{"location":"api/model_manager/#focoos.model_manager.BackboneManager","title":"<code>BackboneManager</code>","text":"<p>Automatic backbone manager with lazy loading.</p> <p>The BackboneManager provides a unified interface for loading neural network backbones (feature extractors) from their configurations. It supports multiple backbone architectures like ResNet, STDC, Swin Transformer, MobileNetV2, and others.</p> <p>The manager maintains a mapping between backbone type names and their implementation paths, and handles the dynamic loading of the appropriate classes.</p> Source code in <code>focoos/model_manager.py</code> <pre><code>class BackboneManager:\n    \"\"\"\n    Automatic backbone manager with lazy loading.\n\n    The BackboneManager provides a unified interface for loading neural network backbones\n    (feature extractors) from their configurations. It supports multiple backbone architectures\n    like ResNet, STDC, Swin Transformer, MobileNetV2, and others.\n\n    The manager maintains a mapping between backbone type names and their implementation paths,\n    and handles the dynamic loading of the appropriate classes.\n    \"\"\"\n\n    _BACKBONE_MAPPING: Dict[str, str] = {\n        \"resnet\": \"resnet.ResNet\",\n        \"stdc\": \"stdc.STDC\",\n        \"swin\": \"swin.Swin\",\n        \"mobilenet_v2\": \"mobilenet_v2.MobileNetV2\",\n        \"mit\": \"mit.MIT\",\n        \"convnextv2\": \"convnextv2.ConvNeXtV2\",\n        \"csp_darknet\": \"csp_darknet.CSPDarknet\",\n    }\n\n    @classmethod\n    def from_config(cls, config: BackboneConfig) -&gt; BaseBackbone:\n        \"\"\"\n        Load a backbone from a configuration.\n\n        This method instantiates a backbone model based on the provided configuration,\n        dynamically loading the appropriate backbone class based on the model_type.\n\n        Args:\n            config: The backbone configuration containing model_type and other parameters\n\n        Returns:\n            BaseBackbone: The instantiated backbone model\n\n        Raises:\n            ValueError: If the backbone type is not supported\n        \"\"\"\n        if config.model_type not in cls._BACKBONE_MAPPING:\n            raise ValueError(f\"Backbone {config.model_type} not supported\")\n        backbone_class = cls.get_model_class(config.model_type)\n        return backbone_class(config)\n\n    @classmethod\n    def get_model_class(cls, model_type: str):\n        \"\"\"\n        Get the model class based on the model type.\n\n        This method dynamically imports and returns the backbone class\n        corresponding to the specified model type.\n\n        Args:\n            model_type: The type of backbone model to load (e.g., \"resnet\", \"swin\")\n\n        Returns:\n            Type[BaseBackbone]: The backbone class\n\n        Raises:\n            ImportError: If the module cannot be imported\n            AttributeError: If the class is not found in the module\n        \"\"\"\n        import importlib\n\n        module_path, class_name = cls._BACKBONE_MAPPING[model_type].split(\".\")\n        module = importlib.import_module(f\".{module_path}\", package=\"focoos.nn.backbone\")\n        return getattr(module, class_name)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.BackboneManager.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Load a backbone from a configuration.</p> <p>This method instantiates a backbone model based on the provided configuration, dynamically loading the appropriate backbone class based on the model_type.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BackboneConfig</code> <p>The backbone configuration containing model_type and other parameters</p> required <p>Returns:</p> Name Type Description <code>BaseBackbone</code> <code>BaseBackbone</code> <p>The instantiated backbone model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the backbone type is not supported</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef from_config(cls, config: BackboneConfig) -&gt; BaseBackbone:\n    \"\"\"\n    Load a backbone from a configuration.\n\n    This method instantiates a backbone model based on the provided configuration,\n    dynamically loading the appropriate backbone class based on the model_type.\n\n    Args:\n        config: The backbone configuration containing model_type and other parameters\n\n    Returns:\n        BaseBackbone: The instantiated backbone model\n\n    Raises:\n        ValueError: If the backbone type is not supported\n    \"\"\"\n    if config.model_type not in cls._BACKBONE_MAPPING:\n        raise ValueError(f\"Backbone {config.model_type} not supported\")\n    backbone_class = cls.get_model_class(config.model_type)\n    return backbone_class(config)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.BackboneManager.get_model_class","title":"<code>get_model_class(model_type)</code>  <code>classmethod</code>","text":"<p>Get the model class based on the model type.</p> <p>This method dynamically imports and returns the backbone class corresponding to the specified model type.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The type of backbone model to load (e.g., \"resnet\", \"swin\")</p> required <p>Returns:</p> Type Description <p>Type[BaseBackbone]: The backbone class</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module cannot be imported</p> <code>AttributeError</code> <p>If the class is not found in the module</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef get_model_class(cls, model_type: str):\n    \"\"\"\n    Get the model class based on the model type.\n\n    This method dynamically imports and returns the backbone class\n    corresponding to the specified model type.\n\n    Args:\n        model_type: The type of backbone model to load (e.g., \"resnet\", \"swin\")\n\n    Returns:\n        Type[BaseBackbone]: The backbone class\n\n    Raises:\n        ImportError: If the module cannot be imported\n        AttributeError: If the class is not found in the module\n    \"\"\"\n    import importlib\n\n    module_path, class_name = cls._BACKBONE_MAPPING[model_type].split(\".\")\n    module = importlib.import_module(f\".{module_path}\", package=\"focoos.nn.backbone\")\n    return getattr(module, class_name)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigBackboneManager","title":"<code>ConfigBackboneManager</code>","text":"<p>Automatic backbone configuration manager with lazy loading.</p> <p>The ConfigBackboneManager provides a specialized manager for handling backbone configurations. It maintains a mapping between backbone type names and their configuration classes, and handles the dynamic loading of these classes.</p> <p>This manager is used primarily by the ConfigManager when processing nested backbone configurations within model configurations.</p> Source code in <code>focoos/model_manager.py</code> <pre><code>class ConfigBackboneManager:\n    \"\"\"\n    Automatic backbone configuration manager with lazy loading.\n\n    The ConfigBackboneManager provides a specialized manager for handling backbone\n    configurations. It maintains a mapping between backbone type names and their\n    configuration classes, and handles the dynamic loading of these classes.\n\n    This manager is used primarily by the ConfigManager when processing nested\n    backbone configurations within model configurations.\n    \"\"\"\n\n    _BACKBONE_MAPPING: Dict[str, str] = {\n        \"resnet\": \"resnet.ResnetConfig\",\n        \"stdc\": \"stdc.STDCConfig\",\n        \"swin\": \"swin.SwinConfig\",\n        \"mobilenet_v2\": \"mobilenet_v2.MobileNetV2Config\",\n        \"mit\": \"mit.MITConfig\",\n        \"convnextv2\": \"convnextv2.ConvNeXtV2Config\",\n        \"csp_darknet\": \"csp_darknet.CSPConfig\",\n    }\n\n    @classmethod\n    def get_model_class(cls, model_type: str):\n        \"\"\"\n        Get the configuration class based on the model type.\n\n        This method dynamically imports and returns the backbone configuration class\n        corresponding to the specified model type.\n\n        Args:\n            model_type: The type of backbone model (e.g., \"resnet\", \"swin\")\n\n        Returns:\n            Type[BackboneConfig]: The backbone configuration class\n\n        Raises:\n            ImportError: If the module cannot be imported\n            AttributeError: If the class is not found in the module\n        \"\"\"\n        import importlib\n\n        module_path, class_name = cls._BACKBONE_MAPPING[model_type].split(\".\")\n        module = importlib.import_module(f\".{module_path}\", package=\"focoos.nn.backbone\")\n        return getattr(module, class_name)\n\n    @classmethod\n    def from_dict(cls, config_dict: dict) -&gt; BackboneConfig:\n        \"\"\"\n        Create a backbone configuration from a dictionary.\n\n        This method instantiates a backbone configuration object based on the\n        model_type specified in the configuration dictionary.\n\n        Args:\n            config_dict: Dictionary containing configuration parameters including model_type\n\n        Returns:\n            BackboneConfig: The instantiated backbone configuration object\n\n        Raises:\n            ValueError: If the backbone type is not supported\n        \"\"\"\n        if config_dict[\"model_type\"] not in cls._BACKBONE_MAPPING:\n            raise ValueError(f\"Backbone {config_dict['model_type']} not supported\")\n\n        config_class = cls.get_model_class(config_dict[\"model_type\"])\n        return_config = config_class(**config_dict)\n        return return_config\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigBackboneManager.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create a backbone configuration from a dictionary.</p> <p>This method instantiates a backbone configuration object based on the model_type specified in the configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>Dictionary containing configuration parameters including model_type</p> required <p>Returns:</p> Name Type Description <code>BackboneConfig</code> <code>BackboneConfig</code> <p>The instantiated backbone configuration object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the backbone type is not supported</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict) -&gt; BackboneConfig:\n    \"\"\"\n    Create a backbone configuration from a dictionary.\n\n    This method instantiates a backbone configuration object based on the\n    model_type specified in the configuration dictionary.\n\n    Args:\n        config_dict: Dictionary containing configuration parameters including model_type\n\n    Returns:\n        BackboneConfig: The instantiated backbone configuration object\n\n    Raises:\n        ValueError: If the backbone type is not supported\n    \"\"\"\n    if config_dict[\"model_type\"] not in cls._BACKBONE_MAPPING:\n        raise ValueError(f\"Backbone {config_dict['model_type']} not supported\")\n\n    config_class = cls.get_model_class(config_dict[\"model_type\"])\n    return_config = config_class(**config_dict)\n    return return_config\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigBackboneManager.get_model_class","title":"<code>get_model_class(model_type)</code>  <code>classmethod</code>","text":"<p>Get the configuration class based on the model type.</p> <p>This method dynamically imports and returns the backbone configuration class corresponding to the specified model type.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The type of backbone model (e.g., \"resnet\", \"swin\")</p> required <p>Returns:</p> Type Description <p>Type[BackboneConfig]: The backbone configuration class</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module cannot be imported</p> <code>AttributeError</code> <p>If the class is not found in the module</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef get_model_class(cls, model_type: str):\n    \"\"\"\n    Get the configuration class based on the model type.\n\n    This method dynamically imports and returns the backbone configuration class\n    corresponding to the specified model type.\n\n    Args:\n        model_type: The type of backbone model (e.g., \"resnet\", \"swin\")\n\n    Returns:\n        Type[BackboneConfig]: The backbone configuration class\n\n    Raises:\n        ImportError: If the module cannot be imported\n        AttributeError: If the class is not found in the module\n    \"\"\"\n    import importlib\n\n    module_path, class_name = cls._BACKBONE_MAPPING[model_type].split(\".\")\n    module = importlib.import_module(f\".{module_path}\", package=\"focoos.nn.backbone\")\n    return getattr(module, class_name)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigManager","title":"<code>ConfigManager</code>","text":"<p>Automatic model configuration management.</p> <p>The ConfigManager provides a centralized system for managing model configurations. It maintains a registry of configuration classes for different model families and handles the creation of appropriate configuration objects from dictionaries.</p> <p>The manager supports dynamic registration of configuration classes and automatic importing of model family modules as needed.</p> Source code in <code>focoos/model_manager.py</code> <pre><code>class ConfigManager:\n    \"\"\"\n    Automatic model configuration management.\n\n    The ConfigManager provides a centralized system for managing model configurations.\n    It maintains a registry of configuration classes for different model families and\n    handles the creation of appropriate configuration objects from dictionaries.\n\n    The manager supports dynamic registration of configuration classes and automatic\n    importing of model family modules as needed.\n    \"\"\"\n\n    _MODEL_CFG_MAPPING: Dict[str, Callable[[], Type[ModelConfig]]] = {}\n\n    @classmethod\n    def register_config(cls, model_family: ModelFamily, model_config_loader: Callable[[], Type[ModelConfig]]):\n        \"\"\"\n        Register a loader for a specific model configuration.\n\n        This method associates a model family with a loader function that returns\n        the configuration class when called. This enables lazy loading of configuration\n        classes.\n\n        Args:\n            model_family: The ModelFamily enum value to register\n            model_config_loader: A callable that returns the configuration class when invoked\n        \"\"\"\n        cls._MODEL_CFG_MAPPING[model_family.value] = model_config_loader\n\n    @classmethod\n    def from_dict(cls, model_family: ModelFamily, config_dict: dict, **kwargs) -&gt; ModelConfig:\n        \"\"\"\n        Create a configuration from a dictionary.\n\n        This method instantiates a model configuration object based on the model family\n        and the provided configuration dictionary. It handles nested configurations\n        like backbone_config and validates the parameters.\n\n        Args:\n            model_family: The model family enum value\n            config_dict: Dictionary containing configuration parameters\n            **kwargs: Additional keyword arguments to override configuration values\n\n        Returns:\n            ModelConfig: The instantiated configuration object\n\n        Raises:\n            ValueError: If the model family is not supported or if invalid parameters are provided\n        \"\"\"\n        if model_family.value not in cls._MODEL_CFG_MAPPING:\n            # Import the family module\n            family_module = importlib.import_module(f\"focoos.models.{model_family.value}\")\n\n            # Iteratively register all models in the family\n            for attr_name in dir(family_module):\n                if attr_name.startswith(\"_register\"):\n                    register_func = getattr(family_module, attr_name)\n                    if callable(register_func):\n                        register_func()\n\n        if model_family.value not in cls._MODEL_CFG_MAPPING:\n            raise ValueError(f\"Model {model_family} not supported\")\n\n        config_class = cls._MODEL_CFG_MAPPING[model_family.value]()  # this return the config class\n\n        # Convert the input dict to the actual config type\n        if \"backbone_config\" in config_dict and config_dict[\"backbone_config\"] is not None:\n            config_dict[\"backbone_config\"] = ConfigBackboneManager.from_dict(config_dict[\"backbone_config\"])\n\n            # Validate the parameters kwargs\n        valid_fields = {f.name for f in fields(config_class)}\n        invalid_kwargs = set(kwargs.keys()) - valid_fields\n        if invalid_kwargs:\n            raise ValueError(\n                f\"Invalid parameters for {config_class.__name__}: {invalid_kwargs}\\nValid parameters: {valid_fields}\"\n            )\n\n        config_dict = config_class(**config_dict)\n\n        # Update the config with the kwargs\n        if kwargs:\n            config_dict.update(kwargs)\n\n        return config_dict\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigManager.from_dict","title":"<code>from_dict(model_family, config_dict, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a configuration from a dictionary.</p> <p>This method instantiates a model configuration object based on the model family and the provided configuration dictionary. It handles nested configurations like backbone_config and validates the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model_family</code> <code>ModelFamily</code> <p>The model family enum value</p> required <code>config_dict</code> <code>dict</code> <p>Dictionary containing configuration parameters</p> required <code>**kwargs</code> <p>Additional keyword arguments to override configuration values</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ModelConfig</code> <code>ModelConfig</code> <p>The instantiated configuration object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model family is not supported or if invalid parameters are provided</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef from_dict(cls, model_family: ModelFamily, config_dict: dict, **kwargs) -&gt; ModelConfig:\n    \"\"\"\n    Create a configuration from a dictionary.\n\n    This method instantiates a model configuration object based on the model family\n    and the provided configuration dictionary. It handles nested configurations\n    like backbone_config and validates the parameters.\n\n    Args:\n        model_family: The model family enum value\n        config_dict: Dictionary containing configuration parameters\n        **kwargs: Additional keyword arguments to override configuration values\n\n    Returns:\n        ModelConfig: The instantiated configuration object\n\n    Raises:\n        ValueError: If the model family is not supported or if invalid parameters are provided\n    \"\"\"\n    if model_family.value not in cls._MODEL_CFG_MAPPING:\n        # Import the family module\n        family_module = importlib.import_module(f\"focoos.models.{model_family.value}\")\n\n        # Iteratively register all models in the family\n        for attr_name in dir(family_module):\n            if attr_name.startswith(\"_register\"):\n                register_func = getattr(family_module, attr_name)\n                if callable(register_func):\n                    register_func()\n\n    if model_family.value not in cls._MODEL_CFG_MAPPING:\n        raise ValueError(f\"Model {model_family} not supported\")\n\n    config_class = cls._MODEL_CFG_MAPPING[model_family.value]()  # this return the config class\n\n    # Convert the input dict to the actual config type\n    if \"backbone_config\" in config_dict and config_dict[\"backbone_config\"] is not None:\n        config_dict[\"backbone_config\"] = ConfigBackboneManager.from_dict(config_dict[\"backbone_config\"])\n\n        # Validate the parameters kwargs\n    valid_fields = {f.name for f in fields(config_class)}\n    invalid_kwargs = set(kwargs.keys()) - valid_fields\n    if invalid_kwargs:\n        raise ValueError(\n            f\"Invalid parameters for {config_class.__name__}: {invalid_kwargs}\\nValid parameters: {valid_fields}\"\n        )\n\n    config_dict = config_class(**config_dict)\n\n    # Update the config with the kwargs\n    if kwargs:\n        config_dict.update(kwargs)\n\n    return config_dict\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ConfigManager.register_config","title":"<code>register_config(model_family, model_config_loader)</code>  <code>classmethod</code>","text":"<p>Register a loader for a specific model configuration.</p> <p>This method associates a model family with a loader function that returns the configuration class when called. This enables lazy loading of configuration classes.</p> <p>Parameters:</p> Name Type Description Default <code>model_family</code> <code>ModelFamily</code> <p>The ModelFamily enum value to register</p> required <code>model_config_loader</code> <code>Callable[[], Type[ModelConfig]]</code> <p>A callable that returns the configuration class when invoked</p> required Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef register_config(cls, model_family: ModelFamily, model_config_loader: Callable[[], Type[ModelConfig]]):\n    \"\"\"\n    Register a loader for a specific model configuration.\n\n    This method associates a model family with a loader function that returns\n    the configuration class when called. This enables lazy loading of configuration\n    classes.\n\n    Args:\n        model_family: The ModelFamily enum value to register\n        model_config_loader: A callable that returns the configuration class when invoked\n    \"\"\"\n    cls._MODEL_CFG_MAPPING[model_family.value] = model_config_loader\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ModelManager","title":"<code>ModelManager</code>","text":"<p>Automatic model manager with lazy loading.</p> <p>The ModelManager provides a unified interface for loading models from various sources: - From ModelInfo objects - From the Focoos Hub (hub:// protocol) - From local directories - From the model registry</p> <p>It handles model registration, configuration management, and weights loading automatically. Models are loaded lazily when requested and can be accessed through the <code>get</code> method.</p> <p>Examples:</p> <p>Load a registered model:</p> <pre><code>&gt;&gt;&gt; model = ModelManager.get(\"model_name\")\n</code></pre> <p>Load a model from hub:</p> <pre><code>&gt;&gt;&gt; model = ModelManager.get(\"hub://username/model_ref\")\n</code></pre> <p>Load a model with custom config:</p> <pre><code>&gt;&gt;&gt; model = ModelManager.get(\"model_name\", config=custom_config)\n</code></pre> Source code in <code>focoos/model_manager.py</code> <pre><code>class ModelManager:\n    \"\"\"Automatic model manager with lazy loading.\n\n    The ModelManager provides a unified interface for loading models from various sources:\n    - From ModelInfo objects\n    - From the Focoos Hub (hub:// protocol)\n    - From local directories\n    - From the model registry\n\n    It handles model registration, configuration management, and weights loading automatically.\n    Models are loaded lazily when requested and can be accessed through the `get` method.\n\n    Examples:\n        Load a registered model:\n        &gt;&gt;&gt; model = ModelManager.get(\"model_name\")\n\n        Load a model from hub:\n        &gt;&gt;&gt; model = ModelManager.get(\"hub://username/model_ref\")\n\n        Load a model with custom config:\n        &gt;&gt;&gt; model = ModelManager.get(\"model_name\", config=custom_config)\n    \"\"\"\n\n    _models_family_map: Dict[str, Callable[[], Type[BaseModelNN]]] = {}  # {\"fai-detr\": load_fai_detr()}\n\n    @classmethod\n    def get(\n        cls,\n        name: Union[str, Path],\n        model_info: Optional[ModelInfo] = None,\n        config: Optional[ModelConfig] = None,\n        hub: Optional[FocoosHUB] = None,\n        cache: bool = True,\n        **kwargs,\n    ) -&gt; FocoosModel:\n        \"\"\"\n        Unified entrypoint to load a model by name or ModelInfo.\n\n        This method provides a single interface for loading models from various sources:\n        - From a ModelInfo object (when model_info is provided)\n        - From the Focoos Hub (when name starts with \"hub://\")\n        - From the ModelRegistry (for pretrained models)\n        - From a local directory (when name is a local path)\n\n        Args:\n            name: Model name, path, or hub reference (e.g., \"hub://username/model_ref\")\n            model_info: Optional ModelInfo object to load the model from directly\n            config: Optional custom model configuration to override defaults\n            hub: Optional FocoosHUB instance to use for hub:// references\n            cache: Optional boolean to cache the model info and weights when loading from hub (defaults to True)\n            **kwargs: Additional keyword arguments passed to the model configuration\n\n        Returns:\n            FocoosModel: The loaded model instance\n\n        Raises:\n            ValueError: If the model cannot be found or loaded\n        \"\"\"\n        if model_info is not None:\n            # Load model directly from provided ModelInfo\n            return cls._from_model_info(model_info=model_info, config=config, **kwargs)\n\n        # If name starts with \"hub://\", load from Focoos Hub\n        if isinstance(name, str) and name.startswith(\"hub://\"):\n            model_info, hub_config = cls._from_hub(hub_uri=name, hub=hub, cache=cache, **kwargs)\n            if config is None:\n                config = hub_config  # Use hub config if no config is provided\n        # If model exists in ModelRegistry, load as pretrained model\n        elif isinstance(name, str) and ModelRegistry.exists(name):\n            model_info = ModelRegistry.get_model_info(name)\n        # Otherwise, attempt to load from a local directory\n        else:\n            model_info = cls._from_local_dir(name=str(name))\n        # Load model from the resolved ModelInfo\n        return cls._from_model_info(model_info=model_info, config=config, **kwargs)\n\n    @classmethod\n    def register_model(cls, model_family: ModelFamily, model_loader: Callable[[], Type[BaseModelNN]]):\n        \"\"\"\n        Register a loader for a specific model family.\n\n        This method associates a model family with a loader function that returns\n        the model class when called. This enables lazy loading of model classes.\n\n        Args:\n            model_family: The ModelFamily enum value to register\n            model_loader: A callable that returns the model class when invoked\n        \"\"\"\n        cls._models_family_map[model_family.value] = model_loader\n\n    @classmethod\n    def _ensure_family_registered(cls, model_family: ModelFamily):\n        \"\"\"\n        Ensure the model family is registered, importing if needed.\n\n        This method checks if a model family is registered and if not, attempts to\n        import and register it automatically by calling any registration functions\n        in the family module.\n\n        Args:\n            model_family: The ModelFamily enum value to ensure is registered\n        \"\"\"\n        if model_family.value in cls._models_family_map:\n            return\n        family_module = importlib.import_module(f\"focoos.models.{model_family.value}\")\n        for attr_name in dir(family_module):\n            if attr_name.startswith(\"_register\"):\n                register_func = getattr(family_module, attr_name)\n                if callable(register_func):\n                    register_func()\n\n    @classmethod\n    def _from_model_info(cls, model_info: ModelInfo, config: Optional[ModelConfig] = None, **kwargs) -&gt; FocoosModel:\n        \"\"\"\n        Load a model from ModelInfo, handling config and weights.\n\n        This method instantiates a model based on the ModelInfo, applying the provided\n        configuration (or using the one from ModelInfo) and loading weights if available.\n\n        Args:\n            model_info: ModelInfo object containing model metadata and references\n            config: Optional model configuration to override the one in ModelInfo\n            **kwargs: Additional keyword arguments passed to the model configuration\n\n        Returns:\n            FocoosModel: The instantiated model with weights loaded if available\n\n        Raises:\n            ValueError: If the model family is not supported\n        \"\"\"\n        cls._ensure_family_registered(model_info.model_family)\n        if model_info.model_family.value not in cls._models_family_map:\n            raise ValueError(f\"Model {model_info.model_family} not supported\")\n        model_class = cls._models_family_map[model_info.model_family.value]()\n        config = config or ConfigManager.from_dict(model_info.model_family, model_info.config, **kwargs)\n        model_info.config = config\n        nn_model = model_class(model_info.config)\n        model = FocoosModel(nn_model, model_info)\n        return model\n\n    @classmethod\n    def _from_local_dir(cls, name: str) -&gt; ModelInfo:\n        \"\"\"\n        Load a model from a local experiment directory.\n\n        This method loads a model from a local directory by reading its ModelInfo file\n        and resolving paths to weights and other artifacts.\n\n        Args:\n            name: Name or path of the model directory relative to models_dir\n\n        Returns:\n            ModelInfo: The model information loaded from the local directory\n\n        Raises:\n            ValueError: If the model directory or ModelInfo file cannot be found\n        \"\"\"\n        run_dir = name\n        if not os.path.exists(run_dir):\n            run_dir = os.path.join(MODELS_DIR, name)\n            if not os.path.exists(run_dir):\n                raise ValueError(f\"Run {run_dir} not exists.\")\n\n        model_info_path = os.path.join(run_dir, ArtifactName.INFO)\n        if not os.path.exists(model_info_path):\n            raise ValueError(f\"Model info not found in {run_dir}\")\n        model_info = ModelInfo.from_json(model_info_path)\n\n        if model_info.weights_uri == ArtifactName.WEIGHTS:\n            model_info.weights_uri = os.path.join(run_dir, model_info.weights_uri)\n\n        return model_info\n\n    @classmethod\n    def _from_hub(\n        cls, hub_uri: str, hub: Optional[FocoosHUB] = None, cache: bool = True, **kwargs\n    ) -&gt; Tuple[ModelInfo, ModelConfig]:\n        \"\"\"\n        Load a model from the Focoos Hub.\n\n        This method downloads a model from the Focoos Hub using the provided URI,\n        which should be in the format \"hub://username/model_ref\".\n\n        Args:\n            hub_uri: Hub URI in the format \"hub://username/model_ref\"\n            hub: Optional FocoosHUB instance to use (creates a new one if not provided)\n            **kwargs: Additional keyword arguments passed to the model configuration\n\n        Returns:\n            Tuple[ModelInfo, ModelConfig]: The model information and configuration\n\n        Raises:\n            ValueError: If the model reference is invalid or the model cannot be downloaded\n        \"\"\"\n        hub = hub or FocoosHUB()\n        model_ref = hub_uri.split(\"hub://\")[1]\n\n        if not model_ref:\n            raise ValueError(\"Model ref is required\")\n\n        model_pth_path = hub.download_model_pth(model_ref=model_ref, skip_if_exists=cache)\n\n        model_info_path = os.path.join(MODELS_DIR, model_ref, ArtifactName.INFO)\n        if not os.path.exists(model_info_path) or not cache:\n            logger.info(f\"\ud83d\udce5 Downloading model info from hub for model: {model_ref}\")\n            remote_model_info = hub.get_model_info(model_ref=model_ref)\n            model_info = ModelInfo.from_json(remote_model_info.model_dump(mode=\"json\"))\n            model_info.dump_json(model_info_path)\n        else:\n            logger.info(f\"\ud83d\udce5 Loading model info from cache: {model_info_path}\")\n            model_info = ModelInfo.from_json(model_info_path)\n\n        if not model_info.weights_uri:\n            model_info.weights_uri = model_pth_path\n\n        model_config = ConfigManager.from_dict(model_info.model_family, model_info.config, **kwargs)\n\n        return (model_info, model_config)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ModelManager.get","title":"<code>get(name, model_info=None, config=None, hub=None, cache=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Unified entrypoint to load a model by name or ModelInfo.</p> <p>This method provides a single interface for loading models from various sources: - From a ModelInfo object (when model_info is provided) - From the Focoos Hub (when name starts with \"hub://\") - From the ModelRegistry (for pretrained models) - From a local directory (when name is a local path)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Union[str, Path]</code> <p>Model name, path, or hub reference (e.g., \"hub://username/model_ref\")</p> required <code>model_info</code> <code>Optional[ModelInfo]</code> <p>Optional ModelInfo object to load the model from directly</p> <code>None</code> <code>config</code> <code>Optional[ModelConfig]</code> <p>Optional custom model configuration to override defaults</p> <code>None</code> <code>hub</code> <code>Optional[FocoosHUB]</code> <p>Optional FocoosHUB instance to use for hub:// references</p> <code>None</code> <code>cache</code> <code>bool</code> <p>Optional boolean to cache the model info and weights when loading from hub (defaults to True)</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the model configuration</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FocoosModel</code> <code>FocoosModel</code> <p>The loaded model instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model cannot be found or loaded</p> Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef get(\n    cls,\n    name: Union[str, Path],\n    model_info: Optional[ModelInfo] = None,\n    config: Optional[ModelConfig] = None,\n    hub: Optional[FocoosHUB] = None,\n    cache: bool = True,\n    **kwargs,\n) -&gt; FocoosModel:\n    \"\"\"\n    Unified entrypoint to load a model by name or ModelInfo.\n\n    This method provides a single interface for loading models from various sources:\n    - From a ModelInfo object (when model_info is provided)\n    - From the Focoos Hub (when name starts with \"hub://\")\n    - From the ModelRegistry (for pretrained models)\n    - From a local directory (when name is a local path)\n\n    Args:\n        name: Model name, path, or hub reference (e.g., \"hub://username/model_ref\")\n        model_info: Optional ModelInfo object to load the model from directly\n        config: Optional custom model configuration to override defaults\n        hub: Optional FocoosHUB instance to use for hub:// references\n        cache: Optional boolean to cache the model info and weights when loading from hub (defaults to True)\n        **kwargs: Additional keyword arguments passed to the model configuration\n\n    Returns:\n        FocoosModel: The loaded model instance\n\n    Raises:\n        ValueError: If the model cannot be found or loaded\n    \"\"\"\n    if model_info is not None:\n        # Load model directly from provided ModelInfo\n        return cls._from_model_info(model_info=model_info, config=config, **kwargs)\n\n    # If name starts with \"hub://\", load from Focoos Hub\n    if isinstance(name, str) and name.startswith(\"hub://\"):\n        model_info, hub_config = cls._from_hub(hub_uri=name, hub=hub, cache=cache, **kwargs)\n        if config is None:\n            config = hub_config  # Use hub config if no config is provided\n    # If model exists in ModelRegistry, load as pretrained model\n    elif isinstance(name, str) and ModelRegistry.exists(name):\n        model_info = ModelRegistry.get_model_info(name)\n    # Otherwise, attempt to load from a local directory\n    else:\n        model_info = cls._from_local_dir(name=str(name))\n    # Load model from the resolved ModelInfo\n    return cls._from_model_info(model_info=model_info, config=config, **kwargs)\n</code></pre>"},{"location":"api/model_manager/#focoos.model_manager.ModelManager.register_model","title":"<code>register_model(model_family, model_loader)</code>  <code>classmethod</code>","text":"<p>Register a loader for a specific model family.</p> <p>This method associates a model family with a loader function that returns the model class when called. This enables lazy loading of model classes.</p> <p>Parameters:</p> Name Type Description Default <code>model_family</code> <code>ModelFamily</code> <p>The ModelFamily enum value to register</p> required <code>model_loader</code> <code>Callable[[], Type[BaseModelNN]]</code> <p>A callable that returns the model class when invoked</p> required Source code in <code>focoos/model_manager.py</code> <pre><code>@classmethod\ndef register_model(cls, model_family: ModelFamily, model_loader: Callable[[], Type[BaseModelNN]]):\n    \"\"\"\n    Register a loader for a specific model family.\n\n    This method associates a model family with a loader function that returns\n    the model class when called. This enables lazy loading of model classes.\n\n    Args:\n        model_family: The ModelFamily enum value to register\n        model_loader: A callable that returns the model class when invoked\n    \"\"\"\n    cls._models_family_map[model_family.value] = model_loader\n</code></pre>"},{"location":"api/model_registry/","title":"ModelRegistry","text":""},{"location":"api/model_registry/#focoos.model_registry.model_registry.ModelRegistry","title":"<code>ModelRegistry</code>","text":"<p>Central registry of pretrained models.</p> <p>This class serves as a centralized registry for all pretrained models in the Focoos system. It provides methods to access model information, list available models, and check model existence.</p> <p>Attributes:</p> Name Type Description <code>_registry_path</code> <code>Path</code> <p>Path to the directory containing model JSON files.</p> <code>_pretrained_models</code> <code>Dict[str, str]</code> <p>Dictionary mapping model names to their JSON file paths.</p> Source code in <code>focoos/model_registry/model_registry.py</code> <pre><code>class ModelRegistry:\n    \"\"\"Central registry of pretrained models.\n\n    This class serves as a centralized registry for all pretrained models in the Focoos system.\n    It provides methods to access model information, list available models, and check model existence.\n\n    Attributes:\n        _registry_path (Path): Path to the directory containing model JSON files.\n        _pretrained_models (Dict[str, str]): Dictionary mapping model names to their JSON file paths.\n    \"\"\"\n\n    _registry_path = Path(__file__).parent\n    _pretrained_models: Optional[Dict[str, str]] = None\n\n    @classmethod\n    def _load_models_cfgs(cls) -&gt; Dict[str, str]:\n        \"\"\"Load model configurations from JSON files.\n\n        Returns:\n            Dict[str, str]: Dictionary mapping model names to their JSON file paths.\n        \"\"\"\n        if cls._pretrained_models is not None:\n            return cls._pretrained_models\n\n        models = {}\n        try:\n            json_files = [f for f in cls._registry_path.iterdir() if f.is_file() and f.suffix == \".json\"]\n\n            for json_file in json_files:\n                model_name = json_file.stem  # Remove .json extension\n                models[model_name] = str(json_file)\n        except OSError as e:\n            logger.error(f\"Failed to load model configurations: {e}\")\n            models = {}\n\n        cls._pretrained_models = models\n        return models\n\n    @classmethod\n    def get_model_info(cls, model_name: str) -&gt; ModelInfo:\n        \"\"\"Get the model information for a given model name.\n\n        Args:\n            model_name (str): The name of the model to retrieve information for.\n                Can be either a pretrained model name or a path to a JSON file.\n\n        Returns:\n            ModelInfo: The model information object containing model details.\n\n        Raises:\n            ValueError: If the model is not found in the registry and the provided\n                path does not exist.\n        \"\"\"\n        models = cls._load_models_cfgs()\n\n        if model_name in models:\n            return ModelInfo.from_json(models[model_name])\n        if not os.path.exists(model_name):\n            logger.warning(f\"\u26a0\ufe0f Model {model_name} not found\")\n            raise ValueError(f\"\u26a0\ufe0f Model {model_name} not found\")\n        return ModelInfo.from_json(model_name)\n\n    @classmethod\n    def list_models(cls) -&gt; List[str]:\n        \"\"\"List all available pretrained models.\n\n        Returns:\n            List[str]: A list of all available pretrained model names.\n        \"\"\"\n        models = cls._load_models_cfgs()\n        return list(models.keys())\n\n    @classmethod\n    def exists(cls, model_name: str) -&gt; bool:\n        \"\"\"Check if a model exists in the registry.\n\n        Args:\n            model_name (str): The name of the model to check.\n\n        Returns:\n            bool: True if the model exists in the pretrained models registry,\n                False otherwise.\n        \"\"\"\n        models = cls._load_models_cfgs()\n        exists = model_name in models\n        if not exists:\n            logger.debug(f\"Model '{model_name}' not found in registry\")\n        return exists\n</code></pre>"},{"location":"api/model_registry/#focoos.model_registry.model_registry.ModelRegistry.exists","title":"<code>exists(model_name)</code>  <code>classmethod</code>","text":"<p>Check if a model exists in the registry.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the model exists in the pretrained models registry, False otherwise.</p> Source code in <code>focoos/model_registry/model_registry.py</code> <pre><code>@classmethod\ndef exists(cls, model_name: str) -&gt; bool:\n    \"\"\"Check if a model exists in the registry.\n\n    Args:\n        model_name (str): The name of the model to check.\n\n    Returns:\n        bool: True if the model exists in the pretrained models registry,\n            False otherwise.\n    \"\"\"\n    models = cls._load_models_cfgs()\n    exists = model_name in models\n    if not exists:\n        logger.debug(f\"Model '{model_name}' not found in registry\")\n    return exists\n</code></pre>"},{"location":"api/model_registry/#focoos.model_registry.model_registry.ModelRegistry.get_model_info","title":"<code>get_model_info(model_name)</code>  <code>classmethod</code>","text":"<p>Get the model information for a given model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to retrieve information for. Can be either a pretrained model name or a path to a JSON file.</p> required <p>Returns:</p> Name Type Description <code>ModelInfo</code> <code>ModelInfo</code> <p>The model information object containing model details.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not found in the registry and the provided path does not exist.</p> Source code in <code>focoos/model_registry/model_registry.py</code> <pre><code>@classmethod\ndef get_model_info(cls, model_name: str) -&gt; ModelInfo:\n    \"\"\"Get the model information for a given model name.\n\n    Args:\n        model_name (str): The name of the model to retrieve information for.\n            Can be either a pretrained model name or a path to a JSON file.\n\n    Returns:\n        ModelInfo: The model information object containing model details.\n\n    Raises:\n        ValueError: If the model is not found in the registry and the provided\n            path does not exist.\n    \"\"\"\n    models = cls._load_models_cfgs()\n\n    if model_name in models:\n        return ModelInfo.from_json(models[model_name])\n    if not os.path.exists(model_name):\n        logger.warning(f\"\u26a0\ufe0f Model {model_name} not found\")\n        raise ValueError(f\"\u26a0\ufe0f Model {model_name} not found\")\n    return ModelInfo.from_json(model_name)\n</code></pre>"},{"location":"api/model_registry/#focoos.model_registry.model_registry.ModelRegistry.list_models","title":"<code>list_models()</code>  <code>classmethod</code>","text":"<p>List all available pretrained models.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of all available pretrained model names.</p> Source code in <code>focoos/model_registry/model_registry.py</code> <pre><code>@classmethod\ndef list_models(cls) -&gt; List[str]:\n    \"\"\"List all available pretrained models.\n\n    Returns:\n        List[str]: A list of all available pretrained model names.\n    \"\"\"\n    models = cls._load_models_cfgs()\n    return list(models.keys())\n</code></pre>"},{"location":"api/ports/","title":"ports","text":""},{"location":"api/ports/#focoos.ports.ApiKey","title":"<code>ApiKey</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>API key for authentication.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ApiKey(PydanticBase):\n    \"\"\"API key for authentication.\"\"\"\n\n    key: str  # type: ignore\n</code></pre>"},{"location":"api/ports/#focoos.ports.ArtifactName","title":"<code>ArtifactName</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Model artifact type.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ArtifactName(str, Enum):\n    \"\"\"Model artifact type.\"\"\"\n\n    WEIGHTS = \"model_final.pth\"\n    ONNX = \"model.onnx\"\n    PT = \"model.pt\"\n    INFO = \"model_info.json\"\n    METRICS = \"metrics.json\"\n    LOGS = \"log.txt\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetLayout","title":"<code>DatasetLayout</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported dataset formats in Focoos.</p> Values <ul> <li>ROBOFLOW_COCO: (Detection,Instance Segmentation)</li> <li>ROBOFLOW_SEG: (Semantic Segmentation)</li> <li>CATALOG: (Any)</li> <li>CLS_FOLDER: (Classification)</li> </ul> <p>Example:     <pre><code>- ROBOFLOW_COCO: (Detection,Instance Segmentation) Roboflow COCO format:\n    root/\n        train/\n            - _annotations.coco.json\n            - img_1.jpg\n            - img_2.jpg\n        valid/\n            - _annotations.coco.json\n            - img_3.jpg\n            - img_4.jpg\n- ROBOFLOW_SEG: (Semantic Segmentation) Roboflow segmentation format:\n    root/\n        train/\n            - _classes.csv (comma separated csv)\n            - img_1.jpg\n            - img_2.jpg\n        valid/\n            - _classes.csv (comma separated csv)\n            - img_3_mask.png\n            - img_4_mask.png\n- CLS_FOLDER: (Classification)\n    root/\n        train/\n            - class_1/\n                - img_1.jpg\n                - img_2.jpg\n            - class_2/\n                - img_3.jpg\n                - img_4.jpg\n        valid/\n            - class_1/\n                - img_14.jpg\n                - img_12.jpg\n            - class_2/\n                - img_5.jpg\n                - img_6.jpg\n</code></pre></p> Source code in <code>focoos/ports.py</code> <pre><code>class DatasetLayout(str, Enum):\n    \"\"\"Supported dataset formats in Focoos.\n\n    Values:\n        - ROBOFLOW_COCO: (Detection,Instance Segmentation)\n        - ROBOFLOW_SEG: (Semantic Segmentation)\n        - CATALOG: (Any)\n        - CLS_FOLDER: (Classification)\n    Example:\n        ```python\n        - ROBOFLOW_COCO: (Detection,Instance Segmentation) Roboflow COCO format:\n            root/\n                train/\n                    - _annotations.coco.json\n                    - img_1.jpg\n                    - img_2.jpg\n                valid/\n                    - _annotations.coco.json\n                    - img_3.jpg\n                    - img_4.jpg\n        - ROBOFLOW_SEG: (Semantic Segmentation) Roboflow segmentation format:\n            root/\n                train/\n                    - _classes.csv (comma separated csv)\n                    - img_1.jpg\n                    - img_2.jpg\n                valid/\n                    - _classes.csv (comma separated csv)\n                    - img_3_mask.png\n                    - img_4_mask.png\n        - CLS_FOLDER: (Classification)\n            root/\n                train/\n                    - class_1/\n                        - img_1.jpg\n                        - img_2.jpg\n                    - class_2/\n                        - img_3.jpg\n                        - img_4.jpg\n                valid/\n                    - class_1/\n                        - img_14.jpg\n                        - img_12.jpg\n                    - class_2/\n                        - img_5.jpg\n                        - img_6.jpg\n        ```\n    \"\"\"\n\n    ROBOFLOW_COCO = \"roboflow_coco\"\n    ROBOFLOW_SEG = \"roboflow_seg\"\n    CATALOG = \"catalog\"\n    CLS_FOLDER = \"cls_folder\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetMetadata","title":"<code>DatasetMetadata</code>  <code>dataclass</code>","text":"<p>Dataclass for storing dataset metadata.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass DatasetMetadata:\n    \"\"\"Dataclass for storing dataset metadata.\"\"\"\n\n    num_classes: int\n    task: Task\n    count: Optional[int] = None\n    name: Optional[str] = None\n    image_root: Optional[str] = None\n    thing_classes: Optional[List[str]] = None\n    _thing_colors: Optional[List[Tuple]] = None\n    stuff_classes: Optional[List[str]] = None\n    _stuff_colors: Optional[List[Tuple]] = None\n    sem_seg_root: Optional[str] = None\n    panoptic_root: Optional[str] = None\n    ignore_label: Optional[int] = None\n    thing_dataset_id_to_contiguous_id: Optional[dict] = None\n    stuff_dataset_id_to_contiguous_id: Optional[dict] = None\n    json_file: Optional[str] = None\n    keypoints: Optional[List[str]] = None\n    keypoints_skeleton: Optional[List[Tuple[int, int]]] = None\n\n    @property\n    def classes(self) -&gt; List[str]:  #!TODO: check if this is correct\n        if self.task == Task.DETECTION or self.task == Task.INSTANCE_SEGMENTATION:\n            assert self.thing_classes is not None, \"thing_classes is required for detection and instance segmentation\"\n            return self.thing_classes\n        if self.task == Task.SEMSEG:\n            # fixme: not sure for panoptic\n            assert self.stuff_classes is not None, \"stuff_classes is required for semantic segmentation\"\n            return self.stuff_classes\n        if self.task == Task.CLASSIFICATION:\n            assert self.thing_classes is not None, \"thing_classes is required for classification\"\n            return self.thing_classes\n        if self.task == Task.KEYPOINT:\n            assert self.thing_classes is not None, \"thing_classes is required for keypoint\"\n            return self.thing_classes\n\n        raise ValueError(f\"Task {self.task} not supported\")\n\n    @property\n    def stuff_colors(self):\n        if self._stuff_colors is not None:\n            return self._stuff_colors\n        if self.stuff_classes is None:\n            return []\n        return [((i * 64) % 255, (i * 128) % 255, (i * 32) % 255) for i in range(len(self.stuff_classes))]\n\n    @stuff_colors.setter\n    def stuff_colors(self, colors):\n        self._stuff_colors = colors\n\n    @property\n    def thing_colors(self):\n        if self._thing_colors is not None:\n            return self._thing_colors\n        if self.thing_classes is None:\n            return []\n        return [((i * 64) % 255, (i * 128) % 255, (i * 32) % 255) for i in range(1, len(self.thing_classes) + 1)]\n\n    @thing_colors.setter\n    def thing_colors(self, colors):\n        self._thing_colors = colors\n\n    @classmethod\n    def from_dict(cls, metadata: dict):\n        \"\"\"Create DatasetMetadata from a dictionary.\n\n        Args:\n            metadata (dict): Dictionary containing metadata.\n\n        Returns:\n            DatasetMetadata: Instance of DatasetMetadata.\n        \"\"\"\n        metadata = {k: v for k, v in metadata.items() if k in inspect.signature(cls).parameters}\n        metadata[\"task\"] = Task(metadata[\"task\"])\n        return cls(**metadata)\n\n    @classmethod\n    def from_json(cls, path: str):\n        \"\"\"Create DatasetMetadata from a json file.\n\n        Args:\n            path (str): Path to json file.\n\n        Returns:\n            DatasetMetadata: Instance of DatasetMetadata.\n        \"\"\"\n        with open(path, encoding=\"utf-8\") as f:\n            metadata = json.load(f)\n        metadata[\"task\"] = Task(metadata[\"task\"])\n        return cls(**metadata)\n\n    def dump_json(self, path: str):\n        \"\"\"Dump DatasetMetadata to a json file.\n\n        Args:\n            path (str): Path to json file.\n        \"\"\"\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(asdict(self), f, ensure_ascii=False, indent=4)\n\n    def get(self, attr, default=None):\n        if hasattr(self, attr):\n            return getattr(self, attr)\n        else:\n            return default\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetMetadata.dump_json","title":"<code>dump_json(path)</code>","text":"<p>Dump DatasetMetadata to a json file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to json file.</p> required Source code in <code>focoos/ports.py</code> <pre><code>def dump_json(self, path: str):\n    \"\"\"Dump DatasetMetadata to a json file.\n\n    Args:\n        path (str): Path to json file.\n    \"\"\"\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(asdict(self), f, ensure_ascii=False, indent=4)\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetMetadata.from_dict","title":"<code>from_dict(metadata)</code>  <code>classmethod</code>","text":"<p>Create DatasetMetadata from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>Dictionary containing metadata.</p> required <p>Returns:</p> Name Type Description <code>DatasetMetadata</code> <p>Instance of DatasetMetadata.</p> Source code in <code>focoos/ports.py</code> <pre><code>@classmethod\ndef from_dict(cls, metadata: dict):\n    \"\"\"Create DatasetMetadata from a dictionary.\n\n    Args:\n        metadata (dict): Dictionary containing metadata.\n\n    Returns:\n        DatasetMetadata: Instance of DatasetMetadata.\n    \"\"\"\n    metadata = {k: v for k, v in metadata.items() if k in inspect.signature(cls).parameters}\n    metadata[\"task\"] = Task(metadata[\"task\"])\n    return cls(**metadata)\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetMetadata.from_json","title":"<code>from_json(path)</code>  <code>classmethod</code>","text":"<p>Create DatasetMetadata from a json file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to json file.</p> required <p>Returns:</p> Name Type Description <code>DatasetMetadata</code> <p>Instance of DatasetMetadata.</p> Source code in <code>focoos/ports.py</code> <pre><code>@classmethod\ndef from_json(cls, path: str):\n    \"\"\"Create DatasetMetadata from a json file.\n\n    Args:\n        path (str): Path to json file.\n\n    Returns:\n        DatasetMetadata: Instance of DatasetMetadata.\n    \"\"\"\n    with open(path, encoding=\"utf-8\") as f:\n        metadata = json.load(f)\n    metadata[\"task\"] = Task(metadata[\"task\"])\n    return cls(**metadata)\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetPreview","title":"<code>DatasetPreview</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Preview information for a Focoos dataset.</p> <p>This class provides metadata about a dataset in the Focoos platform, including its identification, task type, and layout format.</p> <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>Unique reference ID for the dataset.</p> <code>name</code> <code>str</code> <p>Human-readable name of the dataset.</p> <code>task</code> <code>FocoosTask</code> <p>The computer vision task this dataset is designed for.</p> <code>layout</code> <code>DatasetLayout</code> <p>The structural format of the dataset (e.g., ROBOFLOW_COCO, ROBOFLOW_SEG).</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the dataset's purpose or contents.</p> <code>spec</code> <code>Optional[DatasetSpec]</code> <p>Detailed specifications about the dataset's composition and size.</p> Source code in <code>focoos/ports.py</code> <pre><code>class DatasetPreview(PydanticBase):\n    \"\"\"Preview information for a Focoos dataset.\n\n    This class provides metadata about a dataset in the Focoos platform,\n    including its identification, task type, and layout format.\n\n    Attributes:\n        ref (str): Unique reference ID for the dataset.\n        name (str): Human-readable name of the dataset.\n        task (FocoosTask): The computer vision task this dataset is designed for.\n        layout (DatasetLayout): The structural format of the dataset (e.g., ROBOFLOW_COCO, ROBOFLOW_SEG).\n        description (Optional[str]): Optional description of the dataset's purpose or contents.\n        spec (Optional[DatasetSpec]): Detailed specifications about the dataset's composition and size.\n    \"\"\"\n\n    ref: str\n    name: str\n    task: Task\n    layout: DatasetLayout\n    description: Optional[str] = None\n    spec: Optional[DatasetSpec] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.DatasetSpec","title":"<code>DatasetSpec</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Specification details for a dataset in the Focoos platform.</p> <p>This class provides information about the dataset's size and composition, including the number of samples in training and validation sets and the total size.</p> <p>Attributes:</p> Name Type Description <code>train_length</code> <code>int</code> <p>Number of samples in the training set.</p> <code>valid_length</code> <code>int</code> <p>Number of samples in the validation set.</p> <code>size_mb</code> <code>float</code> <p>Total size of the dataset in megabytes.</p> Source code in <code>focoos/ports.py</code> <pre><code>class DatasetSpec(PydanticBase):\n    \"\"\"Specification details for a dataset in the Focoos platform.\n\n    This class provides information about the dataset's size and composition,\n    including the number of samples in training and validation sets and the total size.\n\n    Attributes:\n        train_length (int): Number of samples in the training set.\n        valid_length (int): Number of samples in the validation set.\n        size_mb (float): Total size of the dataset in megabytes.\n    \"\"\"\n\n    train_length: int\n    valid_length: int\n    size_mb: float\n</code></pre>"},{"location":"api/ports/#focoos.ports.DictClass","title":"<code>DictClass</code>","text":"<p>               Bases: <code>OrderedDict</code></p> Source code in <code>focoos/ports.py</code> <pre><code>class DictClass(OrderedDict):\n    def to_tuple(self) -&gt; tuple[Any]:\n        \"\"\"\n        Convert self to a tuple containing all the attributes/keys that are not `None`.\n        \"\"\"\n        return tuple(\n            self[k] for k in self.keys() if self[k] is not None\n        )  # without this check we are unable to export models with None values\n\n    def __getitem__(self, k):\n        if isinstance(k, str):\n            inner_dict = dict(self.items())\n            return inner_dict[k]\n        else:\n            return self.to_tuple()[k]\n\n    def __setattr__(self, name, value):\n        if name in self.keys() and value is not None:\n            # Don't call self.__setitem__ to avoid recursion errors\n            super().__setitem__(name, value)\n        super().__setattr__(name, value)\n\n    def __setitem__(self, key, value):\n        # Will raise a KeyException if needed\n        super().__setitem__(key, value)\n        # Don't call self.__setattr__ to avoid recursion errors\n        super().__setattr__(key, value)\n\n    def __post_init__(self):\n        \"\"\"Check the BasicContainer dataclass.\n\n        Only occurs if @dataclass decorator has been used.\n        \"\"\"\n        class_fields = fields(self)\n\n        # Safety and consistency checks\n        if not len(class_fields):\n            raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n\n        for _field in class_fields:\n            v = getattr(self, _field.name)\n            # if v is not None:  # without this check we are unable to export models with None values\n            #    self[_field.name] = v\n            self[_field.name] = v\n\n    def __reduce__(self):\n        state_dict = {field.name: getattr(self, field.name) for field in fields(self)}\n        return (self.__class__.__new__, (self.__class__,), state_dict)\n</code></pre>"},{"location":"api/ports/#focoos.ports.DictClass.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check the BasicContainer dataclass.</p> <p>Only occurs if @dataclass decorator has been used.</p> Source code in <code>focoos/ports.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Check the BasicContainer dataclass.\n\n    Only occurs if @dataclass decorator has been used.\n    \"\"\"\n    class_fields = fields(self)\n\n    # Safety and consistency checks\n    if not len(class_fields):\n        raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n\n    for _field in class_fields:\n        v = getattr(self, _field.name)\n        # if v is not None:  # without this check we are unable to export models with None values\n        #    self[_field.name] = v\n        self[_field.name] = v\n</code></pre>"},{"location":"api/ports/#focoos.ports.DictClass.to_tuple","title":"<code>to_tuple()</code>","text":"<p>Convert self to a tuple containing all the attributes/keys that are not <code>None</code>.</p> Source code in <code>focoos/ports.py</code> <pre><code>def to_tuple(self) -&gt; tuple[Any]:\n    \"\"\"\n    Convert self to a tuple containing all the attributes/keys that are not `None`.\n    \"\"\"\n    return tuple(\n        self[k] for k in self.keys() if self[k] is not None\n    )  # without this check we are unable to export models with None values\n</code></pre>"},{"location":"api/ports/#focoos.ports.DynamicAxes","title":"<code>DynamicAxes</code>  <code>dataclass</code>","text":"<p>Dynamic axes for model export.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass DynamicAxes:\n    \"\"\"Dynamic axes for model export.\"\"\"\n\n    input_names: list[str]\n    output_names: list[str]\n    dynamic_axes: dict\n</code></pre>"},{"location":"api/ports/#focoos.ports.ExportCfg","title":"<code>ExportCfg</code>  <code>dataclass</code>","text":"<p>Configuration for model export.</p> <p>Parameters:</p> Name Type Description Default <code>out_dir</code> <code>str</code> <p>Output directory for exported model</p> required <code>onnx_opset</code> <code>int</code> <p>ONNX opset version to use</p> <code>17</code> <code>onnx_dynamic</code> <code>bool</code> <p>Whether to use dynamic axes in ONNX export</p> <code>True</code> <code>onnx_simplify</code> <code>bool</code> <p>Whether to simplify ONNX model</p> <code>True</code> <code>model_fuse</code> <code>bool</code> <p>Whether to fuse model layers</p> <code>True</code> <code>format</code> <code>Literal['onnx', 'torchscript']</code> <p>Export format (\"onnx\" or \"torchscript\")</p> <code>'onnx'</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for export</p> <code>'cuda'</code> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass ExportCfg:\n    \"\"\"Configuration for model export.\n\n    Args:\n        out_dir: Output directory for exported model\n        onnx_opset: ONNX opset version to use\n        onnx_dynamic: Whether to use dynamic axes in ONNX export\n        onnx_simplify: Whether to simplify ONNX model\n        model_fuse: Whether to fuse model layers\n        format: Export format (\"onnx\" or \"torchscript\")\n        device: Device to use for export\n    \"\"\"\n\n    out_dir: str\n    onnx_opset: int = 17\n    onnx_dynamic: bool = True\n    onnx_simplify: bool = True\n    model_fuse: bool = True\n    format: Literal[\"onnx\", \"torchscript\"] = \"onnx\"\n    device: Optional[str] = \"cuda\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.ExportFormat","title":"<code>ExportFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Available export formats for model inference.</p> Values <ul> <li>ONNX: ONNX format</li> <li>TORCHSCRIPT: TorchScript format</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class ExportFormat(str, Enum):\n    \"\"\"Available export formats for model inference.\n\n    Values:\n        - ONNX: ONNX format\n        - TORCHSCRIPT: TorchScript format\n\n    \"\"\"\n\n    ONNX = \"onnx\"\n    TORCHSCRIPT = \"torchscript\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.FocoosDet","title":"<code>FocoosDet</code>  <code>dataclass</code>","text":"<p>Represents a single result from a Focoos model.</p> <p>This dataclass encapsulates all relevant information for a detected object, including:   - Bounding box coordinates (bbox) in [x1, y1, x2, y2] format, where (x1, y1) is the top-left and (x2, y2) is the bottom-right.   - Confidence score (conf) between 0 and 1.   - Class ID (cls_id) corresponding to the model's class list.   - Human-readable label (label) for the detected class.   - Optional segmentation mask (mask) as a base64-encoded PNG, cropped to the bbox region.   - Optional keypoints for pose estimation or similar tasks.</p> Notes <ul> <li>The mask field is only present for instance or semantic segmentation models.</li> <li>The mask is a base64-encoded PNG string, with its origin at the top-left of the bbox and dimensions matching the bbox.</li> <li>Keypoints, if present, are a list of (x, y, visibility) tuples.</li> </ul> <p>Attributes:</p> Name Type Description <code>bbox</code> <code>Optional[list[int]]</code> <p>Bounding box [x1, y1, x2, y2].</p> <code>conf</code> <code>Optional[float]</code> <p>Detection confidence score.</p> <code>cls_id</code> <code>Optional[int]</code> <p>Class index.</p> <code>label</code> <code>Optional[str]</code> <p>Class label.</p> <code>mask</code> <code>Optional[str]</code> <p>Base64-encoded PNG mask (cropped to bbox).</p> <code>keypoints</code> <code>Optional[list[tuple[int, int, float]]]</code> <p>Optional keypoints.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass FocoosDet:\n    \"\"\"\n    Represents a single result from a Focoos model.\n\n    This dataclass encapsulates all relevant information for a detected object, including:\n      - Bounding box coordinates (bbox) in [x1, y1, x2, y2] format, where (x1, y1) is the top-left and (x2, y2) is the bottom-right.\n      - Confidence score (conf) between 0 and 1.\n      - Class ID (cls_id) corresponding to the model's class list.\n      - Human-readable label (label) for the detected class.\n      - Optional segmentation mask (mask) as a base64-encoded PNG, cropped to the bbox region.\n      - Optional keypoints for pose estimation or similar tasks.\n\n    Notes:\n        - The mask field is only present for instance or semantic segmentation models.\n        - The mask is a base64-encoded PNG string, with its origin at the top-left of the bbox and dimensions matching the bbox.\n        - Keypoints, if present, are a list of (x, y, visibility) tuples.\n\n    Attributes:\n        bbox (Optional[list[int]]): Bounding box [x1, y1, x2, y2].\n        conf (Optional[float]): Detection confidence score.\n        cls_id (Optional[int]): Class index.\n        label (Optional[str]): Class label.\n        mask (Optional[str]): Base64-encoded PNG mask (cropped to bbox).\n        keypoints (Optional[list[tuple[int, int, float]]]): Optional keypoints.\n    \"\"\"\n\n    bbox: Optional[list[int]] = None\n    conf: Optional[float] = None\n    cls_id: Optional[int] = None\n    label: Optional[str] = None\n    mask: Optional[str] = None\n    keypoints: Optional[list[tuple[int, int, float]]] = None  # TODO: check if float visibility is used or not\n\n    @classmethod\n    def from_json(cls, data: Union[str, dict]):\n        if isinstance(data, str):\n            with open(data, encoding=\"utf-8\") as f:\n                data_dict = json.load(f)\n        else:\n            data_dict = data\n\n        bbox = data_dict.get(\"bbox\")\n        if bbox is not None:  # Retrocompatibility fix for remote results with float bbox, !TODO remove asap\n            data_dict[\"bbox\"] = list(map(int, bbox))\n\n        return cls(**data_dict)\n\n    def __repr__(self):\n        # Show \"hidden\" if mask is not None, else show the actual mask value\n        mask_repr = \"hidden\" if self.mask is not None else self.mask\n        return (\n            f\"FocoosDet(bbox={self.bbox}, conf={self.conf}, cls_id={self.cls_id}, \"\n            f\"label={self.label}, mask={mask_repr}, keypoints={self.keypoints})\"\n        )\n</code></pre>"},{"location":"api/ports/#focoos.ports.FocoosDetections","title":"<code>FocoosDetections</code>  <code>dataclass</code>","text":"<p>Represents a collection of detection or segmentation results from a Focoos model.</p> <p>This dataclass holds a list of FocoosDet objects, and optionally:   - The image (as a base64 string or numpy array) associated with the detections.   - Latency information for the inference process, such as time spent in preprocessing,     inference, postprocessing, and annotation.</p> <p>Attributes:</p> Name Type Description <code>detections</code> <code>list[FocoosDet]</code> <p>List of detection results.</p> <code>image</code> <code>Optional[Union[str, ndarray]]</code> <p>The image associated with the detections, either as a base64-encoded string or a numpy array. If present, the string is typically a base64-encoded annotated image.</p> <code>latency</code> <code>Optional[dict]</code> <p>Dictionary with timing information for each inference step. Keys may include 'inference', 'preprocess', 'postprocess', and 'annotate', with values in seconds.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass FocoosDetections:\n    \"\"\"\n    Represents a collection of detection or segmentation results from a Focoos model.\n\n    This dataclass holds a list of FocoosDet objects, and optionally:\n      - The image (as a base64 string or numpy array) associated with the detections.\n      - Latency information for the inference process, such as time spent in preprocessing,\n        inference, postprocessing, and annotation.\n\n    Attributes:\n        detections (list[FocoosDet]): List of detection results.\n        image (Optional[Union[str, np.ndarray]]): The image associated with the detections,\n            either as a base64-encoded string or a numpy array. If present, the string is\n            typically a base64-encoded annotated image.\n        latency (Optional[dict]): Dictionary with timing information for each inference step.\n            Keys may include 'inference', 'preprocess', 'postprocess', and 'annotate', with\n            values in seconds.\n    \"\"\"\n\n    detections: list[FocoosDet]\n    image: Optional[Union[str, np.ndarray]] = None  # can be Base64 encoded image or numpy array\n    latency: Optional[InferLatency] = None\n\n    def __len__(self):\n        return len(self.detections)\n\n    def model_dump(self):\n        return {\n            \"detections\": [asdict(det) for det in self.detections],\n            \"image\": self.image if isinstance(self.image, str) else None,\n            \"latency\": asdict(self.latency) if self.latency is not None else None,\n        }\n\n    def __repr__(self):\n        # Show \"hidden\" if image is not None, else exclude 'image'\n        fields = []\n        for field_name in self.__dataclass_fields__:\n            if field_name == \"image\":\n                value = getattr(self, field_name)\n                if value is not None:\n                    fields.append(f\"{field_name}=hidden\")\n                continue\n            value = getattr(self, field_name)\n            fields.append(f\"{field_name}={value!r}\")\n        return f\"{self.__class__.__name__}({', '.join(fields)})\"\n\n    def infer_print(self):\n        \"\"\"Print a formatted summary of the detections and timing information.\"\"\"\n        num_detections = len(self.detections)\n\n        # Handle special case of zero detections\n        if num_detections == 0:\n            print(\"\\nNo detections!\")\n        else:\n            # Count detections by class\n            class_counts = {}\n            for det in self.detections:\n                # Determine class key with fallback logic\n                if det.label is not None and det.label != \"\":\n                    class_key = det.label\n                elif det.cls_id is not None:\n                    class_key = f\"(id_class={det.cls_id})\"\n                else:\n                    class_key = \"unknown\"\n\n                class_counts[class_key] = class_counts.get(class_key, 0) + 1\n\n            # Format class counts as comma-separated string\n            if not class_counts:\n                formatted_classes = \"no_classes\"\n            else:\n                sorted_classes = sorted(class_counts.items())\n                formatted_classes = \", \".join(f\"{count} {class_name}\" for class_name, count in sorted_classes)\n\n            print(f\"\\n{formatted_classes}\")\n\n        # Print latency information with total time at the end\n        if self.latency is not None:\n            times = [\n                self.latency.imload or 0,\n                self.latency.preprocess or 0,\n                self.latency.inference or 0,\n                self.latency.postprocess or 0,\n                self.latency.annotate or 0,\n            ]\n            total_time_ms = sum(times) * 1000\n            total_time_str = f\"{total_time_ms:.0f}ms\"\n\n            latency_parts = []\n            if self.latency.imload is not None:\n                latency_parts.append(f\"imload {self.latency.imload * 1000:.0f}ms\")\n            if self.latency.preprocess is not None:\n                latency_parts.append(f\"preprocess {self.latency.preprocess * 1000:.0f}ms\")\n            if self.latency.inference is not None:\n                latency_parts.append(f\"inference {self.latency.inference * 1000:.0f}ms\")\n            if self.latency.postprocess is not None:\n                latency_parts.append(f\"postprocess {self.latency.postprocess * 1000:.0f}ms\")\n            if self.latency.annotate is not None:\n                latency_parts.append(f\"annotate {self.latency.annotate * 1000:.0f}ms\")\n\n            if latency_parts:\n                print(f\"Latency: {', '.join(latency_parts)}, total {total_time_str}\")\n            else:\n                print(f\"Latency: total {total_time_str}\")\n\n    def pprint(self):\n        print(\"\\n\" + \"=\" * 50)\n        print(\"DETECTION RESULTS\")\n        print(\"=\" * 50)\n\n        num_detections = len(self.detections)\n        print(f\"Found {num_detections} detections{': ' if num_detections &gt; 0 else ''}\")\n        print()\n\n        for i, det in enumerate(self.detections):\n            # Get values from FocoosDet object\n            x1, y1, x2, y2 = det.bbox if det.bbox else [-1, -1, -1, -1]\n            conf = det.conf if det.conf is not None else -1\n\n            print(f\"  {i + 1}. {det.label or f'Class {det.cls_id}'}\")\n            print(f\"     Confidence: {conf:.3f}\")\n            print(f\"     Bbox: [{x1}, {y1}, {x2}, {y2}]\")\n            print(f\"     Size: {x2 - x1} x {y2 - y1}\")\n            if det.mask:\n                print(\"     Has mask: Yes (base64 encoded)\")\n            print()\n\n        # Print latency information if available\n        if self.latency is not None:\n            print(\"Latencies:\")\n            print(\"-\" * 50)\n            for key, value in asdict(self.latency).items():\n                if isinstance(value, (int, float)):\n                    print(f\"  {key}: {value:.3f}s\")\n                else:\n                    print(f\"  {key}: {value}\")\n        print()\n        print(\"=\" * 50 + \"\\n\")\n</code></pre>"},{"location":"api/ports/#focoos.ports.FocoosDetections.infer_print","title":"<code>infer_print()</code>","text":"<p>Print a formatted summary of the detections and timing information.</p> Source code in <code>focoos/ports.py</code> <pre><code>def infer_print(self):\n    \"\"\"Print a formatted summary of the detections and timing information.\"\"\"\n    num_detections = len(self.detections)\n\n    # Handle special case of zero detections\n    if num_detections == 0:\n        print(\"\\nNo detections!\")\n    else:\n        # Count detections by class\n        class_counts = {}\n        for det in self.detections:\n            # Determine class key with fallback logic\n            if det.label is not None and det.label != \"\":\n                class_key = det.label\n            elif det.cls_id is not None:\n                class_key = f\"(id_class={det.cls_id})\"\n            else:\n                class_key = \"unknown\"\n\n            class_counts[class_key] = class_counts.get(class_key, 0) + 1\n\n        # Format class counts as comma-separated string\n        if not class_counts:\n            formatted_classes = \"no_classes\"\n        else:\n            sorted_classes = sorted(class_counts.items())\n            formatted_classes = \", \".join(f\"{count} {class_name}\" for class_name, count in sorted_classes)\n\n        print(f\"\\n{formatted_classes}\")\n\n    # Print latency information with total time at the end\n    if self.latency is not None:\n        times = [\n            self.latency.imload or 0,\n            self.latency.preprocess or 0,\n            self.latency.inference or 0,\n            self.latency.postprocess or 0,\n            self.latency.annotate or 0,\n        ]\n        total_time_ms = sum(times) * 1000\n        total_time_str = f\"{total_time_ms:.0f}ms\"\n\n        latency_parts = []\n        if self.latency.imload is not None:\n            latency_parts.append(f\"imload {self.latency.imload * 1000:.0f}ms\")\n        if self.latency.preprocess is not None:\n            latency_parts.append(f\"preprocess {self.latency.preprocess * 1000:.0f}ms\")\n        if self.latency.inference is not None:\n            latency_parts.append(f\"inference {self.latency.inference * 1000:.0f}ms\")\n        if self.latency.postprocess is not None:\n            latency_parts.append(f\"postprocess {self.latency.postprocess * 1000:.0f}ms\")\n        if self.latency.annotate is not None:\n            latency_parts.append(f\"annotate {self.latency.annotate * 1000:.0f}ms\")\n\n        if latency_parts:\n            print(f\"Latency: {', '.join(latency_parts)}, total {total_time_str}\")\n        else:\n            print(f\"Latency: total {total_time_str}\")\n</code></pre>"},{"location":"api/ports/#focoos.ports.GPUDevice","title":"<code>GPUDevice</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Information about a GPU device.</p> Source code in <code>focoos/ports.py</code> <pre><code>class GPUDevice(PydanticBase):\n    \"\"\"Information about a GPU device.\"\"\"\n\n    gpu_id: Optional[int] = None\n    gpu_name: Optional[str] = None\n    gpu_memory_total_gb: Optional[float] = None\n    gpu_memory_used_percentage: Optional[float] = None\n    gpu_temperature: Optional[float] = None\n    gpu_load_percentage: Optional[float] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.GPUInfo","title":"<code>GPUInfo</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Information about a GPU driver.</p> Source code in <code>focoos/ports.py</code> <pre><code>class GPUInfo(PydanticBase):\n    \"\"\"Information about a GPU driver.\"\"\"\n\n    gpu_count: Optional[int] = None\n    gpu_driver: Optional[str] = None\n    gpu_cuda_version: Optional[str] = None\n    total_gpu_memory_gb: Optional[float] = None\n    devices: Optional[list[GPUDevice]] = None\n    mps_available: Optional[bool] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.InferLatency","title":"<code>InferLatency</code>  <code>dataclass</code>","text":"<p>Represents the latency data for a Focoos model.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass InferLatency:\n    \"\"\"\n    Represents the latency data for a Focoos model.\n    \"\"\"\n\n    imload: Optional[float] = None\n    preprocess: Optional[float] = None\n    inference: Optional[float] = None\n    postprocess: Optional[float] = None\n    annotate: Optional[float] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.LatencyMetrics","title":"<code>LatencyMetrics</code>  <code>dataclass</code>","text":"<p>Performance metrics for model inference.</p> <p>This class provides performance metrics for model inference, including frames per second (FPS), engine used, minimum latency, maximum latency, mean latency, standard deviation of latency, input image size, and device type.</p> <p>Attributes:</p> Name Type Description <code>fps</code> <code>int</code> <p>Frames per second (FPS) of the inference process.</p> <code>engine</code> <code>str</code> <p>The inference engine used (e.g., \"onnx\", \"torchscript\").</p> <code>min</code> <code>float</code> <p>Minimum latency in milliseconds.</p> <code>max</code> <code>float</code> <p>Maximum latency in milliseconds.</p> <code>mean</code> <code>float</code> <p>Mean latency in milliseconds.</p> <code>std</code> <code>float</code> <p>Standard deviation of latency in milliseconds.</p> <code>im_size</code> <code>int</code> <p>Input image size.</p> <code>device</code> <code>str</code> <p>Device type.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass LatencyMetrics:\n    \"\"\"Performance metrics for model inference.\n\n    This class provides performance metrics for model inference, including frames per second (FPS),\n    engine used, minimum latency, maximum latency, mean latency, standard deviation of latency,\n    input image size, and device type.\n\n    Attributes:\n        fps (int): Frames per second (FPS) of the inference process.\n        engine (str): The inference engine used (e.g., \"onnx\", \"torchscript\").\n        min (float): Minimum latency in milliseconds.\n        max (float): Maximum latency in milliseconds.\n        mean (float): Mean latency in milliseconds.\n        std (float): Standard deviation of latency in milliseconds.\n        im_size (int): Input image size.\n        device (str): Device type.\n    \"\"\"\n\n    fps: int\n    engine: str\n    min: float\n    max: float\n    mean: float\n    std: float\n    im_size: int\n    device: str\n</code></pre>"},{"location":"api/ports/#focoos.ports.Metrics","title":"<code>Metrics</code>  <code>dataclass</code>","text":"<p>Collection of training and inference metrics.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass Metrics:\n    \"\"\"\n    Collection of training and inference metrics.\n    \"\"\"\n\n    infer_metrics: list[dict] = field(default_factory=list)\n    valid_metrics: list[dict] = field(default_factory=list)\n    train_metrics: list[dict] = field(default_factory=list)\n    iterations: Optional[int] = None\n    best_valid_metric: Optional[dict] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelExtension","title":"<code>ModelExtension</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported model extension.</p> Values <ul> <li>ONNX: ONNX format</li> <li>TORCHSCRIPT: TorchScript format</li> <li>WEIGHTS: Weights format</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class ModelExtension(str, Enum):\n    \"\"\"Supported model extension.\n\n    Values:\n        - ONNX: ONNX format\n        - TORCHSCRIPT: TorchScript format\n        - WEIGHTS: Weights format\n    \"\"\"\n\n    ONNX = \"onnx\"\n    TORCHSCRIPT = \"pt\"\n    WEIGHTS = \"pth\"\n\n    @classmethod\n    def from_runtime_type(cls, runtime_type: RuntimeType):\n        if runtime_type in [\n            RuntimeType.ONNX_CUDA32,\n            RuntimeType.ONNX_TRT32,\n            RuntimeType.ONNX_TRT16,\n            RuntimeType.ONNX_CPU,\n            RuntimeType.ONNX_COREML,\n        ]:\n            return cls.ONNX\n        elif runtime_type == RuntimeType.TORCHSCRIPT_32:\n            return cls.TORCHSCRIPT\n        else:\n            raise ValueError(f\"Invalid runtime type: {runtime_type}\")\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelFamily","title":"<code>ModelFamily</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumerazione delle famiglie di modelli disponibili</p> Source code in <code>focoos/ports.py</code> <pre><code>class ModelFamily(str, Enum):\n    \"\"\"Enumerazione delle famiglie di modelli disponibili\"\"\"\n\n    DETR = \"fai_detr\"\n    MASKFORMER = \"fai_mf\"\n    BISENETFORMER = \"bisenetformer\"\n    IMAGE_CLASSIFIER = \"fai_cls\"\n    RTMO = \"rtmo\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelInfo","title":"<code>ModelInfo</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DictClass</code></p> <p>Comprehensive metadata for a Focoos model.</p> <p>This dataclass encapsulates all relevant information required to identify, configure, and evaluate a model within the Focoos platform. It is used for serialization, deserialization, and programmatic access to model properties.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Human-readable name or unique identifier for the model.</p> <code>model_family</code> <code>ModelFamily</code> <p>The model's architecture family (e.g., RTDETR, M2F).</p> <code>classes</code> <code>list[str]</code> <p>List of class names that the model can detect or segment.</p> <code>im_size</code> <code>int</code> <p>Input image size (usually square, e.g., 640).</p> <code>task</code> <code>Task</code> <p>Computer vision task performed by the model (e.g., detection, segmentation).</p> <code>config</code> <code>dict</code> <p>Model-specific configuration parameters.</p> <code>ref</code> <code>Optional[str]</code> <p>Optional unique reference string for the model.</p> <code>focoos_model</code> <code>Optional[str]</code> <p>Optional Focoos base model identifier.</p> <code>status</code> <code>Optional[ModelStatus]</code> <p>Current status of the model (e.g., training, ready).</p> <code>description</code> <code>Optional[str]</code> <p>Optional human-readable description of the model.</p> <code>train_args</code> <code>Optional[TrainerArgs]</code> <p>Optional training arguments used to train the model.</p> <code>weights_uri</code> <code>Optional[str]</code> <p>Optional URI or path to the model weights.</p> <code>val_dataset</code> <code>Optional[str]</code> <p>Optional name or reference of the validation dataset.</p> <code>val_metrics</code> <code>Optional[dict]</code> <p>Optional dictionary of validation metrics (e.g., mAP, accuracy).</p> <code>focoos_version</code> <code>Optional[str]</code> <p>Optional Focoos version string.</p> <code>latency</code> <code>Optional[list[LatencyMetrics]]</code> <p>Optional list of latency measurements for different runtimes.</p> <code>updated_at</code> <code>Optional[str]</code> <p>Optional ISO timestamp of the last update.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass ModelInfo(DictClass):\n    \"\"\"\n    Comprehensive metadata for a Focoos model.\n\n    This dataclass encapsulates all relevant information required to identify, configure, and evaluate a model\n    within the Focoos platform. It is used for serialization, deserialization, and programmatic access to model\n    properties.\n\n    Attributes:\n        name (str): Human-readable name or unique identifier for the model.\n        model_family (ModelFamily): The model's architecture family (e.g., RTDETR, M2F).\n        classes (list[str]): List of class names that the model can detect or segment.\n        im_size (int): Input image size (usually square, e.g., 640).\n        task (Task): Computer vision task performed by the model (e.g., detection, segmentation).\n        config (dict): Model-specific configuration parameters.\n        ref (Optional[str]): Optional unique reference string for the model.\n        focoos_model (Optional[str]): Optional Focoos base model identifier.\n        status (Optional[ModelStatus]): Current status of the model (e.g., training, ready).\n        description (Optional[str]): Optional human-readable description of the model.\n        train_args (Optional[TrainerArgs]): Optional training arguments used to train the model.\n        weights_uri (Optional[str]): Optional URI or path to the model weights.\n        val_dataset (Optional[str]): Optional name or reference of the validation dataset.\n        val_metrics (Optional[dict]): Optional dictionary of validation metrics (e.g., mAP, accuracy).\n        focoos_version (Optional[str]): Optional Focoos version string.\n        latency (Optional[list[LatencyMetrics]]): Optional list of latency measurements for different runtimes.\n        updated_at (Optional[str]): Optional ISO timestamp of the last update.\n    \"\"\"\n\n    name: str\n    model_family: ModelFamily\n    classes: list[str]\n    im_size: int\n    task: Task\n    config: dict\n    ref: Optional[str] = None\n    focoos_model: Optional[str] = None\n    status: Optional[ModelStatus] = None\n    description: Optional[str] = None\n    train_args: Optional[TrainerArgs] = None\n    weights_uri: Optional[str] = None\n    val_dataset: Optional[str] = None\n    val_metrics: Optional[dict] = None  # TODO: Consider making metrics explicit in the future\n    focoos_version: Optional[str] = None\n    latency: Optional[list[LatencyMetrics]] = None\n    training_info: Optional[TrainingInfo] = None\n    updated_at: Optional[str] = None\n\n    @classmethod\n    def from_json(cls, data: Union[str, dict]):\n        \"\"\"\n        Load ModelInfo from a JSON file.\n\n        Args:\n            path (Optional[str]): Path to the JSON file containing model metadata.\n            data (Optional[dict]): Dictionary containing model metadata.\n\n        Returns:\n            ModelInfo: An instance of ModelInfo populated with data from the file.\n        \"\"\"\n        assert isinstance(data, dict) or isinstance(data, str), \"data must be a dictionary or a path to a JSON file\"\n        if isinstance(data, str):\n            with open(data, encoding=\"utf-8\") as f:\n                model_info_json = json.load(f)\n        else:\n            model_info_json = data\n\n        training_info = None\n        if \"training_info\" in model_info_json and model_info_json[\"training_info\"] is not None:\n            training_info = TrainingInfo(\n                **{k: v for k, v in model_info_json[\"training_info\"].items() if k in TrainingInfo.__dataclass_fields__}\n            )\n\n        model_info = cls(\n            name=model_info_json[\"name\"],\n            ref=model_info_json.get(\"ref\", None),\n            model_family=ModelFamily(model_info_json[\"model_family\"]),\n            classes=model_info_json[\"classes\"],\n            im_size=int(model_info_json[\"im_size\"]),\n            status=ModelStatus(model_info_json.get(\"status\")) if model_info_json.get(\"status\") else None,\n            task=Task(model_info_json[\"task\"]),\n            focoos_model=model_info_json.get(\"focoos_model\", None),\n            config=model_info_json[\"config\"],\n            description=model_info_json.get(\"description\", None),\n            train_args=TrainerArgs(\n                **{k: v for k, v in model_info_json[\"train_args\"].items() if k in TrainerArgs.__dataclass_fields__}\n            )\n            if \"train_args\" in model_info_json and model_info_json[\"train_args\"] is not None\n            else None,\n            weights_uri=model_info_json.get(\"weights_uri\", None),\n            val_dataset=model_info_json.get(\"val_dataset\", None),\n            latency=[LatencyMetrics(**latency) for latency in model_info_json.get(\"latency\", [])]\n            if \"latency\" in model_info_json and model_info_json[\"latency\"] is not None\n            else None,\n            updated_at=model_info_json.get(\"updated_at\", None),\n            focoos_version=model_info_json.get(\"focoos_version\", None),\n            val_metrics=model_info_json.get(\"val_metrics\", None),\n            training_info=training_info,\n        )\n        return model_info\n\n    def dump_json(self, path: str):\n        \"\"\"\n        Serialize ModelInfo to a JSON file.\n\n        Args:\n            path (str): Path where the JSON file will be saved.\n        \"\"\"\n        data = asdict(self)\n        # Note: config_class is not included; if needed, convert to string here.\n\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, ensure_ascii=False, indent=4)\n\n    def pprint(self):\n        \"\"\"\n        Pretty-print the main model information using the Focoos logger.\n        \"\"\"\n        from focoos.utils.logger import get_logger\n\n        logger = get_logger(\"model_info\")\n        logger.info(\n            f\"\"\"\n            \ud83d\udccb Name: {self.name}\n            \ud83d\udcdd Description: {self.description}\n            \ud83d\udc6a Family: {self.model_family}\n            \ud83d\udd17 Focoos Model: {self.focoos_model}\n            \ud83c\udfaf Task: {self.task}\n            \ud83c\udff7\ufe0f Classes: {self.classes}\n            \ud83d\uddbc\ufe0f Im size: {self.im_size}\n            \"\"\"\n        )\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelInfo.dump_json","title":"<code>dump_json(path)</code>","text":"<p>Serialize ModelInfo to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where the JSON file will be saved.</p> required Source code in <code>focoos/ports.py</code> <pre><code>def dump_json(self, path: str):\n    \"\"\"\n    Serialize ModelInfo to a JSON file.\n\n    Args:\n        path (str): Path where the JSON file will be saved.\n    \"\"\"\n    data = asdict(self)\n    # Note: config_class is not included; if needed, convert to string here.\n\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, ensure_ascii=False, indent=4)\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelInfo.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Load ModelInfo from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Path to the JSON file containing model metadata.</p> required <code>data</code> <code>Optional[dict]</code> <p>Dictionary containing model metadata.</p> required <p>Returns:</p> Name Type Description <code>ModelInfo</code> <p>An instance of ModelInfo populated with data from the file.</p> Source code in <code>focoos/ports.py</code> <pre><code>@classmethod\ndef from_json(cls, data: Union[str, dict]):\n    \"\"\"\n    Load ModelInfo from a JSON file.\n\n    Args:\n        path (Optional[str]): Path to the JSON file containing model metadata.\n        data (Optional[dict]): Dictionary containing model metadata.\n\n    Returns:\n        ModelInfo: An instance of ModelInfo populated with data from the file.\n    \"\"\"\n    assert isinstance(data, dict) or isinstance(data, str), \"data must be a dictionary or a path to a JSON file\"\n    if isinstance(data, str):\n        with open(data, encoding=\"utf-8\") as f:\n            model_info_json = json.load(f)\n    else:\n        model_info_json = data\n\n    training_info = None\n    if \"training_info\" in model_info_json and model_info_json[\"training_info\"] is not None:\n        training_info = TrainingInfo(\n            **{k: v for k, v in model_info_json[\"training_info\"].items() if k in TrainingInfo.__dataclass_fields__}\n        )\n\n    model_info = cls(\n        name=model_info_json[\"name\"],\n        ref=model_info_json.get(\"ref\", None),\n        model_family=ModelFamily(model_info_json[\"model_family\"]),\n        classes=model_info_json[\"classes\"],\n        im_size=int(model_info_json[\"im_size\"]),\n        status=ModelStatus(model_info_json.get(\"status\")) if model_info_json.get(\"status\") else None,\n        task=Task(model_info_json[\"task\"]),\n        focoos_model=model_info_json.get(\"focoos_model\", None),\n        config=model_info_json[\"config\"],\n        description=model_info_json.get(\"description\", None),\n        train_args=TrainerArgs(\n            **{k: v for k, v in model_info_json[\"train_args\"].items() if k in TrainerArgs.__dataclass_fields__}\n        )\n        if \"train_args\" in model_info_json and model_info_json[\"train_args\"] is not None\n        else None,\n        weights_uri=model_info_json.get(\"weights_uri\", None),\n        val_dataset=model_info_json.get(\"val_dataset\", None),\n        latency=[LatencyMetrics(**latency) for latency in model_info_json.get(\"latency\", [])]\n        if \"latency\" in model_info_json and model_info_json[\"latency\"] is not None\n        else None,\n        updated_at=model_info_json.get(\"updated_at\", None),\n        focoos_version=model_info_json.get(\"focoos_version\", None),\n        val_metrics=model_info_json.get(\"val_metrics\", None),\n        training_info=training_info,\n    )\n    return model_info\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelInfo.pprint","title":"<code>pprint()</code>","text":"<p>Pretty-print the main model information using the Focoos logger.</p> Source code in <code>focoos/ports.py</code> <pre><code>def pprint(self):\n    \"\"\"\n    Pretty-print the main model information using the Focoos logger.\n    \"\"\"\n    from focoos.utils.logger import get_logger\n\n    logger = get_logger(\"model_info\")\n    logger.info(\n        f\"\"\"\n        \ud83d\udccb Name: {self.name}\n        \ud83d\udcdd Description: {self.description}\n        \ud83d\udc6a Family: {self.model_family}\n        \ud83d\udd17 Focoos Model: {self.focoos_model}\n        \ud83c\udfaf Task: {self.task}\n        \ud83c\udff7\ufe0f Classes: {self.classes}\n        \ud83d\uddbc\ufe0f Im size: {self.im_size}\n        \"\"\"\n    )\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelNotFound","title":"<code>ModelNotFound</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when a requested model is not found.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ModelNotFound(Exception):\n    \"\"\"Exception raised when a requested model is not found.\"\"\"\n\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelOutput","title":"<code>ModelOutput</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DictClass</code></p> <p>Model output base container.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass ModelOutput(DictClass):\n    \"\"\"Model output base container.\"\"\"\n\n    loss: Optional[dict]\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelPreview","title":"<code>ModelPreview</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Preview information for a Focoos model.</p> <p>This class provides a lightweight preview of model information in the Focoos platform, containing essential details like reference ID, name, task type, and status.</p> <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>Unique reference ID for the model.</p> <code>name</code> <code>str</code> <p>Human-readable name of the model.</p> <code>task</code> <code>FocoosTask</code> <p>The computer vision task this model is designed for.</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the model's purpose or capabilities.</p> <code>status</code> <code>ModelStatus</code> <p>Current status of the model (e.g., training, ready, failed).</p> <code>focoos_model</code> <code>str</code> <p>The base model architecture identifier.</p> Source code in <code>focoos/ports.py</code> <pre><code>class ModelPreview(PydanticBase):\n    \"\"\"Preview information for a Focoos model.\n\n    This class provides a lightweight preview of model information in the Focoos platform,\n    containing essential details like reference ID, name, task type, and status.\n\n    Attributes:\n        ref (str): Unique reference ID for the model.\n        name (str): Human-readable name of the model.\n        task (FocoosTask): The computer vision task this model is designed for.\n        description (Optional[str]): Optional description of the model's purpose or capabilities.\n        status (ModelStatus): Current status of the model (e.g., training, ready, failed).\n        focoos_model (str): The base model architecture identifier.\n    \"\"\"\n\n    ref: str\n    name: str\n    task: Task\n    description: Optional[str] = None\n    status: ModelStatus\n    focoos_model: str\n</code></pre>"},{"location":"api/ports/#focoos.ports.ModelStatus","title":"<code>ModelStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Status of a Focoos model during its lifecycle.</p> Values <ul> <li>CREATED: Model has been created</li> <li>TRAINING_STARTING: Training is about to start</li> <li>TRAINING_RUNNING: Training is in progress</li> <li>TRAINING_ERROR: Training encountered an error</li> <li>TRAINING_COMPLETED: Training finished successfully</li> <li>TRAINING_STOPPED: Training was stopped</li> <li>DEPLOYED: Model is deployed</li> <li>DEPLOY_ERROR: Deployment encountered an error</li> </ul> Example <pre><code>from focoos import Focoos\n\nfocoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\nmodel = focoos.get_remote_model(\"my-model\")\n\nif model.status == ModelStatus.DEPLOYED:\n    print(\"Model is deployed and ready for inference\")\nelif model.status == ModelStatus.TRAINING_RUNNING:\n    print(\"Model is currently training\")\nelif model.status == ModelStatus.TRAINING_ERROR:\n    print(\"Model training encountered an error\")\n</code></pre> Source code in <code>focoos/ports.py</code> <pre><code>class ModelStatus(str, Enum):\n    \"\"\"Status of a Focoos model during its lifecycle.\n\n    Values:\n        - CREATED: Model has been created\n        - TRAINING_STARTING: Training is about to start\n        - TRAINING_RUNNING: Training is in progress\n        - TRAINING_ERROR: Training encountered an error\n        - TRAINING_COMPLETED: Training finished successfully\n        - TRAINING_STOPPED: Training was stopped\n        - DEPLOYED: Model is deployed\n        - DEPLOY_ERROR: Deployment encountered an error\n\n    Example:\n        ```python\n        from focoos import Focoos\n\n        focoos = Focoos(api_key=\"&lt;YOUR-API-KEY&gt;\")\n        model = focoos.get_remote_model(\"my-model\")\n\n        if model.status == ModelStatus.DEPLOYED:\n            print(\"Model is deployed and ready for inference\")\n        elif model.status == ModelStatus.TRAINING_RUNNING:\n            print(\"Model is currently training\")\n        elif model.status == ModelStatus.TRAINING_ERROR:\n            print(\"Model training encountered an error\")\n        ```\n    \"\"\"\n\n    CREATED = \"CREATED\"\n    TRAINING_STARTING = \"TRAINING_STARTING\"\n    TRAINING_RUNNING = \"TRAINING_RUNNING\"\n    TRAINING_ERROR = \"TRAINING_ERROR\"\n    TRAINING_COMPLETED = \"TRAINING_COMPLETED\"\n    TRAINING_STOPPED = \"TRAINING_STOPPED\"\n    DEPLOYED = \"DEPLOYED\"\n    DEPLOY_ERROR = \"DEPLOY_ERROR\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.OnnxRuntimeOpts","title":"<code>OnnxRuntimeOpts</code>  <code>dataclass</code>","text":"<p>ONNX runtime configuration options.</p> <p>This class provides configuration options for the ONNX runtime used for model inference.</p> <p>Attributes:</p> Name Type Description <code>fp16</code> <code>Optional[bool]</code> <p>Enable FP16 precision. Default is False.</p> <code>cuda</code> <code>Optional[bool]</code> <p>Enable CUDA acceleration for GPU inference. Default is False.</p> <code>vino</code> <code>Optional[bool]</code> <p>Enable OpenVINO acceleration for Intel hardware. Default is False.</p> <code>verbose</code> <code>Optional[bool]</code> <p>Enable verbose logging during inference. Default is False.</p> <code>trt</code> <code>Optional[bool]</code> <p>Enable TensorRT acceleration for NVIDIA GPUs. Default is False.</p> <code>coreml</code> <code>Optional[bool]</code> <p>Enable CoreML acceleration for Apple hardware. Default is False.</p> <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations to run before benchmarking. Default is 0.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass OnnxRuntimeOpts:\n    \"\"\"ONNX runtime configuration options.\n\n    This class provides configuration options for the ONNX runtime used for model inference.\n\n    Attributes:\n        fp16 (Optional[bool]): Enable FP16 precision. Default is False.\n        cuda (Optional[bool]): Enable CUDA acceleration for GPU inference. Default is False.\n        vino (Optional[bool]): Enable OpenVINO acceleration for Intel hardware. Default is False.\n        verbose (Optional[bool]): Enable verbose logging during inference. Default is False.\n        trt (Optional[bool]): Enable TensorRT acceleration for NVIDIA GPUs. Default is False.\n        coreml (Optional[bool]): Enable CoreML acceleration for Apple hardware. Default is False.\n        warmup_iter (int): Number of warmup iterations to run before benchmarking. Default is 0.\n\n    \"\"\"\n\n    fp16: Optional[bool] = False\n    cuda: Optional[bool] = False\n    vino: Optional[bool] = False\n    verbose: Optional[bool] = False\n    trt: Optional[bool] = False\n    coreml: Optional[bool] = False\n    warmup_iter: int = 0\n</code></pre>"},{"location":"api/ports/#focoos.ports.Quotas","title":"<code>Quotas</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Usage quotas and limits for a user account.</p> <p>Attributes:</p> Name Type Description <code>total_inferences</code> <code>int</code> <p>Total number of inferences allowed.</p> <code>max_inferences</code> <code>int</code> <p>Maximum number of inferences allowed.</p> <code>used_storage_gb</code> <code>float</code> <p>Used storage in gigabytes.</p> <code>max_storage_gb</code> <code>float</code> <p>Maximum storage in gigabytes.</p> <code>active_training_jobs</code> <code>list[str]</code> <p>List of active training job IDs.</p> <code>max_active_training_jobs</code> <code>int</code> <p>Maximum number of active training jobs allowed.</p> Source code in <code>focoos/ports.py</code> <pre><code>class Quotas(PydanticBase):\n    \"\"\"Usage quotas and limits for a user account.\n\n    Attributes:\n        total_inferences (int): Total number of inferences allowed.\n        max_inferences (int): Maximum number of inferences allowed.\n        used_storage_gb (float): Used storage in gigabytes.\n        max_storage_gb (float): Maximum storage in gigabytes.\n        active_training_jobs (list[str]): List of active training job IDs.\n        max_active_training_jobs (int): Maximum number of active training jobs allowed.\n    \"\"\"\n\n    # INFERENCE\n    total_inferences: int\n    max_inferences: int\n    # STORAGE\n    used_storage_gb: float\n    max_storage_gb: float\n    # TRAINING\n    active_training_jobs: list[str]\n    max_active_training_jobs: int\n\n    # ML_G4DN_XLARGE TRAINING HOURS\n    used_mlg4dnxlarge_training_jobs_hours: float\n    max_mlg4dnxlarge_training_jobs_hours: float\n</code></pre>"},{"location":"api/ports/#focoos.ports.RemoteModelInfo","title":"<code>RemoteModelInfo</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>Complete metadata for a Focoos model.</p> <p>This class contains comprehensive information about a model in the Focoos platform, including its identification, configuration, performance metrics, and training details.</p> <p>Attributes:</p> Name Type Description <code>ref</code> <code>str</code> <p>Unique reference ID for the model.</p> <code>name</code> <code>str</code> <p>Human-readable name of the model.</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the model's purpose or capabilities.</p> <code>owner_ref</code> <code>str</code> <p>Reference ID of the model owner.</p> <code>focoos_model</code> <code>str</code> <p>The base model architecture used.</p> <code>task</code> <code>FocoosTask</code> <p>The task type the model is designed for (e.g., DETECTION, SEMSEG).</p> <code>created_at</code> <code>datetime</code> <p>Timestamp when the model was created.</p> <code>updated_at</code> <code>datetime</code> <p>Timestamp when the model was last updated.</p> <code>status</code> <code>ModelStatus</code> <p>Current status of the model (e.g., TRAINING, DEPLOYED).</p> <code>metrics</code> <code>Optional[dict]</code> <p>Performance metrics of the model (e.g., mAP, accuracy).</p> <code>latencies</code> <code>Optional[list[dict]]</code> <p>Inference latency measurements across different configurations.</p> <code>classes</code> <code>Optional[list[str]]</code> <p>List of class names the model can detect or segment.</p> <code>im_size</code> <code>Optional[int]</code> <p>Input image size the model expects.</p> <code>training_info</code> <code>Optional[TrainingInfo]</code> <p>Information about the training process.</p> <code>location</code> <code>Optional[str]</code> <p>Storage location of the model.</p> <code>dataset</code> <code>Optional[DatasetPreview]</code> <p>Information about the dataset used for training.</p> Source code in <code>focoos/ports.py</code> <pre><code>class RemoteModelInfo(PydanticBase):\n    \"\"\"Complete metadata for a Focoos model.\n\n    This class contains comprehensive information about a model in the Focoos platform,\n    including its identification, configuration, performance metrics, and training details.\n\n    Attributes:\n        ref (str): Unique reference ID for the model.\n        name (str): Human-readable name of the model.\n        description (Optional[str]): Optional description of the model's purpose or capabilities.\n        owner_ref (str): Reference ID of the model owner.\n        focoos_model (str): The base model architecture used.\n        task (FocoosTask): The task type the model is designed for (e.g., DETECTION, SEMSEG).\n        created_at (datetime): Timestamp when the model was created.\n        updated_at (datetime): Timestamp when the model was last updated.\n        status (ModelStatus): Current status of the model (e.g., TRAINING, DEPLOYED).\n        metrics (Optional[dict]): Performance metrics of the model (e.g., mAP, accuracy).\n        latencies (Optional[list[dict]]): Inference latency measurements across different configurations.\n        classes (Optional[list[str]]): List of class names the model can detect or segment.\n        im_size (Optional[int]): Input image size the model expects.\n        training_info (Optional[TrainingInfo]): Information about the training process.\n        location (Optional[str]): Storage location of the model.\n        dataset (Optional[DatasetPreview]): Information about the dataset used for training.\n    \"\"\"\n\n    ref: str\n    name: str\n    description: Optional[str] = None\n    is_managed: bool\n    owner_ref: str\n    focoos_model: str\n    config: Optional[dict] = None\n    task: Task\n    created_at: datetime\n    updated_at: datetime\n    status: ModelStatus\n    model_family: Optional[str] = None\n    metrics: Optional[dict] = None\n    classes: Optional[list[str]] = None\n    im_size: Optional[int] = None\n    training_info: Optional[TrainingInfo] = None\n    dataset: Optional[DatasetPreview] = None\n    hyperparameters: Optional[dict] = None\n    focoos_version: Optional[str] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.RuntimeType","title":"<code>RuntimeType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Available runtime configurations for model inference.</p> Values <ul> <li>ONNX_CUDA32: ONNX with CUDA FP32</li> <li>ONNX_TRT32: ONNX with TensorRT FP32</li> <li>ONNX_TRT16: ONNX with TensorRT FP16</li> <li>ONNX_CPU: ONNX on CPU</li> <li>ONNX_COREML: ONNX with CoreML</li> <li>TORCHSCRIPT_32: TorchScript FP32</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class RuntimeType(str, Enum):\n    \"\"\"Available runtime configurations for model inference.\n\n    Values:\n        - ONNX_CUDA32: ONNX with CUDA FP32\n        - ONNX_TRT32: ONNX with TensorRT FP32\n        - ONNX_TRT16: ONNX with TensorRT FP16\n        - ONNX_CPU: ONNX on CPU\n        - ONNX_COREML: ONNX with CoreML\n        - TORCHSCRIPT_32: TorchScript FP32\n\n    \"\"\"\n\n    ONNX_CUDA32 = \"onnx_cuda32\"\n    ONNX_TRT32 = \"onnx_trt32\"\n    ONNX_TRT16 = \"onnx_trt16\"\n    ONNX_CPU = \"onnx_cpu\"\n    ONNX_COREML = \"onnx_coreml\"\n    TORCHSCRIPT_32 = \"torchscript_32\"\n\n    def __str__(self) -&gt; str:\n        return self.value\n\n    def __repr__(self) -&gt; str:\n        return self.value\n\n    def to_export_format(self) -&gt; ExportFormat:\n        if self == RuntimeType.TORCHSCRIPT_32:\n            return ExportFormat.TORCHSCRIPT\n        else:\n            return ExportFormat.ONNX\n</code></pre>"},{"location":"api/ports/#focoos.ports.SystemInfo","title":"<code>SystemInfo</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>System information including hardware and software details.</p> Source code in <code>focoos/ports.py</code> <pre><code>class SystemInfo(PydanticBase):\n    \"\"\"System information including hardware and software details.\"\"\"\n\n    focoos_host: Optional[str] = None\n    focoos_version: Optional[str] = None\n    python_version: Optional[str] = None\n    system: Optional[str] = None\n    system_name: Optional[str] = None\n    cpu_type: Optional[str] = None\n    cpu_cores: Optional[int] = None\n    memory_gb: Optional[float] = None\n    memory_used_percentage: Optional[float] = None\n    available_onnx_providers: Optional[list[str]] = None\n    disk_space_total_gb: Optional[float] = None\n    disk_space_used_percentage: Optional[float] = None\n    pytorch_info: Optional[str] = None\n    gpu_info: Optional[GPUInfo] = None\n    packages_versions: Optional[dict[str, str]] = None\n    environment: Optional[dict[str, str]] = None\n\n    def pprint(self, level: Literal[\"INFO\", \"DEBUG\"] = \"DEBUG\"):\n        \"\"\"Pretty print the system info.\"\"\"\n        from focoos.utils.logger import get_logger\n\n        logger = get_logger(\"SystemInfo\", level=level)\n\n        output_lines = [\"\\n================ \ud83d\udd0d SYSTEM INFO \ud83d\udd0d ====================\"]\n        model_data = self.model_dump()\n\n        if \"focoos_host\" in model_data and \"focoos_version\" in model_data:\n            output_lines.append(f\"focoos: {model_data.get('focoos_host')} (v{model_data.get('focoos_version')})\")\n            model_data.pop(\"focoos_host\", None)\n            model_data.pop(\"focoos_version\", None)\n\n        if \"system\" in model_data and \"system_name\" in model_data:\n            output_lines.append(f\"system: {model_data.get('system')} ({model_data.get('system_name')})\")\n            model_data.pop(\"system\", None)\n            model_data.pop(\"system_name\", None)\n\n        if \"cpu_type\" in model_data and \"cpu_cores\" in model_data:\n            output_lines.append(f\"cpu: {model_data.get('cpu_type')} ({model_data.get('cpu_cores')} cores)\")\n            model_data.pop(\"cpu_type\", None)\n            model_data.pop(\"cpu_cores\", None)\n\n        if \"memory_gb\" in model_data and \"memory_used_percentage\" in model_data:\n            output_lines.append(\n                f\"memory_gb: {model_data.get('memory_gb')} ({model_data.get('memory_used_percentage')}% used)\"\n            )\n            model_data.pop(\"memory_gb\", None)\n            model_data.pop(\"memory_used_percentage\", None)\n\n        if \"disk_space_total_gb\" in model_data and \"disk_space_used_percentage\" in model_data:\n            output_lines.append(\n                f\"disk_space_total_gb: {model_data.get('disk_space_total_gb')} ({model_data.get('disk_space_used_percentage')}% used)\"\n            )\n            model_data.pop(\"disk_space_total_gb\", None)\n            model_data.pop(\"disk_space_used_percentage\", None)\n\n        for key, value in model_data.items():\n            if key == \"gpu_info\" and value is not None:\n                output_lines.append(f\"{key}:\")\n                output_lines.append(f\"  - gpu_count: {value.get('gpu_count')}\")\n                output_lines.append(f\"  - total_memory_gb: {value.get('total_gpu_memory_gb')} GB\")\n                output_lines.append(f\"  - gpu_driver: {value.get('gpu_driver')}\")\n                output_lines.append(f\"  - gpu_cuda_version: {value.get('gpu_cuda_version')}\")\n                output_lines.append(f\"  - mps_available: {value.get('mps_available')}\")\n                if value.get(\"devices\"):\n                    output_lines.append(\"  - devices:\")\n                    for device in value.get(\"devices\", []):\n                        gpu_memory_used = (\n                            f\"{device.get('gpu_memory_used_percentage')}%\"\n                            if device.get(\"gpu_memory_used_percentage\") is not None\n                            else \"N/A\"\n                        )\n                        gpu_load = (\n                            f\"{device.get('gpu_load_percentage')}%\"\n                            if device.get(\"gpu_load_percentage\") is not None\n                            else \"N/A\"\n                        )\n                        gpu_memory_total = (\n                            f\"{device.get('gpu_memory_total_gb')} GB\"\n                            if device.get(\"gpu_memory_total_gb\") is not None\n                            else \"N/A\"\n                        )\n\n                        output_lines.append(\n                            f\"    - GPU {device.get('gpu_id')}: {device.get('gpu_name')}, Memory: {gpu_memory_total} ({gpu_memory_used} used), Load: {gpu_load}\"\n                        )\n            elif isinstance(value, list):\n                output_lines.append(f\"{key}: {value}\")\n            elif isinstance(value, dict) and key == \"packages_versions\":  # Special formatting for packages_versions\n                output_lines.append(f\"{key}:\")\n                for pkg_name, pkg_version in value.items():\n                    output_lines.append(f\"  - {pkg_name}: {pkg_version}\")\n            elif isinstance(value, dict) and key == \"environment\":  # Special formatting for environment\n                output_lines.append(f\"{key}:\")\n                for env_key, env_value in value.items():\n                    output_lines.append(f\"  - {env_key}: {env_value}\")\n            else:\n                output_lines.append(f\"{key}: {value}\")\n        output_lines.append(\"================================================\")\n\n        logger.info(\"\\n\".join(output_lines))\n</code></pre>"},{"location":"api/ports/#focoos.ports.SystemInfo.pprint","title":"<code>pprint(level='DEBUG')</code>","text":"<p>Pretty print the system info.</p> Source code in <code>focoos/ports.py</code> <pre><code>def pprint(self, level: Literal[\"INFO\", \"DEBUG\"] = \"DEBUG\"):\n    \"\"\"Pretty print the system info.\"\"\"\n    from focoos.utils.logger import get_logger\n\n    logger = get_logger(\"SystemInfo\", level=level)\n\n    output_lines = [\"\\n================ \ud83d\udd0d SYSTEM INFO \ud83d\udd0d ====================\"]\n    model_data = self.model_dump()\n\n    if \"focoos_host\" in model_data and \"focoos_version\" in model_data:\n        output_lines.append(f\"focoos: {model_data.get('focoos_host')} (v{model_data.get('focoos_version')})\")\n        model_data.pop(\"focoos_host\", None)\n        model_data.pop(\"focoos_version\", None)\n\n    if \"system\" in model_data and \"system_name\" in model_data:\n        output_lines.append(f\"system: {model_data.get('system')} ({model_data.get('system_name')})\")\n        model_data.pop(\"system\", None)\n        model_data.pop(\"system_name\", None)\n\n    if \"cpu_type\" in model_data and \"cpu_cores\" in model_data:\n        output_lines.append(f\"cpu: {model_data.get('cpu_type')} ({model_data.get('cpu_cores')} cores)\")\n        model_data.pop(\"cpu_type\", None)\n        model_data.pop(\"cpu_cores\", None)\n\n    if \"memory_gb\" in model_data and \"memory_used_percentage\" in model_data:\n        output_lines.append(\n            f\"memory_gb: {model_data.get('memory_gb')} ({model_data.get('memory_used_percentage')}% used)\"\n        )\n        model_data.pop(\"memory_gb\", None)\n        model_data.pop(\"memory_used_percentage\", None)\n\n    if \"disk_space_total_gb\" in model_data and \"disk_space_used_percentage\" in model_data:\n        output_lines.append(\n            f\"disk_space_total_gb: {model_data.get('disk_space_total_gb')} ({model_data.get('disk_space_used_percentage')}% used)\"\n        )\n        model_data.pop(\"disk_space_total_gb\", None)\n        model_data.pop(\"disk_space_used_percentage\", None)\n\n    for key, value in model_data.items():\n        if key == \"gpu_info\" and value is not None:\n            output_lines.append(f\"{key}:\")\n            output_lines.append(f\"  - gpu_count: {value.get('gpu_count')}\")\n            output_lines.append(f\"  - total_memory_gb: {value.get('total_gpu_memory_gb')} GB\")\n            output_lines.append(f\"  - gpu_driver: {value.get('gpu_driver')}\")\n            output_lines.append(f\"  - gpu_cuda_version: {value.get('gpu_cuda_version')}\")\n            output_lines.append(f\"  - mps_available: {value.get('mps_available')}\")\n            if value.get(\"devices\"):\n                output_lines.append(\"  - devices:\")\n                for device in value.get(\"devices\", []):\n                    gpu_memory_used = (\n                        f\"{device.get('gpu_memory_used_percentage')}%\"\n                        if device.get(\"gpu_memory_used_percentage\") is not None\n                        else \"N/A\"\n                    )\n                    gpu_load = (\n                        f\"{device.get('gpu_load_percentage')}%\"\n                        if device.get(\"gpu_load_percentage\") is not None\n                        else \"N/A\"\n                    )\n                    gpu_memory_total = (\n                        f\"{device.get('gpu_memory_total_gb')} GB\"\n                        if device.get(\"gpu_memory_total_gb\") is not None\n                        else \"N/A\"\n                    )\n\n                    output_lines.append(\n                        f\"    - GPU {device.get('gpu_id')}: {device.get('gpu_name')}, Memory: {gpu_memory_total} ({gpu_memory_used} used), Load: {gpu_load}\"\n                    )\n        elif isinstance(value, list):\n            output_lines.append(f\"{key}: {value}\")\n        elif isinstance(value, dict) and key == \"packages_versions\":  # Special formatting for packages_versions\n            output_lines.append(f\"{key}:\")\n            for pkg_name, pkg_version in value.items():\n                output_lines.append(f\"  - {pkg_name}: {pkg_version}\")\n        elif isinstance(value, dict) and key == \"environment\":  # Special formatting for environment\n            output_lines.append(f\"{key}:\")\n            for env_key, env_value in value.items():\n                output_lines.append(f\"  - {env_key}: {env_value}\")\n        else:\n            output_lines.append(f\"{key}: {value}\")\n    output_lines.append(\"================================================\")\n\n    logger.info(\"\\n\".join(output_lines))\n</code></pre>"},{"location":"api/ports/#focoos.ports.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of computer vision tasks supported by Focoos.</p> Values <ul> <li>DETECTION: Object detection</li> <li>SEMSEG: Semantic segmentation</li> <li>INSTANCE_SEGMENTATION: Instance segmentation</li> <li>CLASSIFICATION: Image classification</li> <li>KEYPOINT: Keypoint detection</li> </ul> Source code in <code>focoos/ports.py</code> <pre><code>class Task(str, Enum):\n    \"\"\"Types of computer vision tasks supported by Focoos.\n\n    Values:\n        - DETECTION: Object detection\n        - SEMSEG: Semantic segmentation\n        - INSTANCE_SEGMENTATION: Instance segmentation\n        - CLASSIFICATION: Image classification\n        - KEYPOINT: Keypoint detection\n    \"\"\"\n\n    DETECTION = \"detection\"\n    SEMSEG = \"semseg\"\n    INSTANCE_SEGMENTATION = \"instseg\"\n    CLASSIFICATION = \"classification\"\n    KEYPOINT = \"keypoint\"\n</code></pre>"},{"location":"api/ports/#focoos.ports.TorchscriptRuntimeOpts","title":"<code>TorchscriptRuntimeOpts</code>  <code>dataclass</code>","text":"<p>TorchScript runtime configuration options.</p> <p>This class provides configuration options for the TorchScript runtime used for model inference.</p> <p>Attributes:</p> Name Type Description <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations to run before benchmarking. Default is 0.</p> <code>optimize_for_inference</code> <code>bool</code> <p>Enable inference optimizations. Default is True.</p> <code>set_fusion_strategy</code> <code>bool</code> <p>Enable operator fusion. Default is True.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass TorchscriptRuntimeOpts:\n    \"\"\"TorchScript runtime configuration options.\n\n    This class provides configuration options for the TorchScript runtime used for model inference.\n\n    Attributes:\n        warmup_iter (int): Number of warmup iterations to run before benchmarking. Default is 0.\n        optimize_for_inference (bool): Enable inference optimizations. Default is True.\n        set_fusion_strategy (bool): Enable operator fusion. Default is True.\n    \"\"\"\n\n    warmup_iter: int = 0\n    optimize_for_inference: bool = True\n    set_fusion_strategy: bool = True\n</code></pre>"},{"location":"api/ports/#focoos.ports.TrainerArgs","title":"<code>TrainerArgs</code>  <code>dataclass</code>","text":"<p>Configuration class for unified model training.</p> <p>Attributes:</p> Name Type Description <code>run_name</code> <code>str</code> <p>Name of the training run</p> <code>output_dir</code> <code>str</code> <p>Directory to save outputs</p> <code>ckpt_dir</code> <code>Optional[str]</code> <p>Directory for checkpoints</p> <code>init_checkpoint</code> <code>Optional[str]</code> <p>Initial checkpoint to load</p> <code>resume</code> <code>bool</code> <p>Whether to resume from checkpoint</p> <code>num_gpus</code> <code>int</code> <p>Number of GPUs to use</p> <code>device</code> <code>str</code> <p>Device to use (cuda/cpu)</p> <code>workers</code> <code>int</code> <p>Number of data loading workers</p> <code>amp_enabled</code> <code>bool</code> <p>Whether to use automatic mixed precision</p> <code>ddp_broadcast_buffers</code> <code>bool</code> <p>Whether to broadcast buffers in DDP</p> <code>ddp_find_unused</code> <code>bool</code> <p>Whether to find unused parameters in DDP</p> <code>checkpointer_period</code> <code>int</code> <p>How often to save checkpoints</p> <code>checkpointer_max_to_keep</code> <code>int</code> <p>Maximum checkpoints to keep</p> <code>eval_period</code> <code>int</code> <p>How often to evaluate</p> <code>log_period</code> <code>int</code> <p>How often to log</p> <code>vis_period</code> <code>int</code> <p>How often to visualize</p> <code>samples</code> <code>int</code> <p>Number of samples for visualization</p> <code>seed</code> <code>int</code> <p>Random seed</p> <code>early_stop</code> <code>bool</code> <p>Whether to use early stopping</p> <code>patience</code> <code>int</code> <p>Early stopping patience</p> <code>ema_enabled</code> <code>bool</code> <p>Whether to use EMA</p> <code>ema_decay</code> <code>float</code> <p>EMA decay rate</p> <code>ema_warmup</code> <code>int</code> <p>EMA warmup period</p> <code>learning_rate</code> <code>float</code> <p>Base learning rate</p> <code>weight_decay</code> <code>float</code> <p>Weight decay</p> <code>max_iters</code> <code>int</code> <p>Maximum training iterations</p> <code>batch_size</code> <code>int</code> <p>Batch size</p> <code>scheduler</code> <code>str</code> <p>Learning rate scheduler type</p> <code>scheduler_extra</code> <code>Optional[dict]</code> <p>Extra scheduler parameters</p> <code>optimizer</code> <code>str</code> <p>Optimizer type</p> <code>optimizer_extra</code> <code>Optional[dict]</code> <p>Extra optimizer parameters</p> <code>weight_decay_norm</code> <code>float</code> <p>Weight decay for normalization layers</p> <code>weight_decay_embed</code> <code>float</code> <p>Weight decay for embeddings</p> <code>backbone_multiplier</code> <code>float</code> <p>Learning rate multiplier for backbone</p> <code>decoder_multiplier</code> <code>float</code> <p>Learning rate multiplier for decoder</p> <code>head_multiplier</code> <code>float</code> <p>Learning rate multiplier for head</p> <code>freeze_bn</code> <code>bool</code> <p>Whether to freeze batch norm</p> <code>clip_gradients</code> <code>float</code> <p>Gradient clipping value</p> <code>size_divisibility</code> <code>int</code> <p>Input size divisibility requirement</p> <code>gather_metric_period</code> <code>int</code> <p>How often to gather metrics</p> <code>zero_grad_before_forward</code> <code>bool</code> <p>Whether to zero gradients before forward pass</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass TrainerArgs:\n    \"\"\"Configuration class for unified model training.\n\n    Attributes:\n        run_name (str): Name of the training run\n        output_dir (str): Directory to save outputs\n        ckpt_dir (Optional[str]): Directory for checkpoints\n        init_checkpoint (Optional[str]): Initial checkpoint to load\n        resume (bool): Whether to resume from checkpoint\n        num_gpus (int): Number of GPUs to use\n        device (str): Device to use (cuda/cpu)\n        workers (int): Number of data loading workers\n        amp_enabled (bool): Whether to use automatic mixed precision\n        ddp_broadcast_buffers (bool): Whether to broadcast buffers in DDP\n        ddp_find_unused (bool): Whether to find unused parameters in DDP\n        checkpointer_period (int): How often to save checkpoints\n        checkpointer_max_to_keep (int): Maximum checkpoints to keep\n        eval_period (int): How often to evaluate\n        log_period (int): How often to log\n        vis_period (int): How often to visualize\n        samples (int): Number of samples for visualization\n        seed (int): Random seed\n        early_stop (bool): Whether to use early stopping\n        patience (int): Early stopping patience\n        ema_enabled (bool): Whether to use EMA\n        ema_decay (float): EMA decay rate\n        ema_warmup (int): EMA warmup period\n        learning_rate (float): Base learning rate\n        weight_decay (float): Weight decay\n        max_iters (int): Maximum training iterations\n        batch_size (int): Batch size\n        scheduler (str): Learning rate scheduler type\n        scheduler_extra (Optional[dict]): Extra scheduler parameters\n        optimizer (str): Optimizer type\n        optimizer_extra (Optional[dict]): Extra optimizer parameters\n        weight_decay_norm (float): Weight decay for normalization layers\n        weight_decay_embed (float): Weight decay for embeddings\n        backbone_multiplier (float): Learning rate multiplier for backbone\n        decoder_multiplier (float): Learning rate multiplier for decoder\n        head_multiplier (float): Learning rate multiplier for head\n        freeze_bn (bool): Whether to freeze batch norm\n        clip_gradients (float): Gradient clipping value\n        size_divisibility (int): Input size divisibility requirement\n        gather_metric_period (int): How often to gather metrics\n        zero_grad_before_forward (bool): Whether to zero gradients before forward pass\n    \"\"\"\n\n    run_name: str\n    output_dir: str = MODELS_DIR\n    ckpt_dir: Optional[str] = None\n    init_checkpoint: Optional[str] = None\n    resume: bool = False\n    # Logistics params\n    num_gpus: int = get_gpus_count()\n    device: str = \"cuda\"\n    workers: int = 4\n    amp_enabled: bool = True\n    ddp_broadcast_buffers: bool = False\n    ddp_find_unused: bool = True\n    checkpointer_period: int = 1000\n    checkpointer_max_to_keep: int = 1\n    eval_period: int = 200\n    log_period: int = 20\n    samples: int = 9\n    seed: int = 42\n    early_stop: bool = True\n    patience: int = 10\n    # EMA\n    ema_enabled: bool = False\n    ema_decay: float = 0.999\n    ema_warmup: int = 2000\n    # Hyperparameters\n    learning_rate: float = 5e-4\n    weight_decay: float = 0.02\n    max_iters: int = 3000\n    batch_size: int = 16\n    scheduler: SchedulerType = \"MULTISTEP\"\n    scheduler_extra: Optional[dict] = None\n    optimizer: OptimizerType = \"ADAMW\"\n    optimizer_extra: Optional[dict] = None\n    weight_decay_norm: float = 0.0\n    weight_decay_embed: float = 0.0\n    backbone_multiplier: float = 0.1\n    decoder_multiplier: float = 1.0\n    head_multiplier: float = 1.0\n    freeze_bn: bool = False\n    clip_gradients: float = 0.1\n    size_divisibility: int = 0\n    # Training specific\n    gather_metric_period: int = 1\n    zero_grad_before_forward: bool = False\n\n    # Sync to hub\n    sync_to_hub: bool = False\n</code></pre>"},{"location":"api/ports/#focoos.ports.TrainingInfo","title":"<code>TrainingInfo</code>  <code>dataclass</code>","text":"<p>Information about a model's training process.</p> <p>This class contains details about the training job configuration, status, and timing.</p> <p>Attributes:</p> Name Type Description <code>algorithm_name</code> <code>Optional[str]</code> <p>The name of the training algorithm used.</p> <code>instance_type</code> <code>Optional[str]</code> <p>The compute instance type used for training.</p> <code>volume_size</code> <code>Optional[int]</code> <p>The storage volume size in GB allocated for the training job.</p> <code>max_runtime_in_seconds</code> <code>Optional[int]</code> <p>Maximum allowed runtime for the training job in seconds.</p> <code>main_status</code> <code>Optional[str]</code> <p>The primary status of the training job (e.g., \"InProgress\", \"Completed\").</p> <code>secondary_status</code> <code>Optional[str]</code> <p>Additional status information about the training job.</p> <code>failure_reason</code> <code>Optional[str]</code> <p>Description of why the training job failed, if applicable.</p> <code>elapsed_time</code> <code>Optional[str]</code> <p>Time elapsed since the start of the training job in seconds.</p> <code>status_transitions</code> <code>Optional[list[dict]]</code> <p>List of status change events during the training process.</p> <code>start_time</code> <code>Optional[str]</code> <p>Timestamp when the training job started.</p> <code>end_time</code> <code>Optional[str]</code> <p>Timestamp when the training job completed or failed.</p> <code>artifact_location</code> <code>Optional[str]</code> <p>Storage location of the training artifacts and model outputs.</p> Source code in <code>focoos/ports.py</code> <pre><code>@dataclass\nclass TrainingInfo:\n    \"\"\"Information about a model's training process.\n\n    This class contains details about the training job configuration, status, and timing.\n\n    Attributes:\n        algorithm_name: The name of the training algorithm used.\n        instance_type: The compute instance type used for training.\n        volume_size: The storage volume size in GB allocated for the training job.\n        max_runtime_in_seconds: Maximum allowed runtime for the training job in seconds.\n        main_status: The primary status of the training job (e.g., \"InProgress\", \"Completed\").\n        secondary_status: Additional status information about the training job.\n        failure_reason: Description of why the training job failed, if applicable.\n        elapsed_time: Time elapsed since the start of the training job in seconds.\n        status_transitions: List of status change events during the training process.\n        start_time: Timestamp when the training job started.\n        end_time: Timestamp when the training job completed or failed.\n        artifact_location: Storage location of the training artifacts and model outputs.\n    \"\"\"\n\n    algorithm_name: Optional[str] = \"\"  # todo: remove\n    instance_device: Optional[str] = None\n    instance_type: Optional[str] = None\n    volume_size: Optional[int] = None\n    main_status: Optional[str] = None\n    failure_reason: Optional[str] = None\n    status_transitions: Optional[list[dict]] = None\n    start_time: Optional[str] = None\n    end_time: Optional[str] = None\n    artifact_location: Optional[str] = None\n</code></pre>"},{"location":"api/ports/#focoos.ports.User","title":"<code>User</code>","text":"<p>               Bases: <code>PydanticBase</code></p> <p>User account information.</p> <p>This class represents a user account in the Focoos platform, containing personal information, API key, and usage quotas.</p> <p>Attributes:</p> Name Type Description <code>email</code> <code>str</code> <p>The user's email address.</p> <code>created_at</code> <code>datetime</code> <p>When the user account was created.</p> <code>updated_at</code> <code>datetime</code> <p>When the user account was last updated.</p> <code>company</code> <code>Optional[str]</code> <p>The user's company name, if provided.</p> <code>api_key</code> <code>ApiKey</code> <p>The API key associated with the user account.</p> <code>quotas</code> <code>Quotas</code> <p>Usage quotas and limits for the user account.</p> Source code in <code>focoos/ports.py</code> <pre><code>class User(PydanticBase):\n    \"\"\"User account information.\n\n    This class represents a user account in the Focoos platform, containing\n    personal information, API key, and usage quotas.\n\n    Attributes:\n        email (str): The user's email address.\n        created_at (datetime): When the user account was created.\n        updated_at (datetime): When the user account was last updated.\n        company (Optional[str]): The user's company name, if provided.\n        api_key (ApiKey): The API key associated with the user account.\n        quotas (Quotas): Usage quotas and limits for the user account.\n    \"\"\"\n\n    email: str\n    created_at: datetime\n    updated_at: datetime\n    company: Optional[str] = None\n    api_key: ApiKey\n    quotas: Quotas\n</code></pre>"},{"location":"api/processor/","title":"Processor","text":""},{"location":"api/processor/#focoos.processor.base_processor.Processor","title":"<code>Processor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for model processors that handle preprocessing and postprocessing.</p> <p>This class defines the interface for processing inputs and outputs for different model types. Subclasses must implement the abstract methods to provide model-specific processing logic.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>ModelConfig</code> <p>Configuration object containing model-specific settings.</p> <code>training</code> <code>bool</code> <p>Flag indicating whether the processor is in training mode.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>class Processor(ABC):\n    \"\"\"Abstract base class for model processors that handle preprocessing and postprocessing.\n\n    This class defines the interface for processing inputs and outputs for different model types.\n    Subclasses must implement the abstract methods to provide model-specific processing logic.\n\n    Attributes:\n        config (ModelConfig): Configuration object containing model-specific settings.\n        training (bool): Flag indicating whether the processor is in training mode.\n    \"\"\"\n\n    def __init__(self, config: ModelConfig, image_size: Optional[int] = None):\n        \"\"\"Initialize the processor with the given configuration.\n\n        Args:\n            config (ModelConfig): Model configuration containing settings and parameters.\n        \"\"\"\n        self.config = config\n        self.training = False\n        self.image_size = image_size\n\n    def eval(self):\n        \"\"\"Set the processor to evaluation mode.\n\n        Returns:\n            Processor: Self reference for method chaining.\n        \"\"\"\n        self.training = False\n        return self\n\n    def train(self, training: bool = True):\n        \"\"\"Set the processor training mode.\n\n        Args:\n            training (bool, optional): Whether to set training mode. Defaults to True.\n\n        Returns:\n            Processor: Self reference for method chaining.\n        \"\"\"\n        self.training = training\n        return self\n\n    @abstractmethod\n    def preprocess(\n        self,\n        inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n        device: Union[Literal[\"cuda\", \"cpu\"], torch.device] = \"cuda\",\n        dtype: torch.dtype = torch.float32,\n    ) -&gt; tuple[torch.Tensor, Any]:\n        \"\"\"Preprocess input data for model inference.\n\n        This method must be implemented by subclasses to handle model-specific preprocessing\n        such as resizing, normalization, and tensor formatting.\n\n        Args:\n            inputs: Input data which can be single or multiple images in various formats.\n            device: Target device for tensor placement. Defaults to \"cuda\".\n            dtype: Target data type for tensors. Defaults to torch.float32.\n            image_size: Optional target image size for resizing. Defaults to None.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Preprocessed tensor and any additional metadata.\n\n        Raises:\n            NotImplementedError: If not implemented by subclass.\n        \"\"\"\n        raise NotImplementedError(\"Pre-processing is not implemented for this model.\")\n\n    @abstractmethod\n    def postprocess(\n        self,\n        outputs: ModelOutput,\n        inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n        class_names: list[str] = [],\n        threshold: float = 0.5,\n        **kwargs,\n    ) -&gt; list[FocoosDetections]:\n        \"\"\"Postprocess model outputs to generate final detection results.\n\n        This method must be implemented by subclasses to convert raw model outputs\n        into structured detection results.\n\n        Args:\n            outputs (ModelOutput): Raw outputs from the model.\n            inputs: Original input data for reference during postprocessing.\n            class_names (list[str], optional): List of class names for detection labels.\n                Defaults to empty list.\n            threshold (float, optional): Confidence threshold for detections. Defaults to 0.5.\n            **kwargs: Additional keyword arguments for model-specific postprocessing.\n\n        Returns:\n            list[FocoosDetections]: List of detection results for each input.\n\n        Raises:\n            NotImplementedError: If not implemented by subclass.\n        \"\"\"\n        raise NotImplementedError(\"Post-processing is not implemented for this model.\")\n\n    @abstractmethod\n    def export_postprocess(\n        self,\n        output: Union[list[torch.Tensor], list[np.ndarray]],\n        inputs: Union[\n            torch.Tensor,\n            np.ndarray,\n            Image.Image,\n            list[Image.Image],\n            list[np.ndarray],\n            list[torch.Tensor],\n        ],\n        threshold: float = 0.5,\n        **kwargs,\n    ) -&gt; list[FocoosDetections]:\n        \"\"\"Postprocess outputs from exported model for inference.\n\n        This method handles postprocessing for models that have been exported\n        (e.g., to ONNX format) and may have different output formats.\n\n        Args:\n            output: Raw outputs from exported model as tensors or numpy arrays.\n            inputs: Original input data for reference during postprocessing.\n            threshold: Optional confidence threshold for detections. Defaults to None.\n            **kwargs: Additional keyword arguments for export-specific postprocessing.\n\n        Returns:\n            list[FocoosDetections]: List of detection results for each input.\n\n        Raises:\n            NotImplementedError: If not implemented by subclass.\n        \"\"\"\n        raise NotImplementedError(\"Export post-processing is not implemented for this model.\")\n\n    @abstractmethod\n    def get_dynamic_axes(self) -&gt; DynamicAxes:\n        \"\"\"Get dynamic axes configuration for model export.\n\n        This method defines which axes can vary in size during model export,\n        typically used for ONNX export with dynamic batch sizes or image dimensions.\n\n        Returns:\n            DynamicAxes: Configuration specifying which axes are dynamic.\n\n        Raises:\n            NotImplementedError: If not implemented by subclass.\n        \"\"\"\n        raise NotImplementedError(\"Export axes are not implemented for this model.\")\n\n    @abstractmethod\n    def eval_postprocess(self, outputs: ModelOutput, inputs: list[DatasetEntry]):\n        \"\"\"Postprocess model outputs for evaluation purposes.\n\n        This method handles postprocessing specifically for model evaluation,\n        which may differ from inference postprocessing.\n\n        Args:\n            outputs (ModelOutput): Raw outputs from the model.\n            inputs (list[DatasetEntry]): List of dataset entries used as inputs.\n\n        Raises:\n            NotImplementedError: If not implemented by subclass.\n        \"\"\"\n        raise NotImplementedError(\"Post-processing is not implemented for this model.\")\n\n    def get_image_sizes(\n        self,\n        inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n    ):\n        \"\"\"Extract image dimensions from various input formats.\n\n        This utility method determines the height and width of images from different\n        input types including tensors, numpy arrays, and PIL images.\n\n        Args:\n            inputs: Input data containing one or more images in various formats.\n\n        Returns:\n            list[tuple[int, int]]: List of (height, width) tuples for each image.\n\n        Raises:\n            ValueError: If input type is not supported.\n        \"\"\"\n        image_sizes = []\n\n        if isinstance(inputs, (torch.Tensor, np.ndarray)):\n            # Single tensor/array input\n            if isinstance(inputs, torch.Tensor):\n                height, width = inputs.shape[-2:]\n            else:  # numpy array\n                height, width = inputs.shape[-3:-1] if inputs.ndim &gt; 3 else inputs.shape[:2]\n            image_sizes.append((height, width))\n        elif isinstance(inputs, Image.Image):\n            # Single PIL image\n            width, height = inputs.size\n            image_sizes.append((height, width))\n        elif isinstance(inputs, list):\n            # List of inputs\n            for img in inputs:\n                if isinstance(img, torch.Tensor):\n                    height, width = img.shape[-2:]\n                elif isinstance(img, np.ndarray):\n                    height, width = img.shape[-3:-1] if img.ndim &gt; 3 else img.shape[:2]\n                elif isinstance(img, Image.Image):\n                    width, height = img.size\n                else:\n                    raise ValueError(f\"Unsupported input type in list: {type(img)}\")\n                image_sizes.append((height, width))\n        else:\n            raise ValueError(f\"Unsupported input type: {type(inputs)}\")\n        return image_sizes\n\n    def get_torch_batch(\n        self,\n        inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n        target_size: Optional[tuple[int, int]] = None,\n        device: Optional[torch.device] = None,\n        dtype: Optional[torch.dtype] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Convert various input formats to a batched PyTorch tensor with optional resizing.\n\n        This utility method standardizes different input types (PIL Images, numpy arrays,\n        PyTorch tensors) into a single batched tensor with consistent format (BCHW).\n        Optionally resizes all images to a target size for memory efficiency.\n\n        Args:\n            inputs: Input data containing one or more images in various formats.\n            target_size: Optional (height, width) to resize all images to. If provided,\n                        images are resized individually before batching for memory efficiency.\n            device: Optional target device for tensor placement.\n            dtype: Optional target data type for tensors.\n\n        Returns:\n            torch.Tensor: Batched tensor with shape (B, C, H, W) where:\n                - B is batch size\n                - C is number of channels (typically 3 for RGB)\n                - H is height (target_size[0] if resizing)\n                - W is width (target_size[1] if resizing)\n\n        Note:\n            When target_size is None, this method may break with different image sizes\n            as it uses torch.cat which requires consistent dimensions across inputs.\n            When target_size is provided, all images are resized for consistent batching.\n        \"\"\"\n        if isinstance(inputs, (Image.Image, np.ndarray, torch.Tensor)):\n            inputs_list = [inputs]\n        else:\n            inputs_list = inputs\n\n        # Process each input based on its type\n        processed_inputs = []\n        for inp in inputs_list:\n            # Convert PIL to numpy array if needed\n            if isinstance(inp, Image.Image):\n                inp = np.array(inp)\n\n            # Convert numpy to tensor efficiently\n            if isinstance(inp, np.ndarray):\n                # Use torch.from_numpy for zero-copy when possible\n                if inp.flags.c_contiguous:\n                    inp = torch.from_numpy(inp)\n                else:\n                    # Make contiguous first, then convert\n                    inp = torch.from_numpy(np.ascontiguousarray(inp))\n\n            # Ensure input has correct shape and type\n            if inp.dim() == 3:  # Add batch dimension if missing\n                inp = inp.unsqueeze(0)\n            if inp.shape[1] != 3 and inp.shape[-1] == 3:  # Convert HWC to CHW if needed\n                inp = inp.permute(0, 3, 1, 2)\n\n            # Apply device and dtype conversion before resizing if specified\n            if device is not None:\n                inp = inp.to(device, non_blocking=True)\n            if dtype is not None:\n                inp = inp.to(dtype)\n\n            # Resize individual images if target_size is specified (more memory efficient)\n            if target_size is not None:\n                inp = torch.nn.functional.interpolate(inp, size=target_size, mode=\"bilinear\", align_corners=False)\n\n            processed_inputs.append(inp)\n\n        images_torch = torch.stack([t.squeeze(0) for t in processed_inputs], dim=0)\n\n        return images_torch\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.__init__","title":"<code>__init__(config, image_size=None)</code>","text":"<p>Initialize the processor with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelConfig</code> <p>Model configuration containing settings and parameters.</p> required Source code in <code>focoos/processor/base_processor.py</code> <pre><code>def __init__(self, config: ModelConfig, image_size: Optional[int] = None):\n    \"\"\"Initialize the processor with the given configuration.\n\n    Args:\n        config (ModelConfig): Model configuration containing settings and parameters.\n    \"\"\"\n    self.config = config\n    self.training = False\n    self.image_size = image_size\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.eval","title":"<code>eval()</code>","text":"<p>Set the processor to evaluation mode.</p> <p>Returns:</p> Name Type Description <code>Processor</code> <p>Self reference for method chaining.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>def eval(self):\n    \"\"\"Set the processor to evaluation mode.\n\n    Returns:\n        Processor: Self reference for method chaining.\n    \"\"\"\n    self.training = False\n    return self\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.eval_postprocess","title":"<code>eval_postprocess(outputs, inputs)</code>  <code>abstractmethod</code>","text":"<p>Postprocess model outputs for evaluation purposes.</p> <p>This method handles postprocessing specifically for model evaluation, which may differ from inference postprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>ModelOutput</code> <p>Raw outputs from the model.</p> required <code>inputs</code> <code>list[DatasetEntry]</code> <p>List of dataset entries used as inputs.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>@abstractmethod\ndef eval_postprocess(self, outputs: ModelOutput, inputs: list[DatasetEntry]):\n    \"\"\"Postprocess model outputs for evaluation purposes.\n\n    This method handles postprocessing specifically for model evaluation,\n    which may differ from inference postprocessing.\n\n    Args:\n        outputs (ModelOutput): Raw outputs from the model.\n        inputs (list[DatasetEntry]): List of dataset entries used as inputs.\n\n    Raises:\n        NotImplementedError: If not implemented by subclass.\n    \"\"\"\n    raise NotImplementedError(\"Post-processing is not implemented for this model.\")\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.export_postprocess","title":"<code>export_postprocess(output, inputs, threshold=0.5, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Postprocess outputs from exported model for inference.</p> <p>This method handles postprocessing for models that have been exported (e.g., to ONNX format) and may have different output formats.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Union[list[Tensor], list[ndarray]]</code> <p>Raw outputs from exported model as tensors or numpy arrays.</p> required <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Original input data for reference during postprocessing.</p> required <code>threshold</code> <code>float</code> <p>Optional confidence threshold for detections. Defaults to None.</p> <code>0.5</code> <code>**kwargs</code> <p>Additional keyword arguments for export-specific postprocessing.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[FocoosDetections]</code> <p>list[FocoosDetections]: List of detection results for each input.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>@abstractmethod\ndef export_postprocess(\n    self,\n    output: Union[list[torch.Tensor], list[np.ndarray]],\n    inputs: Union[\n        torch.Tensor,\n        np.ndarray,\n        Image.Image,\n        list[Image.Image],\n        list[np.ndarray],\n        list[torch.Tensor],\n    ],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; list[FocoosDetections]:\n    \"\"\"Postprocess outputs from exported model for inference.\n\n    This method handles postprocessing for models that have been exported\n    (e.g., to ONNX format) and may have different output formats.\n\n    Args:\n        output: Raw outputs from exported model as tensors or numpy arrays.\n        inputs: Original input data for reference during postprocessing.\n        threshold: Optional confidence threshold for detections. Defaults to None.\n        **kwargs: Additional keyword arguments for export-specific postprocessing.\n\n    Returns:\n        list[FocoosDetections]: List of detection results for each input.\n\n    Raises:\n        NotImplementedError: If not implemented by subclass.\n    \"\"\"\n    raise NotImplementedError(\"Export post-processing is not implemented for this model.\")\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.get_dynamic_axes","title":"<code>get_dynamic_axes()</code>  <code>abstractmethod</code>","text":"<p>Get dynamic axes configuration for model export.</p> <p>This method defines which axes can vary in size during model export, typically used for ONNX export with dynamic batch sizes or image dimensions.</p> <p>Returns:</p> Name Type Description <code>DynamicAxes</code> <code>DynamicAxes</code> <p>Configuration specifying which axes are dynamic.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>@abstractmethod\ndef get_dynamic_axes(self) -&gt; DynamicAxes:\n    \"\"\"Get dynamic axes configuration for model export.\n\n    This method defines which axes can vary in size during model export,\n    typically used for ONNX export with dynamic batch sizes or image dimensions.\n\n    Returns:\n        DynamicAxes: Configuration specifying which axes are dynamic.\n\n    Raises:\n        NotImplementedError: If not implemented by subclass.\n    \"\"\"\n    raise NotImplementedError(\"Export axes are not implemented for this model.\")\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.get_image_sizes","title":"<code>get_image_sizes(inputs)</code>","text":"<p>Extract image dimensions from various input formats.</p> <p>This utility method determines the height and width of images from different input types including tensors, numpy arrays, and PIL images.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Input data containing one or more images in various formats.</p> required <p>Returns:</p> Type Description <p>list[tuple[int, int]]: List of (height, width) tuples for each image.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input type is not supported.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>def get_image_sizes(\n    self,\n    inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n):\n    \"\"\"Extract image dimensions from various input formats.\n\n    This utility method determines the height and width of images from different\n    input types including tensors, numpy arrays, and PIL images.\n\n    Args:\n        inputs: Input data containing one or more images in various formats.\n\n    Returns:\n        list[tuple[int, int]]: List of (height, width) tuples for each image.\n\n    Raises:\n        ValueError: If input type is not supported.\n    \"\"\"\n    image_sizes = []\n\n    if isinstance(inputs, (torch.Tensor, np.ndarray)):\n        # Single tensor/array input\n        if isinstance(inputs, torch.Tensor):\n            height, width = inputs.shape[-2:]\n        else:  # numpy array\n            height, width = inputs.shape[-3:-1] if inputs.ndim &gt; 3 else inputs.shape[:2]\n        image_sizes.append((height, width))\n    elif isinstance(inputs, Image.Image):\n        # Single PIL image\n        width, height = inputs.size\n        image_sizes.append((height, width))\n    elif isinstance(inputs, list):\n        # List of inputs\n        for img in inputs:\n            if isinstance(img, torch.Tensor):\n                height, width = img.shape[-2:]\n            elif isinstance(img, np.ndarray):\n                height, width = img.shape[-3:-1] if img.ndim &gt; 3 else img.shape[:2]\n            elif isinstance(img, Image.Image):\n                width, height = img.size\n            else:\n                raise ValueError(f\"Unsupported input type in list: {type(img)}\")\n            image_sizes.append((height, width))\n    else:\n        raise ValueError(f\"Unsupported input type: {type(inputs)}\")\n    return image_sizes\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.get_torch_batch","title":"<code>get_torch_batch(inputs, target_size=None, device=None, dtype=None)</code>","text":"<p>Convert various input formats to a batched PyTorch tensor with optional resizing.</p> <p>This utility method standardizes different input types (PIL Images, numpy arrays, PyTorch tensors) into a single batched tensor with consistent format (BCHW). Optionally resizes all images to a target size for memory efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Input data containing one or more images in various formats.</p> required <code>target_size</code> <code>Optional[tuple[int, int]]</code> <p>Optional (height, width) to resize all images to. If provided,         images are resized individually before batching for memory efficiency.</p> <code>None</code> <code>device</code> <code>Optional[device]</code> <p>Optional target device for tensor placement.</p> <code>None</code> <code>dtype</code> <code>Optional[dtype]</code> <p>Optional target data type for tensors.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Batched tensor with shape (B, C, H, W) where: - B is batch size - C is number of channels (typically 3 for RGB) - H is height (target_size[0] if resizing) - W is width (target_size[1] if resizing)</p> Note <p>When target_size is None, this method may break with different image sizes as it uses torch.cat which requires consistent dimensions across inputs. When target_size is provided, all images are resized for consistent batching.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>def get_torch_batch(\n    self,\n    inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n    target_size: Optional[tuple[int, int]] = None,\n    device: Optional[torch.device] = None,\n    dtype: Optional[torch.dtype] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Convert various input formats to a batched PyTorch tensor with optional resizing.\n\n    This utility method standardizes different input types (PIL Images, numpy arrays,\n    PyTorch tensors) into a single batched tensor with consistent format (BCHW).\n    Optionally resizes all images to a target size for memory efficiency.\n\n    Args:\n        inputs: Input data containing one or more images in various formats.\n        target_size: Optional (height, width) to resize all images to. If provided,\n                    images are resized individually before batching for memory efficiency.\n        device: Optional target device for tensor placement.\n        dtype: Optional target data type for tensors.\n\n    Returns:\n        torch.Tensor: Batched tensor with shape (B, C, H, W) where:\n            - B is batch size\n            - C is number of channels (typically 3 for RGB)\n            - H is height (target_size[0] if resizing)\n            - W is width (target_size[1] if resizing)\n\n    Note:\n        When target_size is None, this method may break with different image sizes\n        as it uses torch.cat which requires consistent dimensions across inputs.\n        When target_size is provided, all images are resized for consistent batching.\n    \"\"\"\n    if isinstance(inputs, (Image.Image, np.ndarray, torch.Tensor)):\n        inputs_list = [inputs]\n    else:\n        inputs_list = inputs\n\n    # Process each input based on its type\n    processed_inputs = []\n    for inp in inputs_list:\n        # Convert PIL to numpy array if needed\n        if isinstance(inp, Image.Image):\n            inp = np.array(inp)\n\n        # Convert numpy to tensor efficiently\n        if isinstance(inp, np.ndarray):\n            # Use torch.from_numpy for zero-copy when possible\n            if inp.flags.c_contiguous:\n                inp = torch.from_numpy(inp)\n            else:\n                # Make contiguous first, then convert\n                inp = torch.from_numpy(np.ascontiguousarray(inp))\n\n        # Ensure input has correct shape and type\n        if inp.dim() == 3:  # Add batch dimension if missing\n            inp = inp.unsqueeze(0)\n        if inp.shape[1] != 3 and inp.shape[-1] == 3:  # Convert HWC to CHW if needed\n            inp = inp.permute(0, 3, 1, 2)\n\n        # Apply device and dtype conversion before resizing if specified\n        if device is not None:\n            inp = inp.to(device, non_blocking=True)\n        if dtype is not None:\n            inp = inp.to(dtype)\n\n        # Resize individual images if target_size is specified (more memory efficient)\n        if target_size is not None:\n            inp = torch.nn.functional.interpolate(inp, size=target_size, mode=\"bilinear\", align_corners=False)\n\n        processed_inputs.append(inp)\n\n    images_torch = torch.stack([t.squeeze(0) for t in processed_inputs], dim=0)\n\n    return images_torch\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.postprocess","title":"<code>postprocess(outputs, inputs, class_names=[], threshold=0.5, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Postprocess model outputs to generate final detection results.</p> <p>This method must be implemented by subclasses to convert raw model outputs into structured detection results.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>ModelOutput</code> <p>Raw outputs from the model.</p> required <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Original input data for reference during postprocessing.</p> required <code>class_names</code> <code>list[str]</code> <p>List of class names for detection labels. Defaults to empty list.</p> <code>[]</code> <code>threshold</code> <code>float</code> <p>Confidence threshold for detections. Defaults to 0.5.</p> <code>0.5</code> <code>**kwargs</code> <p>Additional keyword arguments for model-specific postprocessing.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[FocoosDetections]</code> <p>list[FocoosDetections]: List of detection results for each input.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>@abstractmethod\ndef postprocess(\n    self,\n    outputs: ModelOutput,\n    inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n    class_names: list[str] = [],\n    threshold: float = 0.5,\n    **kwargs,\n) -&gt; list[FocoosDetections]:\n    \"\"\"Postprocess model outputs to generate final detection results.\n\n    This method must be implemented by subclasses to convert raw model outputs\n    into structured detection results.\n\n    Args:\n        outputs (ModelOutput): Raw outputs from the model.\n        inputs: Original input data for reference during postprocessing.\n        class_names (list[str], optional): List of class names for detection labels.\n            Defaults to empty list.\n        threshold (float, optional): Confidence threshold for detections. Defaults to 0.5.\n        **kwargs: Additional keyword arguments for model-specific postprocessing.\n\n    Returns:\n        list[FocoosDetections]: List of detection results for each input.\n\n    Raises:\n        NotImplementedError: If not implemented by subclass.\n    \"\"\"\n    raise NotImplementedError(\"Post-processing is not implemented for this model.\")\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.preprocess","title":"<code>preprocess(inputs, device='cuda', dtype=torch.float32)</code>  <code>abstractmethod</code>","text":"<p>Preprocess input data for model inference.</p> <p>This method must be implemented by subclasses to handle model-specific preprocessing such as resizing, normalization, and tensor formatting.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tensor, ndarray, Image, list[Image], list[ndarray], list[Tensor]]</code> <p>Input data which can be single or multiple images in various formats.</p> required <code>device</code> <code>Union[Literal['cuda', 'cpu'], device]</code> <p>Target device for tensor placement. Defaults to \"cuda\".</p> <code>'cuda'</code> <code>dtype</code> <code>dtype</code> <p>Target data type for tensors. Defaults to torch.float32.</p> <code>float32</code> <code>image_size</code> <p>Optional target image size for resizing. Defaults to None.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Any]</code> <p>tuple[torch.Tensor, Any]: Preprocessed tensor and any additional metadata.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by subclass.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>@abstractmethod\ndef preprocess(\n    self,\n    inputs: Union[torch.Tensor, np.ndarray, Image.Image, list[Image.Image], list[np.ndarray], list[torch.Tensor]],\n    device: Union[Literal[\"cuda\", \"cpu\"], torch.device] = \"cuda\",\n    dtype: torch.dtype = torch.float32,\n) -&gt; tuple[torch.Tensor, Any]:\n    \"\"\"Preprocess input data for model inference.\n\n    This method must be implemented by subclasses to handle model-specific preprocessing\n    such as resizing, normalization, and tensor formatting.\n\n    Args:\n        inputs: Input data which can be single or multiple images in various formats.\n        device: Target device for tensor placement. Defaults to \"cuda\".\n        dtype: Target data type for tensors. Defaults to torch.float32.\n        image_size: Optional target image size for resizing. Defaults to None.\n\n    Returns:\n        tuple[torch.Tensor, Any]: Preprocessed tensor and any additional metadata.\n\n    Raises:\n        NotImplementedError: If not implemented by subclass.\n    \"\"\"\n    raise NotImplementedError(\"Pre-processing is not implemented for this model.\")\n</code></pre>"},{"location":"api/processor/#focoos.processor.base_processor.Processor.train","title":"<code>train(training=True)</code>","text":"<p>Set the processor training mode.</p> <p>Parameters:</p> Name Type Description Default <code>training</code> <code>bool</code> <p>Whether to set training mode. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Processor</code> <p>Self reference for method chaining.</p> Source code in <code>focoos/processor/base_processor.py</code> <pre><code>def train(self, training: bool = True):\n    \"\"\"Set the processor training mode.\n\n    Args:\n        training (bool, optional): Whether to set training mode. Defaults to True.\n\n    Returns:\n        Processor: Self reference for method chaining.\n    \"\"\"\n    self.training = training\n    return self\n</code></pre>"},{"location":"api/processor/#focoos.processor.processor_manager.ProcessorManager","title":"<code>ProcessorManager</code>","text":"<p>Automatic processor manager with lazy loading</p> Source code in <code>focoos/processor/processor_manager.py</code> <pre><code>class ProcessorManager:\n    \"\"\"Automatic processor manager with lazy loading\"\"\"\n\n    _PROCESSOR_MAPPING: Dict[str, Callable[[], Type[Processor]]] = {}\n\n    @classmethod\n    def register_processor(cls, model_family: ModelFamily, processor_loader: Callable[[], Type[Processor]]):\n        \"\"\"\n        Register a loader for a specific processor\n        \"\"\"\n        cls._PROCESSOR_MAPPING[model_family.value] = processor_loader\n\n    @classmethod\n    def _ensure_family_registered(cls, model_family: ModelFamily):\n        \"\"\"Ensure the processor family is registered, importing if needed.\"\"\"\n        if model_family.value not in cls._PROCESSOR_MAPPING:\n            family_module = importlib.import_module(f\"focoos.models.{model_family.value}\")\n            for attr_name in dir(family_module):\n                if attr_name.startswith(\"_register\"):\n                    register_func = getattr(family_module, attr_name)\n                    if callable(register_func):\n                        register_func()\n\n    @classmethod\n    def get_processor(\n        cls, model_family: ModelFamily, model_config: ModelConfig, image_size: Optional[int] = None\n    ) -&gt; Processor:\n        \"\"\"\n        Get a processor instance for the given model family.\n        \"\"\"\n        cls._ensure_family_registered(model_family)\n        if model_family.value not in cls._PROCESSOR_MAPPING:\n            raise ValueError(f\"Processor for {model_family} not supported\")\n        processor_class = cls._PROCESSOR_MAPPING[model_family.value]()\n        return processor_class(config=model_config, image_size=image_size)\n</code></pre>"},{"location":"api/processor/#focoos.processor.processor_manager.ProcessorManager.get_processor","title":"<code>get_processor(model_family, model_config, image_size=None)</code>  <code>classmethod</code>","text":"<p>Get a processor instance for the given model family.</p> Source code in <code>focoos/processor/processor_manager.py</code> <pre><code>@classmethod\ndef get_processor(\n    cls, model_family: ModelFamily, model_config: ModelConfig, image_size: Optional[int] = None\n) -&gt; Processor:\n    \"\"\"\n    Get a processor instance for the given model family.\n    \"\"\"\n    cls._ensure_family_registered(model_family)\n    if model_family.value not in cls._PROCESSOR_MAPPING:\n        raise ValueError(f\"Processor for {model_family} not supported\")\n    processor_class = cls._PROCESSOR_MAPPING[model_family.value]()\n    return processor_class(config=model_config, image_size=image_size)\n</code></pre>"},{"location":"api/processor/#focoos.processor.processor_manager.ProcessorManager.register_processor","title":"<code>register_processor(model_family, processor_loader)</code>  <code>classmethod</code>","text":"<p>Register a loader for a specific processor</p> Source code in <code>focoos/processor/processor_manager.py</code> <pre><code>@classmethod\ndef register_processor(cls, model_family: ModelFamily, processor_loader: Callable[[], Type[Processor]]):\n    \"\"\"\n    Register a loader for a specific processor\n    \"\"\"\n    cls._PROCESSOR_MAPPING[model_family.value] = processor_loader\n</code></pre>"},{"location":"api/runtimes/","title":"runtimes","text":""},{"location":"api/runtimes/#focoos.infer.runtimes.base.BaseRuntime","title":"<code>BaseRuntime</code>","text":"<p>Abstract base class for runtime implementations.</p> <p>This class defines the interface that all runtime implementations must follow. It provides methods for model initialization, inference, and performance benchmarking.</p> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>str</code> <p>Path to the model file.</p> <code>opts</code> <code>Any</code> <p>Runtime-specific options.</p> <code>model_info</code> <code>RemoteModelInfo</code> <p>Metadata about the model.</p> Source code in <code>focoos/infer/runtimes/base.py</code> <pre><code>class BaseRuntime:\n    \"\"\"\n    Abstract base class for runtime implementations.\n\n    This class defines the interface that all runtime implementations must follow.\n    It provides methods for model initialization, inference, and performance benchmarking.\n\n    Attributes:\n        model_path (str): Path to the model file.\n        opts (Any): Runtime-specific options.\n        model_info (RemoteModelInfo): Metadata about the model.\n    \"\"\"\n\n    def __init__(self, model_path: str, opts: Any, model_info: RemoteModelInfo):\n        \"\"\"\n        Initialize the runtime with model path, options and metadata.\n\n        Args:\n            model_path (str): Path to the model file.\n            opts (Any): Runtime-specific configuration options.\n            model_info (RemoteModelInfo): Metadata about the model.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n        \"\"\"\n        Run inference on the input image.\n\n        Args:\n            im (np.ndarray): Input image as a numpy array.\n\n        Returns:\n            np.ndarray: Model output as a numpy array.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _get_device_info(self) -&gt; tuple[str, str]:\n        \"\"\"\n        Get the engine and device name.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def benchmark(self, iterations: int, size: Union[int, Tuple[int, int]]) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model performance.\n\n        Args:\n            iterations (int): Number of inference iterations to run.\n            size (float): Input image size for benchmarking.\n\n        Returns:\n            LatencyMetrics: Performance metrics including mean, median, and percentile latencies.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.base.BaseRuntime.__call__","title":"<code>__call__(im)</code>  <code>abstractmethod</code>","text":"<p>Run inference on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>Input image as a numpy array.</p> required <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>np.ndarray: Model output as a numpy array.</p> Source code in <code>focoos/infer/runtimes/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n    \"\"\"\n    Run inference on the input image.\n\n    Args:\n        im (np.ndarray): Input image as a numpy array.\n\n    Returns:\n        np.ndarray: Model output as a numpy array.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.base.BaseRuntime.__init__","title":"<code>__init__(model_path, opts, model_info)</code>","text":"<p>Initialize the runtime with model path, options and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model file.</p> required <code>opts</code> <code>Any</code> <p>Runtime-specific configuration options.</p> required <code>model_info</code> <code>RemoteModelInfo</code> <p>Metadata about the model.</p> required Source code in <code>focoos/infer/runtimes/base.py</code> <pre><code>def __init__(self, model_path: str, opts: Any, model_info: RemoteModelInfo):\n    \"\"\"\n    Initialize the runtime with model path, options and metadata.\n\n    Args:\n        model_path (str): Path to the model file.\n        opts (Any): Runtime-specific configuration options.\n        model_info (RemoteModelInfo): Metadata about the model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.base.BaseRuntime.benchmark","title":"<code>benchmark(iterations, size)</code>  <code>abstractmethod</code>","text":"<p>Benchmark the model performance.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of inference iterations to run.</p> required <code>size</code> <code>float</code> <p>Input image size for benchmarking.</p> required <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Performance metrics including mean, median, and percentile latencies.</p> Source code in <code>focoos/infer/runtimes/base.py</code> <pre><code>@abstractmethod\ndef benchmark(self, iterations: int, size: Union[int, Tuple[int, int]]) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model performance.\n\n    Args:\n        iterations (int): Number of inference iterations to run.\n        size (float): Input image size for benchmarking.\n\n    Returns:\n        LatencyMetrics: Performance metrics including mean, median, and percentile latencies.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.load_runtime.load_runtime","title":"<code>load_runtime(runtime_type, model_path, model_info, warmup_iter=50, device='auto')</code>","text":"<p>Creates and returns a runtime instance based on the specified runtime type. Supports both ONNX and TorchScript runtimes with various execution providers.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_type</code> <code>RuntimeTypes</code> <p>The type of runtime to use. Can be one of: - ONNX_CUDA32: ONNX runtime with CUDA FP32 - ONNX_TRT32: ONNX runtime with TensorRT FP32 - ONNX_TRT16: ONNX runtime with TensorRT FP16 - ONNX_CPU: ONNX runtime with CPU - ONNX_COREML: ONNX runtime with CoreML - TORCHSCRIPT_32: TorchScript runtime with FP32</p> required <code>model_path</code> <code>str</code> <p>Path to the model file (.onnx or .pt)</p> required <code>model_metadata</code> <code>ModelMetadata</code> <p>Model metadata containing task type, classes etc.</p> required <code>warmup_iter</code> <code>int</code> <p>Number of warmup iterations before inference. Defaults to 0.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>BaseRuntime</code> <code>BaseRuntime</code> <p>A configured runtime instance (ONNXRuntime or TorchscriptRuntime)</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required dependencies (torch/onnxruntime) are not installed</p> Source code in <code>focoos/infer/runtimes/load_runtime.py</code> <pre><code>def load_runtime(\n    runtime_type: RuntimeType,\n    model_path: str,\n    model_info: ModelInfo,\n    warmup_iter: int = 50,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n) -&gt; BaseRuntime:\n    \"\"\"\n    Creates and returns a runtime instance based on the specified runtime type.\n    Supports both ONNX and TorchScript runtimes with various execution providers.\n\n    Args:\n        runtime_type (RuntimeTypes): The type of runtime to use. Can be one of:\n            - ONNX_CUDA32: ONNX runtime with CUDA FP32\n            - ONNX_TRT32: ONNX runtime with TensorRT FP32\n            - ONNX_TRT16: ONNX runtime with TensorRT FP16\n            - ONNX_CPU: ONNX runtime with CPU\n            - ONNX_COREML: ONNX runtime with CoreML\n            - TORCHSCRIPT_32: TorchScript runtime with FP32\n        model_path (str): Path to the model file (.onnx or .pt)\n        model_metadata (ModelMetadata): Model metadata containing task type, classes etc.\n        warmup_iter (int, optional): Number of warmup iterations before inference. Defaults to 0.\n\n    Returns:\n        BaseRuntime: A configured runtime instance (ONNXRuntime or TorchscriptRuntime)\n\n    Raises:\n        ImportError: If required dependencies (torch/onnxruntime) are not installed\n    \"\"\"\n    if runtime_type == RuntimeType.TORCHSCRIPT_32:\n        if not TORCH_AVAILABLE:\n            logger.error(\n                \"\u26a0\ufe0f Pytorch not found =(  please install focoos with ['torch'] extra. See https://focoosai.github.io/focoos/setup/ for more details\"\n            )\n            raise ImportError(\"Pytorch not found\")\n        from focoos.infer.runtimes.torchscript import TorchscriptRuntime\n\n        opts = TorchscriptRuntimeOpts(warmup_iter=warmup_iter)\n        return TorchscriptRuntime(model_path=model_path, opts=opts, model_info=model_info, device=device)\n    else:\n        if not ORT_AVAILABLE:\n            logger.error(\n                \"\u26a0\ufe0f onnxruntime not found =(  please install focoos with one of 'onnx', 'onnx-cpu', extra. See https://focoosai.github.io/focoos/setup/ for more details\"\n            )\n            raise ImportError(\"onnxruntime not found\")\n        from focoos.infer.runtimes.onnx import ONNXRuntime\n\n        opts = OnnxRuntimeOpts(\n            cuda=runtime_type == RuntimeType.ONNX_CUDA32,\n            trt=runtime_type in [RuntimeType.ONNX_TRT32, RuntimeType.ONNX_TRT16],\n            fp16=runtime_type == RuntimeType.ONNX_TRT16,\n            warmup_iter=warmup_iter,\n            coreml=runtime_type == RuntimeType.ONNX_COREML,\n            verbose=False,\n        )\n    return ONNXRuntime(model_path=model_path, opts=opts, model_info=model_info)\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.onnx.ONNXRuntime","title":"<code>ONNXRuntime</code>","text":"<p>               Bases: <code>BaseRuntime</code></p> <p>ONNX Runtime wrapper for model inference with different execution providers.</p> <p>This class implements the BaseRuntime interface for ONNX models, supporting various execution providers like CUDA, TensorRT, OpenVINO, and CoreML. It handles model initialization, provider configuration, warmup, inference, and performance benchmarking.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the model derived from the model path.</p> <code>opts</code> <code>OnnxRuntimeOpts</code> <p>Configuration options for the ONNX runtime.</p> <code>model_info</code> <code>RemoteModelInfo</code> <p>Metadata about the model.</p> <code>ort_sess</code> <code>InferenceSession</code> <p>ONNX Runtime inference session.</p> <code>active_providers</code> <code>list</code> <p>List of active execution providers.</p> <code>dtype</code> <code>dtype</code> <p>Input data type for the model.</p> Source code in <code>focoos/infer/runtimes/onnx.py</code> <pre><code>class ONNXRuntime(BaseRuntime):\n    \"\"\"\n    ONNX Runtime wrapper for model inference with different execution providers.\n\n    This class implements the BaseRuntime interface for ONNX models, supporting\n    various execution providers like CUDA, TensorRT, OpenVINO, and CoreML.\n    It handles model initialization, provider configuration, warmup, inference,\n    and performance benchmarking.\n\n    Attributes:\n        name (str): Name of the model derived from the model path.\n        opts (OnnxRuntimeOpts): Configuration options for the ONNX runtime.\n        model_info (RemoteModelInfo): Metadata about the model.\n        ort_sess (ort.InferenceSession): ONNX Runtime inference session.\n        active_providers (list): List of active execution providers.\n        dtype (np.dtype): Input data type for the model.\n    \"\"\"\n\n    def __init__(self, model_path: Union[str, Path], opts: OnnxRuntimeOpts, model_info: ModelInfo):\n        logger.debug(f\"\ud83d\udd27 [onnxruntime device] {ort.get_device()}\")\n\n        self.name = Path(model_path).stem\n        self.opts = opts\n        self.model_info = model_info\n\n        # Setup session options\n        options = ort.SessionOptions()\n        options.log_severity_level = 0 if opts.verbose else 2\n        options.enable_profiling = opts.verbose\n\n        # Setup providers\n        self.providers = self._setup_providers(model_dir=Path(model_path).parent)\n        self.active_provider = self.providers[0][0]\n        logger.info(f\" using: {self.active_provider}\")\n        # Create session\n        self.ort_sess = ort.InferenceSession(model_path, options, providers=self.providers)\n\n        if self.opts.trt and self.providers[0][0] == \"TensorrtExecutionProvider\":\n            logger.info(\"\ud83d\udfe2  TensorRT enabled. First execution may take longer as it builds the TRT engine.\")\n        # Set input type\n        self.dtype = np.uint8 if self.ort_sess.get_inputs()[0].type == \"tensor(uint8)\" else np.float32\n\n        # Warmup\n        if self.opts.warmup_iter &gt; 0:\n            self._warmup()\n\n        # inputs = self.ort_sess.get_inputs()\n        # outputs = self.ort_sess.get_outputs()\n        # for input in inputs:\n        #     logger.debug(f\"\ud83d\udd27 Input: {input.name} {input.type} {input.shape}\")\n        # for output in outputs:\n        #     logger.debug(f\"\ud83d\udd27 Output: {output.name} {output.type} {output.shape}\")\n\n    def _setup_providers(self, model_dir: Path):\n        providers = []\n        available = ort.get_available_providers()\n        logger.debug(f\"Available providers:{available}\")\n        _dir = Path(model_dir)\n        models_root = _dir.parent\n        # Check and add providers in order of preference\n        provider_configs = [\n            (\n                \"TensorrtExecutionProvider\",\n                self.opts.trt,\n                {\n                    \"device_id\": GPU_ID,\n                    \"trt_fp16_enable\": self.opts.fp16,\n                    \"trt_force_sequential_engine_build\": False,\n                    \"trt_engine_cache_enable\": True,\n                    \"trt_engine_cache_path\": str(_dir / \".trt_cache\"),\n                    \"trt_ep_context_file_path\": str(_dir),\n                    \"trt_timing_cache_enable\": True,  # Timing cache can be shared across multiple models if layers are the same\n                    \"trt_builder_optimization_level\": 3,\n                    \"trt_timing_cache_path\": str(models_root / \".trt_timing_cache\"),\n                },\n            ),\n            (\n                \"OpenVINOExecutionProvider\",\n                self.opts.vino,\n                {\"device_type\": \"MYRIAD_FP16\", \"enable_vpu_fast_compile\": True, \"num_of_threads\": 1},\n            ),\n            (\n                \"CUDAExecutionProvider\",\n                self.opts.cuda,\n                {\n                    \"device_id\": GPU_ID,\n                    \"arena_extend_strategy\": \"kSameAsRequested\",\n                    \"gpu_mem_limit\": 16 * 1024 * 1024 * 1024,\n                    \"cudnn_conv_algo_search\": \"EXHAUSTIVE\",\n                    \"do_copy_in_default_stream\": True,\n                },\n            ),\n            (\"CoreMLExecutionProvider\", self.opts.coreml, {}),\n        ]\n\n        for provider, enabled, config in provider_configs:\n            if enabled and provider in available:\n                providers.append((provider, config))\n            elif enabled:\n                logger.warning(f\"{provider} not found.\")\n\n        providers.append((\"CPUExecutionProvider\", {}))\n        return providers\n\n    def _warmup(self):\n        size = self.model_info.im_size\n        logger.info(f\"\u23f1\ufe0f Warming up model {self.name} on {self.active_provider}, size: {size}x{size}..\")\n        np_image = np.random.rand(1, 3, size, size).astype(self.dtype)\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n        for _ in range(self.opts.warmup_iter):\n            self.ort_sess.run(out_name, {input_name: np_image})\n\n    def __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n        \"\"\"\n        Run inference on the input image.\n\n        Args:\n            im (np.ndarray): Input image as a numpy array.\n\n        Returns:\n            list[np.ndarray]: Model outputs as a list of numpy arrays.\n        \"\"\"\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n        out = self.ort_sess.run(out_name, {input_name: im.cpu().numpy()})\n        return out\n\n    def benchmark(self, iterations: int = 50, size: Union[int, Tuple[int, int]] = 640) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model performance.\n\n        Runs multiple inference iterations and measures execution time to calculate\n        performance metrics like FPS, mean latency, and other statistics.\n\n        Args:\n            iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n            size (int or tuple, optional): Input image size for benchmarking. Defaults to 640.\n\n        Returns:\n            LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n        \"\"\"\n        engine = f\"onnx.{self.active_provider}\"\n        device_name = get_device_name()\n        if self.active_provider == \"CPUExecutionProvider\":\n            device_name = get_cpu_name()\n        if isinstance(size, int):\n            size = (size, size)\n\n        logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device_name}, size: {size}..\")\n\n        np_input = (255 * np.random.random((1, 3, size[0], size[1]))).astype(self.dtype)\n        input_name = self.ort_sess.get_inputs()[0].name\n        out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n        durations = []\n        for step in range(iterations + 5):\n            start = perf_counter()\n            self.ort_sess.run(out_name, {input_name: np_input})\n            end = perf_counter()\n\n            if step &gt;= 5:  # Skip first 5 iterations\n                durations.append((end - start) * 1000)\n\n        durations = np.array(durations)\n\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean()),\n            engine=engine,\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n            device=device_name,\n        )\n        logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.onnx.ONNXRuntime.__call__","title":"<code>__call__(im)</code>","text":"<p>Run inference on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>Input image as a numpy array.</p> required <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>list[np.ndarray]: Model outputs as a list of numpy arrays.</p> Source code in <code>focoos/infer/runtimes/onnx.py</code> <pre><code>def __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n    \"\"\"\n    Run inference on the input image.\n\n    Args:\n        im (np.ndarray): Input image as a numpy array.\n\n    Returns:\n        list[np.ndarray]: Model outputs as a list of numpy arrays.\n    \"\"\"\n    input_name = self.ort_sess.get_inputs()[0].name\n    out_name = [output.name for output in self.ort_sess.get_outputs()]\n    out = self.ort_sess.run(out_name, {input_name: im.cpu().numpy()})\n    return out\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.onnx.ONNXRuntime.benchmark","title":"<code>benchmark(iterations=50, size=640)</code>","text":"<p>Benchmark the model performance.</p> <p>Runs multiple inference iterations and measures execution time to calculate performance metrics like FPS, mean latency, and other statistics.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of inference iterations to run. Defaults to 20.</p> <code>50</code> <code>size</code> <code>int or tuple</code> <p>Input image size for benchmarking. Defaults to 640.</p> <code>640</code> <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Performance metrics including FPS, mean, min, max, and std latencies.</p> Source code in <code>focoos/infer/runtimes/onnx.py</code> <pre><code>def benchmark(self, iterations: int = 50, size: Union[int, Tuple[int, int]] = 640) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model performance.\n\n    Runs multiple inference iterations and measures execution time to calculate\n    performance metrics like FPS, mean latency, and other statistics.\n\n    Args:\n        iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n        size (int or tuple, optional): Input image size for benchmarking. Defaults to 640.\n\n    Returns:\n        LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n    \"\"\"\n    engine = f\"onnx.{self.active_provider}\"\n    device_name = get_device_name()\n    if self.active_provider == \"CPUExecutionProvider\":\n        device_name = get_cpu_name()\n    if isinstance(size, int):\n        size = (size, size)\n\n    logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device_name}, size: {size}..\")\n\n    np_input = (255 * np.random.random((1, 3, size[0], size[1]))).astype(self.dtype)\n    input_name = self.ort_sess.get_inputs()[0].name\n    out_name = [output.name for output in self.ort_sess.get_outputs()]\n\n    durations = []\n    for step in range(iterations + 5):\n        start = perf_counter()\n        self.ort_sess.run(out_name, {input_name: np_input})\n        end = perf_counter()\n\n        if step &gt;= 5:  # Skip first 5 iterations\n            durations.append((end - start) * 1000)\n\n    durations = np.array(durations)\n\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean()),\n        engine=engine,\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size[0],  # FIXME: this is a hack to get the im_size as int, assuming it's a square\n        device=device_name,\n    )\n    logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.torchscript.TorchscriptRuntime","title":"<code>TorchscriptRuntime</code>","text":"<p>               Bases: <code>BaseRuntime</code></p> <p>TorchScript Runtime wrapper for model inference.</p> <p>This class implements the BaseRuntime interface for TorchScript models, supporting both CPU and CUDA devices. It handles model initialization, device placement, warmup, inference, and performance benchmarking.</p> <p>Attributes:</p> Name Type Description <code>device</code> <code>device</code> <p>Device to run inference on (CPU or CUDA).</p> <code>opts</code> <code>TorchscriptRuntimeOpts</code> <p>Configuration options for the TorchScript runtime.</p> <code>model</code> <code>ScriptModule</code> <p>Loaded TorchScript model.</p> <code>model_info</code> <code>RemoteModelInfo</code> <p>Metadata about the model.</p> Source code in <code>focoos/infer/runtimes/torchscript.py</code> <pre><code>class TorchscriptRuntime(BaseRuntime):\n    \"\"\"\n    TorchScript Runtime wrapper for model inference.\n\n    This class implements the BaseRuntime interface for TorchScript models,\n    supporting both CPU and CUDA devices. It handles model initialization,\n    device placement, warmup, inference, and performance benchmarking.\n\n    Attributes:\n        device (torch.device): Device to run inference on (CPU or CUDA).\n        opts (TorchscriptRuntimeOpts): Configuration options for the TorchScript runtime.\n        model (torch.jit.ScriptModule): Loaded TorchScript model.\n        model_info (RemoteModelInfo): Metadata about the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        opts: TorchscriptRuntimeOpts,\n        model_info: ModelInfo,\n        device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n    ):\n        if device == \"auto\":\n            self.device = torch.device(get_device_type())\n        else:\n            self.device = torch.device(device)\n        logger.info(f\"\ud83d\udd27 Device: {self.device}\")\n        self.opts = opts\n        self.model_info = model_info\n\n        map_location = None if torch.cuda.is_available() else \"cpu\"\n\n        self.model = torch.jit.load(model_path, map_location=map_location)\n        self.model = self.model.to(self.device)\n\n        if self.opts.warmup_iter &gt; 0:\n            size = (\n                self.model_info.im_size\n                if self.model_info.task in [Task.DETECTION, Task.CLASSIFICATION] and self.model_info.im_size\n                else 640\n            )\n            logger.info(f\"\u23f1\ufe0f Warming up model {self.model_info.name} on {self.device}, size: {size}x{size}..\")\n            with torch.no_grad():\n                np_image = torch.rand(1, 3, size, size).to(self.device)\n                for _ in range(self.opts.warmup_iter):\n                    self.model(np_image)\n\n    def __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n        \"\"\"\n        Run inference on the input image.\n\n        Args:\n            im (np.ndarray): Input image as a numpy array.\n\n        Returns:\n            list[np.ndarray]: Model outputs as a list of numpy arrays.\n        \"\"\"\n        with torch.no_grad():\n            res = self.model(im)\n            return res\n\n    def benchmark(self, iterations: int = 20, size: Union[int, Tuple[int, int]] = 640) -&gt; LatencyMetrics:\n        \"\"\"\n        Benchmark the model performance.\n\n        Runs multiple inference iterations and measures execution time to calculate\n        performance metrics like FPS, mean latency, and other statistics.\n\n        Args:\n            iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n\n        Returns:\n            LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n        \"\"\"\n        engine = \"torchscript\"\n        if self.device.type == \"cpu\":\n            device_name = get_cpu_name()\n        else:\n            device_name = get_device_name()\n        logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device_name}, size: {size}x{size}..\")\n\n        if isinstance(size, int):\n            size = (size, size)\n\n        torch_input = torch.rand(1, 3, size[0], size[1], device=self.device)\n        durations = []\n\n        with torch.no_grad():\n            for step in range(iterations + 5):\n                start = perf_counter()\n                self.model(torch_input)\n                end = perf_counter()\n\n                if step &gt;= 5:  # Skip first 5 iterations\n                    durations.append((end - start) * 1000)\n\n        durations = np.array(durations)\n\n        metrics = LatencyMetrics(\n            fps=int(1000 / durations.mean().astype(float)),\n            engine=engine,\n            mean=round(durations.mean().astype(float), 3),\n            max=round(durations.max().astype(float), 3),\n            min=round(durations.min().astype(float), 3),\n            std=round(durations.std().astype(float), 3),\n            im_size=size[0],\n            device=device_name,\n        )\n        logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n        return metrics\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.torchscript.TorchscriptRuntime.__call__","title":"<code>__call__(im)</code>","text":"<p>Run inference on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>Input image as a numpy array.</p> required <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>list[np.ndarray]: Model outputs as a list of numpy arrays.</p> Source code in <code>focoos/infer/runtimes/torchscript.py</code> <pre><code>def __call__(self, im: torch.Tensor) -&gt; list[np.ndarray]:\n    \"\"\"\n    Run inference on the input image.\n\n    Args:\n        im (np.ndarray): Input image as a numpy array.\n\n    Returns:\n        list[np.ndarray]: Model outputs as a list of numpy arrays.\n    \"\"\"\n    with torch.no_grad():\n        res = self.model(im)\n        return res\n</code></pre>"},{"location":"api/runtimes/#focoos.infer.runtimes.torchscript.TorchscriptRuntime.benchmark","title":"<code>benchmark(iterations=20, size=640)</code>","text":"<p>Benchmark the model performance.</p> <p>Runs multiple inference iterations and measures execution time to calculate performance metrics like FPS, mean latency, and other statistics.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of inference iterations to run. Defaults to 20.</p> <code>20</code> <p>Returns:</p> Name Type Description <code>LatencyMetrics</code> <code>LatencyMetrics</code> <p>Performance metrics including FPS, mean, min, max, and std latencies.</p> Source code in <code>focoos/infer/runtimes/torchscript.py</code> <pre><code>def benchmark(self, iterations: int = 20, size: Union[int, Tuple[int, int]] = 640) -&gt; LatencyMetrics:\n    \"\"\"\n    Benchmark the model performance.\n\n    Runs multiple inference iterations and measures execution time to calculate\n    performance metrics like FPS, mean latency, and other statistics.\n\n    Args:\n        iterations (int, optional): Number of inference iterations to run. Defaults to 20.\n\n    Returns:\n        LatencyMetrics: Performance metrics including FPS, mean, min, max, and std latencies.\n    \"\"\"\n    engine = \"torchscript\"\n    if self.device.type == \"cpu\":\n        device_name = get_cpu_name()\n    else:\n        device_name = get_device_name()\n    logger.info(f\"\u23f1\ufe0f Benchmarking latency on {device_name}, size: {size}x{size}..\")\n\n    if isinstance(size, int):\n        size = (size, size)\n\n    torch_input = torch.rand(1, 3, size[0], size[1], device=self.device)\n    durations = []\n\n    with torch.no_grad():\n        for step in range(iterations + 5):\n            start = perf_counter()\n            self.model(torch_input)\n            end = perf_counter()\n\n            if step &gt;= 5:  # Skip first 5 iterations\n                durations.append((end - start) * 1000)\n\n    durations = np.array(durations)\n\n    metrics = LatencyMetrics(\n        fps=int(1000 / durations.mean().astype(float)),\n        engine=engine,\n        mean=round(durations.mean().astype(float), 3),\n        max=round(durations.max().astype(float), 3),\n        min=round(durations.min().astype(float), 3),\n        std=round(durations.std().astype(float), 3),\n        im_size=size[0],\n        device=device_name,\n    )\n    logger.info(f\"\ud83d\udd25 FPS: {metrics.fps} Mean latency: {metrics.mean} ms \")\n    return metrics\n</code></pre>"},{"location":"api/trainer_evaluation/","title":"evaluation","text":""},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator","title":"<code>ClassificationEvaluator</code>","text":"<p>               Bases: <code>DatasetEvaluator</code></p> <p>Evaluator for classification tasks with comprehensive metrics computation.</p> <p>This evaluator computes various classification metrics including accuracy, precision, recall, and F1 score both per-class and as macro/weighted averages. It supports distributed evaluation across multiple processes.</p> <p>Attributes:</p> Name Type Description <code>dataset_dict</code> <code>DictDataset</code> <p>Dataset containing ground truth annotations.</p> <code>metadata</code> <p>Metadata from the dataset containing class information.</p> <code>num_classes</code> <code>int</code> <p>Number of classes in the classification task.</p> <code>class_names</code> <code>List[str]</code> <p>Names of the classes.</p> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>class ClassificationEvaluator(DatasetEvaluator):\n    \"\"\"Evaluator for classification tasks with comprehensive metrics computation.\n\n    This evaluator computes various classification metrics including accuracy, precision,\n    recall, and F1 score both per-class and as macro/weighted averages. It supports\n    distributed evaluation across multiple processes.\n\n    Attributes:\n        dataset_dict (DictDataset): Dataset containing ground truth annotations.\n        metadata: Metadata from the dataset containing class information.\n        num_classes (int): Number of classes in the classification task.\n        class_names (List[str]): Names of the classes.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_dict: DictDataset,\n        distributed=True,\n    ):\n        \"\"\"Initialize the ClassificationEvaluator.\n\n        Args:\n            dataset_dict (DictDataset): Dataset in DictDataset format containing\n                the ground truth annotations.\n            distributed (bool, optional): If True, evaluation will be distributed\n                across multiple processes. Defaults to True.\n        \"\"\"\n        self.dataset_dict = dataset_dict\n        self.metadata = self.dataset_dict.metadata\n        self._distributed = distributed\n        self._cpu_device = torch.device(\"cpu\")\n        self.num_classes = self.metadata.num_classes\n        self.class_names = self.metadata.thing_classes\n\n        self._predictions = []\n        self._targets = []\n\n    @classmethod\n    def from_datasetdict(cls, dataset_dict, **kwargs):\n        \"\"\"Create ClassificationEvaluator instance from a dataset dictionary.\n\n        Args:\n            dataset_dict: Dataset dictionary containing the data and metadata.\n            **kwargs: Additional keyword arguments passed to the constructor.\n\n        Returns:\n            ClassificationEvaluator: New instance of the evaluator.\n        \"\"\"\n        return cls(dataset_dict=dataset_dict, **kwargs)\n\n    def reset(self):\n        \"\"\"Clear stored predictions and targets.\n\n        This method resets the internal state of the evaluator by clearing\n        all accumulated predictions and ground truth targets.\n        \"\"\"\n        self._predictions = []\n        self._targets = []\n\n    def process(self, inputs: List[ClassificationDatasetDict], outputs: List[ClassificationModelOutput]):\n        \"\"\"Process a batch of inputs and outputs for evaluation.\n\n        This method extracts predictions and ground truth labels from the provided\n        inputs and outputs, then stores them for later evaluation.\n\n        Args:\n            inputs (List[ClassificationDatasetDict]): List of input dictionaries,\n                each containing ground truth information. Expected to have 'label'\n                field or 'annotations' with category_id.\n            outputs (List[ClassificationModelOutput]): List of model outputs,\n                each containing 'logits' field with predicted class logits or\n                ClassificationModelOutput instances.\n\n        Note:\n            - Ground truth labels are extracted from input['label'] or\n              input['annotations'] with category_id (supports multi-label)\n            - Predictions are extracted from output['logits'] or output.logits\n            - Items with missing labels or logits are skipped with warnings\n        \"\"\"\n        for input_item, output_item in zip(inputs, outputs):\n            # Get ground truth label from input\n            label_list = None\n            if \"label\" in input_item:\n                label_list = input_item[\"label\"]\n            elif hasattr(input_item, \"label\"):\n                label_list = input_item.label\n            elif \"annotations\" in input_item and len(input_item[\"annotations\"]) &gt; 0:\n                # Handle label from annotations format (multi-label)\n                label_list = [ann.get(\"category_id\", None) for ann in input_item[\"annotations\"]]\n            if label_list is None:\n                raise ValueError(f\"Could not find label in input item: {input_item}\")\n            label = torch.zeros((self.num_classes), dtype=torch.int)\n            for label_i in label_list:\n                if label_i is not None:\n                    label[label_i] = 1\n\n            if label is None:\n                logger.warning(f\"Could not find label in input item: {input_item}\")\n                continue\n\n            # Get model predictions from output\n            logits = None\n            if isinstance(output_item, dict) and \"logits\" in output_item:\n                logits = output_item[\"logits\"]\n            elif hasattr(output_item, \"logits\"):\n                # Handle ClassificationModelOutput objects\n                logits = output_item.logits\n\n            if logits is None:\n                logger.warning(f\"Could not find logits in output item: {output_item}\")\n                continue\n\n            # Move tensors to CPU for evaluation\n            logits = logits.to(self._cpu_device)\n\n            # For multi-label classification, convert logits to binary predictions\n            # Use sigmoid + threshold for multi-label prediction\n            predicted_labels = (logits &gt; 0.5).int()\n\n            # Store prediction and ground truth\n            self._predictions.append(predicted_labels)\n            self._targets.append(label)\n\n    # def _compute_confusion_matrix(self, y_true, y_pred, num_classes):\n    #     \"\"\"Compute confusion matrix for classification evaluation.\n\n    #     Args:\n    #         targets: List of target label lists\n    #         num_classes (int): Number of classes\n\n    #     Returns:\n    #         torch.Tensor: Binary matrix of shape (num_samples, num_classes)\n    #     \"\"\"\n    #     binary_targets = torch.zeros(len(targets), num_classes, dtype=torch.int)\n    #     for i, target_labels in enumerate(targets):\n    #         for label in target_labels:\n    #             if 0 &lt;= label &lt; num_classes:\n    #                 binary_targets[i, label] = 1\n    #     return binary_targets\n\n    def evaluate(self):\n        \"\"\"Evaluate multi-label classification metrics on accumulated predictions.\n\n        Computes comprehensive multi-label classification metrics including subset accuracy,\n        per-class precision/recall/F1, and macro/weighted averages.\n\n        Returns:\n            OrderedDict: Dictionary containing evaluation metrics with the following keys:\n                - 'Subset-Accuracy': Exact match accuracy (%)\n                - 'Hamming-Loss': Hamming loss (lower is better)\n                - 'Macro-Precision': Macro-averaged precision (%)\n                - 'Macro-Recall': Macro-averaged recall (%)\n                - 'Macro-F1': Macro-averaged F1 score (%)\n                - 'Micro-Precision': Micro-averaged precision (%)\n                - 'Micro-Recall': Micro-averaged recall (%)\n                - 'Micro-F1': Micro-averaged F1 score (%)\n                - 'Precision-{class_name}': Per-class precision (%)\n                - 'Recall-{class_name}': Per-class recall (%)\n                - 'F1-{class_name}': Per-class F1 score (%)\n\n        Note:\n            - In distributed mode, only the main process returns results\n            - All metrics are expressed as percentages except Hamming-Loss\n            - Returns empty dict if no predictions are available\n            - Results are wrapped in 'classification' key for trainer compatibility\n        \"\"\"\n        if self._distributed:\n            synchronize()\n            predictions_list = all_gather(self._predictions)\n            targets_list = all_gather(self._targets)\n\n            # Flatten gathered lists\n            predictions = []\n            targets = []\n            for p_list, t_list in zip(predictions_list, targets_list):\n                if p_list is not None:\n                    predictions.extend(p_list)\n                if t_list is not None:\n                    targets.extend(t_list)\n\n            if not is_main_process():\n                return\n        else:\n            predictions = self._predictions\n            targets = self._targets\n\n        # Check if we have predictions to evaluate\n        if len(predictions) == 0:\n            logger.warning(\"No predictions to evaluate\")\n            return OrderedDict({\"classification\": {}})\n\n        # Convert predictions to tensor (already binary from process method)\n        y_pred = torch.stack(predictions)  # [num_samples, num_classes]\n\n        # Convert targets to binary matrix\n        y_true = torch.stack(targets).to(dtype=y_pred.dtype, device=y_pred.device)\n\n        # Calculate per-class metrics\n        tp = (y_true * y_pred).sum(dim=0)  # True positives for each class\n        fp = ((1 - y_true) * y_pred).sum(dim=0)  # False positives for each class\n        fn = (y_true * (1 - y_pred)).sum(dim=0)  # False negatives for each class\n\n        # Precision for each class\n        precision_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n        for i in range(self.num_classes):\n            if tp[i] + fp[i] &gt; 0:\n                precision_per_class[i] = 100.0 * tp[i].float() / (tp[i] + fp[i]).float()\n\n        # Recall for each class\n        recall_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n        for i in range(self.num_classes):\n            if tp[i] + fn[i] &gt; 0:\n                recall_per_class[i] = 100.0 * tp[i].float() / (tp[i] + fn[i]).float()\n\n        # F1 score for each class\n        f1_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n        for i in range(self.num_classes):\n            if precision_per_class[i] + recall_per_class[i] &gt; 0:\n                f1_per_class[i] = (\n                    2 * (precision_per_class[i] * recall_per_class[i]) / (precision_per_class[i] + recall_per_class[i])\n                )\n\n        # Calculate macro averages (average across classes)\n        valid_classes = (tp + fn &gt; 0).sum().item()  # Classes that appear in ground truth\n        macro_precision = precision_per_class.sum() / max(valid_classes, 1)\n        macro_recall = recall_per_class.sum() / max(valid_classes, 1)\n\n        if macro_precision + macro_recall &gt; 0:\n            macro_f1 = 2 * (macro_precision * macro_recall) / (macro_precision + macro_recall)\n        else:\n            macro_f1 = torch.tensor(0.0)\n\n        # Create results dictionary\n        results = OrderedDict()\n        results[\"F1\"] = macro_f1.item()\n        results[\"Precision\"] = macro_precision.item()\n        results[\"Recall\"] = macro_recall.item()\n\n        # Add per-class metrics\n        if self.class_names is not None:\n            for i, class_name in enumerate(self.class_names):\n                if i &lt; self.num_classes:\n                    results[f\"F1-{class_name}\"] = f1_per_class[i].item()\n                    results[f\"Precision-{class_name}\"] = precision_per_class[i].item()\n                    results[f\"Recall-{class_name}\"] = recall_per_class[i].item()\n\n        # Log results\n        logger.info(\"Multi-label Classification Evaluation Results:\")\n        for k, v in results.items():\n            logger.info(f\"  {k}: {v:.2f}\")\n\n        # Return results in the expected format for trainer\n        return OrderedDict({\"classification\": results})\n</code></pre>"},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator.__init__","title":"<code>__init__(dataset_dict, distributed=True)</code>","text":"<p>Initialize the ClassificationEvaluator.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dict</code> <code>DictDataset</code> <p>Dataset in DictDataset format containing the ground truth annotations.</p> required <code>distributed</code> <code>bool</code> <p>If True, evaluation will be distributed across multiple processes. Defaults to True.</p> <code>True</code> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>def __init__(\n    self,\n    dataset_dict: DictDataset,\n    distributed=True,\n):\n    \"\"\"Initialize the ClassificationEvaluator.\n\n    Args:\n        dataset_dict (DictDataset): Dataset in DictDataset format containing\n            the ground truth annotations.\n        distributed (bool, optional): If True, evaluation will be distributed\n            across multiple processes. Defaults to True.\n    \"\"\"\n    self.dataset_dict = dataset_dict\n    self.metadata = self.dataset_dict.metadata\n    self._distributed = distributed\n    self._cpu_device = torch.device(\"cpu\")\n    self.num_classes = self.metadata.num_classes\n    self.class_names = self.metadata.thing_classes\n\n    self._predictions = []\n    self._targets = []\n</code></pre>"},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator.evaluate","title":"<code>evaluate()</code>","text":"<p>Evaluate multi-label classification metrics on accumulated predictions.</p> <p>Computes comprehensive multi-label classification metrics including subset accuracy, per-class precision/recall/F1, and macro/weighted averages.</p> <p>Returns:</p> Name Type Description <code>OrderedDict</code> <p>Dictionary containing evaluation metrics with the following keys: - 'Subset-Accuracy': Exact match accuracy (%) - 'Hamming-Loss': Hamming loss (lower is better) - 'Macro-Precision': Macro-averaged precision (%) - 'Macro-Recall': Macro-averaged recall (%) - 'Macro-F1': Macro-averaged F1 score (%) - 'Micro-Precision': Micro-averaged precision (%) - 'Micro-Recall': Micro-averaged recall (%) - 'Micro-F1': Micro-averaged F1 score (%) - 'Precision-{class_name}': Per-class precision (%) - 'Recall-{class_name}': Per-class recall (%) - 'F1-{class_name}': Per-class F1 score (%)</p> Note <ul> <li>In distributed mode, only the main process returns results</li> <li>All metrics are expressed as percentages except Hamming-Loss</li> <li>Returns empty dict if no predictions are available</li> <li>Results are wrapped in 'classification' key for trainer compatibility</li> </ul> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>def evaluate(self):\n    \"\"\"Evaluate multi-label classification metrics on accumulated predictions.\n\n    Computes comprehensive multi-label classification metrics including subset accuracy,\n    per-class precision/recall/F1, and macro/weighted averages.\n\n    Returns:\n        OrderedDict: Dictionary containing evaluation metrics with the following keys:\n            - 'Subset-Accuracy': Exact match accuracy (%)\n            - 'Hamming-Loss': Hamming loss (lower is better)\n            - 'Macro-Precision': Macro-averaged precision (%)\n            - 'Macro-Recall': Macro-averaged recall (%)\n            - 'Macro-F1': Macro-averaged F1 score (%)\n            - 'Micro-Precision': Micro-averaged precision (%)\n            - 'Micro-Recall': Micro-averaged recall (%)\n            - 'Micro-F1': Micro-averaged F1 score (%)\n            - 'Precision-{class_name}': Per-class precision (%)\n            - 'Recall-{class_name}': Per-class recall (%)\n            - 'F1-{class_name}': Per-class F1 score (%)\n\n    Note:\n        - In distributed mode, only the main process returns results\n        - All metrics are expressed as percentages except Hamming-Loss\n        - Returns empty dict if no predictions are available\n        - Results are wrapped in 'classification' key for trainer compatibility\n    \"\"\"\n    if self._distributed:\n        synchronize()\n        predictions_list = all_gather(self._predictions)\n        targets_list = all_gather(self._targets)\n\n        # Flatten gathered lists\n        predictions = []\n        targets = []\n        for p_list, t_list in zip(predictions_list, targets_list):\n            if p_list is not None:\n                predictions.extend(p_list)\n            if t_list is not None:\n                targets.extend(t_list)\n\n        if not is_main_process():\n            return\n    else:\n        predictions = self._predictions\n        targets = self._targets\n\n    # Check if we have predictions to evaluate\n    if len(predictions) == 0:\n        logger.warning(\"No predictions to evaluate\")\n        return OrderedDict({\"classification\": {}})\n\n    # Convert predictions to tensor (already binary from process method)\n    y_pred = torch.stack(predictions)  # [num_samples, num_classes]\n\n    # Convert targets to binary matrix\n    y_true = torch.stack(targets).to(dtype=y_pred.dtype, device=y_pred.device)\n\n    # Calculate per-class metrics\n    tp = (y_true * y_pred).sum(dim=0)  # True positives for each class\n    fp = ((1 - y_true) * y_pred).sum(dim=0)  # False positives for each class\n    fn = (y_true * (1 - y_pred)).sum(dim=0)  # False negatives for each class\n\n    # Precision for each class\n    precision_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n    for i in range(self.num_classes):\n        if tp[i] + fp[i] &gt; 0:\n            precision_per_class[i] = 100.0 * tp[i].float() / (tp[i] + fp[i]).float()\n\n    # Recall for each class\n    recall_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n    for i in range(self.num_classes):\n        if tp[i] + fn[i] &gt; 0:\n            recall_per_class[i] = 100.0 * tp[i].float() / (tp[i] + fn[i]).float()\n\n    # F1 score for each class\n    f1_per_class = torch.zeros(self.num_classes, dtype=torch.float)\n    for i in range(self.num_classes):\n        if precision_per_class[i] + recall_per_class[i] &gt; 0:\n            f1_per_class[i] = (\n                2 * (precision_per_class[i] * recall_per_class[i]) / (precision_per_class[i] + recall_per_class[i])\n            )\n\n    # Calculate macro averages (average across classes)\n    valid_classes = (tp + fn &gt; 0).sum().item()  # Classes that appear in ground truth\n    macro_precision = precision_per_class.sum() / max(valid_classes, 1)\n    macro_recall = recall_per_class.sum() / max(valid_classes, 1)\n\n    if macro_precision + macro_recall &gt; 0:\n        macro_f1 = 2 * (macro_precision * macro_recall) / (macro_precision + macro_recall)\n    else:\n        macro_f1 = torch.tensor(0.0)\n\n    # Create results dictionary\n    results = OrderedDict()\n    results[\"F1\"] = macro_f1.item()\n    results[\"Precision\"] = macro_precision.item()\n    results[\"Recall\"] = macro_recall.item()\n\n    # Add per-class metrics\n    if self.class_names is not None:\n        for i, class_name in enumerate(self.class_names):\n            if i &lt; self.num_classes:\n                results[f\"F1-{class_name}\"] = f1_per_class[i].item()\n                results[f\"Precision-{class_name}\"] = precision_per_class[i].item()\n                results[f\"Recall-{class_name}\"] = recall_per_class[i].item()\n\n    # Log results\n    logger.info(\"Multi-label Classification Evaluation Results:\")\n    for k, v in results.items():\n        logger.info(f\"  {k}: {v:.2f}\")\n\n    # Return results in the expected format for trainer\n    return OrderedDict({\"classification\": results})\n</code></pre>"},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator.from_datasetdict","title":"<code>from_datasetdict(dataset_dict, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create ClassificationEvaluator instance from a dataset dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dict</code> <p>Dataset dictionary containing the data and metadata.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ClassificationEvaluator</code> <p>New instance of the evaluator.</p> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>@classmethod\ndef from_datasetdict(cls, dataset_dict, **kwargs):\n    \"\"\"Create ClassificationEvaluator instance from a dataset dictionary.\n\n    Args:\n        dataset_dict: Dataset dictionary containing the data and metadata.\n        **kwargs: Additional keyword arguments passed to the constructor.\n\n    Returns:\n        ClassificationEvaluator: New instance of the evaluator.\n    \"\"\"\n    return cls(dataset_dict=dataset_dict, **kwargs)\n</code></pre>"},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator.process","title":"<code>process(inputs, outputs)</code>","text":"<p>Process a batch of inputs and outputs for evaluation.</p> <p>This method extracts predictions and ground truth labels from the provided inputs and outputs, then stores them for later evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[ClassificationDatasetDict]</code> <p>List of input dictionaries, each containing ground truth information. Expected to have 'label' field or 'annotations' with category_id.</p> required <code>outputs</code> <code>List[ClassificationModelOutput]</code> <p>List of model outputs, each containing 'logits' field with predicted class logits or ClassificationModelOutput instances.</p> required Note <ul> <li>Ground truth labels are extracted from input['label'] or   input['annotations'] with category_id (supports multi-label)</li> <li>Predictions are extracted from output['logits'] or output.logits</li> <li>Items with missing labels or logits are skipped with warnings</li> </ul> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>def process(self, inputs: List[ClassificationDatasetDict], outputs: List[ClassificationModelOutput]):\n    \"\"\"Process a batch of inputs and outputs for evaluation.\n\n    This method extracts predictions and ground truth labels from the provided\n    inputs and outputs, then stores them for later evaluation.\n\n    Args:\n        inputs (List[ClassificationDatasetDict]): List of input dictionaries,\n            each containing ground truth information. Expected to have 'label'\n            field or 'annotations' with category_id.\n        outputs (List[ClassificationModelOutput]): List of model outputs,\n            each containing 'logits' field with predicted class logits or\n            ClassificationModelOutput instances.\n\n    Note:\n        - Ground truth labels are extracted from input['label'] or\n          input['annotations'] with category_id (supports multi-label)\n        - Predictions are extracted from output['logits'] or output.logits\n        - Items with missing labels or logits are skipped with warnings\n    \"\"\"\n    for input_item, output_item in zip(inputs, outputs):\n        # Get ground truth label from input\n        label_list = None\n        if \"label\" in input_item:\n            label_list = input_item[\"label\"]\n        elif hasattr(input_item, \"label\"):\n            label_list = input_item.label\n        elif \"annotations\" in input_item and len(input_item[\"annotations\"]) &gt; 0:\n            # Handle label from annotations format (multi-label)\n            label_list = [ann.get(\"category_id\", None) for ann in input_item[\"annotations\"]]\n        if label_list is None:\n            raise ValueError(f\"Could not find label in input item: {input_item}\")\n        label = torch.zeros((self.num_classes), dtype=torch.int)\n        for label_i in label_list:\n            if label_i is not None:\n                label[label_i] = 1\n\n        if label is None:\n            logger.warning(f\"Could not find label in input item: {input_item}\")\n            continue\n\n        # Get model predictions from output\n        logits = None\n        if isinstance(output_item, dict) and \"logits\" in output_item:\n            logits = output_item[\"logits\"]\n        elif hasattr(output_item, \"logits\"):\n            # Handle ClassificationModelOutput objects\n            logits = output_item.logits\n\n        if logits is None:\n            logger.warning(f\"Could not find logits in output item: {output_item}\")\n            continue\n\n        # Move tensors to CPU for evaluation\n        logits = logits.to(self._cpu_device)\n\n        # For multi-label classification, convert logits to binary predictions\n        # Use sigmoid + threshold for multi-label prediction\n        predicted_labels = (logits &gt; 0.5).int()\n\n        # Store prediction and ground truth\n        self._predictions.append(predicted_labels)\n        self._targets.append(label)\n</code></pre>"},{"location":"api/trainer_evaluation/#focoos.trainer.evaluation.classification_evaluation.ClassificationEvaluator.reset","title":"<code>reset()</code>","text":"<p>Clear stored predictions and targets.</p> <p>This method resets the internal state of the evaluator by clearing all accumulated predictions and ground truth targets.</p> Source code in <code>focoos/trainer/evaluation/classification_evaluation.py</code> <pre><code>def reset(self):\n    \"\"\"Clear stored predictions and targets.\n\n    This method resets the internal state of the evaluator by clearing\n    all accumulated predictions and ground truth targets.\n    \"\"\"\n    self._predictions = []\n    self._targets = []\n</code></pre>"},{"location":"api/trainer_hooks/","title":"hooks","text":""},{"location":"api/trainer_hooks/#focoos.trainer.hooks.early_stop.EarlyStoppingHook","title":"<code>EarlyStoppingHook</code>","text":"<p>               Bases: <code>HookBase</code></p> Source code in <code>focoos/trainer/hooks/early_stop.py</code> <pre><code>class EarlyStoppingHook(HookBase):\n    def __init__(\n        self,\n        enabled: bool,\n        eval_period: int,\n        patience: int,\n        val_metric: str,\n        mode: str = \"max\",\n    ):\n        \"\"\"\n        Initializes the EarlyStoppingHook.\n\n        This hook is designed to monitor a specific validation metric during the training process\n        and stop training when no improvement is observed in the metric for a specified number of\n        iterations. This is particularly useful for preventing overfitting by halting training\n        once the model's performance on the validation set no longer improves.\n\n        Args:\n            eval_period (int): The frequency (in iterations) at which the validation metric is evaluated.\n                               For example, if `eval_period` is 100, the validation metric will be checked\n                               every 100 iterations.\n            patience (int): Number of consecutive evaluations with no improvement after which training will be stopped.\n                            For example, if `patience` is set to 5, training will stop if the validation metric does not\n                            improve for 5 consecutive evaluations.\n            val_metric (str): The name of the validation metric to monitor. This should correspond to one of the metrics\n                              calculated during the validation phase, such as \"accuracy\", \"loss\", etc.\n            mode (str, optional): One of \"min\" or \"max\". This parameter dictates the direction of improvement\n                                  for the validation metric. In \"min\" mode, the training will stop when the monitored\n                                  quantity (e.g., loss) stops decreasing. In \"max\" mode, training will stop when\n                                  the monitored quantity (e.g., accuracy) stops increasing. Defaults to \"max\".\n\n        \"\"\"\n        self.enabled = enabled\n        self.patience = patience\n        self.val_metric = val_metric\n        self.mode = mode\n        self.best_metric = None\n        self.num_bad_epochs = 0\n        self._period = eval_period\n        self._logger = get_logger(__name__)\n\n    def after_step(self):\n        next_iter = self.trainer.iter + 1\n\n        if self._period &gt; 0 and next_iter % self._period == 0 and next_iter != self.trainer.max_iter and self.enabled:\n            metric_tuple = self.trainer.storage.latest().get(self.val_metric)\n\n            if metric_tuple is None:\n                return\n            else:\n                current_metric, metric_iter = metric_tuple\n\n            if (\n                self.best_metric is None\n                or (self.mode == \"max\" and current_metric &gt; self.best_metric)\n                or (self.mode == \"min\" and current_metric &lt; self.best_metric)\n            ):\n                self.best_metric = current_metric\n                self.num_bad_epochs = 0\n            else:\n                self.num_bad_epochs += 1\n                self._logger.info(f\"{self.num_bad_epochs}/{self.patience} without improvements..\")\n\n            if self.num_bad_epochs &gt;= self.patience:\n                self.trainer.storage.put_scalar(\"early_stopping\", True)\n                raise EarlyStopException(\"Early Stopping Exception to stop the training..\")\n</code></pre>"},{"location":"api/trainer_hooks/#focoos.trainer.hooks.early_stop.EarlyStoppingHook.__init__","title":"<code>__init__(enabled, eval_period, patience, val_metric, mode='max')</code>","text":"<p>Initializes the EarlyStoppingHook.</p> <p>This hook is designed to monitor a specific validation metric during the training process and stop training when no improvement is observed in the metric for a specified number of iterations. This is particularly useful for preventing overfitting by halting training once the model's performance on the validation set no longer improves.</p> <p>Parameters:</p> Name Type Description Default <code>eval_period</code> <code>int</code> <p>The frequency (in iterations) at which the validation metric is evaluated.                For example, if <code>eval_period</code> is 100, the validation metric will be checked                every 100 iterations.</p> required <code>patience</code> <code>int</code> <p>Number of consecutive evaluations with no improvement after which training will be stopped.             For example, if <code>patience</code> is set to 5, training will stop if the validation metric does not             improve for 5 consecutive evaluations.</p> required <code>val_metric</code> <code>str</code> <p>The name of the validation metric to monitor. This should correspond to one of the metrics               calculated during the validation phase, such as \"accuracy\", \"loss\", etc.</p> required <code>mode</code> <code>str</code> <p>One of \"min\" or \"max\". This parameter dictates the direction of improvement                   for the validation metric. In \"min\" mode, the training will stop when the monitored                   quantity (e.g., loss) stops decreasing. In \"max\" mode, training will stop when                   the monitored quantity (e.g., accuracy) stops increasing. Defaults to \"max\".</p> <code>'max'</code> Source code in <code>focoos/trainer/hooks/early_stop.py</code> <pre><code>def __init__(\n    self,\n    enabled: bool,\n    eval_period: int,\n    patience: int,\n    val_metric: str,\n    mode: str = \"max\",\n):\n    \"\"\"\n    Initializes the EarlyStoppingHook.\n\n    This hook is designed to monitor a specific validation metric during the training process\n    and stop training when no improvement is observed in the metric for a specified number of\n    iterations. This is particularly useful for preventing overfitting by halting training\n    once the model's performance on the validation set no longer improves.\n\n    Args:\n        eval_period (int): The frequency (in iterations) at which the validation metric is evaluated.\n                           For example, if `eval_period` is 100, the validation metric will be checked\n                           every 100 iterations.\n        patience (int): Number of consecutive evaluations with no improvement after which training will be stopped.\n                        For example, if `patience` is set to 5, training will stop if the validation metric does not\n                        improve for 5 consecutive evaluations.\n        val_metric (str): The name of the validation metric to monitor. This should correspond to one of the metrics\n                          calculated during the validation phase, such as \"accuracy\", \"loss\", etc.\n        mode (str, optional): One of \"min\" or \"max\". This parameter dictates the direction of improvement\n                              for the validation metric. In \"min\" mode, the training will stop when the monitored\n                              quantity (e.g., loss) stops decreasing. In \"max\" mode, training will stop when\n                              the monitored quantity (e.g., accuracy) stops increasing. Defaults to \"max\".\n\n    \"\"\"\n    self.enabled = enabled\n    self.patience = patience\n    self.val_metric = val_metric\n    self.mode = mode\n    self.best_metric = None\n    self.num_bad_epochs = 0\n    self._period = eval_period\n    self._logger = get_logger(__name__)\n</code></pre>"},{"location":"api/trainer_hooks/#focoos.trainer.hooks.sync_to_hub.SyncToHubHook","title":"<code>SyncToHubHook</code>","text":"<p>               Bases: <code>HookBase</code></p> Source code in <code>focoos/trainer/hooks/sync_to_hub.py</code> <pre><code>class SyncToHubHook(HookBase):\n    def __init__(\n        self,\n        remote_model: RemoteModel,\n        model_info: ModelInfo,\n        output_dir: str,\n        sync_period: int = 100,\n        eval_period: int = 100,\n    ):\n        self.remote_model = remote_model\n        self.model_info = model_info\n        self.output_dir = output_dir\n        self.sync_period = sync_period\n        self.eval_period = eval_period\n\n    @property\n    def iteration(self):\n        try:\n            _iter = self.trainer.iter\n        except Exception:\n            _iter = 1\n        return _iter\n\n    def before_train(self):\n        \"\"\"\n        Called before the first iteration.\n        \"\"\"\n        info = HubSyncLocalTraining(\n            status=ModelStatus.TRAINING_RUNNING,  # type: ignore\n            iterations=0,\n            metrics=None,\n        )\n\n        self._sync_train_job(info)\n\n    def after_step(self):\n        if (self.iteration % self.sync_period == 0) and self.iteration &gt; 0:\n            self._sync_train_job(\n                sync_info=HubSyncLocalTraining(\n                    status=ModelStatus.TRAINING_RUNNING,  # type: ignore\n                    iterations=self.iteration,\n                    training_info=self.model_info.training_info,\n                )\n            )\n\n        elif (self.iteration % (self.eval_period + 3) == 0) and self.iteration &gt; 0:\n            self._sync_train_job(\n                sync_info=HubSyncLocalTraining(\n                    status=ModelStatus.TRAINING_RUNNING,  # type: ignore\n                    iterations=self.iteration,\n                    training_info=self.model_info.training_info,\n                ),\n                upload_artifacts=[ArtifactName.WEIGHTS],\n            )\n\n    def after_train(self):\n        # Catch exception and sync training info, final weights will be synced in main trainer fn\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        if exc_type is not None:\n            logger.error(\n                f\"Exception during training, status set to TRAINING_ERROR: {str(exc_type.__name__)} {str(exc_value)}\"\n            )\n            status = ModelStatus.TRAINING_ERROR\n            self.model_info.status = status\n            if self.model_info.training_info is not None:\n                self.model_info.training_info.main_status = status\n                self.model_info.training_info.failure_reason = str(exc_value)\n                self.model_info.training_info.end_time = datetime.now().isoformat()\n                if self.model_info.training_info.status_transitions is None:\n                    self.model_info.training_info.status_transitions = []\n                self.model_info.training_info.status_transitions.append(\n                    dict(\n                        status=status,\n                        timestamp=datetime.now().isoformat(),\n                        detail=f\"{str(exc_type.__name__)}:  {str(exc_value)}\",\n                    )\n                )\n            self.model_info.dump_json(os.path.join(self.output_dir, ArtifactName.INFO))\n            self._sync_train_job(\n                sync_info=HubSyncLocalTraining(\n                    status=status,\n                    iterations=self.iteration,\n                    training_info=self.model_info.training_info,\n                ),\n                upload_artifacts=[\n                    ArtifactName.WEIGHTS,\n                    ArtifactName.LOGS,\n                    ArtifactName.INFO,\n                    ArtifactName.METRICS,\n                ],\n            )\n\n    def _sync_train_job(self, sync_info: HubSyncLocalTraining, upload_artifacts: Optional[List[ArtifactName]] = None):\n        try:\n            self.remote_model.sync_local_training_job(sync_info, self.output_dir, upload_artifacts)\n            # logger.debug(f\"Sync: {self.iteration} {self.model_info.name} ref: {self.model_info.ref}\")\n        except Exception as e:\n            logger.error(f\"[sync_train_job] failed to sync train job: {str(e)}\")\n</code></pre>"},{"location":"api/trainer_hooks/#focoos.trainer.hooks.sync_to_hub.SyncToHubHook.before_train","title":"<code>before_train()</code>","text":"<p>Called before the first iteration.</p> Source code in <code>focoos/trainer/hooks/sync_to_hub.py</code> <pre><code>def before_train(self):\n    \"\"\"\n    Called before the first iteration.\n    \"\"\"\n    info = HubSyncLocalTraining(\n        status=ModelStatus.TRAINING_RUNNING,  # type: ignore\n        iterations=0,\n        metrics=None,\n    )\n\n    self._sync_train_job(info)\n</code></pre>"},{"location":"development/changelog/","title":"Changelog","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"development/code_of_conduct/","title":"Code of conduct","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"development/contributing/","title":"Contributing","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"helpers/wip/","title":"Wip","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p> <p>This page is currently being developed and may not be complete.</p> <p>Feel free to contribute to this page! If you have suggestions or would like to help improve it, please contact us.</p>"},{"location":"hub/","title":"\ud83d\ude80 Focoos HUB Overview","text":"<p>The Focoos HUB is a cloud-based platform that provides seamless integration between your local development environment and the Focoos AI ecosystem. It enables you to manage models, datasets, perform remote inference operations, and monitor training progress through a unified API.</p> <p></p>"},{"location":"hub/#what-is-focoos-hub","title":"What is Focoos HUB?","text":"<p>Focoos HUB serves as your gateway to:</p> <ul> <li>Model Management: Store, version, and share your trained computer vision models</li> <li>Remote Inference: Run inference on cloud infrastructure without local GPU requirements</li> <li>Dataset Management: Upload, download, and manage datasets in the cloud</li> <li>Collaboration: Share models and datasets with team members</li> <li>Monitoring: Track model performance and usage metrics</li> </ul>"},{"location":"hub/#key-components","title":"Key Components","text":""},{"location":"hub/#focooshub-client","title":"FocoosHUB Client","text":"<p>The main interface for interacting with the Focoos platform. It provides authentication and access to all HUB services.</p> <pre><code>from focoos import FocoosHUB\n\n# Initialize the HUB client\nhub = FocoosHUB()\n\n# Get user information\nuser_info = hub.get_user_info()\nprint(f\"Welcome {user_info.email}!\")\n</code></pre>"},{"location":"hub/#remote-models","title":"Remote Models","text":"<p>Access and manage models stored in the cloud:</p> <ul> <li>List your available models</li> <li>Get detailed model information</li> <li>Download models for local use</li> <li>Perform remote inference without downloading</li> </ul>"},{"location":"hub/#remote-datasets","title":"Remote Datasets","text":"<p>Manage datasets in the cloud:</p> <ul> <li>Upload local datasets to the cloud</li> <li>Download shared datasets</li> <li>Access dataset metadata and specifications</li> <li>List available datasets (owned and shared)</li> </ul>"},{"location":"hub/#getting-started","title":"Getting Started","text":"<p>To start using Focoos HUB:</p> <ol> <li>Authentication: Set up your API key in the configuration</li> <li>Initialize: Create a FocoosHUB instance</li> <li>Explore: List your models and datasets</li> <li>Use: Run inference or manage your ML artifacts</li> </ol> <p>See the HUB section for detailed usage examples and the Remote Inference guide for cloud-based inference workflows.</p>"},{"location":"hub/#benefits","title":"Benefits","text":"<ul> <li>Scalability: Access to cloud GPU resources for inference</li> <li>Collaboration: Easy sharing of models and datasets</li> <li>Cost Efficiency: Pay-per-use inference without maintaining infrastructure</li> <li>Integration: Seamless workflow between local development and cloud deployment</li> </ul>"},{"location":"hub/hub/","title":"\ud83d\ude80 Focoos HUB","text":"<p>The FocoosHUB class is your main interface for interacting with the Focoos cloud platform. It provides comprehensive functionality for managing models, datasets, and performing cloud operations.</p>"},{"location":"hub/hub/#getting-started","title":"Getting Started","text":""},{"location":"hub/hub/#authentication","title":"Authentication","text":"<p>Before using the HUB, you need to authenticate with your API key:</p> <pre><code>from focoos import FocoosHUB\n\n# Option 1: Use API key from configuration\nhub = FocoosHUB()\n\n# Option 2: Explicitly provide API key\nhub = FocoosHUB(api_key=\"your-api-key-here\")\n</code></pre>"},{"location":"hub/hub/#get-user-information","title":"Get User Information","text":"<p>Check your account details and quotas:</p> <pre><code>user_info = hub.get_user_info()\n\nprint(f\"Email: {user_info.email}\")\nprint(f\"Company: {user_info.company}\")\nprint(f\"Storage used: {user_info.quotas.used_storage_gb}GB\")\nprint(f\"Inferences used: {user_info.quotas.total_inferences}\")\n</code></pre>"},{"location":"hub/hub/#working-with-models","title":"Working with Models","text":""},{"location":"hub/hub/#list-your-models","title":"List Your Models","text":"<p>Get an overview of all models in your account:</p> <pre><code>models = hub.list_remote_models()\n\nfor model in models:\n    print(f\"Model: {model.name}\")\n    print(f\"  - Reference: {model.ref}\")\n    print(f\"  - Task: {model.task}\")\n    print(f\"  - Status: {model.status}\")\n    print(f\"  - Created: {model.created_at}\")\n</code></pre>"},{"location":"hub/hub/#get-model-information","title":"Get Model Information","text":"<p>Retrieve detailed information about a specific model:</p> <pre><code>model_ref = \"your-model-reference\"\nmodel_info = hub.get_model_info(model_ref)\n\nprint(f\"Model Name: {model_info.name}\")\nprint(f\"Description: {model_info.description}\")\nprint(f\"Task: {model_info.task}\")\nprint(f\"Classes: {model_info.classes}\")\nprint(f\"Image Size: {model_info.im_size}\")\nprint(f\"Status: {model_info.status}\")\n\n# Access training information if available\nif model_info.training:\n    print(f\"Training Status: {model_info.training.status}\")\n    print(f\"Training Progress: {model_info.training.progress}%\")\n</code></pre>"},{"location":"hub/hub/#get-remote-model-instance","title":"Get Remote Model Instance","text":"<p>Get a remote model instance for cloud-based operations:</p> <pre><code>model_ref = \"your-model-reference\"\nremote_model = hub.get_remote_model(model_ref)\n\n# This model can be used for remote inference\nresults = remote_model.infer(\"path/to/image.jpg\", threshold=0.5)\n\n# Get model training information\ntraining_info = remote_model.train_info()\nprint(f\"Training status: {training_info.status}\")\n\n# Get model metrics\nmetrics = remote_model.metrics()\nprint(f\"Validation mAP: {metrics.map}\")\n</code></pre>"},{"location":"hub/hub/#working-with-datasets","title":"Working with Datasets","text":""},{"location":"hub/hub/#list-available-datasets","title":"List Available Datasets","text":"<p>View datasets you own and optionally shared datasets:</p> <pre><code># List only your datasets\nmy_datasets = hub.list_remote_datasets()\n\n# List your datasets and shared datasets\nall_datasets = hub.list_remote_datasets(include_shared=True)\n\nfor dataset in all_datasets:\n    print(f\"Dataset: {dataset.name}\")\n    print(f\"  - Reference: {dataset.ref}\")\n    print(f\"  - Task: {dataset.task}\")\n    print(f\"  - Layout: {dataset.layout}\")\n    if dataset.spec:\n        print(f\"  - spec.train_length: {dataset.spec.train_length}\")\n        print(f\"  - spec.valid_length: {dataset.spec.valid_length}\")\n        print(f\"  - spec.size_mb: {dataset.spec.size_mb}\")\n</code></pre>"},{"location":"hub/hub/#working-with-remote-datasets","title":"Working with Remote Datasets","text":"<p>Get a remote dataset instance and work with it:</p> <pre><code>dataset_ref = \"your-dataset-reference\"\nremote_dataset = hub.get_remote_dataset(dataset_ref)\n\nprint(f\"Dataset Name: {remote_dataset.name}\")\nprint(f\"Task: {remote_dataset.task}\")\nprint(f\"Layout: {remote_dataset.layout}\")\n\n# Download dataset data\nlocal_path = remote_dataset.download_data(\"./datasets\")\nprint(f\"Dataset downloaded to: {local_path}\")\n</code></pre>"},{"location":"hub/hub/#error-handling","title":"Error Handling","text":"<p>The HUB client raises <code>ValueError</code> exceptions for API errors:</p> <pre><code>try:\n    model_info = hub.get_model_info(\"non-existent-model\")\nexcept ValueError as e:\n    print(f\"Error retrieving model: {e}\")\n\ntry:\n    models = hub.list_remote_models()\nexcept ValueError as e:\n    print(f\"Error listing models: {e}\")\n</code></pre>"},{"location":"hub/hub/#configuration","title":"Configuration","text":"<p>The HUB client uses configuration from the global <code>FOCOOS_CONFIG</code>:</p> <pre><code>from focoos import FOCOOS_CONFIG\n\n# Check current configuration\nprint(f\"API Key: {FOCOOS_CONFIG.focoos_api_key}\")\nprint(f\"Log Level: {FOCOOS_CONFIG.focoos_log_level}\")\nprint(f\"Runtime type: {FOCOOS_CONFIG.runtime_type}\")\nprint(f\"Warmup iter: {FOCOOS_CONFIG.warmup_iter}\")\n\n# The HUB client will use these values by default\nhub = FocoosHUB()  # Uses FOCOOS_CONFIG values\n</code></pre>"},{"location":"hub/hub/#advanced-usage","title":"Advanced Usage","text":""},{"location":"hub/hub/#model-training-integration","title":"Model Training Integration","text":"<p>When training models locally, you can sync them to the HUB:</p> <pre><code>from focoos.models.focoos_model import FocoosModel\nfrom focoos.ports import TrainerArgs\nfrom focoos import ModelManager\n\n# Load a model for training\nmodel = ModelManager.get(\"fai-detr-l-obj365\")\n\n# Configure training with HUB sync\ntrain_args = TrainerArgs(\n    max_iters=1000,\n    batch_size=16,\n    sync_to_hub=True  # This will automatically create a remote model\n)\n\n# Train the model (this will sync to HUB)\nmodel.train(train_args, train_dataset, val_dataset, hub=hub)\n</code></pre>"},{"location":"hub/hub/#monitoring-training","title":"Monitoring Training","text":"<p>Monitor training progress of remote models:</p> <pre><code>remote_model = hub.get_remote_model(\"training-model-ref\")\n\n# Get training info\ntraining_info = remote_model.train_info()\n\nif training_info:\n    print(f\"Algorithm Name: {training_info.algorithm_name}\")\n    print(f\"Instance Device: {training_info.instance_device}\")\n    print(f\"Instance Type: {training_info.instance_type}\")\n    print(f\"Volume Size: {training_info.volume_size}\")\n    print(f\"Main Status: {training_info.main_status}\")\n    print(f\"Failure Reason: {training_info.failure_reason}\")\n    print(f\"Status Transitions: {training_info.status_transitions}\")\n    print(f\"Start Time: {training_info.start_time}\")\n    print(f\"End Time: {training_info.end_time}\")\n    print(f\"Artifact Location: {training_info.artifact_location}\")\n\n# Get training logs\nlogs = remote_model.train_logs()\nfor log_entry in logs:\n    print(log_entry)\n</code></pre>"},{"location":"hub/hub/#see-also","title":"See Also","text":"<ul> <li>Remote Inference - Learn about cloud-based inference</li> <li>Overview - Understand the HUB architecture</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"hub/remote_inference/","title":"\ud83c\udf0d Remote Inference","text":"<p>Remote inference allows you to run computer vision models in the cloud without needing local GPU resources. This is perfect for production deployments, edge devices, or when you want to avoid the overhead of managing local model inference.</p>"},{"location":"hub/remote_inference/#what-is-remote-inference","title":"What is Remote Inference?","text":"<p>Remote inference uses the Focoos cloud infrastructure to run your models. Instead of loading models locally, you send images to the cloud API and receive inference results. This provides several advantages:</p> <ul> <li>No Local GPU Required: Run inference on any device, including CPU-only machines</li> <li>Scalability: Handle varying inference loads without managing infrastructure</li> <li>Always Updated: Use the latest version of your models automatically</li> <li>Cost Efficient: Pay per inference without maintaining dedicated hardware</li> <li>Low Latency: Optimized cloud infrastructure for fast inference</li> </ul>"},{"location":"hub/remote_inference/#getting-started","title":"Getting Started","text":""},{"location":"hub/remote_inference/#basic-remote-inference","title":"Basic Remote Inference","text":"<p>Here's how to perform remote inference with a model:</p> <pre><code>from focoos import FocoosHUB\n\n# Initialize the HUB client\nhub = FocoosHUB()\n\n# Get a remote model instance\nmodel_ref = \"fai-detr-l-obj365\"  # Use any available model\nremote_model = hub.get_remote_model(model_ref)\n\n# Perform inference\nresults = remote_model.infer(\"path/to/image.jpg\", threshold=0.5)\n\n# Process results\nfor detection in results.detections:\n    print(f\"Class ID: {detection.cls_id}\")\n    print(f\"Confidence: {detection.conf:.3f}\")\n    print(f\"Bounding Box: {detection.bbox}\")\n</code></pre>"},{"location":"hub/remote_inference/#using-the-callable-interface","title":"Using the Callable Interface","text":"<p>Remote models can also be called directly like functions:</p> <pre><code># This is equivalent to calling remote_model.infer()\nresults = remote_model.infer(\"path/to/image.jpg\", threshold=0.5)\n</code></pre>"},{"location":"hub/remote_inference/#supported-input-types","title":"Supported Input Types","text":"<p>Remote inference accepts various input types:</p>"},{"location":"hub/remote_inference/#file-paths","title":"File Paths","text":"<pre><code>results = remote_model.infer(\"./images/photo.jpg\")\n</code></pre>"},{"location":"hub/remote_inference/#numpy-arrays","title":"NumPy Arrays","text":"<pre><code>import cv2\nimport numpy as np\n\n# Load image as numpy array\nimage = cv2.imread(\"photo.jpg\")\nresults = remote_model.infer(image, threshold=0.3)\n</code></pre>"},{"location":"hub/remote_inference/#pil-images","title":"PIL Images","text":"<pre><code>from PIL import Image\n\n# Load with PIL\npil_image = Image.open(\"photo.jpg\")\nresults = remote_model.infer(pil_image)\n</code></pre>"},{"location":"hub/remote_inference/#raw-bytes","title":"Raw Bytes","text":"<pre><code># Image as bytes\nwith open(\"photo.jpg\", \"rb\") as f:\n    image_bytes = f.read()\n\nresults = remote_model.infer(image_bytes)\n</code></pre>"},{"location":"hub/remote_inference/#inference-parameters","title":"Inference Parameters","text":""},{"location":"hub/remote_inference/#threshold-control","title":"Threshold Control","text":"<p>Control detection sensitivity with the threshold parameter:</p> <pre><code># High threshold - only very confident detections\nresults = remote_model.infer(\"image.jpg\", threshold=0.8)\n\n# Low threshold - more detections, potentially less accurate\nresults = remote_model.infer(\"image.jpg\", threshold=0.2)\n\n# Default threshold (usually 0.5)\nresults = remote_model.infer(\"image.jpg\")\n</code></pre>"},{"location":"hub/remote_inference/#working-with-results","title":"Working with Results","text":""},{"location":"hub/remote_inference/#detection-results","title":"Detection Results","text":"<p>For object detection models, results contain bounding boxes and classifications:</p> <pre><code>results = remote_model.infer(\"image.jpg\")\n\nprint(f\"Found {len(results.detections)} objects\")\n\nfor i, detection in enumerate(results.detections):\n    print(f\"Detection {i+1}:\")\n    print(f\"  Class ID: {detection.cls_id}\")\n    print(f\"  Confidence: {detection.conf:.3f}\")\n    print(f\"  Bounding Box: {detection.bbox}\")\n\n    # Box coordinates\n    if detection.bbox:\n        x1, y1, x2, y2 = detection.bbox[0], detection.bbox[1], detection.bbox[2], detection.bbox[3]\n    print(f\"  Coordinates: ({x1}, {y1}) to ({x2}, {y2})\")\n</code></pre>"},{"location":"hub/remote_inference/#visualization","title":"Visualization","text":"<p>Visualize results using the built-in utilities:</p> <pre><code>results = model.infer(image=image, threshold=0.5,annotate=True)\n\nImage.fromarray(results.image)\n</code></pre>"},{"location":"hub/remote_inference/#model-management-for-remote-inference","title":"Model Management for Remote Inference","text":""},{"location":"hub/remote_inference/#checking-model-status","title":"Checking Model Status","text":"<p>Before using a model for inference, check its status:</p> <pre><code>model_info = remote_model.get_info()\n\nif model_info.status == ModelStatus.TRAINING_COMPLETED:\n    print(\"Model is ready for inference\")\n    results = remote_model.infer(\"image.jpg\")\nelif model_info.status == ModelStatus.TRAINING_RUNNING:\n    print(\"Model is still training\")\nelif model_info.status == ModelStatus.TRAINING_ERROR:\n    print(\"Model has an error\")\n</code></pre>"},{"location":"hub/remote_inference/#model-information","title":"Model Information","text":"<p>Get detailed information about the remote model:</p> <pre><code>model_info = remote_model.get_info()\n\nprint(f\"Model: {model_info.name}\")\nprint(f\"Task: {model_info.task}\")\nprint(f\"Classes: {model_info.classes}\")\nprint(f\"Image Size: {model_info.im_size}\")\nprint(f\"Status: {model_info.status}\")\n</code></pre>"},{"location":"hub/remote_inference/#comparison-remote-vs-local-inference","title":"Comparison: Remote vs Local Inference","text":"Aspect Remote Inference Local Inference Hardware No GPU required GPU recommended Setup Instant Model download required Scalability Automatic Manual scaling Cost Pay per use Infrastructure costs Latency Network dependent Very low Privacy Data sent to cloud Data stays local Offline Requires internet Works offline"},{"location":"hub/remote_inference/#best-practices","title":"Best Practices","text":"<ol> <li>Optimize Images: Resize large images to reduce upload time and costs</li> <li>Handle Errors: Implement retry logic for network issues</li> <li>Batch Smartly: Group related inferences to minimize overhead</li> <li>Monitor Usage: Track inference costs and quotas</li> <li>Cache Results: Store results for identical inputs when appropriate</li> <li>Use Appropriate Thresholds: Tune detection thresholds for your use case</li> </ol>"},{"location":"hub/remote_inference/#see-also","title":"See Also","text":"<ul> <li>HUB - Complete HUB documentation</li> <li>Overview - HUB architecture overview</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"models/","title":"Focoos Models \ud83e\udde0","text":"<p>With the Focoos SDK, you can take advantage of a collection of foundational models that are optimized for a range of computer vision tasks. These pre-trained models, covering detection and semantic segmentation across various domains, provide an excellent starting point for your specific use case. Whether you need to fine-tune for custom requirements or adapt them to your application, these models offer a solid foundation to accelerate your development process.</p>"},{"location":"models/#semantic-segmentation","title":"Semantic Segmentation \ud83d\uddbc\ufe0f","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-mf-l-ade Mask2Former (Resnet-101) Common Scene (150) ADE20K mIoU: 48.27mAcc: 62.15 73 fai-mf-m-ade Mask2Former (STDC-2) Common Scene (150) ADE20K mIoU: 45.32mACC: 57.75 127 bisenetformer-l-ade BisenetFormer (STDC-2) Common Scene (150) ADE20K mIoU: 45.07mAcc: 58.03 - bisenetformer-m-ade BisenetFormer (STDC-2) Common Scene (150) ADE20K mIoU: 43.43mACC: 57.01 - bisenetformer-s-ade BisenetFormer (STDC-1) Common Scene (150) ADE20K mIoU: 42.91mACC: 56.55 - <p> mIoU = Intersection over Union averaged by class   mAcc = Pixel Accuracy averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"models/#object-detection","title":"Object Detection \ud83d\udd75\ufe0f\u200d\u2642\ufe0f","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-detr-l-coco RT-DETR (Resnet-50) Common Objects (80) COCO bbox/AP: 53.06bbox/AP50: 70.91 87 fai-detr-m-coco RT-DETR (STDC-2) Common Objects (80) COCO bbox/AP: 44.69bbox/AP50: 61.63 181 fai-detr-l-obj365 RT-DETR (Resnet50) Common Objects (365) Objects365 bbox/AP: 34.60bbox/AP50: 45.81 87 <p> AP = Average Precision averaged by class   AP50 = Average Precision at IoU threshold 0.50 averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"models/#instance-segmentation","title":"Instance Segmentation \ud83c\udfad","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-mf-s-coco-ins Mask2Former (Resnet-50) Common Objects (80) COCO segm/AP: 41.45segm/AP50: 64.12 86 fai-mf-m-coco-ins Mask2Former (Resnet-101) Common Objects (80) COCO segm/AP: 43.09segm/AP50: 65.87 70 fai-mf-l-coco-ins Mask2Former (Resnet-101) Common Objects (80) COCO segm/AP: 44.23segm/AP50: 67.53 55 <p> AP = Average Precision averaged by class   AP50 = Average Precision at IoU threshold 0.50 averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"models/#keypoint-detection","title":"Keypoint Detection \ud83e\udd77","text":"Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 rtmo-s-coco RTMO (CSP-Darknet) Persons (1) COCO keypoints/AP: 67.94keypoints/AP50: 87.86 104 rtmo-m-coco RTMO (CSP-Darknet) Persons (1) COCO keypoints/AP: 70.94keypoints/AP50: 89.47 89 rtmo-l-coco RTMO (CSP-Darknet) Persons (1) COCO keypoints/AP: 72.14keypoints/AP50: 89.85 63 <p> AP = Average Precision averaged by class   AP50 = Average Precision at IoU threshold 0.50 averaged by class   FPS = Frames per second computed using TensorRT with resolution 640x640  </p>"},{"location":"models/bisenetformer/","title":"BisenetFormer","text":""},{"location":"models/bisenetformer/#overview","title":"Overview","text":"<p>BisenetFormer is an advanced segmentation model that combines the efficiency of BiSeNet (Bilateral Segmentation Network) with the power of transformer architectures. Developed by FocoosAI, this model is designed for real-time semantic segmentation tasks requiring both high accuracy and computational efficiency.</p> <p>The model employs a dual-path architecture where spatial details are preserved through one path while semantic information is processed through another, then fused with transformer-based attention mechanisms for superior segmentation performance.</p>"},{"location":"models/bisenetformer/#neural-network-architecture","title":"Neural Network Architecture","text":"<p> The BisenetFormer architecture consists of four main components working in concert:</p>"},{"location":"models/bisenetformer/#backbone","title":"Backbone","text":"<ul> <li>Purpose: Feature extraction from input images</li> <li>Design: Configurable backbone network (e.g., ResNet, STDC)</li> <li>Output: Multi-scale features at different resolutions (1/4, 1/8, 1/16, 1/32)</li> </ul>"},{"location":"models/bisenetformer/#context-path","title":"Context Path","text":"<ul> <li>Component: Global context extraction path</li> <li>Features:</li> <li>Attention Refinement Module (ARM) for feature enhancement</li> <li>Global Average Pooling for context aggregation</li> <li>Multi-scale feature fusion with upsampling</li> <li>Purpose: Captures high-level semantic information</li> </ul>"},{"location":"models/bisenetformer/#spatial-path-detail-branch","title":"Spatial Path (Detail Branch)","text":"<ul> <li>Component: Spatial detail preservation path</li> <li>Features:</li> <li>Bilateral structure maintaining spatial resolution</li> <li>ConvBNReLU blocks for efficient processing</li> <li>Feature Fusion Module (FFM) for combining paths</li> <li>Purpose: Preserves fine-grained spatial details</li> </ul>"},{"location":"models/bisenetformer/#transformer-decoder","title":"Transformer Decoder","text":"<ul> <li>Design: Lightweight transformer decoder with attention mechanisms</li> <li>Components:</li> <li>Self-attention layers for feature refinement</li> <li>Cross-attention layers for multi-scale feature integration</li> <li>Feed-forward networks (FFN) for feature transformation</li> <li>100 learnable object queries (configurable)</li> <li>Layers: Configurable number of decoder layers (default: 6)</li> </ul>"},{"location":"models/bisenetformer/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"models/bisenetformer/#core-model-parameters","title":"Core Model Parameters","text":"<ul> <li><code>num_classes</code> (int): Number of segmentation classes</li> <li><code>num_queries</code> (int, default=100): Number of learnable object queries</li> <li><code>backbone_config</code> (BackboneConfig): Backbone network configuration</li> </ul>"},{"location":"models/bisenetformer/#architecture-dimensions","title":"Architecture Dimensions","text":"<ul> <li><code>pixel_decoder_out_dim</code> (int, default=256): Pixel decoder output channels</li> <li><code>pixel_decoder_feat_dim</code> (int, default=256): Pixel decoder feature channels</li> <li><code>transformer_predictor_hidden_dim</code> (int, default=256): Transformer hidden dimension</li> <li><code>transformer_predictor_dec_layers</code> (int, default=6): Number of decoder layers</li> <li><code>transformer_predictor_dim_feedforward</code> (int, default=1024): FFN dimension</li> <li><code>head_out_dim</code> (int, default=256): Prediction head output dimension</li> </ul>"},{"location":"models/bisenetformer/#inference-configuration","title":"Inference Configuration","text":"<ul> <li><code>postprocessing_type</code> (str): Either \"semantic\" or \"instance\" segmentation</li> <li><code>mask_threshold</code> (float, default=0.5): Binary mask threshold</li> <li><code>threshold</code> (float, default=0.5): Confidence threshold for detections</li> <li><code>top_k</code> (int, default=300): Maximum number of detections to return</li> <li><code>use_mask_score</code> (bool, default=False): Whether to use mask quality scores</li> <li><code>predict_all_pixels</code> (bool, default=False): Predict class for every pixel, usually better for semantic segmentation</li> </ul>"},{"location":"models/bisenetformer/#losses","title":"Losses","text":"<p>The model employs three complementary loss functions as described in the Mask2Former paper:</p> <ol> <li>Cross-entropy Loss (<code>loss_ce</code>): Classification of object classes</li> <li>Dice Loss (<code>loss_dice</code>): Shape-aware segmentation loss</li> <li>Mask Loss (<code>loss_mask</code>): Binary cross-entropy on predicted masks</li> </ol>"},{"location":"models/bisenetformer/#supported-tasks","title":"Supported Tasks","text":""},{"location":"models/bisenetformer/#semantic-segmentation","title":"Semantic Segmentation","text":"<ul> <li>Output: Dense pixel-wise class predictions</li> <li>Use Cases: Scene understanding, autonomous driving, medical imaging</li> <li>Configuration: Set <code>postprocessing_type=\"semantic\"</code></li> </ul>"},{"location":"models/bisenetformer/#instance-segmentation","title":"Instance Segmentation","text":"<ul> <li>Output: Individual object instances with masks and bounding boxes</li> <li>Use Cases: Object detection and counting, robotics applications</li> <li>Configuration: Set <code>postprocessing_type=\"instance\"</code></li> </ul>"},{"location":"models/bisenetformer/#model-outputs","title":"Model Outputs","text":""},{"location":"models/bisenetformer/#internal-output-bisenetformeroutput","title":"Internal Output (<code>BisenetFormerOutput</code>)","text":"<ul> <li><code>masks</code> (torch.Tensor): Shape [B, num_queries, H, W] - Query mask predictions</li> <li><code>logits</code> (torch.Tensor): Shape [B, num_queries, num_classes] - Class predictions</li> <li><code>loss</code> (Optional[dict]): Training losses including:<ul> <li><code>loss_ce</code>: Cross-entropy classification loss</li> <li><code>loss_mask</code>: Binary cross-entropy mask loss</li> <li><code>loss_dice</code>: Dice coefficient loss</li> </ul> </li> </ul>"},{"location":"models/bisenetformer/#inference-output-focoosdetections","title":"Inference Output (<code>FocoosDetections</code>)","text":"<p>For each detected object:</p> <ul> <li><code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2]</li> <li><code>conf</code> (float): Confidence score</li> <li><code>cls_id</code> (int): Class identifier</li> <li><code>mask</code> (str): Base64-encoded binary mask</li> <li><code>label</code> (Optional[str]): Human-readable class name</li> </ul>"},{"location":"models/bisenetformer/#available-models","title":"Available Models","text":"<p>Currently, you can find 3 bisenetformer models on the Focoos Hub, all for the semantic segmentation task.</p> Model Name Architecture Dataset Metric FPS Nvidia-T4 bisenetformer-l-ade BisenetFormer (STDC-2) ADE20K mIoU: 45.07mAcc: 58.03 - bisenetformer-m-ade BisenetFormer (STDC-2) ADE20K mIoU: 43.43mACC: 57.01 - bisenetformer-s-ade BisenetFormer (STDC-1) ADE20K mIoU: 42.91mACC: 56.55 -"},{"location":"models/bisenetformer/#example-usage","title":"Example Usage","text":""},{"location":"models/bisenetformer/#quick-start-with-pre-trained-model","title":"Quick Start with Pre-trained Model","text":"<pre><code>from focoos import ASSETS_DIR, ModelManager\nfrom PIL import Image\n\n# Load a pre-trained BisenetFormer model\nmodel = ModelManager.get(\"bisenetformer-m-ade\")\n\n# Run inference on an image\nimage = ASSETS_DIR / \"ADE_val_00000034\"\nresult = model.infer(image, threshold=0.5, annotate=True)\n\n# Process results\nfor detection in result.detections:\n    print(f\"Class: {detection.label}, Confidence: {detection.conf:.3f}\")\n# Visualize image\nImage.fromarray(result.image)\n</code></pre>"},{"location":"models/bisenetformer/#custom-model-configuration","title":"Custom Model Configuration","text":"<pre><code>from focoos.models.bisenetformer.config import BisenetFormerConfig\nfrom focoos.models.bisenetformer.modelling import BisenetFormer\nfrom focoos.nn.backbone.stdc import STDCConfig\n\n# Configure the backbone\nbackbone_config = STDCConfig(\n    model_type=\"stdc\",\n    use_pretrained=True,\n)\n\n# Configure the BisenetFormer model\nconfig = BisenetFormerConfig(\n    backbone_config=backbone_config,\n    num_classes=150,  # ADE20K classes\n    num_queries=100,\n    postprocessing_type=\"semantic\",\n    predict_all_pixels=True,  # Better for semantic segmentation\n    transformer_predictor_dec_layers=6,\n    threshold=0.5,\n)\n\n# Create the model\nmodel = BisenetFormer(config)\n</code></pre>"},{"location":"models/fai_cls/","title":"FAI-CLS (FocoosAI Classification)","text":""},{"location":"models/fai_cls/#overview","title":"Overview","text":"<p>Fai-cls is a versatile image classification model developed by FocoosAI that can utilize any backbone architecture for feature extraction. This model is designed for both single-label and multi-label image classification tasks, offering flexibility in architecture choices and training configurations.</p> <p>The model employs a simple yet effective approach: a configurable backbone extracts features from input images, followed by a classification head that produces class predictions. This design enables easy adaptation to different domains and datasets while maintaining high performance and computational efficiency.</p>"},{"location":"models/fai_cls/#available-models","title":"Available Models","text":"<p>Currently, you can find 3 fai-cls models on the Focoos Hub, all trained on COCO dataset for image classification.</p> Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-cls-n-coco Classification (STDC-Small) Common Objects (80) COCO F1: 48.66Precision: 58.48Recall: 41.66 - fai-cls-s-coco Classification (STDC-Small) Common Objects (80) COCO F1: 61.92Precision: 68.69Recall: 56.37 - fai-cls-m-coco Classification (STDC-Large) Common Objects (80) COCO F1: 66.98Precision: 73.00Recall: 61.88 -"},{"location":"models/fai_cls/#supported-dataset","title":"Supported dataset","text":"<ul> <li> <p>ROBOFLOW_COCO (multi-class)</p> </li> <li> <p>CLASSIFICATION_FOLDER</p> </li> </ul>"},{"location":"models/fai_cls/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The FAI-CLS architecture consists of two main components:</p>"},{"location":"models/fai_cls/#backbone","title":"Backbone","text":"<ul> <li>Purpose: Feature extraction from input images</li> <li>Design: Configurable backbone network (ResNet, EfficientNet, STDC, etc.)</li> <li>Output: High-level feature representations</li> <li>Feature Selection: Uses specified feature level (default: \"res5\" for highest-level features)</li> <li>Flexibility: Supports any backbone that provides the required output shape</li> </ul>"},{"location":"models/fai_cls/#classification-head","title":"Classification Head","text":"<ul> <li>Architecture: Multi-layer perceptron (MLP) with configurable depth</li> <li> <p>Components:</p> </li> <li> <p>Global Average Pooling (AdaptiveAvgPool2d) for spatial dimension reduction</p> </li> <li>Flatten layer to convert 2D features to 1D</li> <li>Linear layers with ReLU activation</li> <li>Dropout for regularization</li> <li>Final linear layer for class predictions</li> <li> <p>Configurations:</p> </li> <li> <p>Single Layer: Direct mapping from features to classes</p> </li> <li>Two Layer: Hidden layer with ReLU and dropout for better feature transformation</li> </ul>"},{"location":"models/fai_cls/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"models/fai_cls/#core-model-parameters","title":"Core Model Parameters","text":"<ul> <li><code>num_classes</code> (int): Number of classification classes</li> <li><code>backbone_config</code> (BackboneConfig): Backbone network configuration</li> </ul>"},{"location":"models/fai_cls/#architecture-configuration","title":"Architecture Configuration","text":"<ul> <li><code>hidden_dim</code> (int, default=512): Hidden layer dimension for two-layer classifier</li> <li><code>dropout_rate</code> (float, default=0.2): Dropout probability for regularization</li> <li><code>features</code> (str, default=\"res5\"): Feature level to extract from backbone</li> <li><code>num_layers</code> (int, default=2): Number of classification layers (1 or 2)</li> </ul>"},{"location":"models/fai_cls/#loss-configuration","title":"Loss Configuration","text":"<ul> <li><code>use_focal_loss</code> (bool, default=False): Use focal loss instead of cross-entropy</li> <li><code>focal_alpha</code> (float, default=0.75): Alpha parameter for focal loss</li> <li><code>focal_gamma</code> (float, default=2.0): Gamma parameter for focal loss</li> <li><code>label_smoothing</code> (float, default=0.0): Label smoothing factor</li> <li><code>multi_label</code> (bool, default=False): Enable multi-label classification</li> </ul>"},{"location":"models/fai_cls/#supported-tasks","title":"Supported Tasks","text":""},{"location":"models/fai_cls/#single-label-classification","title":"Single-Label Classification","text":"<ul> <li>Output: Single class prediction per image</li> <li> <p>Use Cases:</p> <ul> <li>Image categorization (animals, objects, scenes)</li> <li>Medical image diagnosis</li> <li>Quality control in manufacturing</li> <li>Content moderation</li> <li>Agricultural crop classification</li> </ul> </li> <li> <p>Loss: Cross-entropy or focal loss</p> </li> <li>Configuration: Set <code>multi_label=False</code></li> </ul>"},{"location":"models/fai_cls/#multi-label-classification","title":"Multi-Label Classification","text":"<ul> <li>Output: Multiple class predictions per image</li> <li> <p>Use Cases:</p> <ul> <li>Multi-object recognition</li> <li>Image tagging and annotation</li> <li>Scene attribute recognition</li> <li>Medical condition classification</li> <li>Content-based image retrieval</li> </ul> </li> <li> <p>Loss: Binary cross-entropy with logits</p> </li> <li>Configuration: Set <code>multi_label=True</code></li> </ul>"},{"location":"models/fai_cls/#model-outputs","title":"Model Outputs","text":""},{"location":"models/fai_cls/#training-output-classificationmodeloutput","title":"Training Output (<code>ClassificationModelOutput</code>)","text":"<ul> <li><code>logits</code> (torch.Tensor): Shape [B, num_classes] - Raw class predictions</li> <li><code>loss</code> (Optional[dict]): Training loss including:<ul> <li><code>loss_cls</code>: Classification loss (cross-entropy, focal, or BCE)</li> </ul> </li> </ul>"},{"location":"models/fai_cls/#inference-output","title":"Inference Output","text":"<p>For each detected object:</p> <ul> <li><code>conf</code> (float): Confidence score</li> <li><code>cls_id</code> (int): Class identifier</li> <li><code>label</code> (Optional[str]): Human-readable class name</li> </ul>"},{"location":"models/fai_cls/#losses","title":"Losses","text":"<p>The model supports multiple loss function configurations:</p>"},{"location":"models/fai_cls/#cross-entropy-loss-default","title":"Cross-Entropy Loss (Default)","text":"<ul> <li>Use Case: Standard single-label classification</li> <li>Features: Optional label smoothing for better generalization</li> <li>Activation: Softmax for probability distribution</li> </ul>"},{"location":"models/fai_cls/#binary-cross-entropy-loss","title":"Binary Cross-Entropy Loss","text":"<ul> <li>Use Case: Multi-label classification tasks</li> <li>Features: Independent probability for each class</li> <li>Activation: Sigmoid for per-class probabilities</li> </ul>"},{"location":"models/fai_cls/#architecture-variants","title":"Architecture Variants","text":""},{"location":"models/fai_cls/#single-layer-classifier","title":"Single-Layer Classifier","text":"<p><pre><code>AdaptiveAvgPool2d(1) \u2192 Flatten \u2192 Dropout \u2192 Linear(features \u2192 num_classes)\n</code></pre> - Benefits: Faster inference, fewer parameters - Use Case: Simple datasets or when computational efficiency is critical</p>"},{"location":"models/fai_cls/#two-layer-classifier","title":"Two-Layer Classifier","text":"<p><pre><code>AdaptiveAvgPool2d(1) \u2192 Flatten \u2192 Linear(features \u2192 hidden_dim) \u2192 ReLU \u2192 Dropout \u2192 Linear(hidden_dim \u2192 num_classes)\n</code></pre> - Benefits: Better feature transformation, improved accuracy - Use Case: Complex datasets requiring more sophisticated feature processing</p>"},{"location":"models/fai_cls/#training-strategies","title":"Training Strategies","text":""},{"location":"models/fai_cls/#standard-training","title":"Standard Training","text":"<ul> <li>Use cross-entropy loss with appropriate learning rate scheduling</li> <li>Apply data augmentation for better generalization</li> <li>Monitor validation accuracy for early stopping</li> </ul>"},{"location":"models/fai_cls/#imbalanced-data","title":"Imbalanced Data","text":"<ul> <li>Enable focal loss with appropriate \u03b1 and \u03b3 parameters</li> <li>Consider class weighting strategies</li> <li>Use stratified sampling for validation</li> </ul>"},{"location":"models/fai_cls/#multi-label-scenarios","title":"Multi-Label Scenarios","text":"<ul> <li>Set <code>multi_label=True</code> in configuration</li> <li>Use appropriate evaluation metrics (F1-score, mAP)</li> <li>Consider threshold optimization for final predictions</li> </ul> <p>This flexible architecture makes FAI-CLS suitable for a wide range of image classification applications, from simple binary classification to complex multi-label scenarios, while maintaining computational efficiency and ease of use.</p>"},{"location":"models/fai_cls/#quick-start-with-pre-trained-model","title":"Quick Start with Pre-trained Model","text":"<p><pre><code>from focoos import ASSETS_DIR, ModelManager\nfrom PIL import Image\n\n# Load a pre-trained model\nmodel = ModelManager.get(\"fai-cls-m-coco\")\n\nimage = ASSETS_DIR / \"federer.jpg\"\nresult = model.infer(image,threshold=0.5, annotate=True)\n\n# Process results\nfor detection in result.detections:\n    print(f\"Class: {detection.label}, Confidence: {detection.conf:.3f}\")\n\n# Visualize image\nImage.fromarray(result.image)\n</code></pre> For the training process, please refer to the specific section of the documentation.</p>"},{"location":"models/fai_cls/#custom-model-configuration","title":"Custom Model Configuration","text":""},{"location":"models/fai_cls/#single-label-classification-setup","title":"Single-Label Classification Setup","text":"<pre><code>from focoos.models.fai_cls.config import ClassificationConfig\nfrom focoos.models.fai_cls.modelling import FAIClassification\nfrom focoos.nn.backbone.resnet import ResnetConfig\n\n# Configure with ResNet backbone\nbackbone_config = ResnetConfig(\n    model_type=\"resnet\",\n    depth=50,\n    pretrained=True,\n)\n\nconfig = ClassificationConfig(\n    backbone_config=backbone_config,\n    num_classes=1000,  # ImageNet classes\n    resolution=224,\n    num_layers=2,\n    hidden_dim=512,\n    dropout_rate=0.2,\n)\n\n# Create model\nmodel = FAIClassification(config)\n</code></pre>"},{"location":"models/fai_cls/#multi-label-classification-setup","title":"Multi-Label Classification Setup","text":"<pre><code>from focoos.models.fai_cls.config import ClassificationConfig\nfrom focoos.models.fai_cls.modelling import FAIClassification\nfrom focoos.nn.backbone.resnet import ResnetConfig\n\n# Configure with ResNet backbone\nbackbone_config = ResnetConfig(\n    model_type=\"resnet\",\n    depth=50,\n    pretrained=True,\n)\n\nconfig = ClassificationConfig(\n    backbone_config=backbone_config,\n    num_classes=1000,  # ImageNet classes\n    resolution=224,\n    num_layers=2,\n    hidden_dim=512,\n    dropout_rate=0.2,\n    multi_label=True,\n)\n\n# Create model\nmodel = FAIClassification(config)\n</code></pre>"},{"location":"models/fai_detr/","title":"FAI-DETR (FocoosAI Detection Transformer)","text":""},{"location":"models/fai_detr/#overview","title":"Overview","text":"<p>FAI-DETR is an advanced object detection model based on the RT-DETR (Detection Transformer) architecture, optimized by FocoosAI for efficient and accurate object detection tasks. This model eliminates the need for hand-crafted components like non-maximum suppression (NMS) and anchor generation by using a transformer-based approach with learnable object queries.</p> <p>The model employs a set-based global loss through bipartite matching and a transformer encoder-decoder architecture that directly predicts bounding boxes and class labels. This end-to-end approach simplifies the detection pipeline while achieving competitive performance.</p>"},{"location":"models/fai_detr/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The FAI-DETR architecture consists of four main components:</p>"},{"location":"models/fai_detr/#backbone","title":"Backbone","text":"<ul> <li>Purpose: Feature extraction from input images</li> <li>Design: Configurable backbone network (ResNet, STDC, etc.)</li> <li>Output: Multi-scale features at different resolutions</li> <li>Integration: Features are processed through the encoder for global context</li> </ul>"},{"location":"models/fai_detr/#encoder","title":"Encoder","text":"<ul> <li>Architecture: Transformer encoder with multi-scale deformable attention</li> <li>Components:</li> <li>Multi-scale deformable self-attention layers</li> <li>Position embeddings (sine-based)</li> <li>Feed-forward networks (FFN)</li> <li>CSPRep layers for efficient feature processing</li> <li>Features: Processes multi-scale features to capture global context</li> <li>Layers: Configurable number of encoder layers (default: 1)</li> </ul>"},{"location":"models/fai_detr/#decoder","title":"Decoder","text":"<ul> <li>Architecture: Multi-scale deformable transformer decoder</li> <li>Components:</li> <li>Self-attention layers for query interaction</li> <li>Cross-attention layers with deformable attention</li> <li>Feed-forward networks</li> <li>Reference point refinement</li> <li>Queries: 300 learnable object queries (configurable)</li> <li>Layers: Configurable number of decoder layers (default: 6)</li> </ul>"},{"location":"models/fai_detr/#detection-head","title":"Detection Head","text":"<ul> <li>Classification Head: Predicts class probabilities for each query</li> <li>Regression Head: Predicts bounding box coordinates (center, width, height)</li> <li>Output Format: Direct box predictions without anchors or post-processing</li> </ul>"},{"location":"models/fai_detr/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"models/fai_detr/#core-model-parameters","title":"Core Model Parameters","text":"<ul> <li><code>num_classes</code> (int): Number of object detection classes</li> <li><code>num_queries</code> (int, default=300): Number of learnable object queries</li> <li><code>backbone_config</code> (BackboneConfig): Backbone network configuration</li> </ul>"},{"location":"models/fai_detr/#encoder-configuration","title":"Encoder Configuration","text":"<ul> <li><code>pixel_decoder_out_dim</code> (int, default=256): Encoder output dimension</li> <li><code>pixel_decoder_feat_dim</code> (int, default=256): Encoder feature dimension</li> <li><code>pixel_decoder_num_encoder_layers</code> (int, default=1): Number of encoder layers</li> <li><code>pixel_decoder_expansion</code> (float, default=1.0): Channel expansion ratio</li> <li><code>pixel_decoder_dim_feedforward</code> (int, default=1024): FFN dimension</li> <li><code>pixel_decoder_dropout</code> (float, default=0.0): Dropout rate</li> <li><code>pixel_decoder_nhead</code> (int, default=8): Number of attention heads</li> </ul>"},{"location":"models/fai_detr/#decoder-configuration","title":"Decoder Configuration","text":"<ul> <li><code>transformer_predictor_hidden_dim</code> (int, default=256): Decoder hidden dimension</li> <li><code>transformer_predictor_dec_layers</code> (int, default=6): Number of decoder layers</li> <li><code>transformer_predictor_dim_feedforward</code> (int, default=1024): FFN dimension</li> <li><code>transformer_predictor_nhead</code> (int, default=8): Number of attention heads</li> <li><code>transformer_predictor_out_dim</code> (int, default=256): Decoder output dimension</li> <li><code>head_out_dim</code> (int, default=256): Detection head output dimension</li> </ul>"},{"location":"models/fai_detr/#inference-configuration","title":"Inference Configuration","text":"<ul> <li><code>threshold</code> (float, default=0.5): Confidence threshold for detections</li> <li><code>top_k</code> (int, default=300): Maximum number of detections to return</li> </ul>"},{"location":"models/fai_detr/#supported-tasks","title":"Supported Tasks","text":""},{"location":"models/fai_detr/#object-detection","title":"Object Detection","text":"<ul> <li>Output: Bounding boxes with class labels and confidence scores</li> <li>Use Cases:</li> <li>General object detection in natural images</li> <li>Autonomous driving (vehicle, pedestrian detection)</li> <li>Surveillance and security applications</li> <li>Industrial quality control</li> <li>Medical image analysis</li> <li>Performance: End-to-end detection without NMS post-processing</li> </ul>"},{"location":"models/fai_detr/#model-outputs","title":"Model Outputs","text":""},{"location":"models/fai_detr/#internal-output-detrmodeloutput","title":"Internal Output (<code>DETRModelOutput</code>)","text":"<ul> <li><code>boxes</code> (torch.Tensor): Shape [B, num_queries, 4] - Bounding boxes in XYXY format normalized to [0, 1]</li> <li><code>logits</code> (torch.Tensor): Shape [B, num_queries, num_classes] - Class predictions</li> <li><code>loss</code> (Optional[dict]): Training losses including:<ul> <li><code>loss_vfl</code>: Varifocal loss for classification</li> <li><code>loss_bbox</code>: L1 loss for bounding box regression</li> <li><code>loss_giou</code>: Generalized IoU loss for box alignment</li> </ul> </li> </ul>"},{"location":"models/fai_detr/#inference-output-focoosdetections","title":"Inference Output (<code>FocoosDetections</code>)","text":"<p>For each detected object:</p> <ul> <li><code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2]</li> <li><code>conf</code> (float): Confidence score</li> <li><code>cls_id</code> (int): Class identifier</li> <li><code>label</code> (Optional[str]): Human-readable class name</li> </ul>"},{"location":"models/fai_detr/#losses","title":"Losses","text":"<p>The model employs three main loss components:</p> <ol> <li>Varifocal Loss (<code>loss_vfl</code>):</li> <li>Advanced focal loss variant for classification</li> <li>Handles foreground-background imbalance</li> <li> <p>Joint optimization of classification and localization quality</p> </li> <li> <p>Bounding Box Loss (<code>loss_bbox</code>):</p> </li> <li>L1 loss for direct coordinate regression</li> <li> <p>Normalized coordinates for scale invariance</p> </li> <li> <p>Generalized IoU Loss (<code>loss_giou</code>):</p> </li> <li>Shape-aware bounding box loss</li> <li>Better gradient flow for overlapping boxes</li> <li>Improved localization accuracy</li> </ol>"},{"location":"models/fai_detr/#available-models","title":"Available Models","text":"<p>Currently, you can find 3 fai-detr models on the Focoos Hub, 2 trained on COCO and one trained on Object365</p> Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 fai-detr-l-coco RT-DETR (Resnet-50) Common Objects (80) COCO bbox/AP: 53.06bbox/AP50: 70.91 87 fai-detr-m-coco RT-DETR (STDC-2) Common Objects (80) COCO bbox/AP: 44.69bbox/AP50: 61.63 181 fai-detr-l-obj365 RT-DETR (Resnet50) Common Objects (365) Objects365 bbox/AP: 34.60bbox/AP50: 45.81 87"},{"location":"models/fai_detr/#example-usage","title":"Example Usage","text":""},{"location":"models/fai_detr/#quick-start-with-pre-trained-model","title":"Quick Start with Pre-trained Model","text":"<pre><code>from focoos import ASSETS_DIR, ModelManager\nfrom PIL import Image\n\n# Load a pre-trained model\nmodel = ModelManager.get(\"fai-detr-m-coco\")\n\n# Run inference on an image\nimage = ASSETS_DIR / \"federer.jpg\"\nresult = model.infer(image,threshold=0.5, annotate=True)\n\n# Process results\nfor detection in result.detections:\n    print(f\"Class: {detection.label}, Confidence: {detection.conf:.3f}\")\n\n# Visualize image\nImage.fromarray(result.image)\n</code></pre>"},{"location":"models/fai_detr/#custom-model-configuration","title":"Custom Model Configuration","text":"<pre><code>from focoos.models.fai_detr.config import DETRConfig\nfrom focoos.models.fai_detr.modelling import FAIDetr\nfrom focoos.nn.backbone.stdc import STDCConfig\n\n# Configure the backbone\nbackbone_config = STDCConfig(\n    model_type=\"stdc\",\n    use_pretrained=True,\n)\n\n# Configure the model\nconfig = DETRConfig(\n    backbone_config=backbone_config,\n    num_classes=80,\n    num_queries=300,\n    transformer_predictor_dec_layers=3,\n    threshold=0.5,\n)\n\n# Create the model\nmodel = FAIDetr(config)\n</code></pre>"},{"location":"models/fai_mf/","title":"FAI-MF (FocoosAI MaskFormer)","text":""},{"location":"models/fai_mf/#overview","title":"Overview","text":"<p>The FAI-MF model is a Mask2Former implementation optimized by FocoosAI for semantic and instance segmentation tasks. Unlike traditional segmentation models such as DeepLab, Mask2Former employs a mask-classification approach where predictions consist of segmentation masks paired with class probabilities.</p>"},{"location":"models/fai_mf/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>The FAI-MF model is built on the Mask2Former architecture, featuring a transformer-based encoder-decoder design with three main components:</p> <p></p>"},{"location":"models/fai_mf/#backbone","title":"Backbone","text":"<ul> <li>Network: Any backbone that can extract multi-scale features from an image</li> <li>Output: Multi-scale features from stages 2-5 at resolutions 1/4, 1/8, 1/16, and 1/32</li> </ul>"},{"location":"models/fai_mf/#pixel-decoder","title":"Pixel Decoder","text":"<ul> <li>Architecture: Feature Pyramid Network (FPN)</li> <li>Input: Features from backbone stages 2-5</li> <li>Modifications: Deformable attention modules removed for improved portability and inference speed</li> <li>Output: Upscaled multi-scale features for mask generation</li> </ul>"},{"location":"models/fai_mf/#transformer-decoder","title":"Transformer Decoder","text":"<ul> <li>Design: Lightweight version of the original Mask2Former decoder</li> <li>Layers: N decoder layer (depending on the speed/accuracy trade-off)</li> <li>Queries: Q learnable object queries (usually 100)</li> <li>Components:</li> <li>Self-attention layers</li> <li>Masked cross-attention layers</li> <li>Feed-forward networks (FFN)</li> </ul>"},{"location":"models/fai_mf/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"models/fai_mf/#core-model-parameters","title":"Core Model Parameters","text":"<ul> <li><code>num_classes</code> (int): Number of segmentation classes</li> <li><code>num_queries</code> (int, default=100): Number of learnable object queries</li> <li><code>resolution</code> (int, default=640): Input image resolution</li> </ul>"},{"location":"models/fai_mf/#backbone-configuration","title":"Backbone Configuration","text":"<ul> <li><code>backbone_config</code> (BackboneConfig): Backbone network configuration</li> </ul>"},{"location":"models/fai_mf/#architecture-dimensions","title":"Architecture Dimensions","text":"<ul> <li><code>pixel_decoder_out_dim</code> (int, default=256): Pixel decoder output channels</li> <li><code>pixel_decoder_feat_dim</code> (int, default=256): Pixel decoder feature channels</li> <li><code>transformer_predictor_hidden_dim</code> (int, default=256): Transformer hidden dimension</li> <li><code>transformer_predictor_dec_layers</code> (int, default=6): Number of decoder layers</li> <li><code>head_out_dim</code> (int, default=256): Prediction head output dimension</li> </ul>"},{"location":"models/fai_mf/#inference-configuration","title":"Inference Configuration","text":"<ul> <li><code>postprocessing_type</code> (str): Either \"semantic\" or \"instance\" segmentation</li> <li><code>mask_threshold</code> (float, default=0.5): Binary mask threshold</li> <li><code>threshold</code> (float, default=0.5): Confidence threshold for the classification scores</li> <li><code>predict_all_pixels</code> (bool, default=False): Predict class for every pixel, this is usually better for semantic segmentation</li> </ul>"},{"location":"models/fai_mf/#supported-tasks","title":"Supported Tasks","text":""},{"location":"models/fai_mf/#semantic-segmentation","title":"Semantic Segmentation","text":"<ul> <li>Output: Dense pixel-wise class predictions</li> <li>Use case: Scene understanding, medical imaging, autonomous driving</li> <li>Configuration: Set <code>postprocessing_type=\"semantic\"</code></li> </ul>"},{"location":"models/fai_mf/#instance-segmentation","title":"Instance Segmentation","text":"<ul> <li>Output: Individual object instances with masks and bounding boxes</li> <li>Use case: Object detection and counting, robotics, surveillance</li> <li>Configuration: Set <code>postprocessing_type=\"instance\"</code></li> </ul>"},{"location":"models/fai_mf/#model-outputs","title":"Model Outputs","text":""},{"location":"models/fai_mf/#inner-model-output-maskformermodeloutput","title":"Inner Model Output (<code>MaskFormerModelOutput</code>)","text":"<ul> <li><code>masks</code> (torch.Tensor): Shape [B, num_queries, H, W] - Query mask predictions</li> <li><code>logits</code> (torch.Tensor): Shape [B, num_queries, num_classes] - Class predictions</li> <li><code>loss</code> (Optional[dict]): Training losses including:<ul> <li><code>loss_ce</code>: Cross-entropy classification loss</li> <li><code>loss_mask</code>: Binary cross-entropy mask loss</li> <li><code>loss_dice</code>: Dice coefficient loss</li> </ul> </li> </ul>"},{"location":"models/fai_mf/#inference-output-focoosdetections","title":"Inference Output (<code>FocoosDetections</code>)","text":"<p>For each detected object:</p> <ul> <li><code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2]</li> <li><code>conf</code> (float): Confidence score</li> <li><code>cls_id</code> (int): Class identifier</li> <li><code>mask</code> (str): Base64-encoded binary mask</li> <li><code>label</code> (Optional[str]): Human-readable class name</li> </ul>"},{"location":"models/fai_mf/#losses","title":"Losses","text":"<p>The model employs three complementary loss functions as described in the original paper:</p> <ol> <li>Cross-entropy Loss (<code>loss_ce</code>): Classification of object classes</li> <li>Dice Loss (<code>loss_dice</code>): Shape-aware segmentation loss</li> <li>Mask Loss (<code>loss_mask</code>): Binary cross-entropy on predicted masks</li> </ol>"},{"location":"models/fai_mf/#available-models","title":"Available Models","text":"<p>Currently, you can find 5 fai-mf models on the Focoos Hub, 2 for semantic segmentation and 3 for instance-segmentation.</p>"},{"location":"models/fai_mf/#semantic-segmentation-models","title":"Semantic Segmentation Models","text":"Model Name Architecture Dataset Metric FPS Nvidia-T4 fai-mf-l-ade Mask2Former (Resnet-101) ADE20K mIoU: 48.27mAcc: 62.15 73 fai-mf-m-ade Mask2Former (STDC-2) ADE20K mIoU: 45.32mACC: 57.75 127"},{"location":"models/fai_mf/#instance-segmentation-models","title":"Instance Segmentation Models","text":"Model Name Architecture Dataset Metric FPS Nvidia-T4 fai-mf-s-coco-ins Mask2Former (Resnet-50) COCO segm/AP: 41.45segm/AP50: 64.12 86 fai-mf-m-coco-ins Mask2Former (Resnet-101) COCO segm/AP: 43.09segm/AP50: 65.87 70 fai-mf-l-coco-ins Mask2Former (Resnet-101) COCO segm/AP: 44.23segm/AP50: 67.53 55"},{"location":"models/fai_mf/#example-usage","title":"Example Usage","text":""},{"location":"models/fai_mf/#quick-start-with-pre-trained-model","title":"Quick Start with Pre-trained Model","text":"<pre><code>from focoos import ASSETS_DIR, ModelManager\nfrom PIL import Image\n\n# Load a pre-trained BisenetFormer model\nmodel = ModelManager.get(\"fai-mf-l-ade\")\n\n# Run inference on an image\nimage = ASSETS_DIR / \"ADE_val_00000034\"\nresult = model.infer(image,threshold=0.5, annotate=True)\n\n# Process results\nfor detection in result.detections:\n    print(f\"Class: {detection.label}, Confidence: {detection.conf:.3f}\")\n\n# Visualize image\nImage.fromarray(result.image)\n</code></pre>"},{"location":"models/fai_mf/#custom-model-configuration","title":"Custom Model Configuration","text":"<pre><code>from focoos.models.fai_mf.config import MaskFormerConfig\nfrom focoos.models.fai_mf.modelling import FAIMaskFormer\nfrom focoos.nn.backbone.stdc import STDCConfig\n\n# Configure the backbone\nbackbone_config = STDCConfig(\n    model_type=\"stdc\",\n    use_pretrained=True,\n)\n\n# Configure the model\nconfig = MaskFormerConfig(\n    backbone_config=backbone_config,\n    num_classes=80,\n    num_queries=300,\n    transformer_predictor_dec_layers=3,\n    threshold=0.5,\n)\n\n# Create the model\nmodel = FAIMaskFormer(config)\n</code></pre>"},{"location":"models/rtmo/","title":"RTMO","text":""},{"location":"models/rtmo/#overview","title":"Overview","text":"<p>RTMO (Real-Time Multi-person One-stage) is a high-performance, one-stage framework for real-time multi-person pose estimation. It integrates a novel coordinate classification method into YOLOXPose architecture, achieving an excellent balance between speed and accuracy.</p> <p>The model avoids the typical slowdown of two-stage (top-down) methods in crowded scenes by directly predicting keypoints for all individuals in a single pass. Its core innovation is the Dynamic Coordinate Classifier (DCC), which represents keypoints using dual 1-D heatmaps, enabling precise localization without the computational overhead of high-resolution feature maps.</p>"},{"location":"models/rtmo/#neural-network-architecture","title":"Neural Network Architecture","text":"<p> The RTMO architecture consists of three main components:</p>"},{"location":"models/rtmo/#backbone","title":"Backbone","text":"<ul> <li>Purpose: To extract multi-scale feature maps from the input image.</li> <li>Design: Configurable backbone network (e.g., ResNet, STDC), default is CSPDarknet.</li> </ul>"},{"location":"models/rtmo/#neck","title":"Neck","text":"<ul> <li>Purpose: To fuse and refine the features from the backbone.</li> <li>Design: A HybridEncoder combines transformer encoder layers with CSP-based feature fusion, processing the last three feature maps from the backbone (with downsampling rates of 8, 16 and 32) to generate enhanced features for the head. The HybridEncoder consists of:</li> <li>Transformer Encoder: Uses DETR-style transformer layers with sinusoidal positional encoding</li> <li>CSP (Cross Stage Partial) layers: For efficient feature processing and fusion</li> <li>FPN (Feature Pyramid Network): Top-down and bottom-up feature fusion for multi-scale feature enhancement</li> </ul>"},{"location":"models/rtmo/#head","title":"Head","text":"<ul> <li>Purpose: To predict the final outputs for each grid cell on the feature map.</li> <li>Components:</li> <li>RTMOHeadModule: Dual-branch architecture that processes shared input features:<ul> <li>Classification Branch: Processes half of input features through stacked convolutions to generate classification scores</li> <li>Pose Branch: Processes the other half through group convolutions to generate pose features, bounding boxes, keypoint offsets, and visibility scores</li> </ul> </li> <li>Dynamic Coordinate Classifier (DCC): This is the core component for keypoint localization. It takes the pose feature and translates it into K pairs of 1-D heatmaps (one for the horizontal axis, one for the vertical).</li> </ul>"},{"location":"models/rtmo/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"models/rtmo/#core-model-parameters","title":"Core Model parameters","text":"<ul> <li><code>num_classes</code> (int): Number of object classes (default=1 for \"person\" in COCO dataset).</li> <li><code>num_keypoints</code> (int): Number of keypoints to predict (default=17 for person body).</li> </ul>"},{"location":"models/rtmo/#backbone-configuration","title":"Backbone Configuration","text":"<ul> <li><code>backbone_config</code> (BackboneConfig): Backbone network configuration (default: <code>CSPConfig</code>)</li> </ul>"},{"location":"models/rtmo/#neck_1","title":"Neck","text":"<ul> <li><code>transformer_embed_dims</code> (int, default=256): Embedding dimension for transformer layers</li> <li><code>transformer_num_heads</code> (int, default=8): Number of attention heads in transformer</li> <li><code>transformer_feedforward_channels</code> (int, default=1024): Feedforward network dimension in transformer</li> <li><code>transformer_dropout</code> (float, default=0.0): Dropout rate in transformer layers</li> <li><code>transformer_encoder_layers</code> (int, default=1): Number of transformer encoder layers</li> </ul>"},{"location":"models/rtmo/#head_1","title":"Head","text":"<ul> <li><code>in_channels</code> (int, default=256): Number of input channels to the head from the neck.</li> <li><code>pose_vec_channels</code> (int, default=256): Dimension of the output pose feature vector for each grid, which is later fed into the DCC.</li> <li><code>cls_feat_channels</code> (int, default=256): Number of channels in the classification branch</li> <li><code>stacked_convs</code> (int, default=2): Number of stacked convolution layers in the classification and regression branches of the head.</li> <li><code>featmap_strides</code> (List[int], default=[16, 8]): Strides of feature maps from backbone</li> <li><code>featmap_strides_pointgenerator</code> (List[int], default=[16, 8]): Strides for point generator</li> <li><code>centralize_points_pointgenerator</code> (bool, default=False): Whether to centralize points in point generator</li> </ul>"},{"location":"models/rtmo/#dynamic-coordinate-classifier-dcc","title":"Dynamic Coordinate Classifier (DCC)","text":"<ul> <li><code>feat_channels_dcc</code> (int, default=128): The feature dimension used within the DCC for keypoint feature representation.</li> <li><code>num_bins</code> (Tuple[int, int], default=(192, 256)): The number of bins for the horizontal (x) and vertical (y) heatmaps, respectively.</li> <li><code>spe_channels</code> (int, default=128): The channel dimension for the Sine Positional Encoding (SPE) used to encode bin coordinates.</li> </ul>"},{"location":"models/rtmo/#gau-gated-attention-unit","title":"GAU (Gated Attention Unit)","text":"<ul> <li><code>gau_s</code> (int, default=128): The self-attention feature dimension.</li> <li><code>gau_expansion_factor</code> (int, default=2): The expansion factor for the hidden dimension in the GAU.</li> <li><code>gau_dropout_rate</code> (float, default=0.0): Dropout rate applied within the GAU.</li> </ul>"},{"location":"models/rtmo/#losses","title":"Losses","text":"<ol> <li>Classification Loss (<code>loss_cls</code>): A VariFocal Loss is used to supervise the classification score of each grid.</li> <li>Bounding Box Loss (<code>loss_bbox</code>): An IoU (Intersection over Union) Loss is applied to the decoded bounding boxes for positive grids.</li> <li>MLE Loss (<code>loss_mle</code>): The core loss for keypoint localization. It is applied to the 1-D heatmaps from the DCC.</li> <li>Keypoint Visibility Loss (<code>loss_vis</code>): A Binary Cross-Entropy (BCE) Loss is used to supervise the visibility prediction for each keypoint.</li> <li>OKS Loss (<code>loss_oks</code>): Auxiliary loss supervising keypoint regression with OKS (Object Keypoint Similarity) Loss. It uses DCC predictions as targets for further refinement.</li> </ol> <p>The total loss is a weighted sum of these components.</p>"},{"location":"models/rtmo/#supported-tasks","title":"Supported Tasks","text":""},{"location":"models/rtmo/#multi-person-pose-estimation","title":"Multi-Person Pose Estimation","text":"<ul> <li>Output: Detects all individuals in an image, providing instance-level predictions. For each person, the model outputs a bounding box, a detection score, and the coordinates and visibility status for all keypoints.</li> <li>Use Cases: Action recognition, sports analytics, augmented reality, human-computer interaction.</li> </ul>"},{"location":"models/rtmo/#model-outputs","title":"Model Outputs","text":""},{"location":"models/rtmo/#internal-output-rtmomodeloutput","title":"Internal Output (<code>RTMOModelOutput</code>)","text":"<ul> <li><code>outputs</code> (KeypointOutput):<ul> <li><code>scores</code>: (torch.Tensor) Shape [B, num_detections] - Detection scores for each instance</li> <li><code>labels</code>: (torch.Tensor) Shape [B, num_detections] - Class labels for each instance</li> <li><code>pred_bboxes</code>: (torch.Tensor) Shape [B, num_detections, 4] - Predicted bounding boxes [x1, y1, x2, y2]</li> <li><code>bbox_scores</code>: (torch.Tensor) Shape [B, num_detections] - Scores for each bounding box</li> <li><code>pred_keypoints</code>: (torch.Tensor) Shape [B, num_detections, num_keypoints, 2] - Predicted keypoint coordinates</li> <li><code>keypoint_scores</code>: (torch.Tensor) Shape [B, num_detections, num_keypoints] - Scores for each keypoint</li> <li><code>keypoints_visible</code>: (torch.Tensor) Shape [B, num_detections, num_keypoints] - Visibility for keypoints</li> </ul> </li> <li><code>loss</code> (RTMOLoss): Training losses including:<ul> <li><code>loss_bbox</code>: IoU loss for bounding box regression</li> <li><code>loss_vis</code>: Binary cross-entropy visibility loss</li> <li><code>loss_mle</code>: Maximum Likelihood Estimation (MLE) loss applied to 1-D keypoint heatmaps for keypoint localization</li> <li><code>loss_oks</code>: OKS (Object Keypoint Similarity) loss for better keypoint localization</li> <li><code>loss_cls</code>: VariFocal classification loss</li> </ul> </li> </ul>"},{"location":"models/rtmo/#inference-output-focoosdetections","title":"Inference Output (<code>FocoosDetections</code>)","text":"<p>For each detected object:</p> <ul> <li><code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2]</li> <li><code>conf</code> (float): Confidence score</li> <li><code>cls_id</code> (int): Class identifier</li> <li><code>label</code> (Optional[str]): Human-readable class name</li> <li><code>keypoints</code> (List[List[int]]): Keypoints [x, y, visibility]</li> </ul>"},{"location":"models/rtmo/#available-models","title":"Available Models","text":"<p>The following RTMO models are available on the Focoos Hub for multi-person pose estimation:</p> Model Name Architecture Domain (Classes) Dataset Metric FPS Nvidia-T4 rtmo-s-coco RTMO (CSP-Darknet) Persons (1) COCO keypoints/AP: 67.94keypoints/AP50: 87.86 104 rtmo-m-coco RTMO (CSP-Darknet) Persons (1) COCO keypoints/AP: 70.94keypoints/AP50: 89.47 89 rtmo-l-coco RTMO (CSP-Darknet) Persons (1) COCO keypoints/AP: 72.14keypoints/AP50: 89.85 63"},{"location":"models/rtmo/#example-usage","title":"Example Usage","text":""},{"location":"models/rtmo/#quick-start-with-pre-trained-model","title":"Quick Start with Pre-trained Model","text":"<pre><code>from PIL import Image\n\nfrom focoos import ModelManager, ASSETS_DIR\n\n# Load a pre-trained RTMO model\nmodel = ModelManager.get(\"rtmo-s-coco\")\n\n# Run inference on an image\nimage = ASSETS_DIR / \"federer.jpg\"\nresult = model.infer(image,threshold=0.5, annotate=True)\n\n# Process results\nfor detection in result.detections:\n    print(f\"Class: {detection.label}, Confidence: {detection.conf:.3f}\")\n\n# Visualize image\nImage.fromarray(result.image)\n</code></pre>"},{"location":"models/rtmo/#custom-model-configuration","title":"Custom Model Configuration","text":"<pre><code>from focoos.models.rtmo.config import RTMOConfig\nfrom focoos.models.rtmo.modelling import RTMO\nfrom focoos.nn.backbone.csp_darknet import CSPConfig\n\n# Configure the backbone\nbackbone_config = CSPConfig(\n    size=\"small\",\n    use_pretrained=True,\n)\n\n# Configure the RTMO model\nconfig = RTMOConfig(\n    backbone_config=backbone_config,\n    num_classes=1,  # COCO Person classes\n    num_keypoints=17 # Number of keypoints for COCO Person body pose\n)\n\n# Create the model\nmodel = RTMO(config)\n</code></pre>"}]}